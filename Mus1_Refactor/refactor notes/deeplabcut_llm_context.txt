--- File: CODE_OF_CONDUCT.md ---
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socioeconomic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
 advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
 address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
 professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at alexander.mathis@epfl.ch. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq


--- File: NOTICE.yml ---
# Main repository license
- header: |
    DeepLabCut Toolbox (deeplabcut.org)
    © A. & M.W. Mathis Labs
    https://github.com/DeepLabCut/DeepLabCut

    Please see AUTHORS for contributors.
    https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS

    Licensed under GNU Lesser General Public License v3.0
  include:
    - 'deeplabcut/**/*.py'
    - 'tests/**/*.py'
    - 'examples/**/*.py'
    - 'docs/**/*.py'
    #- 'conda-environments/**/*.yaml'

# License for files adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow

# Applies to most files in deeplabcut.pose_estimation_tensorflow
- header: |
    DeepLabCut Toolbox (deeplabcut.org)
    © A. & M.W. Mathis Labs
    https://github.com/DeepLabCut/DeepLabCut

    Please see AUTHORS for contributors.
    https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS

    Adapted from DeeperCut by Eldar Insafutdinov
    https://github.com/eldar/pose-tensorflow

    Licensed under GNU Lesser General Public License v3.0
  include:
    # This filelist was generated by running
    # find deeplabcut/pose_estimation_tensorflow -iname '*.py' | xargs grep 'Eldar Insafutdinov'
    # from the repo base directory.
    - deeplabcut/pose_estimation_tensorflow/config.py
    - deeplabcut/pose_estimation_tensorflow/datasets/factory.py
    - deeplabcut/pose_estimation_tensorflow/vis_dataset.py
    - deeplabcut/pose_estimation_tensorflow/core/train.py
    - deeplabcut/pose_estimation_tensorflow/core/predict.py
    - deeplabcut/pose_estimation_tensorflow/core/test.py
    - deeplabcut/pose_estimation_tensorflow/default_config.py
    - deeplabcut/pose_estimation_tensorflow/util/visualize.py
    - deeplabcut/pose_estimation_tensorflow/util/__init__.py
    - deeplabcut/pose_estimation_tensorflow/util/logging.py
    - deeplabcut/pose_estimation_tensorflow/nnets/resnet.py
    - deeplabcut/pose_estimation_tensorflow/__init__.py
  exclude: []

# Tensorflow licenses
- header: |
    Copyright 2019 The TensorFlow Authors. All Rights Reserved.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
  include:
    - deeplabcut/pose_estimation_tensorflow/backbones/*.py
    - deeplabcut/pose_estimation_tensorflow/nnets/utils.py

- header: |
    Copyright 2018 The TensorFlow Authors. All Rights Reserved.

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

        http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
  include:
    - deeplabcut/pose_estimation_tensorflow/nnets/conv_blocks.py
    - deeplabcut/pose_estimation_tensorflow/backbones/mobilenet.py
    - deeplabcut/pose_estimation_tensorflow/backbones/mobilenet_v2.py

# TIMM license
- header: |
    Copyright 2019 Ross Wightman

    Licensed under the Apache License, Version 2.0 (the "License");
    you may not use this file except in compliance with the License.
    You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.

    Hacked together by / Copyright 2020 Ross Wightman
    https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py
  include:
    - deeplabcut/pose_tracking_pytorch/solver/scheduler_factory.py
    - deeplabcut/pose_tracking_pytorch/model/backones/vit_pytorch.py

# PyTorch license 

- header: |
    See https://github.com/pytorch/pytorch/blob/main/LICENSE


--- File: LICENSE ---
                   GNU LESSER GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.


  This version of the GNU Lesser General Public License incorporates
the terms and conditions of version 3 of the GNU General Public
License, supplemented by the additional permissions listed below.

  0. Additional Definitions.

  As used herein, "this License" refers to version 3 of the GNU Lesser
General Public License, and the "GNU GPL" refers to version 3 of the GNU
General Public License.

  "The Library" refers to a covered work governed by this License,
other than an Application or a Combined Work as defined below.

  An "Application" is any work that makes use of an interface provided
by the Library, but which is not otherwise based on the Library.
Defining a subclass of a class defined by the Library is deemed a mode
of using an interface provided by the Library.

  A "Combined Work" is a work produced by combining or linking an
Application with the Library.  The particular version of the Library
with which the Combined Work was made is also called the "Linked
Version".

  The "Minimal Corresponding Source" for a Combined Work means the
Corresponding Source for the Combined Work, excluding any source code
for portions of the Combined Work that, considered in isolation, are
based on the Application, and not on the Linked Version.

  The "Corresponding Application Code" for a Combined Work means the
object code and/or source code for the Application, including any data
and utility programs needed for reproducing the Combined Work from the
Application, but excluding the System Libraries of the Combined Work.

  1. Exception to Section 3 of the GNU GPL.

  You may convey a covered work under sections 3 and 4 of this License
without being bound by section 3 of the GNU GPL.

  2. Conveying Modified Versions.

  If you modify a copy of the Library, and, in your modifications, a
facility refers to a function or data to be supplied by an Application
that uses the facility (other than as an argument passed when the
facility is invoked), then you may convey a copy of the modified
version:

   a) under this License, provided that you make a good faith effort to
   ensure that, in the event an Application does not supply the
   function or data, the facility still operates, and performs
   whatever part of its purpose remains meaningful, or

   b) under the GNU GPL, with none of the additional permissions of
   this License applicable to that copy.

  3. Object Code Incorporating Material from Library Header Files.

  The object code form of an Application may incorporate material from
a header file that is part of the Library.  You may convey such object
code under terms of your choice, provided that, if the incorporated
material is not limited to numerical parameters, data structure
layouts and accessors, or small macros, inline functions and templates
(ten or fewer lines in length), you do both of the following:

   a) Give prominent notice with each copy of the object code that the
   Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the object code with a copy of the GNU GPL and this license
   document.

  4. Combined Works.

  You may convey a Combined Work under terms of your choice that,
taken together, effectively do not restrict modification of the
portions of the Library contained in the Combined Work and reverse
engineering for debugging such modifications, if you also do each of
the following:

   a) Give prominent notice with each copy of the Combined Work that
   the Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the Combined Work with a copy of the GNU GPL and this license
   document.

   c) For a Combined Work that displays copyright notices during
   execution, include the copyright notice for the Library among
   these notices, as well as a reference directing the user to the
   copies of the GNU GPL and this license document.

   d) Do one of the following:

       0) Convey the Minimal Corresponding Source under the terms of this
       License, and the Corresponding Application Code in a form
       suitable for, and under terms that permit, the user to
       recombine or relink the Application with a modified version of
       the Linked Version to produce a modified Combined Work, in the
       manner specified by section 6 of the GNU GPL for conveying
       Corresponding Source.

       1) Use a suitable shared library mechanism for linking with the
       Library.  A suitable mechanism is one that (a) uses at run time
       a copy of the Library already present on the user's computer
       system, and (b) will operate properly with a modified version
       of the Library that is interface-compatible with the Linked
       Version.

   e) Provide Installation Information, but only if you would otherwise
   be required to provide such information under section 6 of the
   GNU GPL, and only to the extent that such information is
   necessary to install and execute a modified version of the
   Combined Work produced by recombining or relinking the
   Application with a modified version of the Linked Version. (If
   you use option 4d0, the Installation Information must accompany
   the Minimal Corresponding Source and Corresponding Application
   Code. If you use option 4d1, you must provide the Installation
   Information in the manner specified by section 6 of the GNU GPL
   for conveying Corresponding Source.)

  5. Combined Libraries.

  You may place library facilities that are a work based on the
Library side by side in a single library together with other library
facilities that are not Applications and are not covered by this
License, and convey such a combined library under terms of your
choice, if you do both of the following:

   a) Accompany the combined library with a copy of the same work based
   on the Library, uncombined with any other library facilities,
   conveyed under the terms of this License.

   b) Give prominent notice with the combined library that part of it
   is a work based on the Library, and explaining where to find the
   accompanying uncombined form of the same work.

  6. Revised Versions of the GNU Lesser General Public License.

  The Free Software Foundation may publish revised and/or new versions
of the GNU Lesser General Public License from time to time. Such new
versions will be similar in spirit to the present version, but may
differ in detail to address new problems or concerns.

  Each version is given a distinguishing version number. If the
Library as you received it specifies that a certain numbered version
of the GNU Lesser General Public License "or any later version"
applies to it, you have the option of following the terms and
conditions either of that published version or of any later version
published by the Free Software Foundation. If the Library as you
received it does not specify a version number of the GNU Lesser
General Public License, you may choose any version of the GNU Lesser
General Public License ever published by the Free Software Foundation.

  If the Library as you received it specifies that a proxy can decide
whether future versions of the GNU Lesser General Public License shall
apply, that proxy's public statement of acceptance of any version is
permanent authorization for you to choose that version for the
Library.



--- File: requirements.txt ---
# novel for pytorch DLC:
albumentations<=1.4.3
einops
pycocotools
timm
wandb

# existing:
dlclibrary
ipython
filterpy
ruamel.yaml>=0.15.0
intel-openmp
imageio-ffmpeg
imgaug>=0.4.0
numba>=0.54.0
matplotlib>=3.3, <3.8.4
networkx>=2.6
numpy>=1.18.5,<2.0.0
pandas>=1.0.1,!=1.5.0
Pillow>=7.1
pyyaml
scikit-image>=0.17
scikit-learn>=1.0
scipy>=1.9
statsmodels>=0.11
tensorflow>=2.0,<2.13.0
tables==3.8.0
tensorpack>=0.11
tf_slim>=1.1.0
torch>=2.0.0
torchvision
tqdm


--- File: AUTHORS ---
DeepLabCut (www.deeplabcut.org) was initially developed by
Alexander & Mackenzie Mathis in collaboration with Matthias Bethge in 2017.
It is actively developed by Alexander & Mackenzie Mathis (steering council and owners).

DeepLabCut is an open-source tool and has benefited from suggestions and edits by many
individuals: DeepLabCut/graphs/contributors

############################################################################################################

DeepLabCut 1.0 Toolbox
A Mathis, alexander.mathis@bethgelab.org | https://github.com/DeepLabCut/DeepLabCut
M Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab

Specific external contributors:
E Insafutdinov and co-authors of DeeperCut (see README) for feature detectors: https://github.com/eldar
- Thus, code in this subdirectory at the time of April 2018, deeplabcut/pose_estimation_tensorflow
was adapted from: https://github.com/eldar/pose-tensorflow.

Products:
DeepLabCut: markerless pose estimation of user-defined body parts with deep learning. Nature Neuroscience, 2018.
https://doi.org/10.1038/s41593-018-0209-y
A. Mathis, P. Mamidanna, K.M. Cury, T. Abe, V.N. Murthy, M.W. Mathis* & M. Bethge*

Contributions:
Conceptualization: A.M., M.W.M. and M.B.
Software: A.M. and M.W.M.
Formal analysis: A.M.
Experiments: A.M. and V.N.M. (trail-tracking), M.W.M. (mouse reaching), K.M.C. (Drosophila).
Image Labeling: P.M., K.M.C., T.A., M.W.M., A.M.
Writing: A.M. and M.W.M. with input from all authors.
These authors jointly directed this work: M. Mathis, M. Bethge

############################################################################################################

DeepLabCut 2.0 Toolbox
A Mathis, alexander.mathis@bethgelab.org | https://github.com/DeepLabCut/DeepLabCut
T Nath, nath@rowland.harvard.edu | https://github.com/meet10may
M Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab

Products:
Using DeepLabCut for 3D markerless pose estimation across species and behaviors. Nature Protocols, 2019.
https://www.nature.com/articles/s41596-019-0176-0
T. Nath*, A. Mathis*, AC. Chen, A. Patel, M. Bethge, M. Mathis

Contributions:
Conceptualization: AM, TN, MWM.
Software: AM, TN and MWM.
Dataset (cheetah): AP.
Image Labeling: ACC.
Formal analysis: ACC, AM and AP analyzed the cheetah data.
Writing: MWM, AM and TN with inputs from all authors.

############################################################################################################

DeepLabCut 2.1 major additions:
A Mathis, alexander.mathis@bethgelab.org | https://github.com/DeepLabCut/DeepLabCut
T Nath, nath@rowland.harvard.edu | https://github.com/meet10may
M Yüksekgönül, mertyuksekgonul@gmail.com | https://github.com/mertyg
M Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab

Specific external contributors:
Tensorpack augmentation: https://github.com/DeepLabCut/DeepLabCut/pull/409 by Katie Rupp

Products:
Pretraining boosts out-of-domain robustness for pose estimation. WACV, 2021.
http://www.mackenziemathislab.org/horse10
A. Mathis, T. Biasi, S. Schneider, M. Yüksekgönül, B. Rogers, M. Bethge, M. Mathis

############################################################################################################

DeepLabCut 2.1 - 2.2 additions:
A Mathis, alexander.mathis@epfl.ch | https://github.com/AlexEMG
J Lauer, jessy@deeplabcut.org | https://github.com/jeylau
M Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab
M Zhou, https://github.com/zhoumu53
S Ye, https://github.com/yeshaokai
S Schneider, https://github.com/stes
T Biasi, https://github.com/tbiasi
G Kane, https://github.com/gkane26
M Yüksekgönül, https://github.com/mertyg
T Nath, https://github.com/meet10may

Preprint:
Multi-animal pose estimation and tracking with DeepLabCut
J Lauer,  M Zhou,  S Ye,  W Menegas,  S Schneider, T Nath,  MM Rahman, V Di Santo,
D Soberanes, G Feng, VN Murthy, G Lauder, C Dulac,  M Mathis, A Mathis (2021).
https://www.biorxiv.org/content/10.1101/2021.04.30.442096v1

Publication:
Multi-animal pose estimation, identification and tracking with DeepLabCut
Lauer, J., Zhou, M., Ye, S., Menegas, W., Schneider, S., Nath, T., Rahman, M.M.,
Di Santo, V., Soberanes, D., Feng, G., Murthy, V.N., Lauder, G.V., Dulac, C.,
Mathis, M.W., & Mathis, A. (2022).
Nature Methods, 19, 496 - 504.

Conceptualization was done by A.M. and M.W.M. Formal analysis and code were done by J.L., A.M. and M.W.M.
New deep architectures were designed by M.Z., S.Y. and A.M. GUIs were done by J.L., M.W.M. and T.N.
Benchmark was set by S.S., M.W.M., A.M. and J.L. Marmoset data were gathered by W.M. and G.F.
Marmoset behavioral analysis was carried out by W.M. Parenting data were gathered by M.M.R., A.M. and C.D.
Tri-mouse data were gathered by D.S., A.M. and V.N.M. Fish data were gathered by V.D.S. and G.L.
The article was written by A.M., M.W.M. and J.L. with input from all authors.
M.W.M. and A.M. co-supervised the project.

############################################################################################################

DeepLabCut 2.2 - 3.0 additions:
A Mathis, alexander.mathis@epfl.ch | https://github.com/AlexEMG
M Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab
J Lauer, jessy@deeplabcut.org | https://github.com/jeylau
N Poulsen, neils.poulsen@epfl.ch | https://github.com/n-poulsen
S Schneider, stes@hey.com | https://github.com/stes
S Ye, shaokai.ye@epfl.ch | https://github.com/yeshaokai

Preprint:
Ye, S., Filippova, A., Lauer, J., Schneider, S., Vidal, M., Qiu, T., Mathis, A., & Mathis, M.W. (2023).
SuperAnimal pretrained pose estimation models for behavioral analysis. https://arxiv.org/abs/2203.07436


############################################################################################################

DeepLabCut 3.0 Toolbox
M Mathis, mackenzie@post.harvard.edu | https://github.com/MMathisLab
A Mathis, alexander.mathis@epfl.ch | https://github.com/AlexEMG
N Poulsen, neils.poulsen@epfl.ch | https://github.com/n-poulsen
S Ye, shaokai.ye@epfl.ch | https://github.com/yeshaokai
A Filippova, anastasiia.filippova@epfl.ch | https://github.com/nastya236
Q Macé | https://github.com/QuentinJGMace
J Lauer, jessy@deeplabcut.org | https://github.com/jeylau
L Stoffl, lucas.stoffl@epfl.ch | https://github.com/LucZot

We also greatly thank the 2023 DeepLabCut AI Residents who contributed:
Anna Teruel-Sanchis | https://github.com/anna-teruel
Riza Rae Pineda | https://github.com/rizarae-p
Konrad Danielewski | https://github.com/KonradDanielewski

Products:
PyTorch backend for DeepLabCut
Expanded SuperAnimal capabilities
New model architectures (WIP: stay tuned, but includes BUCTD)


--- File: .pre-commit-config.yaml ---
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: check-added-large-files
      - id: check-yaml
      - id: end-of-file-fixer
      - id: name-tests-test
      - id: trailing-whitespace
  - repo: https://github.com/PyCQA/isort
    rev: 5.12.0
    hooks:
      - id: isort
  - repo: https://github.com/psf/black
    rev: 22.3.0
    hooks:
      - id: black
        language_version: python3


--- File: pyproject.toml ---
[tool.yapf]
 based_on_style = "google"
 indent_width = 4

 [tool.isort]
 multi_line_output = 3
 include_trailing_comma = true
 force_sort_within_sections = false
 lexicographical = true
 single_line_exclusions = ['typing']
 order_by_type = false
 group_by_package = true
 line_length = 88
 skip = [
     "__init__.py",
 ] 
[tool.pytest.ini_options]
markers = [
    "require_models: mark test as requiring models to run"
]

--- File: .codespellrc ---
[codespell]
skip = .git,*.pdf,*.svg,deeplabcut/pose_estimation_tensorflow/models/pretrained
# MOT,SIE - legit acronyms
# tThe - for \tThe. codespell is not good detecting those yet
ignore-words-list = mot,sie,tthe,assertin,bu,td,ctd,wither


--- File: reinstall.sh ---
pip uninstall deeplabcut
python3 setup.py sdist bdist_wheel
pip install dist/deeplabcut-3.0.0rc7-py3-none-any.whl


--- File: README.md ---
<div align="center">
  

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1628250004229-KVYD7JJVHYEFDJ32L9VJ/DLClogo2021.jpg?format=1000w" width="95%">
</p>

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1665060917309-V0YVY2UKVLKSS6O18XDI/MousereachGIF.gif?format=1000w?format=180w" height="150">

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/daed7f16-527f-4150-8bdd-cbb20e267451/cheetah-ezgif.com-video-to-gif-converter.gif?format=180w" height="150">


<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1534797521117-EIEUED03C68241QZ4KCK/ke17ZwdGBToddI8pDm48kAx9qLOWpcHWRGxWsJQSczRZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpwdr4GYy30vFzf31Oe7KAPZKkqgaiEgc5jBNdhZmDPlzxdkDSclo6ofuXZm6YCEhUo/MATHIS_2018_fly.gif?format=180w" height="150">

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1619609897110-TKSTWKEM6HTGXID9D489/ke17ZwdGBToddI8pDm48kAvjv6tW_eojYQmNU0ncbllZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVHBSTXHtjUKlhRtWJ1Vo6l1B2bxJtByvWSjL6Vz3amc5yb8BodarTVrzIWCp72ioWw/triMouseDLC.gif?format=180w" height="150">


  



[📚Documentation](https://deeplabcut.github.io/DeepLabCut/README.html) |
[🛠️ Installation](https://deeplabcut.github.io/DeepLabCut/docs/installation.html) |
[🌎 Home Page](https://www.deeplabcut.org) |
[🐿🐴🐁🐘🐆 Model Zoo](http://www.mackenziemathislab.org/deeplabcut/) |
[🚨 News](https://deeplabcut.github.io/DeepLabCut/README.html#news-and-in-the-news) |
[🪲 Reporting Issues](https://github.com/DeepLabCut/DeepLabCut/issues) 


[🫶 Getting Assistance](https://deeplabcut.github.io/DeepLabCut/README.html#be-part-of-the-dlc-community) | 
[∞ DeepLabCut Online Course](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials/blob/master/DLCcourse.md) | 
[📝 Publications](https://deeplabcut.github.io/DeepLabCut/README.html#references) | 
[👩🏾‍💻👨‍💻 DeepLabCut AI Residency](https://www.deeplabcutairesidency.org/) 


![Version](https://img.shields.io/badge/python_version-3.10-purple)
[![Downloads](https://pepy.tech/badge/deeplabcut)](https://pepy.tech/project/deeplabcut)
[![Downloads](https://pepy.tech/badge/deeplabcut/month)](https://pepy.tech/project/deeplabcut)
[![PyPI version](https://badge.fury.io/py/deeplabcut.svg)](https://badge.fury.io/py/deeplabcut)
![Python package](https://github.com/DeepLabCut/DeepLabCut/workflows/Python%20package/badge.svg)
[![License: LGPL v3](https://img.shields.io/badge/License-LGPL%20v3-blue.svg)](https://www.gnu.org/licenses/lgpl-3.0)
<a href="https://github.com/psf/black"><img alt="Code style: black" src="https://img.shields.io/badge/code%20style-black-000000.svg"></a>
[![GitHub stars](https://img.shields.io/github/stars/DeepLabCut/DeepLabCut.svg?style=social&label=Star)](https://github.com/DeepLabCut/DeepLabCut)
[![Average time to resolve an issue](http://isitmaintained.com/badge/resolution/deeplabcut/deeplabcut.svg)](http://isitmaintained.com/project/deeplabcut/deeplabcut "Average time to resolve an issue")
[![Percentage of issues still open](http://isitmaintained.com/badge/open/deeplabcut/deeplabcut.svg)](http://isitmaintained.com/project/deeplabcut/deeplabcut "Percentage of issues still open")
[![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/deeplabcut)
[![Gitter](https://badges.gitter.im/DeepLabCut/community.svg)](https://gitter.im/DeepLabCut/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)
[![Twitter Follow](https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&style=social)](https://twitter.com/DeepLabCut)
[![Generic badge](https://img.shields.io/badge/Contributions-Welcome-brightgreen.svg)](CONTRIBUTING.md)
[![CZI's Essential Open Source Software for Science](https://chanzuckerberg.github.io/open-science/badges/CZI-EOSS.svg)](https://czi.co/EOSS)

</div>

# Welcome! 👋

**DeepLabCut™️** is a toolbox for state-of-the-art markerless pose estimation of animals performing various behaviors. As long as you can see (label) what you want to track, you can use this toolbox, as it is animal and object agnostic. [Read a short development and application summary below](https://github.com/DeepLabCut/DeepLabCut#why-use-deeplabcut). 

# [Installation: how to install DeepLabCut](https://deeplabcut.github.io/DeepLabCut/docs/installation.html)

Please click the link above for all the information you need to get started! Please note that currently we support only Python 3.10+ (see conda files for guidance).

Developers Stable Release: very quick start (Python 3.10+ required) to install 
DeepLabCut with the PyTorch engine

- [Install PyTorch](https://pytorch.org/get-started/locally/) (**select the desired
CUDA version if you want to use a GPU**): `pip install torch torchvision`
- Then, [install `pytables`](https://www.pytables.org/usersguide/installation.html): `conda install -c conda-forge pytables==3.8.0`
- Finally, install `DeepLabCut` (with all functions + the GUI): 
`pip install --pre  "deeplabcut[gui]"` or `pip install  --pre "deeplabcut"` (headless 
version with PyTorch)!

To use the TensorFlow engine (requires Python 3.10; TF up to v2.10 supported on Windows,
up to v2.12 on other platforms): you'll need to run `pip install "deeplabcut[gui,tf]"` 
(which includes all functions plus GUIs) or `pip install "deeplabcut[tf]"` (headless
version with PyTorch and TensorFlow).

We recommend using our conda file, see [here](https://github.com/DeepLabCut/DeepLabCut/blob/main/conda-environments/README.md) or the new [`deeplabcut-docker` package](https://github.com/DeepLabCut/DeepLabCut/tree/main/docker). 

# [Documentation: The DeepLabCut Process](https://deeplabcut.github.io/DeepLabCut/README.html)

Our docs walk you through using DeepLabCut, and key API points. For an overview of the toolbox and workflow for project management, see our step-by-step at [Nature Protocols paper](https://doi.org/10.1038/s41596-019-0176-0).

For a deeper understanding and more resources for you to get started with Python and DeepLabCut, please check out our free online course! http://DLCcourse.deeplabcut.org

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1609244903687-US1SN063QIFJS4BP4IJD/ke17ZwdGBToddI8pDm48kFG9xAYub2PPnmh56PTVg7gUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcAju5e7u9RZJEVbVQPZRu9xb_m-kUO2M3I1IeDqD4l8YcGqu2nZPx1UhKV8wc1ELN/dlc_overview_whitebkgrnd.png?format=2500w" width="95%">
</p>

# [DEMO the code](/examples)

🐭 pose tracking of single animals demo [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_DEMO_mouse_openfield.ipynb)

See [more demos here](https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/README.md). We provide data and several Jupyter Notebooks: one that walks you through a demo dataset to test your installation, and another Notebook to run DeepLabCut from the beginning on your own data. We also show you how to use the code in Docker, and on Google Colab.

# Why use DeepLabCut?

In 2018, we demonstrated the capabilities for [trail tracking](https://vnmurthylab.org/), [reaching in mice](http://www.mousemotorlab.org/) and various Drosophila behaviors during egg-laying (see [Mathis et al.](https://www.nature.com/articles/s41593-018-0209-y) for details). There is, however, nothing specific that makes the toolbox only applicable to these tasks and/or species. The toolbox has already been successfully applied (by us and others) to [rats](http://www.mousemotorlab.org/deeplabcut), humans, various fish species, bacteria, leeches, various robots, cheetahs, [mouse whiskers](http://www.mousemotorlab.org/deeplabcut) and [race horses](http://www.mousemotorlab.org/deeplabcut). DeepLabCut utilized the feature detectors (ResNets + readout layers) of one of the state-of-the-art algorithms for human pose estimation by Insafutdinov et al., called DeeperCut, which inspired the name for our toolbox (see references below). Since this time, the package has changed substantially.  The code has been re-tooled and re-factored since 2.1+: We have added faster and higher performance variants with MobileNetV2s, EfficientNets, and our own DLCRNet backbones (see [Pretraining boosts out-of-domain robustness for pose estimation](https://arxiv.org/abs/1909.11229) and [Lauer et al 2022](https://www.nature.com/articles/s41592-022-01443-0)). Additionally, we have improved the inference speed and provided both additional and novel augmentation methods, added real-time, and multi-animal support.
In v3.0+ we have changed the backend to support PyTorch. This brings not only an easier installation process for users, but performance gains, developer flexibility, and a lot of new tools! Importantly, the high-level API stays the same, so it will be a seamless transition for users 💜!
We currently provide state-of-the-art performance for animal pose estimation and the labs (M. Mathis Lab and A. Mathis Group) have both top journal and computer vision conference papers.

<p align="center">
<img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3e47258a922d548c483247/1547585339819/ErrorvsTrainingsetSize.png?format=750w" height="160">
<img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3e469d8a922d548c4828fa/1547585194560/compressionrobustness.png?format=750w" height="160">
<img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3fbed74fa51acecd63deeb/1547681534736/MouseLocomotion_warren.gif?format=500w" height="160">  
<img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3fc1c6758d46950ce7eec7/1547682383595/cheetah.png?format=750w" height="160">
</p>

**Left:** Due to transfer learning it requires **little training data** for multiple, challenging behaviors (see [Mathis et al. 2018](https://www.nature.com/articles/s41593-018-0209-y) for details). **Mid Left:** The feature detectors are robust to video compression (see [Mathis/Warren](https://www.biorxiv.org/content/early/2018/10/30/457242) for details). **Mid Right:** It allows 3D pose estimation with a single network and camera (see [Mathis/Warren](https://www.biorxiv.org/content/early/2018/10/30/457242)). **Right:** It allows 3D pose estimation with a single network trained on data from multiple cameras together with standard triangulation methods (see [Nath* and Mathis* et al. 2019](https://doi.org/10.1038/s41596-019-0176-0)).

**DeepLabCut** is embedding in a larger open-source eco-system, providing behavioral tracking for neuroscience, ecology, medical, and technical applications. Moreover, many new tools are being actively developed. See [DLC-Utils](https://github.com/DeepLabCut/DLCutils) for some helper code.

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588292233203-FD1DVKAQYNV2TU91CO7R/ke17ZwdGBToddI8pDm48kIX24IsDPzy6M4KUaihfICJZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIxtGUdkzp028KVNnpOijF3PweOM5su6FUQHO6Wkh72Nw/dlc_eco.gif?format=1000w" width="80%">
</p>

## Code contributors:

DLC code was originally developed by [Alexander Mathis](https://github.com/AlexEMG) & [Mackenzie Mathis](https://github.com/MMathisLab), and was extended in 2.0 with the core dev team consisting of [Tanmay Nath](https://github.com/meet10may) (2.0-2.1), and currently (2.1+) with [Jessy Lauer](https://github.com/jeylau) and (2.3+) [Niels Poulsen](https://github.com/n-poulsen).
DeepLabCut is an open-source tool and has benefited from suggestions and edits by many individuals including  Mert Yuksekgonul, Tom Biasi, Richard Warren, Ronny Eichler, Hao Wu, Federico Claudi, Gary Kane and Jonny Saunders as well as the [100+ contributors](https://github.com/DeepLabCut/DeepLabCut/graphs/contributors). Please see [AUTHORS](https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS) for more details!

This is an actively developed package and we welcome community development and involvement.

# Get Assistance & be part of the DLC Community✨:

| 🚉 Platform                                                 | 🎯 Goal                                                                      | ⏱️ Estimated Response Time | 📢 Support Squad                        |
|------------------------------------------------------------|-----------------------------------------------------------------------------|---------------------------|----------------------------------------|
| [![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/deeplabcut) <br /> 🐭Tag: DeepLabCut | To ask help and support questions👋                                          | Promptly🔥                 | DLC Team and The DLC Community |
| GitHub DeepLabCut/[Issues](https://github.com/DeepLabCut/DeepLabCut/issues)                                  | To report bugs and code issues🐛   (we encourage you to search issues first) | 2-3 days                  | DLC Team                               |
|[![Gitter](https://badges.gitter.im/DeepLabCut/community.svg)](https://gitter.im/DeepLabCut/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)                               | To discuss with other users,  share ideas and collaborate💡                  | 2 days                    | The DLC Community                      |
| GitHub DeepLabCut/[Contributing](https://github.com/DeepLabCut/DeepLabCut/blob/master/CONTRIBUTING.md)                          | To contribute your expertise and experience🙏💯                               | Promptly🔥                 | DLC Team                               |
| 🚧 GitHub DeepLabCut/[Roadmap](https://github.com/DeepLabCut/DeepLabCut/blob/master/docs/roadmap.md)                             | To learn more about our journey✈️                                            | N/A                       | N/A                                    |
| [![Twitter Follow](https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&style=social)](https://twitter.com/DeepLabCut)                                                   | To keep up with our latest news and updates 📢                               | Daily                     | DLC Team                               |
| The DeepLabCut [AI Residency Program](https://www.deeplabcutairesidency.org/)                        | To come and work with us next summer👏                                       | Annually                  | DLC Team                               |


## References \& Citations:

Please see our [dedicated page](https://deeplabcut.github.io/DeepLabCut/docs/citation.html) on how to **cite DeepLabCut** 🙏 and our suggestions for your Methods section!

## License:

This project is primarily licensed under the GNU Lesser General Public License v3.0. Note that the software is provided "as is", without warranty of any kind, express or implied. If you use the code or data, please cite us! Note, artwork (DeepLabCut logo) and images are copyrighted; please do not take or use these images without written permission.

SuperAnimal models are provided for research use only (non-commercial use).

## Major Versions:

- For all versions, please see [here](https://github.com/DeepLabCut/DeepLabCut/releases).

VERSION 3.0: A whole new experience with PyTorch🔥. While the high-level API remains the same, the backend and developer friendliness have greatly improved, along with performance gains!

VERSION 2.3: Model Zoo SuperAnimals, and a whole new GUI experience.

VERSION 2.2: Multi-animal pose estimation, identification, and tracking with DeepLabCut is supported (as well as single-animal projects).

VERSION 2.0-2.1: This is the **Python package** of [DeepLabCut](https://www.nature.com/articles/s41593-018-0209-y) that was originally released in Oct 2018 with our [Nature Protocols](https://doi.org/10.1038/s41596-019-0176-0) paper (preprint [here](https://www.biorxiv.org/content/10.1101/476531v1)).
This package includes graphical user interfaces to label your data, and take you from data set creation to automatic behavioral analysis. It also introduces an active learning framework to efficiently use DeepLabCut on large experimental projects, and data augmentation tools that improve network performance, especially in challenging cases (see [panel b](https://camo.githubusercontent.com/77c92f6b89d44ca758d815bdd7e801247437060b/68747470733a2f2f737461746963312e73717561726573706163652e636f6d2f7374617469632f3537663664353163396637343536366635356563663237312f742f3563336663316336373538643436393530636537656563372f313534373638323338333539352f636865657461682e706e673f666f726d61743d37353077)).

VERSION 1.0: The initial, Nature Neuroscience version of [DeepLabCut](https://www.nature.com/articles/s41593-018-0209-y) can be found in the history of git, or here: https://github.com/DeepLabCut/DeepLabCut/releases/tag/1.11

# News (and in the news):

:purple_heart: We released a major update, moving from 2.x --> 3.x with the backend change to PyTorch

:purple_heart: The DeepLabCut Model Zoo launches SuperAnimals, see more [here](http://www.mackenziemathislab.org/dlc-modelzoo/).

:purple_heart: **DeepLabCut supports multi-animal pose estimation!** maDLC is out of beta/rc mode and beta is deprecated, thanks to the testers out there for feedback! Your labeled data will be backwards compatible, but not all other steps. Please see the [new `2.2+` releases](https://github.com/DeepLabCut/DeepLabCut/releases) for what's new & how to install it, please see our new [paper, Lauer et al 2022](https://www.nature.com/articles/s41592-022-01443-0), and the [new docs]( https://deeplabcut.github.io/DeepLabCut) on how to use it!

:purple_heart: We support multi-animal re-identification, see [Lauer et al 2022](https://www.nature.com/articles/s41592-022-01443-0).

:purple_heart: We have a **real-time** package available! http://DLClive.deeplabcut.org


- June 2024: Our second DLC paper ['Using DeepLabCut for 3D markerless pose estimation across species and behaviors'](https://www.nature.com/articles/s41596-019-0176-0) in Nature Protocols has surpassed 1,000 Google Scholar citations!
- May 2024: DeepLabCut was featured in Nature: ['DeepLabCut: the motion-tracking tool that went viral'](https://www.nature.com/articles/d41586-024-01474-x)
- January 2024: Our original paper ['DeepLabCut: markerless pose estimation of user-defined body parts with deep learning'](https://www.nature.com/articles/s41593-018-0209-y) in Nature Neuroscience has surpassed 3,000 Google Scholar citations! 
- December 2023: DeepLabCut hit 600,000 downloads!
- October 2023: DeepLabCut celebrates a milestone with 4,000 🌟 in Github!
- July 2023: The user forum is very active with more than 1k questions and answers: [![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/deeplabcut)
- May 2023: The Model Zoo is now fully integrated into the DeepLabCut GUI, making it easier than ever to access a variety of pre-trained models. Check out the accompanying paper: [SuperAnimal pretrained pose estimation models for behavioral analysis by Ye et al.](https://arxiv.org/abs/2203.07436)
- December 2022: DeepLabCut hits 450,000 downloads and 2.3 is the new stable release
- August 2022: DeepLabCut hit 400,000 downloads
- August 2021: 2.2 becomes the new stable release for DeepLabCut.
- July 2021: Docs are now at https://deeplabcut.github.io/DeepLabCut and we now include TensorFlow 2 support!
- May 2021: DeepLabCut hit 200,000 downloads! Also, Our preprint on 2.2, multi-animal DeepLabCut is released!
- Jan 2021: [Pretraining boosts out-of-domain robustness for pose estimation](https://openaccess.thecvf.com/content/WACV2021/html/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html) published in the IEEE Winter Conference on Applications of Computer Vision. We also added EfficientNet backbones to DeepLabCut, those are best trained with cosine decay (see paper). To use them, just pass "`efficientnet-b0`" to "`efficientnet-b6`" when creating the trainingset!
- Dec 2020: We released a real-time package that allows for online pose estimation and real-time feedback. See [DLClive.deeplabcut.org](http://DLClive.deeplabcut.org).
- 5/22 2020: We released 2.2beta5. This beta release has some of the features of DeepLabCut 2.2, whose major goal is to integrate multi-animal pose estimation to DeepLabCut.
- Mar 2020: Inspired by suggestions we heard at this weeks CZI's Essential Open Source Software meeting in Berkeley, CA we updated our [docs](docs/UseOverviewGuide.md). Let us know what you think!
- Feb 2020: Our [review on animal pose estimation is published!](https://www.sciencedirect.com/science/article/pii/S0959438819301151)
- Nov 2019: DeepLabCut was recognized by the Chan Zuckerberg Initiative (CZI) with funding to support this project. Read more in the [Harvard Gazette](https://news.harvard.edu/gazette/story/newsplus/harvard-researchers-awarded-czi-open-source-award/), on [CZI's Essential Open Source Software for Science site](https://chanzuckerberg.com/eoss/proposals/) and in their [Medium post](https://medium.com/@cziscience/how-open-source-software-contributors-are-accelerating-biomedicine-1a5f50f6846a)
- Oct 2019: DLC 2.1 released with lots of updates. In particular, a Project Manager GUI, MobileNetsV2, and augmentation packages (Imgaug and Tensorpack). For detailed updates see [releases](https://github.com/DeepLabCut/DeepLabCut/releases)
- Sept 2019: We published two preprints. One showing that [ImageNet pretraining contributes to robustness](https://arxiv.org/abs/1909.11229) and a [review on animal pose estimation](https://arxiv.org/abs/1909.13868). Check them out!
- Jun 2019: DLC 2.0.7 released with lots of updates. For updates see [releases](https://github.com/DeepLabCut/DeepLabCut/releases)
- Feb 2019: DeepLabCut joined [twitter](https://twitter.com/deeplabcut) [![Twitter Follow](https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&style=social)](https://twitter.com/DeepLabCut)
- Jan 2019: We hosted workshops for DLC in Warsaw, Munich and Cambridge. The materials are available [here](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials)
- Jan 2019: We joined the Image Source Forum for user help: [![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftag%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tag/deeplabcut)

- Nov 2018: We posted a detailed guide for DeepLabCut 2.0 on [BioRxiv](https://www.biorxiv.org/content/early/2018/11/24/476531). It also contains a case study for 3D pose estimation in cheetahs.
- Nov 2018: Various (post-hoc) analysis scripts contributed by users (and us) will be gathered at [DLCutils](https://github.com/DeepLabCut/DLCutils). Feel free to contribute! In particular, there is a script guiding you through
importing a project into the new data format for DLC 2.0
- Oct 2018: new pre-print on the speed video-compression and robustness of DeepLabCut on [BioRxiv](https://www.biorxiv.org/content/early/2018/10/30/457242)
- Sept 2018: Nature Lab Animal covers DeepLabCut: [Behavior tracking cuts deep](https://www.nature.com/articles/s41684-018-0164-y)
- Kunlin Wei & Konrad Kording write a very nice News & Views on our paper: [Behavioral Tracking Gets Real](https://www.nature.com/articles/s41593-018-0215-0)
- August 2018: Our [preprint](https://arxiv.org/abs/1804.03142) appeared in [Nature Neuroscience](https://www.nature.com/articles/s41593-018-0209-y)
- August 2018: NVIDIA AI Developer News: [AI Enables Markerless Animal Tracking](https://news.developer.nvidia.com/ai-enables-markerless-animal-tracking/)
- July 2018: Ed Yong covered DeepLabCut and interviewed several users for the [Atlantic](https://www.theatlantic.com/science/archive/2018/07/deeplabcut-tracking-animal-movements/564338).
- April 2018: first DeepLabCut preprint on [arXiv.org](https://arxiv.org/abs/1804.03142)

  ## Funding

  We are grateful for the follow support over the years! This software project was supported in part by the Essential Open Source Software for Science (EOSS) program at Chan Zuckerberg Initiative (cycles 1, 3, 3-DEI, 4), and jointly with the Kavli Foundation for EOSS Cycle 6!  We also thank the Rowland Institute at Harvard for funding from 2017-2020, and EPFL from 2020-present.


--- File: _toc.yml ---
format: jb-book
root: README
parts:
- caption: Getting Started
  chapters:
  - file: docs/UseOverviewGuide
  - file: docs/course
- caption: Installation
  chapters:
  - file: docs/installation
  - file: docs/recipes/installTips
  - file: docs/docker
- caption: Main User Guides
  chapters:
  - file: docs/standardDeepLabCut_UserGuide
  - file: docs/maDLC_UserGuide
  - file: docs/Overviewof3D
  - file: docs/HelperFunctions
- caption: Graphical User Interfaces (GUIs)
  chapters:
  - file: docs/gui/PROJECT_GUI
  - file: docs/gui/napari_GUI
- caption: DLC3 PyTorch Specific Docs
  chapters:
  - file: docs/pytorch/user_guide.md
  - file: docs/pytorch/pytorch_config.md
  - file: docs/pytorch/architectures.md
- caption: Quick Start Tutorials
  chapters:
  - file: docs/quick-start/single_animal_quick_guide
  - file: docs/quick-start/tutorial_maDLC
- caption: 🚀 Beginner's Guide to DeepLabCut
  chapters:
  - file: docs/beginner-guides/beginners-guide
  - file: docs/beginner-guides/manage-project
  - file: docs/beginner-guides/labeling
  - file: docs/beginner-guides/Training-Evaluation
  - file: docs/beginner-guides/video-analysis
- caption: Hardware Tips
  chapters:
  - file: docs/recipes/TechHardware
- caption: DeepLabCut-Live!
  chapters:
  - file: docs/deeplabcutlive
- caption: 🦄 DeepLabCut Model Zoo
  chapters:
  - file: docs/ModelZoo
  - file: docs/recipes/UsingModelZooPupil
- caption: 🧑‍🍳 Cookbook (detailed helper guides)
  chapters:
  - file: docs/convert_maDLC
  - file: docs/recipes/OtherData
  - file: docs/recipes/io
  - file: docs/recipes/nn
  - file: docs/recipes/post
  - file: docs/recipes/BatchProcessing
  - file: docs/recipes/DLCMethods
  - file: docs/recipes/ClusteringNapari
  - file: docs/recipes/OpenVINO
  - file: docs/recipes/flip_and_rotate
  - file: docs/recipes/pose_cfg_file_breakdown
  - file: docs/recipes/publishing_notebooks_into_the_DLC_main_cookbook
- caption: DeepLabCut Benchmarking
  chapters:
  - file: docs/benchmark
  - file: docs/pytorch/Benchmarking_shuffle_guide
- caption: Mission & Contribute
  chapters:
  - file: docs/MISSION_AND_VALUES
  - file: docs/roadmap
  - file: docs/Governance
- caption: Citations for DeepLabCut
  chapters:
  - file: docs/citation


--- File: setup.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepLabCut2.0-3.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.
https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""
from __future__ import annotations

import setuptools
from pathlib import Path


with open("README.md", encoding="utf-8", errors="replace") as fh:
    long_description = fh.read()


def super_animal_config_paths() -> list[str]:
    config_dirs = [
        Path("deeplabcut") / "modelzoo" / "model_configs",
        Path("deeplabcut") / "modelzoo" / "project_configs",
    ]

    configs = []
    for subdir in config_dirs:
        for p in subdir.iterdir():
            if p.suffix == ".yaml":
                configs.append(str(p))

    return configs


def pytorch_config_paths() -> list[str]:
    pytorch_configs = []
    config_dir = Path("deeplabcut") / "pose_estimation_pytorch" / "config"
    config_subdirs = [p for p in config_dir.iterdir() if p.is_dir()]
    for subdir in config_subdirs:
        for p in subdir.iterdir():
            if p.suffix == ".yaml":
                pytorch_configs.append(str(p))

    return pytorch_configs


setuptools.setup(
    name="deeplabcut",
    version="3.0.0rc7",
    author="A. & M.W. Mathis Labs",
    author_email="alexander@deeplabcut.org",
    description="Markerless pose-estimation of user-defined features with deep learning",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/DeepLabCut/DeepLabCut",
    install_requires=[
        "albumentations<=1.4.3",
        "dlclibrary>=0.0.7",
        "einops",
        "dlclibrary>=0.0.6",
        "filterpy>=1.4.4",
        "ruamel.yaml>=0.15.0",
        "imgaug>=0.4.0",
        "imageio-ffmpeg",
        "numba>=0.54",
        "matplotlib>=3.3,<3.9,!=3.7.0,!=3.7.1",
        "networkx>=2.6",
        "numpy>=1.18.5,<2.0.0",
        "pandas>=1.0.1,!=1.5.0",
        "scikit-image>=0.17",
        "scikit-learn>=1.0",
        "scipy>=1.9",
        "statsmodels>=0.11",
        "tables==3.8.0",
        "timm",
        "torch>=2.0.0",
        "torchvision",
        "tqdm",
        "pycocotools",
        "pyyaml",
        "Pillow>=7.1",
    ],
    extras_require={
        "gui": [
            "pyside6==6.4.2",
            "qdarkstyle==3.1",
            "napari-deeplabcut>=0.2.1.6",
        ],
        "openvino": ["openvino-dev==2022.1.0"],
        "docs": ["numpydoc"],
        "tf": [
            "tensorflow>=2.0,<=2.10;platform_system=='Windows'",
            "tensorflow>=2.0,<=2.12;platform_system!='Windows'",
            "tensorpack>=0.11",
            "tf_slim>=1.1.0",
        ],  # Last supported TF version on Windows Native is 2.10
        "apple_mchips": [
            "tensorflow-macos<2.13.0",
            "tensorflow-metal",
            "tensorpack>=0.11",
            "tf_slim>=1.1.0",
        ],
        "modelzoo": ["huggingface_hub"],
        "wandb": ["wandb"],
    },
    scripts=["deeplabcut/pose_estimation_tensorflow/models/pretrained/download.sh"],
    packages=setuptools.find_packages(),
    data_files=[
        (
            "deeplabcut",
            [
                "deeplabcut/pose_cfg.yaml",
                "deeplabcut/inference_cfg.yaml",
                "deeplabcut/reid_cfg.yaml",
                "deeplabcut/modelzoo/models_to_framework.json",
                "deeplabcut/pose_estimation_tensorflow/models/pretrained/pretrained_model_urls.yaml",
                "deeplabcut/gui/style.qss",
                "deeplabcut/gui/media/logo.png",
                "deeplabcut/gui/media/dlc_1-01.png",
                "deeplabcut/gui/media/dlc-pt.png",
                "deeplabcut/gui/media/dlc-tf.png",
                "deeplabcut/gui/assets/logo.png",
                "deeplabcut/gui/assets/logo_transparent.png",
                "deeplabcut/gui/assets/welcome.png",
                "deeplabcut/gui/assets/icons/help.png",
                "deeplabcut/gui/assets/icons/help2.png",
                "deeplabcut/gui/assets/icons/new_project.png",
                "deeplabcut/gui/assets/icons/new_project2.png",
                "deeplabcut/gui/assets/icons/open.png",
                "deeplabcut/gui/assets/icons/open2.png",
            ] + super_animal_config_paths() + pytorch_config_paths(),
        )
    ],
    include_package_data=True,
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)",
        "Operating System :: OS Independent",
    ],
    entry_points="""[console_scripts]
            dlc=deeplabcut.__main__:main""",
)

# https://www.python.org/dev/peps/pep-0440/#compatible-release


--- File: .gitignore ---
# Docker specific
logs/
#Jupyter book build directory
_build/*
#Data and examples
/examples/open*
/examples/Reac*
/examples/TES*
/examples/multi*
/examples/3D*
/examples/m3*
/examples/OUT
/examples/pretrained*
.local
.DS_Store
examples/.DS_Store
*~
# Tensorflow checkpoints
*.ckpt
snapshot-*

# Modelzoo checkpoints
deeplabcut/modelzoo/checkpoints/

# PyTorch backbone weights
deeplabcut/pose_estimation_pytorch/models/backbones/pretrained_weights/

# Wandb files
wandb/

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
#lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/

# Translations
*.mo
*.pot

# Django stuff:
*.log*.ckpt
snapshot-*

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
local_settings.py

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# dotenv
.env

# virtualenv
.venv
venv/
ENV/

# Spyder project settings
.spyderproject
.spyproject

# IDEs configurations
.vscode/*
.idea/*

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/


--- File: CONTRIBUTING.md ---
# How to Contribute to DeepLabCut

DeepLabCut is an actively developed package and we welcome community development and involvement. We are especially seeking people from underrepresented backgrounds in OSS to contribute their expertise and experience. Please get in touch if you want to discuss specific contributions you are interested in developing, and we can help shape a road-map.

We are happy to receive code extensions, bug fixes, documentation updates, etc.

If you are a new user, we recommend checking out the detailed [Github Guides](https://guides.github.com).

## Setting up a development installation

In order to make changes to `deeplabcut`, you will need to [fork](https://guides.github.com/activities/forking/#fork) the
[repository](https://github.com/deeplabcut/deeplabcut).

If you are not familiar with `git`, we recommend reading up on [this guide](https://guides.github.com/introduction/git-handbook/#basic-git).

Here are guidelines for installing deeplabcut locally on your own computer, where you can make changes to the code! We often update the master deeplabcut code base on github, and then ~1 a month we push out a stable release on pypi. This is what most users turn to on a daily basis (i.e. pypi is where you get your `pip install deeplabcut` code from! 

But, sometimes we add things to the repo that are not yet integrated, or you might want to edit the code yourself, or you will need to do this to contribute. Here, we show you how to do this. 

**Step 1:**

- git clone the repo into a folder on your computer:  

- click on this green button and copy the link:

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1581984907363-G8AFGX4V20Y1XD1PSZAK/ke17ZwdGBToddI8pDm48kGJBV0_F4LE4_UtCip_K_3lZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVE0ejQCe16973Pm-pux3j5_Oqt57D2H0YbaJ3tl8vn_eR926scO3xePJoa6uVJa9B4/gitclone.png?format=500w)

- then in the terminal type: `git clone https://github.com/DeepLabCut/DeepLabCut.git`

**Step 2:**

- Now you will work from the terminal inside this cloned folder:

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1581985288123-V8XUAY0C0ZDNJ5WBHB7Y/ke17ZwdGBToddI8pDm48kIsGBOdR9tS_SxF6KQXIcDtZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpz3c8X74DzCy4P3pv-ZANOdh-3ZL9iVkcryTbbTskaGvEc42UcRKU-PHxLXKM6ZekE/terminal.png?format=750w)

- Now, when you start `ipython` and `import deeplabcut` you are importing the folder "deeplabcut" - so any changes you make, or any changes we made before adding it to the pip package, are here.

- You can also check which deeplabcut you are importing by running: `deeplabcut.__file__`

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1581985466026-94OCSZJ5TL8U52JLB5VU/ke17ZwdGBToddI8pDm48kNdOD5iqmBzHwUaWGKS6qHBZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpyQPoegsR7K4odW9xcCi1MIHmvHh95_BFXYdKinJaRhV61R4G3qaUq94yWmtQgdj1A/importlocal.png?format=750w)

If you make changes to the code/first use the code, be sure you run `./resinstall.sh`, which you find in the main DeepLabCut folder:

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1609353210708-FRNREI7HUNS4GLDSJ00G/ke17ZwdGBToddI8pDm48kAya1IcSd32bok4WHvykeicUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dq18t0tDkB2HMfL2JGcLHN27k5rSOPIU8nEAZT0p1MiSCjLISwBs8eEdxAxTptZAUg/Screen+Shot+2020-12-30+at+7.33.16+PM.png?format=2500w)



Note, before committing to DeepLabCut, please be sure your code is formatted according to `black`. To learn more,
see [`black`'s documentation](https://black.readthedocs.io/en/stable/).

Now, please make a [pull request](https://github.com/DeepLabCut/DeepLabCut/pull/new/) that includes both a **summary of and changes to**:

- How you modified the code and what new functionality it has.
- DOCSTRING update for your change
- A working example of how it works for users. 
- If it's a function that also can be used in downstream steps (i.e. could be plotted) we ask you (1) highlight this, and (2) ideally you provide that functionality as well. If you have any questions, please reach out: admin@deeplabcut.org 

**TestScript outputs:**

- The **OS it has been tested on**
- the **output of the [testscript.py](/examples/testscript.py)** and if you are editing the **3D code the [testscript_3d.py](/examples/testscript_3d.py)**, and if you edit multi-animal code please run the [maDLC test script](https://github.com/DeepLabCut/DeepLabCut/blob/master/examples/testscript_multianimal.py).

**Review & Formatting:**

- Please run black on the code to conform to our Black code style (see more at https://pypi.org/project/black/). 
- Please assign a reviewer, typically @AlexEMG, @mmathislab, or @jeylau (i/e. the [core-developers](https://github.com/orgs/DeepLabCut/teams/core-developers/members))

**Code headers**

- The code headers can be standardized by running `python tools/update_license_headers.py`
- Edit `NOTICE.yml` to update the header. 

**DeepLabCut is an open-source tool and has benefited from suggestions and edits by many individuals:**

- the [authors](/AUTHORS)
- [code contributors](https://github.com/DeepLabCut/DeepLabCut/graphs/contributors) 



--- File: _config.yml ---
title: DeepLabCut
author: The DeepLabCut Team
logo: docs/images/logo.png
only_build_toc_files: true

sphinx:
  config:
    autodoc_mock_imports: ["wx", "matplotlib", "qtpy", "PySide6", "napari", "shiboken6"]
    mermaid_output_format: raw
  extra_extensions:
    - numpydoc
    - sphinxcontrib.mermaid

execute:
  execute_notebooks: "off"

html:
  extra_navbar: ""
  use_issues_button: true
  use_repository_button: true
  extra_footer: |
    <div>Powered by <a href="https://jupyterbook.org/">Jupyter Book</a>.</div>

repository:
  url: https://github.com/DeepLabCut/DeepLabCut
  path_to_book: docs
  branch: main

launch_buttons:
  colab_url: "https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb"


--- File: dlc.py ---
"""
DeepLabCut2.0-2.2 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut

Please see AUTHORS for contributors.
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""

from deeplabcut import cli


def main():
    cli.main()


if __name__ == "__main__":
    main()


--- File: testscript_cli.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
modified from: https://github.com/DeepLabCut/DeepLabCut-core/testscript_cli.py
by Mackenzie.

DEVELOPERS:
This script tests various functionalities in an automatic way.
It produces nothing of interest scientifically.
"""

task = "Testcore"  # Enter the name of your experiment Task
scorer = "Mackenzie"  # Enter the name of the experimenter/labeler

import os, subprocess, sys


# def install(package):
#    subprocess.check_call([sys.executable, "-m", "pip", "install", package])
# install("tensorflow==1.13.1")

import deeplabcut as dlc
from deeplabcut.core.engine import Engine

from pathlib import Path
import pandas as pd
import numpy as np
import platform

print("Imported DLC!")

engine = Engine.TF

basepath = os.path.dirname(os.path.abspath("testscript_cli.py"))
videoname = "reachingvideo1"
video = [
    os.path.join(
        basepath,
        "examples",
        "Reaching-Mackenzie-2018-08-30",
        "videos",
        videoname + ".avi",
    )
]
# For testing a color video:
# videoname='baby4hin2min'
# video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]
# to test destination folder:
# dfolder=basepath
print(video)

dfolder = None
net_type = "resnet_50"  #'mobilenet_v2_0.35' #'resnet_50'
augmenter_type = "default"
augmenter_type2 = "imgaug"

if platform.system() == "Darwin" or platform.system() == "Windows":
    print("On Windows/OSX tensorpack is not tested by default.")
    augmenter_type3 = "imgaug"
else:
    augmenter_type3 = "tensorpack"  # Does not work on WINDOWS

numiter = 3

print("CREATING PROJECT")
path_config_file = dlc.create_new_project(task, scorer, video, copy_videos=True)

cfg = dlc.auxiliaryfunctions.read_config(path_config_file)
cfg["numframes2pick"] = 5
cfg["pcutoff"] = 0.01
cfg["TrainingFraction"] = [0.8]
cfg["skeleton"] = [["bodypart1", "bodypart2"], ["bodypart1", "bodypart3"]]

dlc.auxiliaryfunctions.write_config(path_config_file, cfg)

print("EXTRACTING FRAMES")
dlc.extract_frames(path_config_file, mode="automatic", userfeedback=False)

print("CREATING SOME LABELS FOR THE FRAMES")
frames = os.listdir(os.path.join(cfg["project_path"], "labeled-data", videoname))
# As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)
for index, bodypart in enumerate(cfg["bodyparts"]):
    columnindex = pd.MultiIndex.from_product(
        [[scorer], [bodypart], ["x", "y"]], names=["scorer", "bodyparts", "coords"]
    )
    frame = pd.DataFrame(
        100 + np.ones((len(frames), 2)) * 50 * index,
        columns=columnindex,
        index=[os.path.join("labeled-data", videoname, fn) for fn in frames],
    )
    if index == 0:
        dataFrame = frame
    else:
        dataFrame = pd.concat([dataFrame, frame], axis=1)

dataFrame.to_csv(
    os.path.join(
        cfg["project_path"],
        "labeled-data",
        videoname,
        "CollectedData_" + scorer + ".csv",
    )
)
dataFrame.to_hdf(
    os.path.join(
        cfg["project_path"],
        "labeled-data",
        videoname,
        "CollectedData_" + scorer + ".h5",
    ),
    "df_with_missing",
    format="table",
    mode="w",
)

print("Plot labels...")

dlc.check_labels(path_config_file)

print("CREATING TRAININGSET")
dlc.create_training_dataset(
    path_config_file, net_type=net_type, augmenter_type=augmenter_type, engine=engine,
)

posefile = os.path.join(
    cfg["project_path"],
    "dlc-models/iteration-"
    + str(cfg["iteration"])
    + "/"
    + cfg["Task"]
    + cfg["date"]
    + "-trainset"
    + str(int(cfg["TrainingFraction"][0] * 100))
    + "shuffle"
    + str(1),
    "train/pose_cfg.yaml",
)

DLC_config = dlc.auxiliaryfunctions.read_plainconfig(posefile)
DLC_config["save_iters"] = numiter
DLC_config["display_iters"] = 2
DLC_config["multi_step"] = [[0.001, numiter]]

print("CHANGING training parameters to end quickly!")
dlc.auxiliaryfunctions.write_plainconfig(posefile, DLC_config)

print("TRAIN")
dlc.train_network(path_config_file)

print("EVALUATE")
dlc.evaluate_network(path_config_file, plotting=True)

videotest = os.path.join(cfg["project_path"], "videos", videoname + ".avi")

print(videotest)

# quicker variant
"""
print("VIDEO ANALYSIS")
dlc.analyze_videos(path_config_file, [videotest], save_as_csv=True)

print("CREATE VIDEO")
dlc.create_labeled_video(path_config_file,[videotest], save_frames=False)

print("Making plots")
dlc.plot_trajectories(path_config_file,[videotest])

print("CREATING TRAININGSET 2")
dlc.create_training_dataset(path_config_file, Shuffles=[2],net_type=net_type,augmenter_type=augmenter_type2)

cfg=dlc.auxiliaryfunctions.read_config(path_config_file)
posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(2),'train/pose_cfg.yaml')
DLC_config=dlc.auxiliaryfunctions.read_plainconfig(posefile)
DLC_config['save_iters']=numiter
DLC_config['display_iters']=1
DLC_config['multi_step']=[[0.001,numiter]]

print("CHANGING training parameters to end quickly!")
dlc.auxiliaryfunctions.write_config(posefile,DLC_config)

print("TRAIN")
dlc.train_network(path_config_file, shuffle=2,allow_growth=True)

print("EVALUATE")
dlc.evaluate_network(path_config_file,Shuffles=[2],plotting=False)


print("ANALYZING some individual frames")
dlc.analyze_time_lapse_frames(path_config_file,os.path.join(cfg['project_path'],'labeled-data/reachingvideo1/'))
"""

print("Export model...")
dlc.export_model(path_config_file, shuffle=1, make_tar=False)

print(
    "ALL DONE!!! - default/imgaug cases of DLCcore training and evaluation are functional (no extract outlier or refinement tested)."
)


--- File: tools/update_license_headers.py ---
"""Apply copyright headers to all code files in the repository.

This file can be called as a python script without arguments. For
configuration, see the instructions in NOTICE.yml.
"""

import tempfile
import glob
import yaml
import fnmatch
import subprocess


def load_config(filename):
    with open(filename, "r") as fh:
        config = yaml.safe_load(fh)
    return config


def walk_directory(entry):
    """Talk the directory"""

    if "header" not in entry:
        raise ValueError("Current entry does not have a header.")
    if "include" not in entry:
        raise ValueError("Current entry does not have an include list.")

    def _list_include():
        """List all files specified in the include list."""
        for include_pattern in entry["include"]:
            for filename in glob.iglob(include_pattern, recursive=True):
                yield filename

    def _filter_exclude(iterable):
        """Filter filenames from an iterator by the exclude patterns."""
        for filename in iterable:
            for exclude_pattern in entry.get("exclude", []):
                if fnmatch.fnmatch(filename, exclude_pattern):
                    break
            else:
                yield filename

    files = _filter_exclude(set(_list_include()))
    return list(files)


def main(input_file="NOTICE.yml"):
    config = load_config(input_file)
    for entry in config:
        filelist = list(walk_directory(entry))
        with tempfile.NamedTemporaryFile(mode="w") as header_file:
            header_file.write(entry["header"])
            header_file.flush()
            header_file.seek(0)
            command = ["licenseheaders", "-t", str(header_file.name), "-f"] + filelist
            result = subprocess.run(command, capture_output=True)
            if result.returncode != 0:
                print(result.stdout.decode())
                print(result.stderr.decode())


if __name__ == "__main__":
    main()


--- File: tools/README.md ---
# Developer tools useful for maintaining the repository

As developer you'll need:

```bash
pip install coverage pytest fnmatch black
```

## Code headers

The code headers can be standardized by running

``` bash
python tools/update_license_headers.py
```

from the repository root.

You can edit the `NOTICE.yml` to update the header.


## Workflow for contributing/checking your code

```bash
black .
```

## Running the tests (locally)

We use the pytest framework. You can just run:

```bash
pytest
```

For coverage run:

```
coverage run -m pytest
coverage report
```


--- File: docker/motd.sh ---
#!/bin/bash
# DLC docker message of the day

check_root() {
    if [[ $(id -un) == "root" ]]; then
        echo !!! Warning: !!!
        echo It seems like you run the container as root, which is not recommended.
        echo If this is not intended, make sure to launch the container with the
        echo '-u $(id -u)' flag set or use the helper scripts in the main
        echo DLC repo, https://github.com/DeepLabCut/DeepLabCut
    fi
}

print_version() {
    DLC_VERSION=$(
        python3 -c "import deeplabcut; print(deeplabcut.__version__)" \
            2>/dev/null ||
            echo [unknown version]
    )
    echo Welcome to DeepLabCut ${DLC_VERSION}!
    echo You are running the container as user $(id -un) \($(id -u)\).
}

cat <<"EOF"
                    .--,       .--,
                    ( (  \.---./  ) )
                     '.__/o   o\__.'
                       `{=  ^  =}´
                         >  u  <
 ____________________.""`-------`"".______________________  
\   ___                   __         __   _____       __  /
/  / _ \ ___  ___  ___   / /  ___ _ / /  / ___/__ __ / /_ \
\ / // // -_)/ -_)/ _ \ / /__/ _ `// _ \/ /__ / // // __/ /
//____/ \__/ \__// .__//____/\_,_//_.__/\___/ \_,_/ \__/  \
\_________________________________________________________/
                       ___)( )(___ `-.___. 
                      (((__) (__)))      ~`

EOF

print_version
check_root


--- File: docker/Dockerfile.test ---
ARG CUDA_VERSION
ARG DEEPLABCUT_VERSION
FROM deeplabcut/deeplabcut:${DEEPLABCUT_VERSION}-core-cuda${CUDA_VERSION}-latest

RUN mkdir test/
WORKDIR test
RUN apt-get update && apt-get install -yy git
RUN git config --global advice.detachedHead false
RUN git clone --depth 1 --branch v${DEEPLABCUT_VERSION} \
    https://github.com/DeepLabCut/DeepLabCut.git /test

RUN pip3 install --no-cache-dir pytest
RUN chmod a+rwx -R /test



--- File: docker/LICENSE ---
                   GNU LESSER GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.


  This version of the GNU Lesser General Public License incorporates
the terms and conditions of version 3 of the GNU General Public
License, supplemented by the additional permissions listed below.

  0. Additional Definitions.

  As used herein, "this License" refers to version 3 of the GNU Lesser
General Public License, and the "GNU GPL" refers to version 3 of the GNU
General Public License.

  "The Library" refers to a covered work governed by this License,
other than an Application or a Combined Work as defined below.

  An "Application" is any work that makes use of an interface provided
by the Library, but which is not otherwise based on the Library.
Defining a subclass of a class defined by the Library is deemed a mode
of using an interface provided by the Library.

  A "Combined Work" is a work produced by combining or linking an
Application with the Library.  The particular version of the Library
with which the Combined Work was made is also called the "Linked
Version".

  The "Minimal Corresponding Source" for a Combined Work means the
Corresponding Source for the Combined Work, excluding any source code
for portions of the Combined Work that, considered in isolation, are
based on the Application, and not on the Linked Version.

  The "Corresponding Application Code" for a Combined Work means the
object code and/or source code for the Application, including any data
and utility programs needed for reproducing the Combined Work from the
Application, but excluding the System Libraries of the Combined Work.

  1. Exception to Section 3 of the GNU GPL.

  You may convey a covered work under sections 3 and 4 of this License
without being bound by section 3 of the GNU GPL.

  2. Conveying Modified Versions.

  If you modify a copy of the Library, and, in your modifications, a
facility refers to a function or data to be supplied by an Application
that uses the facility (other than as an argument passed when the
facility is invoked), then you may convey a copy of the modified
version:

   a) under this License, provided that you make a good faith effort to
   ensure that, in the event an Application does not supply the
   function or data, the facility still operates, and performs
   whatever part of its purpose remains meaningful, or

   b) under the GNU GPL, with none of the additional permissions of
   this License applicable to that copy.

  3. Object Code Incorporating Material from Library Header Files.

  The object code form of an Application may incorporate material from
a header file that is part of the Library.  You may convey such object
code under terms of your choice, provided that, if the incorporated
material is not limited to numerical parameters, data structure
layouts and accessors, or small macros, inline functions and templates
(ten or fewer lines in length), you do both of the following:

   a) Give prominent notice with each copy of the object code that the
   Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the object code with a copy of the GNU GPL and this license
   document.

  4. Combined Works.

  You may convey a Combined Work under terms of your choice that,
taken together, effectively do not restrict modification of the
portions of the Library contained in the Combined Work and reverse
engineering for debugging such modifications, if you also do each of
the following:

   a) Give prominent notice with each copy of the Combined Work that
   the Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the Combined Work with a copy of the GNU GPL and this license
   document.

   c) For a Combined Work that displays copyright notices during
   execution, include the copyright notice for the Library among
   these notices, as well as a reference directing the user to the
   copies of the GNU GPL and this license document.

   d) Do one of the following:

       0) Convey the Minimal Corresponding Source under the terms of this
       License, and the Corresponding Application Code in a form
       suitable for, and under terms that permit, the user to
       recombine or relink the Application with a modified version of
       the Linked Version to produce a modified Combined Work, in the
       manner specified by section 6 of the GNU GPL for conveying
       Corresponding Source.

       1) Use a suitable shared library mechanism for linking with the
       Library.  A suitable mechanism is one that (a) uses at run time
       a copy of the Library already present on the user's computer
       system, and (b) will operate properly with a modified version
       of the Library that is interface-compatible with the Linked
       Version.

   e) Provide Installation Information, but only if you would otherwise
   be required to provide such information under section 6 of the
   GNU GPL, and only to the extent that such information is
   necessary to install and execute a modified version of the
   Combined Work produced by recombining or relinking the
   Application with a modified version of the Linked Version. (If
   you use option 4d0, the Installation Information must accompany
   the Minimal Corresponding Source and Corresponding Application
   Code. If you use option 4d1, you must provide the Installation
   Information in the manner specified by section 6 of the GNU GPL
   for conveying Corresponding Source.)

  5. Combined Libraries.

  You may place library facilities that are a work based on the
Library side by side in a single library together with other library
facilities that are not Applications and are not covered by this
License, and convey such a combined library under terms of your
choice, if you do both of the following:

   a) Accompany the combined library with a copy of the same work based
   on the Library, uncombined with any other library facilities,
   conveyed under the terms of this License.

   b) Give prominent notice with the combined library that part of it
   is a work based on the Library, and explaining where to find the
   accompanying uncombined form of the same work.

  6. Revised Versions of the GNU Lesser General Public License.

  The Free Software Foundation may publish revised and/or new versions
of the GNU Lesser General Public License from time to time. Such new
versions will be similar in spirit to the present version, but may
differ in detail to address new problems or concerns.

  Each version is given a distinguishing version number. If the
Library as you received it specifies that a certain numbered version
of the GNU Lesser General Public License "or any later version"
applies to it, you have the option of following the terms and
conditions either of that published version or of any later version
published by the Free Software Foundation. If the Library as you
received it does not specify a version number of the GNU Lesser
General Public License, you may choose any version of the GNU Lesser
General Public License ever published by the Free Software Foundation.

  If the Library as you received it specifies that a proxy can decide
whether future versions of the GNU Lesser General Public License shall
apply, that proxy's public statement of acceptance of any version is
permanent authorization for you to choose that version for the
Library.



--- File: docker/Dockerfile.core ---
ARG CUDA_VERSION
ARG DEEPLABCUT_VERSION
FROM deeplabcut/deeplabcut:${DEEPLABCUT_VERSION}-base-cuda${CUDA_VERSION}-latest

ENV DLClight True

COPY motd.sh /home/motd.sh
RUN echo "source /home/motd.sh" >> /etc/profile


--- File: docker/Makefile ---
clean:
	rm -rf dist/
	rm -f PYPI_README.md

prepare_build:
	python3 -m pip install --upgrade twine build
	cp ../docs/docker.md PYPI_README.md

build: clean prepare_build  
	python3 -m build

upload_test: prepare_build
	python3 -m twine upload --repository testpypi dist/*

upload: prepare_build
	python3 -m twine upload dist/*


--- File: docker/deeplabcut_docker.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepLabCut2.0-2.2 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""

import argparse
import os
import pty
import sys

__version__ = "0.0.10-alpha"

_MOTD = r"""
                    .--,       .--,
                    ( (  \.---./  ) )
                     '.__/o   o\__.'
                       `{=  ^  =}´
                         >  u  <
 ____________________.""`-------`"".______________________  
\   ___                   __         __   _____       __  /
/  / _ \ ___  ___  ___   / /  ___ _ / /  / ___/__ __ / /_ \
\ / // // -_)/ -_)/ _ \ / /__/ _ `// _ \/ /__ / // // __/ /
//____/ \__/ \__// .__//____/\_,_//_.__/\___/ \_,_/ \__/  \
\_________________________________________________________/
                       ___)( )(___ `-.___. 
                      (((__) (__)))      ~`

Welcome to DeepLabCut docker!
"""


def _parse_args():
    parser = argparse.ArgumentParser(
        "deeplabcut-docker",
        description=(
            "Utility tool for launching DeepLabCut docker containers. "
            "Only a single argument is given to specify the container type. "
            "By default, the current directory is mounted into the container "
            "and used as the current working directory. You can additionally "
            "specify any additional docker argument specified in "
            "https://docs.docker.com/engine/reference/commandline/cli/."
        ),
    )
    parser.add_argument(
        "container",
        type=str,
        choices=["gui", "notebook", "bash"],
        help=(
            "The container to launch. A list of all containers is available on "
            "https://hub.docker.com/r/deeplabcut/deeplabcut/tags. By default, the "
            "latest DLC version will be selected and automatically updated, if "
            "possible. All containers are currently launched in interactive mode "
            "by default, meaning you can use Ctrl+C in your terminal session to "
            "terminate a command."
        ),
    )
    return parser.parse_known_args()


def main():
    """Main entry point. Parse arguments and launch container."""
    launch_args, docker_arguments = _parse_args()
    argv = ["deeplabcut_docker.sh", launch_args.container, *docker_arguments]
    print(_MOTD, file=sys.stderr)
    pty.spawn(argv)
    print("Container stopped.", file=sys.stderr)


if __name__ == "__main__":
    main()


--- File: docker/pyproject.toml ---
[build-system]
requires = ["setuptools", "wheel"]
build-backend = "setuptools.build_meta"

--- File: docker/MANIFEST.in ---
include pyproject.toml
include PYPI_README.md
include LICENSE
include deeplabcut_docker.sh


--- File: docker/README.md ---
# DeepLabCut Dockerfiles

**Note that this README is mainly intended for DeepLabCut developers. The main documentation contains its own user documentation on the provided docker images.**

This repo contains build routines for the following official DeepLabCut docker images:
- `deeplabcut/deeplabcut:base`: Base image with TF2.5, cuDNN8 and DLC
- `deeplabcut/deeplabcut:latest-core`: DLC in light mode
- `deeplabcut/deeplabcut:latest-gui`: DLC in GUI mode
- `deeplabcut/deeplabcut:latest-gui-jupyter`: DLC in GUI mode, with jupyter installed

All images are based on Python 3.8.
The images are synced to DockerHub: https://hub.docker.com/r/deeplabcut/deeplabcut

## Quickstart

You can use the images fully standalone, without the need of cloning the DeepLabCut repo.
A helper package called `deeplabcut-docker` is available on PyPI and can be installed by running

``` bash
pip install deeplabcut-docker
```

*Note: Advanced users can also directly download and use the `deeplabcut-docker.sh` script if this is preferred over a python helper script.*

We provide docker containers for three different use cases outlined below.

In all cases, your current directory will be mounted in the container, and the container
will be started with your current username and group.

- To launch the DLC GUI directly, run
  ```bash
  deeplabcut-docker gui
  ```
- Interactive console with DLC in light mode
  ```bash
  deeplabcut-docker bash
  ```
- A Jupyter notebook server can be launched with
  ```bash
  deeplabcut-docker notebook
  ```

## For developers

Make sure your docker daemon is running and navigate to the repository root directory.
You can build the images by running

```
docker/build.sh build
```

Note that this assumes that you have rights to execute `docker build` and `docker run` commands which requires either `sudo` access or membership in the `docker` group on your local machine. If you are not in the `docker` group, run the script with the environment variable `DOCKER="sudo docker"` set to override the default docker command.

Images can be verified by running

```
docker/build.sh test
``` 

Built images can be pushed to DockerHub by running

```
docker/build.sh push
``` 

## Prerequisites (if you don't have Docker installed already)

**(1)** Install Docker. See https://docs.docker.com/install/ & for Ubuntu: https://docs.docker.com/install/linux/docker-ce/ubuntu/
Test docker: 

    $ sudo docker run hello-world
    
 The output should be: ``Hello from Docker! This message shows that your installation appears to be working correctly.``

*if you get the error ``docker: Error response from daemon: Unknown runtime specified nvidia.`` just simply restart docker: 
  
       $ sudo systemctl daemon-reload
       $ sudo systemctl restart docker

    
**(2)** Add your user to the docker group (https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user)
Quick guide  to create the docker group and add your user: 
Create the docker group.

    $ sudo groupadd docker
Add your user to the docker group.

    $ sudo usermod -aG docker $USER

(perhaps restart your computer (best) or (at min) open a new terminal to make sure that you are added from now on)

## Acknowledgements

Ascii art in the MOTD is adapted from https://ascii.co.uk/art/mice and https://patorjk.com/software/taag/#p=display&f=Small%20Slant&t=DeepLabCut.

```
                    .--,       .--,
                    ( (  \.---./  ) )
                     '.__/o   o\__.'
                       `{=  ^  =}´
                         >  u  <
 ____________________.""`-------`"".______________________  
\   ___                   __         __   _____       __  /
/  / _ \ ___  ___  ___   / /  ___ _ / /  / ___/__ __ / /_ \
\ / // // -_)/ -_)/ _ \ / /__/ _ `// _ \/ /__ / // // __/ /
//____/ \__/ \__// .__//____/\_,_//_.__/\___/ \_,_/ \__/  \
\_________________________________________________________/
                       ___)( )(___ `-.___. 
                      (((__) (__)))      ~`
```


--- File: docker/build.sh ---
#!/bin/bash
# Build script for deeplabcut docker images.
# > docker/build.sh [build|test|push]

set -e

# Set default Docker binary
export DOCKER=${DOCKER:-'docker'}
export DOCKER_BUILD="$DOCKER build"
export BASENAME=deeplabcut/deeplabcut
export DOCKERDIR=docker

# Check if script is being run from the correct directory
if [[ ! -d ./${DOCKERDIR} ]]; then
    echo >&2 Run from repository root. Current pwd is
    pwd >&2
    exit 1
fi

# List Docker images related to DeepLabCut
list_images() {
    $DOCKER images |
        grep '^deeplabcut ' |
        sed -s 's/\s\s\+/\t/g' |
        cut -f1,2 -d$'\t' --output-delimiter ':' |
        grep core
}

# Run tests inside Docker containers
run_test() {
    kwargs=(
        -u $(id -u) --tmpfs /.local --tmpfs /.cache
        --tmpfs /test/.pytest_cache
        --env DLClight=True -t
        $1
    )

    # Unit tests
    $DOCKER run ${kwargs[@]} python3 -m pytest -v tests || return 255

    # Functional tests
    $DOCKER run ${kwargs[@]} python3 testscript_cli.py || return 255

    return 0
}
export -f run_test

# Iterate through build matrix and perform actions
iterate_build_matrix() {
    ## TODO(stes): Consider adding legacy versions for CUDA
    ## if there is demand from users:

    #[add other dlc versions to build here]
    dlc_versions=(
        "2.3.5"
        #"2.3.2"
    )

    #[add other cuda versions to build here]
    cuda_versions=(
        #"11.4.3-cudnn8-runtime-ubuntu20.04"
        "11.7.1-cudnn8-runtime-ubuntu20.04"
    )

    docker_types=(
        "base"
        "test"
        "core"
        #"gui"
    )

    mode=${1:-build}
    for cuda_version in \
        ${cuda_versions[@]}; do
        for deeplabcut_version in \
            ${dlc_versions[@]}; do
            for stage in \
                ${docker_types[@]}; do
                tag=${deeplabcut_version}-${stage}-cuda${cuda_version}-latest
                case "$mode" in
                build)
                    echo \
                        --build-arg=CUDA_VERSION=${cuda_version} \
                        --build-arg=DEEPLABCUT_VERSION=${deeplabcut_version} \
                        "--tag=${BASENAME}:$tag" \
                        -f "Dockerfile.${stage}" \.
                    # --no-cache \
                    ;;
                push | clean | test)
                    echo ${BASENAME}:${tag}
                    ;;
                esac
            done
        done
    done
}

# Get Git hash
githash() {
    git log -1 --pretty=format:"%h"
}

# Create logs directory and set log file name
mkdir -p logs
logfile=logs/$(date +%y%m%d-%H%M%S)-$(githash)
echo "Logging to $logdir.*"

if [ $# -eq 0 ]; then
    echo "Help: Provide arguments to this script."
    echo "Usage: $0 [build|test|push]"
    exit 1
fi

# Iterate through command line arguments
for arg in "$@"; do
    case $1 in
    clean)
        iterate_build_matrix clean |
            tr '\n' '\0' |
            xargs -I@ -0 bash -c "docker image rm @ |& grep -v 'No such image'"
        ;;
    build)
        echo "DeepLabCut docker build:: $(git log -1 --oneline)"
        cp -r examples ${DOCKERDIR}
        (
            cd ${DOCKERDIR}
            iterate_build_matrix |
                tr '\n' '\0' |
                xargs -I@ -0 bash -c "echo Building @; $DOCKER build @ || exit 255"
            echo Successful build.
        ) |& tee ${logfile}.build
        ;;
    test)
        (
            echo "DeepLabCut docker build:: $(git log -1 --oneline)"
            iterate_build_matrix test |
                grep '\-test\-' |
                tr '\n' '\0' |
                xargs -0 -I@ bash -c "run_test @ || exit 255"
            echo Successful test.
        ) |& tee ${logfile}.test
        ;;
    push)
        iterate_build_matrix push |
            grep -v '\-test\-' |
            tr '\n' '\0' |
            xargs -I@ -0 bash -c "echo docker push @; \
				docker push @; \
				docker image rm @ |& grep -v 'No such image'"
        ;;
    *)
        echo "Usage: $0 [build|test|push]"
        exit 1
        ;;
    esac
done


--- File: docker/Dockerfile.gui ---
# NOTE: This dockerfile is currently not included in the build process
# It is still left for reference, but currently untested.
ARG CUDA_VERSION
ARG DEEPLABCUT_VERSION

FROM deeplabcut/deeplabcut:${DEEPLABCUT_VERSION}-base-cuda${CUDA_VERSION}-latest

RUN DEBIAN_FRONTEND=noninteractive apt-get update -yy \ 
    && apt-get install -yy --no-install-recommends libgtk-3-dev python3-wxgtk4.0 locales \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* \
    && locale-gen en_US.UTF-8 en_GB.UTF-8

ARG DEEPLABCUT_VERSION
RUN pip3 install --no-cache-dir --upgrade deeplabcut[gui]==${DEEPLABCUT_VERSION} \
 && pip3 list

ENV DLClight=False
CMD ["python3", "-m", "deeplabcut"]


--- File: docker/.gitignore ---
examples/
PYPI_README.md


--- File: docker/setup.cfg ---
[metadata]
name = deeplabcut-docker
version = attr: deeplabcut_docker.__version__
author = A. & M. Mathis Labs
author_email = alexander@deeplabcut.org
maintainer = Steffen Schneider
maintainer_email = stes@hey.com
description = A helper package to launch DeepLabCut docker images
url = https://github.com/DeepLabCut/DeepLabCut/tree/main/docker
project_urls =
    Bug Tracker = https://github.com/DeepLabCut/DeepLabCut/issues 
classifiers =
    Programming Language :: Python :: 3
    License :: OSI Approved :: GNU Lesser General Public License v3 (LGPLv3)
    Operating System :: MacOS
    Operating System :: POSIX :: Linux
    Topic :: Utilities
license = LGPLv3
long_description = file: PYPI_README.md
long_description_content_type = text/markdown
platform = any

[options]
package_dir =
    = . 
py_modules = deeplabcut_docker
python_requires = >=3.6
include_package_data = True

[options.entry_points]
console_scripts =
    deeplabcut-docker = deeplabcut_docker:main

[options.packages.find]
where = .

[options.data_files]
bin = deeplabcut_docker.sh

[bdist_wheel]
universal = 1


--- File: docker/deeplabcut_docker.sh ---
#!/bin/bash
#
# Helper script for launching deeplabcut docker UI containers
# Usage:
#   $ ./deeplabcut-docker.sh [gui|notebook|bash]

DOCKER=${DOCKER:-docker}
DLC_VERSION=${DLC_VERSION:-"latest"}
DLC_NOTEBOOK_PORT=${DLC_NOTEBOOK_PORT:-8888}

# Check if the current users has privileges to start
# a docker container.
check_system() {
    if [[ $(uname -s) == Linux ]]; then
        if [ $(groups | grep -c docker) -eq 0 ]; then
            if [[ "$DOCKER" == "sudo docker" ]]; then
                return 0
            fi
            err "The current user $(id -u) is not                      "
            err "part of the \"docker\" group.                         "
            err "Please either:                                        "
            err " 1) Launch this script with the DOCKER environment    "
            err "    variable set to DOCKER=\"sudo docker\" (use this  "
            err "    with care)!                                       "
            err " 2) Add your user to the docker group. You might need "
            err "    to log in and out again to see the effect of the  "
            err "    change.                                           "
            exit 1
        fi
    elif [[ $(uname -s) == Darwin ]]; then
        err "Please note that macOSX support is currently experimental"
        err "If you encounter errors, please open an issue on"
        err "https://github.com/DeepLabCut/DeepLabCut/issues"
        err "Thanks for testing the package!"
    fi
}

# Select docker parameters based on the system.
# Display variable and bind paths slightly differ
# between macOS and Linux. Further systems should
# be added here.
get_x11_args() {
    if [[ $(uname -s) == Linux ]]; then
        err "Using Linux config"
        args=(
            "-e DISPLAY=unix$DISPLAY"
            "-v /tmp/.X11-unix:/tmp/.X11-unix"
            "-v $XAUTHORITY:/home/developer/.Xauthority"
        )
    elif [[ $(uname -s) == Darwin ]]; then
        err "Using OSX config"
        # TODO(stes) This is most likely not robust for all users;
        #            We need to replace "en0" by some dynamic way
        #            of figuring out the active network interface.
        #            Even better would be to use 127.0.0.1, if this
        #            is possible with the correct external config
        ip=$(ifconfig en0 | grep inet | awk '$1=="inet" {print $2}')
        display_id=$(echo $DISPLAY | sed -e 's/.*\(:[0-9]\)/\1/')
        args=(
            "-e DISPLAY=${ip}${display_id}"
        )
    else
        err "Unknown operating system:"
        err "$(uname -s)"
        err "Please open an issue on "
        err "https://github.com/DeepLabCut/DeepLabCut/issues"
        err "And attach your full console output."
        exit 1
    fi
    echo "${args[@]}"
}

get_mount_args() {
    args=(
        "-v $(pwd):/app -w /app"
    )
    echo "${args[@]}"
}

get_container_name() {
    echo deeplabcut/deeplabcut:${DLC_VERSION}-$1
}

get_local_container_name() {
    echo deeplabcut-${DLC_VERSION}-$1
}

### Start of helper functions ###

# Print error messages to stderr
# Ref. https://google.github.io/styleguide/shellguide.html#stdout-vs-stderr
err() {
    echo "[$(date +'%Y-%m-%dT%H:%M:%S%z')]: $*" >&2
}

# Update the docker container
update() {
    $DOCKER pull $(get_container_name $1)
}

# Build the docker container
# Usage: build [core|gui|gui-jupyter]
build() {
    tag=$1
    _build $(get_container_name $tag) $(get_local_container_name $tag) || exit 1
}

_build() {
    remote_name=$1
    local_name=$2

    uname=$(id -un)
    uid=$(id -u)
    gname=$(id -gn)
    gid=$(id -g)

    err "Configuring a local container for user $uname ($uid) in group $gname ($gid)"
    $DOCKER build -q -t ${local_name} - <<EOF
    from ${remote_name}

    # Create same user as on the host system
    run mkdir -p /home
    run mkdir -p /app
    run groupadd -g $gid $gname || groupmod -o -g $gid $gname
    run useradd -d /home -s /bin/bash -u $uid -g $gid $uname
    run chown -R $uname:$gname /home
    run chown -R $uname:$gname /app

    # Switch to the local user from now on
    user $uname
EOF
    if [ $? -ne 0 ]; then
        err Build failed.
        exit 1
    fi
    err Build succeeded
}

### Start of CLI functions ###

# Launch the UI version of DeepLabCut
gui() {
    extra_args="$@"
    update gui || exit 1
    build gui || exit 1
    args="$(get_x11_args) $(get_mount_args) ${extra_args}"
    $DOCKER run -it --rm ${args} $(get_local_container_name gui) ||
        err "Failed to launch the DLC GUI. Used args: \"${args}\""
}

# Launch a Jupyter Server in the current directory
notebook() {
    extra_args="$@"
    update gui-jupyter || exit 1
    build gui-jupyter || exit 1
    args="$(get_x11_args) $(get_mount_args) ${extra_args} -v /app/examples"
    err "Starting the notebook server."
    err "Open your browser at"
    err "http://127.0.0.1:${DLC_NOTEBOOK_PORT}"
    err "If prompted for a password, enter 'deeplabcut'."
    $DOCKER run -p 127.0.0.1:${DLC_NOTEBOOK_PORT}:8888 -it --rm ${args} $(get_local_container_name gui-jupyter) ||
        err "Failed to launch the notebook server. Used args: \"${args}\""
}

# Launch the command line, using DLC in light mode
bash() {
    extra_args="$@"
    update core || exit 1
    build core || exit 1
    args="$(get_mount_args) ${extra_args}"
    $DOCKER run -it $args $(get_local_container_name core) bash
}

# Launch a custom docker image (for developers)
# Takes a local image name as the first argument.
custom() {
    image=$1
    shift 1
    extra_args="$@"
    _build $image "${image}-custom" || exit 1
    args="$(get_mount_args) ${extra_args}"
    $DOCKER run -it $args ${image}-custom bash
}

check_system
subcommand=${1:-gui}
shift 1
case "${subcommand}" in
gui) gui "$@" ;;
notebook) notebook "$@" ;;
bash) bash "$@" ;;
custom) custom "$@" ;;
*)
    echo "Usage"
    echo "$0 [gui|notebook|bash|help]"
    ;;
esac


--- File: docker/Dockerfile.base ---
ARG CUDA_VERSION
FROM nvidia/cuda:${CUDA_VERSION}

ARG DEEPLABCUT_VERSION
ENV DEBIAN_FRONTEND=noninteractive

ARG PYTHON_VERSION=3.9
RUN apt-get update -yy && \
    apt-get install -yy --no-install-recommends python${PYTHON_VERSION} python3-pip ffmpeg libsm6 libxext6 && \
    ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python3 && \
    ln -s -f /usr/bin/python${PYTHON_VERSION} /usr/bin/python && \
    ln -s -f /usr/bin/pip3 /usr/bin/pip && \
    rm -rf /var/lib/apt/lists/* && \
    apt-get clean

RUN pip3 install --upgrade \
		deeplabcut==${DEEPLABCUT_VERSION} \
		numpy==1.24.0 \
		decorator==4.4.2 \ 
		tensorflow==2.10 \
		torch==1.12 \
	&& pip3 list

# The installed tensorflow version will not work with the latest protocol buffer version,
# hence we are fixing the version to 3.20.
# See https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates
# for details on why this is needed. (re: Aug 21, 2023: retested, still required)
RUN pip3 install protobuf==3.20.1

# TODO required to fix permission errors when running the container with limited permission.
RUN chmod a+rwx -R /usr/local/lib/python${PYTHON_VERSION}/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained

ENV CUDA_VERSION=${CUDA_VERSION}
ENV DEEPLABCUT_VERSION=${DEEPLABCUT_VERSION}


--- File: docker/Dockerfile.jupyter ---
ARG CUDA_VERSION
ARG DEEPLABCUT_VERSION
FROM deeplabcut/deeplabcut:${DEEPLABCUT_VERSION}-core-cuda${CUDA_VERSION}-latest

RUN pip3 install --no-cache-dir \
        notebook==6.4.12 \
        && pip3 list

ENV PYTHONPATH "${PYTHONPATH}:/usr/lib/python3"

ARG USER=docker_user
RUN useradd -m ${USER} \
    && cp /root/.bashrc /home/${USER}/ \
    && mkdir /app /data /codebase \
    && chown -R --from=root ${USER} /home/${SUSER} \
                                /app /data /codebase
ENV HOME /home/${USER}
WORKDIR ${HOME}
USER ${USER}

RUN jupyter notebook --generate-config
EXPOSE 8888
ENTRYPOINT ["jupyter", "notebook", "--no-browser",  "--ip", "0.0.0.0"]


--- File: tests/test_auxfun_models.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


from pathlib import Path
from tempfile import TemporaryDirectory
import unittest
from unittest.mock import patch

from deeplabcut.utils.auxfun_models import MODELTYPE_FILEPATH_MAP, check_for_weights


class CheckForWeightsTestCase(unittest.TestCase):
    def test_filepaths_for_modeltypes(self):
        with TemporaryDirectory() as tmpdir:
            with patch(
                "deeplabcut.utils.auxfun_models.download_weights"
            ) as mocked_download:
                for modeltype, expected_path in MODELTYPE_FILEPATH_MAP.items():
                    actual_path = check_for_weights(modeltype, Path(tmpdir))
                self.assertIn(str(expected_path), actual_path)
                if "efficientnet" in modeltype:
                    mocked_download.assert_called_with(
                        modeltype, tmpdir / expected_path.parent
                    )
                else:
                    mocked_download.assert_called_with(
                        modeltype, tmpdir / expected_path
                    )

    def test_bad_modeltype(self):
        actual_path = check_for_weights("dummymodel", "nonexistentpath")
        self.assertEqual(actual_path, "nonexistentpath")


--- File: tests/conftest.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import os
import pickle
import pytest
import shutil
import urllib.request
import zipfile
from deeplabcut.core import inferenceutils
from io import BytesIO
from PIL import Image
from tqdm import tqdm


TEST_DATA_DIR = os.path.join(os.path.dirname(os.path.realpath(__file__)), "data")


def unzip_from_url(url, dest_folder):
    """Directly extract files without writing the archive to disk."""
    os.makedirs(dest_folder, exist_ok=True)
    resp = urllib.request.urlopen(url)
    with zipfile.ZipFile(BytesIO(resp.read())) as zf:
        for member in tqdm(zf.infolist(), desc="Extracting"):
            try:
                zf.extract(member, path=dest_folder)
            except zipfile.error:
                pass


def pytest_sessionstart(session):
    unzip_from_url(
        "https://github.com/DeepLabCut/UnitTestData/raw/main/data.zip",
        os.path.split(TEST_DATA_DIR)[0],
    )
    session.__DATA_FOLDER = TEST_DATA_DIR


def pytest_sessionfinish(session, exitstatus):
    shutil.rmtree(session.__DATA_FOLDER)


@pytest.fixture(scope="function")
def ground_truth_detections():
    with open(os.path.join(TEST_DATA_DIR, "dets.pickle"), "rb") as file:
        return pickle.load(file)


@pytest.fixture(scope="function")
def model_outputs():
    with open(os.path.join(TEST_DATA_DIR, "outputs.pickle"), "rb") as file:
        scmaps, locrefs, pafs = pickle.load(file)
    locrefs = np.reshape(locrefs, (*locrefs.shape[:3], -1, 2))
    locrefs *= 7.2801
    pafs = np.reshape(pafs, (*pafs.shape[:3], -1, 2))
    return scmaps, locrefs, pafs


@pytest.fixture(scope="function")
def sample_image():
    return np.asarray(Image.open(os.path.join(TEST_DATA_DIR, "image.png")))


@pytest.fixture(scope="function")
def sample_keypoints():
    with open(os.path.join(TEST_DATA_DIR, "trimouse_assemblies.pickle"), "rb") as file:
        temp = pickle.load(file)
    return np.concatenate(temp[0])[:, :2]


@pytest.fixture(scope="function")
def real_assemblies():
    with open(os.path.join(TEST_DATA_DIR, "trimouse_assemblies.pickle"), "rb") as file:
        temp = pickle.load(file)
    data = np.stack(list(temp.values()))
    return inferenceutils._parse_ground_truth_data(data)


@pytest.fixture(scope="function")
def real_assemblies_montblanc():
    with open(os.path.join(TEST_DATA_DIR, "montblanc_assemblies.pickle"), "rb") as file:
        temp = pickle.load(file)
    single = temp.pop("single")
    data = np.full((max(temp) + 1, 3, 4, 4), np.nan)
    for k, assemblies in temp.items():
        for i, assembly in enumerate(assemblies):
            data[k, i] = assembly
    return inferenceutils._parse_ground_truth_data(data), single


@pytest.fixture(scope="function")
def real_tracklets():
    with open(os.path.join(TEST_DATA_DIR, "trimouse_tracklets.pickle"), "rb") as file:
        return pickle.load(file)


@pytest.fixture(scope="function")
def real_tracklets_montblanc():
    with open(os.path.join(TEST_DATA_DIR, "montblanc_tracklets.pickle"), "rb") as file:
        return pickle.load(file)


@pytest.fixture(scope="function")
def evaluation_data_and_metadata():
    full_data_file = os.path.join(TEST_DATA_DIR, "trimouse_eval.pickle")
    metadata_file = full_data_file.replace("eval", "meta")
    with open(full_data_file, "rb") as file:
        data = pickle.load(file)
    with open(metadata_file, "rb") as file:
        metadata = pickle.load(file)
    return data, metadata


@pytest.fixture(scope="function")
def evaluation_data_and_metadata_montblanc():
    full_data_file = os.path.join(TEST_DATA_DIR, "montblanc_eval.pickle")
    metadata_file = full_data_file.replace("eval", "meta")
    with open(full_data_file, "rb") as file:
        data = pickle.load(file)
    with open(metadata_file, "rb") as file:
        metadata = pickle.load(file)
    return data, metadata


--- File: tests/test_dataset_augmentation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import imgaug.augmenters as iaa
import numpy as np
import pytest
from deeplabcut.pose_estimation_tensorflow.datasets import augmentation


@pytest.mark.parametrize(
    "width, height",
    [
        (200, 200),
        (300, 300),
        (400, 400),
    ],
)
def test_keypoint_aware_cropping(
    sample_image,
    sample_keypoints,
    width,
    height,
):
    aug = augmentation.KeypointAwareCropToFixedSize(width=width, height=height)
    images_aug, keypoints_aug = aug(
        images=[sample_image],
        keypoints=[sample_keypoints],
    )
    assert len(images_aug) == len(keypoints_aug) == 1
    assert all(im.shape[:2] == (height, width) for im in images_aug)
    # Ensure at least a keypoint is visible in each crop
    assert all(len(kpts) for kpts in keypoints_aug)

    # Test passing in a batch of frames
    n_samples = 8
    images_aug, keypoints_aug = aug(
        images=[sample_image] * n_samples,
        keypoints=[sample_keypoints] * n_samples,
    )
    assert len(images_aug) == len(keypoints_aug) == n_samples


@pytest.mark.parametrize(
    "width, height",
    [
        (200, 200),
        (300, 300),
        (400, 400),
    ],
)
def test_sequential(
    sample_image,
    sample_keypoints,
    width,
    height,
):
    # Guarantee that images smaller than crop size are handled fine
    very_small_image = sample_image[:50, :50]
    aug = iaa.Sequential(
        [
            iaa.PadToFixedSize(width, height),
            augmentation.KeypointAwareCropToFixedSize(width, height),
        ]
    )
    images_aug, keypoints_aug = aug(
        images=[very_small_image],
        keypoints=[sample_keypoints],
    )
    assert len(images_aug) == len(keypoints_aug) == 1
    assert all(im.shape[:2] == (height, width) for im in images_aug)
    # Ensure at least a keypoint is visible in each crop
    assert all(len(kpts) for kpts in keypoints_aug)

    # Test passing in a batch of frames
    n_samples = 8
    images_aug, keypoints_aug = aug(
        images=[very_small_image] * n_samples,
        keypoints=[sample_keypoints] * n_samples,
    )
    assert len(images_aug) == len(keypoints_aug) == n_samples


def test_keypoint_horizontal_flip(
    sample_image,
    sample_keypoints,
):
    keypoints_flipped = sample_keypoints.copy()
    keypoints_flipped[:, 0] = sample_image.shape[1] - keypoints_flipped[:, 0]
    pairs = [(0, 1), (2, 3), (4, 5), (6, 7), (8, 9), (10, 11)]
    aug = augmentation.KeypointFliplr(
        keypoints=list(map(str, range(12))),
        symmetric_pairs=pairs,
    )
    keypoints_aug = aug(
        images=[sample_image],
        keypoints=[sample_keypoints],
    )[
        1
    ][0]
    temp = keypoints_aug.reshape((3, 12, 2))
    for pair in pairs:
        temp[:, pair] = temp[:, pair[::-1]]
    keypoints_unaug = temp.reshape((-1, 2))
    np.testing.assert_allclose(keypoints_unaug, keypoints_flipped)


def test_keypoint_horizontal_flip_with_nans(
    sample_image,
    sample_keypoints,
):
    sample_keypoints[::12] = np.nan
    sample_keypoints[2::12] = np.nan
    keypoints_flipped = sample_keypoints.copy()
    keypoints_flipped[:, 0] = sample_image.shape[1] - keypoints_flipped[:, 0]
    pairs = [(0, 1), (2, 3)]
    aug = augmentation.KeypointFliplr(
        keypoints=list(map(str, range(12))),
        symmetric_pairs=pairs,
    )
    keypoints_aug = aug(
        images=[sample_image],
        keypoints=[sample_keypoints],
    )[
        1
    ][0]
    temp = keypoints_aug.reshape((3, 12, 2))
    for pair in pairs:
        temp[:, pair] = temp[:, pair[::-1]]
    keypoints_unaug = temp.reshape((-1, 2))
    np.testing.assert_allclose(keypoints_unaug, keypoints_flipped)


--- File: tests/test_frame_selection_tools.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
""" Tests for frame selection tools """
import math
from unittest.mock import Mock
import pytest
import deeplabcut.utils.frameselectiontools as fst


@pytest.mark.parametrize(
    "fps, duration, n_to_pick, start, end, index",
    [
        (32, 10, 10, 0, 1, None),
        (16, 100, 50, 0, 1, list(range(100, 500, 5))),
        (16, 100, 5, 0.25, 0.3, list(range(100, 500, 5))),
    ],
)
def test_uniform_frames(fps, duration, n_to_pick, start, end, index):
    start_idx = int(math.floor(start * duration * fps))
    end_idx = int(math.ceil(end * duration * fps))
    if index is None:
        valid_indices = list(range(start_idx, end_idx))
    else:
        valid_indices = [idx for idx in index if start_idx <= idx <= end_idx]

    clip = Mock()
    clip.fps = fps
    clip.duration = duration
    frames = fst.UniformFrames(clip, n_to_pick, start, end, index)
    print(f"FPS: {fps}")
    print(f"Duration: {duration}")
    print(f"Selected Frames: {frames}")
    print(f"Valid Indices: {valid_indices}")

    # Check that we get the number of frames we asked for
    assert len(frames) == n_to_pick, f"Wrong nb. of frames: {n_to_pick}!={len(frames)}"
    # Check that all indices are valid
    for index in frames:
        assert index in valid_indices, f"Invalid index: {index} not in {valid_indices}"
    # Check that all frames are unique
    assert len(set(frames)) == len(frames), "Duplicate indices found"


@pytest.mark.parametrize(
    "fps, nframes, n_to_pick, start, end, index",
    [
        (32, 320, 10, 0, 1, None),
        (16, 1600, 50, 0, 1, list(range(100, 500, 5))),
        (16, 1600, 5, 0.25, 0.3, list(range(100, 500, 5))),
    ],
)
def test_uniform_frames_cv2(fps, nframes, n_to_pick, start, end, index):
    start_idx = int(math.floor(start * nframes))
    end_idx = int(math.ceil(end * nframes))
    if index is None:
        valid_indices = list(range(start_idx, end_idx))
    else:
        valid_indices = [idx for idx in index if start_idx <= idx <= end_idx]

    cap = Mock()
    cap.fps = fps
    cap.__len__ = Mock(return_value=nframes)
    frames = fst.UniformFramescv2(cap, n_to_pick, start, end, index)
    print(f"FPS: {fps}")
    print(f"Nframes: {nframes}")
    print(f"Selected Frames: {frames}")
    print(f"Valid Indices: {valid_indices}")

    # Check that we get the number of frames we asked for
    assert len(frames) == n_to_pick, f"Wrong nb. of frames: {n_to_pick}!={len(frames)}"
    # Check that all indices are valid
    for index in frames:
        assert index in valid_indices, f"Invalid index: {index} not in {valid_indices}"
    # Check that all frames are unique
    assert len(set(frames)) == len(frames), "Duplicate indices found"


--- File: tests/test_video.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import pytest
from conftest import TEST_DATA_DIR
from deeplabcut.utils.auxfun_videos import VideoWriter


POS_FRAMES = 1  # Equivalent to cv2.CAP_PROP_POS_FRAMES


@pytest.fixture()
def video_clip():
    return VideoWriter(os.path.join(TEST_DATA_DIR, "vid.avi"))


def test_reader_wrong_inputs(tmp_path):
    with pytest.raises(ValueError):
        VideoWriter(str(tmp_path))
    fake_vid = tmp_path / "fake.avi"
    fake_vid.write_bytes(b"42")
    with pytest.raises(IOError):
        VideoWriter(str(fake_vid))


def test_reader_check_integrity(video_clip):
    video_clip.check_integrity()
    log_file = os.path.join(video_clip.directory, f"{video_clip.name}.log")
    assert os.path.getsize(log_file) == 0


def test_reader_video_path(video_clip):
    assert video_clip.name == "vid"
    assert video_clip.format == ".avi"
    assert video_clip.directory == TEST_DATA_DIR


def test_reader_metadata(video_clip):
    metadata = video_clip.metadata
    assert metadata["n_frames"] == video_clip.get_n_frames(True) == 256
    assert metadata["fps"] == 30
    assert metadata["width"] == 416
    assert metadata["height"] == 374


def test_reader_wrong_fps(video_clip):
    with pytest.raises(ValueError):
        video_clip.fps = 0


def test_reader_duration(video_clip):
    assert video_clip.calc_duration() == pytest.approx(
        video_clip.calc_duration(robust=False), abs=0.01
    )


def test_reader_set_frame(video_clip):
    with pytest.raises(ValueError):
        video_clip.set_to_frame(-1)
    video_clip.set_to_frame(2)
    assert int(video_clip.video.get(POS_FRAMES)) == 2
    video_clip.set_to_frame(len(video_clip) + 10)
    assert int(video_clip.video.get(POS_FRAMES)) == len(video_clip) - 1
    video_clip.reset()
    assert int(video_clip.video.get(POS_FRAMES)) == 0


@pytest.mark.parametrize("shrink, crop", [(1, False), (1, True), (2, False), (2, True)])
def test_reader_read_frame(video_clip, shrink, crop):
    if crop:
        video_clip.set_bbox(0, 0.5, 0, 0.5, relative=True)
    frame = video_clip.read_frame(shrink, crop)
    height, width, _ = frame.shape
    assert height == video_clip.height // shrink
    assert width == video_clip.width // shrink


def test_writer_bbox(video_clip):
    bbox = 0, 100, 0, 100
    video_clip.set_bbox(*bbox)
    assert video_clip.get_bbox() == bbox
    with pytest.raises(ValueError):
        video_clip.set_bbox(200, 100, 0, 100, relative=False)
    video_clip.set_bbox(0, 1, 0, 1.01, relative=True)
    assert video_clip.get_bbox(relative=True) == (0, 1, 0, 1)


@pytest.mark.parametrize(
    "start, end", [(0, 10), ("0:0", "0:10"), ("00:00:00", "00:00:10")]
)
def test_writer_shorten_invalid_timestamps(video_clip, start, end):
    with pytest.raises(ValueError):
        video_clip.shorten(start, end)


def test_writer_shorten(tmp_path, video_clip):
    file = video_clip.shorten("00:00:00", "00:00:02", dest_folder=str(tmp_path))
    vid = VideoWriter(file)
    assert pytest.approx(vid.calc_duration(), abs=0.1) == 2


def test_writer_split(tmp_path, video_clip):
    with pytest.raises(ValueError):
        video_clip.split(1)
    n_splits = 3
    clips = video_clip.split(n_splits, dest_folder=str(tmp_path))
    assert len(clips) == n_splits
    vid = VideoWriter(clips[0])
    assert pytest.approx(len(vid), abs=1) == len(video_clip) // n_splits


def test_writer_crop(tmp_path, video_clip):
    x1, x2, y1, y2 = 0, 50, 0, 100
    video_clip.set_bbox(x1, x2, y1, y2)
    file = video_clip.crop(dest_folder=str(tmp_path))
    vid = VideoWriter(file)
    assert vid.dimensions == (x2 - x1, y2 - y1)


@pytest.mark.parametrize("target_height", [200, 177])
def test_writer_rescale(tmp_path, video_clip, target_height):
    file = video_clip.rescale(width=-1, height=target_height, dest_folder=str(tmp_path))
    vid = VideoWriter(file)
    assert vid.height == target_height
    # Verify the aspect ratio is preserved
    ar = video_clip.height / target_height
    assert vid.width == pytest.approx(video_clip.width // ar, abs=1)


--- File: tests/test_auxiliaryfunctions.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from pathlib import Path
import pytest
from deeplabcut.utils import auxiliaryfunctions
from deeplabcut.utils.auxfun_videos import SUPPORTED_VIDEOS


def test_find_analyzed_data(tmpdir_factory):
    fake_folder = tmpdir_factory.mktemp("videos")
    SUPPORTED_VIDEOS = ["avi"]
    n_ext = len(SUPPORTED_VIDEOS)

    SCORER = "DLC_dlcrnetms5_multi_mouseApr11shuffle1_5"
    WRONG_SCORER = "DLC_dlcrnetms5_multi_mouseApr11shuffle3_5"

    def _create_fake_file(filename):
        path = str(fake_folder.join(filename))
        with open(path, "w") as f:
            f.write("")
        return path

    for ind, ext in enumerate(SUPPORTED_VIDEOS):
        vname = "video" + str(ind)
        _ = _create_fake_file(vname + "." + ext)
        _ = _create_fake_file(vname + SCORER + ".pickle")
        _ = _create_fake_file(vname + SCORER + ".h5")

    for ind, ext in enumerate(SUPPORTED_VIDEOS):
        # test if existing models are found:
        assert auxiliaryfunctions.find_analyzed_data(
            fake_folder, "video" + str(ind), SCORER
        )

        # Test if nonexisting models are not found
        with pytest.raises(FileNotFoundError):
            auxiliaryfunctions.find_analyzed_data(
                fake_folder, "video" + str(ind), WRONG_SCORER
            )

        with pytest.raises(FileNotFoundError):
            auxiliaryfunctions.find_analyzed_data(
                fake_folder, "video" + str(ind), SCORER, filtered=True
            )


def test_get_list_of_videos(tmpdir_factory):
    fake_folder = tmpdir_factory.mktemp("videos")
    n_ext = len(SUPPORTED_VIDEOS)

    def _create_fake_file(filename):
        path = str(fake_folder.join(filename))
        with open(path, "w") as f:
            f.write("")
        return path

    fake_videos = []
    for ext in SUPPORTED_VIDEOS:
        path = _create_fake_file(f"fake.{ext}")
        fake_videos.append(path)

    # Add some other office files:
    path = _create_fake_file("fake.xls")
    path = _create_fake_file("fake.pptx")

    # Add a .pickle and .h5 files
    _ = _create_fake_file("fake.pickle")
    _ = _create_fake_file("fake.h5")

    # By default, all videos with common extensions are taken from a directory
    videos = auxiliaryfunctions.get_list_of_videos(
        str(fake_folder),
        videotype="",
    )
    assert len(videos) == n_ext

    # A list of extensions can also be passed in
    videos = auxiliaryfunctions.get_list_of_videos(
        str(fake_folder),
        videotype=SUPPORTED_VIDEOS,
    )
    assert len(videos) == n_ext

    for ext in SUPPORTED_VIDEOS:
        videos = auxiliaryfunctions.get_list_of_videos(
            str(fake_folder),
            videotype=ext,
        )
        assert len(videos) == 1

    videos = auxiliaryfunctions.get_list_of_videos(
        str(fake_folder),
        videotype="unknown",
    )
    assert not len(videos)

    videos = auxiliaryfunctions.get_list_of_videos(
        fake_videos,
        videotype="",
    )
    assert len(videos) == n_ext

    for video in fake_videos:
        videos = auxiliaryfunctions.get_list_of_videos([video], videotype="")
        assert len(videos) == 1

    for ext in SUPPORTED_VIDEOS:
        videos = auxiliaryfunctions.get_list_of_videos(
            fake_videos,
            videotype=ext,
        )
        assert len(videos) == 1


def test_write_config_has_skeleton(tmpdir_factory):
    """Required for backward compatibility"""
    fake_folder = tmpdir_factory.mktemp("fakeConfigs")
    fake_config_file = fake_folder / Path("fakeConfig")
    auxiliaryfunctions.write_config(fake_config_file, {})
    config_data = auxiliaryfunctions.read_config(fake_config_file)
    assert "skeleton" in config_data


@pytest.mark.parametrize(
    "multianimal, bodyparts, ma_bpts, unique_bpts, comparison_bpts, expected_bpts",
    [
        (
            False,
            ["head", "shoulders", "knees", "toes"],
            None,
            None,
            {"knees", "others", "toes"},
            ["knees", "toes"],
        ),
        (
            True,
            None,
            ["head", "shoulders", "knees"],
            ["toes"],
            {"knees", "others", "toes"},
            ["knees", "toes"],
        ),
    ],
)
def test_intersection_of_body_parts_and_ones_given_by_user(
    multianimal, bodyparts, ma_bpts, unique_bpts, comparison_bpts, expected_bpts
):
    cfg = {
        "multianimalproject": multianimal,
        "bodyparts": bodyparts,
        "multianimalbodyparts": ma_bpts,
        "uniquebodyparts": unique_bpts,
    }

    if multianimal:
        all_bodyparts = list(set(ma_bpts + unique_bpts))
    else:
        all_bodyparts = bodyparts

    filtered_bpts = (
        auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
            cfg, comparisonbodyparts="all"
        )
    )
    print(all_bodyparts)
    print(filtered_bpts)
    assert len(all_bodyparts) == len(filtered_bpts)
    assert all([bpt in all_bodyparts for bpt in filtered_bpts])

    filtered_bpts = (
        auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
            cfg,
            comparisonbodyparts=comparison_bpts,
        )
    )
    print(filtered_bpts)
    assert len(expected_bpts) == len(filtered_bpts)
    assert all([bpt in expected_bpts for bpt in filtered_bpts])


class MockPath:
    def __init__(self, path: Path, st_mtime: int):
        self.path = path
        self.parent = self.path.parent
        self.st_mtime = st_mtime

    def lstat(self):
        return self


# labeled_folders: (has_H5, H5_st_mtime, folder_name)
@pytest.mark.parametrize(
    "labeled_folders, next_folder_name",
    [
        ([(True, 1, "a"), (False, None, "b"), (False, None, "c")], "b"),
        ([(False, None, "a"), (True, 123, "d"), (False, None, "f")], "f"),
    ],
)
def test_find_next_unlabeled_folder(
    tmpdir_factory,
    monkeypatch,
    labeled_folders,
    next_folder_name,
):
    project_folder = tmpdir_factory.mktemp("project")
    fake_cfg = Path(project_folder / "cfg.yaml")
    auxiliaryfunctions.write_config(fake_cfg, {"project_path": str(project_folder)})

    data_folder = project_folder / "labeled-data"
    data_folder.mkdir()
    rglob_results = []
    for has_h5, h5_last_mod_time, folder_name in labeled_folders:
        labeled_folder_path = Path(data_folder / folder_name)
        labeled_folder_path.mkdir()
        if has_h5:
            h5_path = Path(labeled_folder_path / "data.h5")
            rglob_results.append(MockPath(h5_path, h5_last_mod_time))

    def get_rglob_results(*args, **kwargs):
        return rglob_results

    monkeypatch.setattr(Path, "rglob", get_rglob_results)
    next_folder = auxiliaryfunctions.find_next_unlabeled_folder(fake_cfg)
    assert str(next_folder) == str(Path(data_folder / next_folder_name))


@pytest.fixture
def mock_snapshot_folder(tmp_path):
    """Mock folder with snapshots."""
    folder = tmp_path / "train"
    folder.mkdir()

    # mock files
    snapshot_files = [
        "snapshot-4.index",
        "snapshot-5.index",
        "snapshot-6.index",
        "snapshot-3.data-00000-of-00001",
        "snapshot-3.index",
        "snapshot-3.meta",
    ]
    for file_name in snapshot_files:
        (folder / file_name).touch()

    return folder


@pytest.fixture
def mock_no_snapshots_folder(tmp_path):
    """Mock folder with no snapshots."""
    folder = tmp_path / "train"
    folder.mkdir()

    # mock files
    snapshot_files = ["log.txt", "pose_cfg.yaml"]
    for file_name in snapshot_files:
        (folder / file_name).touch()

    return folder


def test_get_snapshots_from_folder(mock_snapshot_folder):
    """Test returns expected snapshots in order."""
    snapshot_names = auxiliaryfunctions.get_snapshots_from_folder(mock_snapshot_folder)
    assert snapshot_names == ["snapshot-3", "snapshot-4", "snapshot-5", "snapshot-6"]


def test_get_snapshots_from_folder_none(mock_no_snapshots_folder):
    """Test raises ValueError if no snapshots are found."""
    with pytest.raises(FileNotFoundError):
        auxiliaryfunctions.get_snapshots_from_folder(mock_no_snapshots_folder)


--- File: tests/test_auxfun_multianimal.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import networkx as nx
import numpy as np
import pandas as pd
import pytest
from deeplabcut.utils import auxfun_multianimal
from itertools import combinations


def test_prune_paf_graph():
    n_bpts = 10  # This corresponds to 45 edges
    edges = [list(edge) for edge in combinations(range(n_bpts), 2)]
    with pytest.raises(ValueError):
        pruned_edges = auxfun_multianimal.prune_paf_graph(edges, n_bpts - 2)
        pruned_edges = auxfun_multianimal.prune_paf_graph(edges, len(edges))

    for target in range(20, 45, 5):
        pruned_edges = auxfun_multianimal.prune_paf_graph(edges, target)
        assert len(pruned_edges) == target

    for degree in (4, 6, 8):
        pruned_edges = auxfun_multianimal.prune_paf_graph(
            edges,
            average_degree=degree,
        )
        G = nx.Graph(pruned_edges)
        assert np.mean(list(dict(G.degree).values())) == degree


def test_reorder_individuals_in_df():
    import random

    # Load sample multi animal data
    df = pd.read_hdf("tests/data/montblanc_tracks.h5")
    individuals = df.columns.get_level_values("individuals").unique().to_list()

    # Generate a random permutation and reorder data. Ignore the unique bodypart
    permutation_indices = random.sample(
        range(len(individuals[:-1])), k=len(individuals[:-1])
    )
    permutation = [individuals[i] for i in permutation_indices]
    permutation.append("single")
    df_reordered = auxfun_multianimal.reorder_individuals_in_df(df, permutation)

    # Get inverse permutation and reorder the modified data to get back
    # to the original
    inverse_permutation_indices = np.argsort(permutation_indices).tolist()
    inverse_permutation = [individuals[i] for i in inverse_permutation_indices]
    inverse_permutation.append("single")
    df_inverse_reordering = auxfun_multianimal.reorder_individuals_in_df(
        df_reordered, inverse_permutation
    )

    # Check
    pd.testing.assert_frame_equal(df, df_inverse_reordering)


--- File: tests/test_predict_multianimal.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import tensorflow as tf
from deeplabcut.pose_estimation_tensorflow.core import predict_multianimal


RADIUS = 5
THRESHOLD = 0.01
STRIDE = 8


def test_extract_detections(model_outputs, ground_truth_detections):
    scmaps, locrefs, _ = model_outputs
    inds_gt = []
    for i in range(scmaps.shape[3]):
        scmap = scmaps[0, ..., i]
        peaks = predict_multianimal.find_local_maxima(scmap, RADIUS, THRESHOLD)
        inds_gt.append(np.c_[peaks, np.ones(len(peaks)).reshape((-1, 1)) * i])
    inds_gt = np.concatenate(inds_gt).astype(int)
    pos_gt = np.concatenate(ground_truth_detections[0]["coordinates"][0])
    prob_gt = np.concatenate(ground_truth_detections[0]["confidence"])
    inds = predict_multianimal.find_local_peak_indices_maxpool_nms(
        scmaps,
        RADIUS,
        THRESHOLD,
    )
    with tf.compat.v1.Session() as sess:
        inds = sess.run(inds)
    pos = predict_multianimal.calc_peak_locations(locrefs, inds, STRIDE)
    s, r, c, b = inds.T
    prob = scmaps[s, r, c, b].reshape((-1, 1))
    idx = np.argsort(inds[:, -1], kind="mergesort")
    np.testing.assert_equal(inds[idx, 1:], inds_gt)
    np.testing.assert_almost_equal(pos[idx], pos_gt, decimal=3)
    np.testing.assert_almost_equal(prob[idx], prob_gt, decimal=5)


def test_association_costs(model_outputs, ground_truth_detections):
    costs_gt = ground_truth_detections[0]["costs"]
    peak_inds = predict_multianimal.find_local_peak_indices_maxpool_nms(
        model_outputs[0],
        RADIUS,
        THRESHOLD,
    )
    with tf.compat.v1.Session() as sess:
        peak_inds = sess.run(peak_inds)
    graph = [[i, j] for i in range(12) for j in range(i + 1, 12)]
    preds = predict_multianimal.compute_peaks_and_costs(
        *model_outputs,
        peak_inds,
        graph=graph,
        paf_inds=np.arange(len(graph)),
        n_id_channels=0,
        stride=STRIDE,
    )[0]
    assert all(k in preds for k in ("coordinates", "confidence", "costs"))
    costs_pred = preds["costs"]
    assert len(costs_pred) == len(costs_gt)
    eq = [
        np.array_equal(np.argmax(v["m1"], axis=0), np.argmax(costs_gt[k]["m1"], axis=0))
        for k, v in costs_pred.items()
    ]
    assert sum(eq) == 60  # 6 arrays are unequal as cost computation was corrected
    assert all(
        np.allclose(v["distance"], costs_gt[k]["distance"], atol=1.5)
        for k, v in costs_pred.items()
    )


def test_compute_peaks_and_costs_no_graph(model_outputs):
    peak_inds = predict_multianimal.find_local_peak_indices_maxpool_nms(
        model_outputs[0],
        RADIUS,
        THRESHOLD,
    )
    with tf.compat.v1.Session() as sess:
        peak_inds = sess.run(peak_inds)
    preds = predict_multianimal.compute_peaks_and_costs(
        *model_outputs,
        peak_inds,
        graph=[],
        paf_inds=[],
        n_id_channels=0,
        stride=STRIDE,
    )[0]
    assert "costs" not in preds


--- File: tests/test_stitcher.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import pandas as pd
import pytest
from deeplabcut.refine_training_dataset.stitch import Tracklet, TrackletStitcher


TRACKLET_LEN = 1000
TRACKLET_START = 50
TRACKLET_ID = 0
N_DETS = 5
N_TRACKLETS = 20


def fake_tracklet():
    inds = np.arange(TRACKLET_START, TRACKLET_START + TRACKLET_LEN)
    data = np.empty((TRACKLET_LEN, N_DETS, 4))
    data[..., :2] = np.arange(N_DETS).reshape(-1, 1) * [1, 1]
    data[..., 2] = 1
    data[..., 3] = TRACKLET_ID
    return Tracklet(data, inds)


def make_fake_tracklets():
    tracklet = fake_tracklet()
    tracklet_single = Tracklet(tracklet.data[:, :1], tracklet.inds)
    return tracklet, tracklet_single


@pytest.fixture()
def fake_stitcher():
    inds = np.arange(TRACKLET_LEN)
    data = np.random.rand(inds.size, N_DETS, 3)
    track = Tracklet(data, inds)
    idx = np.linspace(0, inds.size, N_TRACKLETS + 1, dtype=int)
    tracklets = TrackletStitcher.split_tracklet(track, idx[1:-1])
    return TrackletStitcher(tracklets, n_tracks=2)


@pytest.mark.parametrize("tracklet", make_fake_tracklets())
def test_tracklet_wrong_inputs(tracklet):
    with pytest.raises(ValueError):
        _ = Tracklet(tracklet.data[..., :2], tracklet.inds)
    with pytest.raises(ValueError):
        _ = Tracklet(tracklet.data[: TRACKLET_LEN - 2], tracklet.inds)


@pytest.mark.parametrize("tracklet", make_fake_tracklets())
def test_tracklet_monotonic_indices(tracklet):
    tracklet_inv = Tracklet(tracklet.data[::-1], tracklet.inds[::-1])
    np.testing.assert_equal(tracklet.inds, tracklet_inv.inds)
    np.testing.assert_equal(tracklet.xy, tracklet_inv.xy)


@pytest.mark.parametrize("tracklet", make_fake_tracklets())
def test_tracklet(tracklet):
    assert len(tracklet) == TRACKLET_LEN
    assert tracklet.likelihood == 1
    assert tracklet.identity == TRACKLET_ID
    assert tracklet.start == TRACKLET_START
    assert tracklet.end == TRACKLET_START + TRACKLET_LEN - 1
    np.testing.assert_equal(
        tracklet.centroid,
        np.full((TRACKLET_LEN, 2), np.arange(tracklet.data.shape[1]).mean()),
    )
    tracklet2 = Tracklet(tracklet.data, tracklet.inds + TRACKLET_LEN)
    assert tracklet not in tracklet2
    tracklet_new = tracklet + tracklet2
    tracklet_new -= tracklet
    np.testing.assert_equal(tracklet_new.data, tracklet2.data)
    np.testing.assert_equal(tracklet_new.inds, tracklet2.inds)
    tracklet2 = tracklet + tracklet
    assert tracklet2.contains_duplicates()


@pytest.mark.parametrize("tracklet", make_fake_tracklets())
def test_tracklet_default_identity(tracklet):
    tracklet.data = tracklet.data[..., :3]
    assert tracklet.identity == -1


@pytest.mark.parametrize("tracklet", make_fake_tracklets())
def test_tracklet_data_access(tracklet):
    np.testing.assert_equal(tracklet.get_data_at(TRACKLET_START), tracklet.data[0])
    tracklet.set_data_at(TRACKLET_START + 1, tracklet.data[0] * 2)
    np.testing.assert_equal(tracklet.data[1], tracklet.data[0] * 2)
    tracklet.del_data_at(TRACKLET_START + 1)
    assert not tracklet.is_continuous
    assert TRACKLET_START + 1 not in tracklet.inds


@pytest.mark.parametrize(
    "tracklet, where, norm",
    list(zip(make_fake_tracklets(), ("head", "tail"), (False, True))),
)
def test_tracklet_calc_velocity(tracklet, where, norm):
    _ = tracklet.calc_velocity(where, norm)


@pytest.mark.parametrize("tracklet", make_fake_tracklets())
def test_tracklet_calc_rate_of_turn(tracklet):
    for where in ("head", "tail"):
        _ = tracklet.calc_rate_of_turn(where)


@pytest.mark.parametrize("tracklet", make_fake_tracklets())
def test_tracklet_affinities(tracklet):
    other_tracklet = Tracklet(tracklet.data, tracklet.inds + TRACKLET_LEN)
    _ = tracklet.dynamic_similarity_with(other_tracklet)
    _ = tracklet.dynamic_dissimilarity_with(other_tracklet)
    _ = tracklet.shape_dissimilarity_with(other_tracklet)
    _ = tracklet.box_overlap_with(other_tracklet)
    _ = tracklet.motion_affinity_with(other_tracklet)
    _ = tracklet.distance_to(other_tracklet)


@pytest.mark.parametrize("tracklet", make_fake_tracklets())
def test_stitcher_wrong_inputs(tracklet):
    with pytest.raises(IOError):
        _ = TrackletStitcher([], n_tracks=2)
    with pytest.raises(ValueError):
        _ = TrackletStitcher([tracklet], n_tracks=2, min_length=2)


@pytest.mark.parametrize("tracklet", make_fake_tracklets())
def test_purify_tracklets(tracklet):
    tracklet.data = np.full_like(tracklet.data, np.nan)
    assert TrackletStitcher.purify_tracklet(tracklet) is None
    tracklet.data[0] = 1
    tracklet_pure = TrackletStitcher.purify_tracklet(tracklet)
    assert len(tracklet_pure) == 1
    assert tracklet_pure.inds == tracklet.inds[0]


def test_stitcher(tmpdir_factory, fake_stitcher):
    assert len(fake_stitcher) == N_TRACKLETS
    assert fake_stitcher.n_frames == TRACKLET_LEN
    assert fake_stitcher.compute_max_gap(fake_stitcher.tracklets) == 1
    fake_stitcher.build_graph(max_gap=1)
    fake_stitcher.stitch(add_back_residuals=True)
    output_name = tmpdir_factory.mktemp("data").join("fake.h5")
    fake_stitcher.write_tracks(output_name)

    # Break the graph to test stitching failure
    fake_stitcher.G.remove_edge("source", "0in")
    with pytest.warns(UserWarning):
        fake_stitcher.stitch(add_back_residuals=True)


def test_stitcher_plot(fake_stitcher):
    fake_stitcher.build_graph(max_gap=1)
    fake_stitcher.draw_graph(with_weights=True)
    fake_stitcher.stitch(add_back_residuals=True)
    fake_stitcher.plot_tracklets()
    fake_stitcher.plot_paths()
    fake_stitcher.plot_tracks()


def test_tracklet_interpolate(real_tracklets):
    data = np.stack(list(real_tracklets[0].values()))[:10]
    inds = np.arange(len(data))
    gap = 2
    inds[len(inds) // 2 :] += gap
    tracklet = Tracklet(data, inds)
    assert len(tracklet) == len(data)
    new_tracklet = tracklet.interpolate(max_gap=1)
    assert len(new_tracklet) == len(data)
    new_tracklet = tracklet.interpolate(max_gap=gap)
    assert len(new_tracklet) == len(data) + gap
    missing_inds = list(set(range(inds.max())).difference(inds))
    assert np.all(new_tracklet.data[missing_inds, :, 2] == 0.5)


def test_stitcher_real(tmpdir_factory, real_tracklets):
    stitcher = TrackletStitcher.from_dict_of_dict(real_tracklets, n_tracks=3)
    assert len(stitcher) == 3
    assert all(tracklet.is_continuous for tracklet in stitcher.tracklets)
    assert all(tracklet.identity == -1 for tracklet in stitcher.tracklets)
    assert not stitcher.residuals
    assert stitcher.compute_max_gap(stitcher.tracklets) == 0

    stitcher.build_graph()
    assert stitcher.G.number_of_edges() == 9
    assert all(weight is None for *_, weight in stitcher.G.edges.data("weight"))

    stitcher.stitch()
    assert len(stitcher.tracks) == 3
    assert all(len(track) == 50 for track in stitcher.tracks)
    assert all(0.998 <= track.likelihood <= 1 for track in stitcher.tracks)

    output_name = tmpdir_factory.mktemp("data").join("fake.h5")
    stitcher.write_tracks(output_name, ["mickey", "minnie", "bianca"])


def test_stitcher_montblanc(real_tracklets_montblanc):
    stitcher = TrackletStitcher.from_dict_of_dict(
        real_tracklets_montblanc,
        n_tracks=3,
    )
    assert len(stitcher) == 5
    assert all(tracklet.is_continuous for tracklet in stitcher.tracklets)
    assert all(tracklet.identity == -1 for tracklet in stitcher.tracklets)
    assert len(stitcher.residuals) == 1
    assert len(stitcher.residuals[0]) == 2
    assert stitcher.compute_max_gap(stitcher.tracklets) == 5

    stitcher.build_graph()
    assert stitcher.G.number_of_edges() == 18
    weights = [w for *_, w in stitcher.G.edges.data("weight") if w]
    assert weights == [2453, 24498, 5428]

    stitcher.stitch()
    assert len(stitcher.tracks) == 3
    assert all(len(track) >= 176 for track in stitcher.tracks)
    assert all(0.996 <= track.likelihood <= 1 for track in stitcher.tracks)

    df_gt = pd.read_hdf("tests/data/montblanc_tracks.h5")
    df = stitcher.format_df()
    np.testing.assert_equal(df.to_numpy(), df_gt.to_numpy())


def test_stitcher_with_identity(real_tracklets):
    # Add fake IDs
    for i in range(3):
        tracklet = real_tracklets[i]
        for v in tracklet.values():
            v[:, -1] = i
    stitcher = TrackletStitcher.from_dict_of_dict(real_tracklets, n_tracks=3)
    tracklets = sorted(stitcher, key=lambda t: t.identity)
    assert all(tracklet.identity == i for i, tracklet in enumerate(tracklets))

    # Split all tracklets in half
    tracklets = [t for track in stitcher for t in stitcher.split_tracklet(track, [25])]
    stitcher = TrackletStitcher(tracklets, n_tracks=3)
    assert len(stitcher) == 6

    stitcher.build_graph()
    weight = stitcher.G.edges[("0out", "3in")]["weight"]

    def weight_func(t1, t2):
        w = 0.01 if t1.identity == t2.identity else 1
        return w * t1.distance_to(t2)

    stitcher.build_graph(weight_func=weight_func)
    assert stitcher.G.number_of_edges() == 27
    new_weight = stitcher.G.edges[("0out", "3in")]["weight"]
    assert new_weight == weight // 100

    stitcher.stitch()
    assert len(stitcher.tracks) == 3
    assert all(len(track) == 50 for track in stitcher.tracks)
    assert all(0.998 <= track.likelihood <= 1 for track in stitcher.tracks)
    tracks = sorted(stitcher.tracks, key=lambda t: t.identity)
    assert all(track.identity == i for i, track in enumerate(tracks))


--- File: tests/test_evaluate.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import pandas as pd
import pytest

import deeplabcut.pose_estimation_tensorflow as pet
from deeplabcut.pose_estimation_tensorflow.core.evaluate import (
    get_available_requested_snapshots,
    get_snapshots_by_index,
)


def make_single_animal_rmse_df(
    bodyparts,
    train_indices,
    test_indices,
    error_data=None,
) -> pd.DataFrame:
    if error_data is None:
        error_data = np.ones((len(train_indices) + len(test_indices), len(bodyparts)))
    return pd.DataFrame(error_data, columns=bodyparts)


def make_multi_animal_rmse_df(
    scorer,
    individuals,
    bodyparts,
    train_indices,
    test_indices,
    error_data=None,
) -> pd.DataFrame:
    columns = pd.MultiIndex.from_product(
        [[scorer], individuals, bodyparts],
        names=["scorer", "individuals", "bodyparts"],
    )
    if error_data is None:
        error_data = np.ones(
            (len(train_indices) + len(test_indices), len(individuals) * len(bodyparts))
        )
    return pd.DataFrame(error_data, columns=columns)


KEYPOINT_ERROR_NAMES = [
    "Train error (px)",
    "Test error (px)",
    "Train error (px) with p-cutoff",
    "Test error (px) with p-cutoff",
]

KEYPOINT_ERROR_TEST_DATA = [
    (
        {
            "df_error": make_single_animal_rmse_df(
                bodyparts=["leg", "arm", "head"],
                train_indices=[0, 1, 3],
                test_indices=[2, 4],
            ),
            "train_indices": [0, 1, 3],
            "test_indices": [2, 4],
        },
        {
            "leg": [1.0, 1.0],  # train, test
            "arm": [1.0, 1.0],  # train, test
            "head": [1.0, 1.0],  # train, test
        },
    ),
    (
        {
            "df_error": make_single_animal_rmse_df(
                bodyparts=["leftHand", "rightHand"],
                train_indices=[0, 2],
                test_indices=[1, 3],
                error_data=[
                    [1.0, np.nan],
                    [1.0, 0.0],
                    [0.0, 10.0],
                    [5.0, 5.0],
                ],
            ),
            "train_indices": [0, 2],
            "test_indices": [1, 3],
        },
        {
            "leftHand": [0.5, 3.0],  # train, test
            "rightHand": [10.0, 2.5],  # train, test
        },
    ),
    (
        {
            "df_error": make_single_animal_rmse_df(
                bodyparts=["leg", "arm", "head"],
                train_indices=[0, 1, 3],
                test_indices=[2, 4],
            ),
            "train_indices": [0, 1, 3],
            "test_indices": [2, 4],
        },
        {
            "leg": [1.0, 1.0],  # train, test
            "arm": [1.0, 1.0],  # train, test
            "head": [1.0, 1.0],  # train, test
        },
    ),
    (
        {
            "df_error": make_multi_animal_rmse_df(
                scorer="john",
                individuals=["individual_1", "individual_2"],
                bodyparts=["leftArm", "rightArm"],
                train_indices=[0, 1, 3],
                test_indices=[2],
                error_data=[
                    # individual_1, individual2
                    # leftArm, rightArm, leftArm, rightArm
                    [1.0, np.nan, 1.0, 2.0],
                    [2.0, 0.0, 1.0, np.nan],
                    [3.0, 10.0, 1.0, np.nan],
                    [10.0, 4.0, np.nan, np.nan],
                ],
            ),
            "train_indices": [0, 1, 3],
            "test_indices": [2],
        },
        {
            "leftArm": [3.0, 2.0],  # train, test
            "rightArm": [2.0, 10.0],  # train, test
        },
    ),
]


@pytest.mark.parametrize("inputs, expected_values", KEYPOINT_ERROR_TEST_DATA)
def test_evaluate_keypoint_error(inputs, expected_values):
    keypoint_error = pet.keypoint_error(
        inputs["df_error"],
        inputs["df_error"],
        inputs["train_indices"],
        inputs["test_indices"],
    )
    print(inputs["df_error"])
    print(keypoint_error)
    for bodypart, mean_errors in expected_values.items():
        for error_name in KEYPOINT_ERROR_NAMES:
            if "train" in error_name.lower():
                mean_error = mean_errors[0]
            else:
                mean_error = mean_errors[1]

            assert keypoint_error.loc[error_name, bodypart] == mean_error


def test_get_available_requested_snapshots_ok():
    """Test that the correct snapshots are returned."""
    available = ["snapshot-1", "snapshot-2"]
    requested = ["snapshot-2", "snapshot-3"]

    snapshots = get_available_requested_snapshots(
        requested_snapshots=requested,
        available_snapshots=available,
    )
    assert snapshots == ["snapshot-2"]


def test_get_available_requested_snapshots_error():
    """Test that a ValueError is raised when requested snapshots are not available."""
    with pytest.raises(ValueError):
        get_available_requested_snapshots(
            requested_snapshots=["snapshot-2"],
            available_snapshots=["snapshot-1", "snapshot-3"],
        )


def test_get_snapshots_by_index_int_ok():
    """Test that the correct snapshots are returned."""
    available = ["snapshot-1", "snapshot-2", "snapshot-3"]

    # positive int
    snapshots = get_snapshots_by_index(
        idx=2,
        available_snapshots=available,
    )
    assert snapshots == ["snapshot-3"]

    # negative int
    snapshots = get_snapshots_by_index(
        idx=-2,
        available_snapshots=available,
    )
    assert snapshots == ["snapshot-2"]

    # all snapshots
    snapshots = get_snapshots_by_index(
        idx="all",
        available_snapshots=available,
    )
    assert snapshots == ["snapshot-1", "snapshot-2", "snapshot-3"]


def test_get_snapshots_by_index_error():
    """Test that a ValueError is raised when the index is out of range or invalid str."""
    available = ["snapshot-1", "snapshot-2", "snapshot-3"]

    # positive int
    with pytest.raises(IndexError):
        get_snapshots_by_index(
            idx=5,
            available_snapshots=available,
        )
    # negative int
    with pytest.raises(IndexError):
        get_snapshots_by_index(
            idx=-4,
            available_snapshots=available,
        )
    # invalid str
    with pytest.raises(IndexError):
        get_snapshots_by_index(
            idx="1",
            available_snapshots=available,
        )


--- File: tests/test_crossvalutils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import pickle
from deeplabcut.core import crossvalutils

BEST_GRAPH = [14, 15, 16, 11, 22, 31, 61, 7, 59, 62, 64]
BEST_GRAPH_MONTBLANC = [1, 0, 2, 5, 4, 3]


def test_get_n_best_paf_graphs(evaluation_data_and_metadata):
    data, metadata = evaluation_data_and_metadata
    params = crossvalutils._set_up_evaluation(data)
    n_graphs = 5
    paf_inds, dict_ = crossvalutils._get_n_best_paf_graphs(
        data, metadata, params["paf_graph"], n_graphs=n_graphs
    )
    assert len(paf_inds) == n_graphs
    assert len(dict_) == len(params["paf_graph"])
    assert len(paf_inds[0]) == 11
    assert paf_inds[0] == BEST_GRAPH
    assert len(paf_inds[-1]) == len(params["paf_graph"])


def test_get_n_best_paf_graphs_montblanc(evaluation_data_and_metadata_montblanc):
    data, metadata = evaluation_data_and_metadata_montblanc
    params = crossvalutils._set_up_evaluation(data)
    paf_inds, dict_ = crossvalutils._get_n_best_paf_graphs(
        data,
        metadata,
        params["paf_graph"],
    )
    assert len(paf_inds) == 4
    assert len(dict_) == len(params["paf_graph"])
    assert [len(inds) for inds in paf_inds] == list(range(3, 7))
    assert paf_inds[-1] == BEST_GRAPH_MONTBLANC
    assert len(paf_inds[-1]) == len(params["paf_graph"])


def test_benchmark_paf_graphs(evaluation_data_and_metadata):
    data, _ = evaluation_data_and_metadata
    cfg = {
        "individuals": ["mickey", "minnie", "bianca"],
        "uniquebodyparts": [],
        "multianimalbodyparts": [
            "snout",
            "leftear",
            "rightear",
            "shoulder",
            "spine1",
            "spine2",
            "spine3",
            "spine4",
            "tailbase",
            "tail1",
            "tail2",
            "tailend",
        ],
    }
    inference_cfg = {"topktoretain": 3, "pcutoff": 0.1, "pafthreshold": 0.1}
    results = crossvalutils._benchmark_paf_graphs(
        cfg, inference_cfg, data, [BEST_GRAPH]
    )
    all_scores = results[0]
    assert len(all_scores) == 1
    assert all_scores[0][1] == BEST_GRAPH
    miss, purity = results[1].xs("mean", level=1).to_numpy().squeeze()
    assert np.isclose(miss, 0.02, atol=1e-2)
    assert np.isclose(purity, 0.98, atol=1e-2)


def test_benchmark_paf_graphs_montblanc(evaluation_data_and_metadata_montblanc):
    data, metadata = evaluation_data_and_metadata_montblanc
    cfg = {
        "individuals": [f"bird{i}" for i in range(1, 9)],
        "uniquebodyparts": ["center"],
        "multianimalbodyparts": [
            "head",
            "tail",
            "leftwing",
            "rightwing",
        ],
    }
    inference_cfg = {"topktoretain": 8, "pcutoff": 0.1, "pafthreshold": 0.1}
    results = crossvalutils._benchmark_paf_graphs(
        cfg,
        inference_cfg,
        data,
        [BEST_GRAPH_MONTBLANC],
        split_inds=[metadata["data"]["trainIndices"], metadata["data"]["testIndices"]],
    )
    with open("tests/data/montblanc_map.pickle", "rb") as f:
        results_gt = pickle.load(f)
    np.testing.assert_equal(
        results[1].loc["purity"].to_numpy().squeeze(),
        [
            results_gt[0][6][("purity", "mean")],
            results_gt[0][6][("purity", "std")],
        ],
    )
    vals = [
        results[2][0][0]["mAP"],
        results[2][0][0]["mAR"],
        results[2][0][1]["mAP"],
        results[2][0][1]["mAR"],
    ]
    np.testing.assert_equal(
        vals,
        [
            results_gt[0][6][("mAP_train", "mean")],
            results_gt[0][6][("mAR_train", "mean")],
            results_gt[0][6][("mAP_test", "mean")],
            results_gt[0][6][("mAR_test", "mean")],
        ],
    )


--- File: tests/test_pose_multianimal_imgaug.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import os
import pytest
from conftest import TEST_DATA_DIR
from deeplabcut.pose_estimation_tensorflow.datasets import (
    Batch,
    pose_multianimal_imgaug,
    PoseDatasetFactory,
)
from deeplabcut.utils import read_plainconfig


def mock_imread(path, mode):
    return (np.random.rand(400, 400, 3) * 255).astype(np.uint8)


pose_multianimal_imgaug.imread = mock_imread


@pytest.fixture()
def ma_dataset():
    cfg = read_plainconfig(os.path.join(TEST_DATA_DIR, "pose_cfg.yaml"))
    cfg["project_path"] = TEST_DATA_DIR
    cfg["dataset"] = "trimouse_train_data.pickle"
    return PoseDatasetFactory.create(cfg)


@pytest.mark.parametrize(
    "scale, stride",
    [
        (0.6, 2),
        (0.6, 4),
        (0.6, 8),
        (0.8, 4),
        (1.0, 8),
        (1.2, 8),
        (0.6, 4),
        (0.8, 8),
    ],
)
def test_calc_target_and_scoremap_sizes(
    ma_dataset,
    scale,
    stride,
):
    ma_dataset.cfg["global_scale"] = scale
    ma_dataset.cfg["stride"] = stride
    # Disable stochastic scale jitter
    ma_dataset.cfg["scale_jitter_lo"] = 1
    ma_dataset.cfg["scale_jitter_up"] = 1
    target_size, sm_size = ma_dataset.calc_target_and_scoremap_sizes()
    np.testing.assert_equal(np.asarray([400, 400]) * scale, target_size)
    np.testing.assert_equal(target_size / stride, sm_size)


def test_get_batch(ma_dataset):
    for batch_size in 1, 4, 8, 16:
        ma_dataset.batch_size = batch_size
        batch_images, joint_ids, batch_joints, data_items = ma_dataset.get_batch()
        assert (
            len(batch_images)
            == len(joint_ids)
            == len(batch_joints)
            == len(data_items)
            == batch_size
        )
        for data_item, joint_id, batch_joint in zip(
            data_items, joint_ids, batch_joints
        ):
            assert len(data_item.joints) == len(joint_id)
            assert len(batch_joint) == len(np.concatenate(joint_id))
            start = 0
            mask = ~np.isnan(batch_joint).any(axis=1)
            for joints, id_ in zip(data_item.joints.values(), joint_id):
                inds = id_ + start
                mask_ = mask[inds]
                np.testing.assert_equal(joints[:, 0], id_[mask_])
                np.testing.assert_equal(joints[:, 1:], batch_joint[inds][mask_])
                start += id_.size


def test_build_augmentation_pipeline(ma_dataset):
    for prob in (0.3, 0.5):
        _ = ma_dataset.build_augmentation_pipeline(prob)


@pytest.mark.parametrize("num_idchannel", range(4))
def test_get_targetmaps(ma_dataset, num_idchannel):
    ma_dataset.cfg["num_idchannel"] = num_idchannel
    batch = ma_dataset.get_batch()[1:]
    target_size, sm_size = ma_dataset.calc_target_and_scoremap_sizes()
    scale = np.mean(target_size / ma_dataset.default_size)
    maps = ma_dataset.get_targetmaps_update(*batch, sm_size, scale)
    assert all(len(map_) == ma_dataset.batch_size for map_ in maps.values())
    assert (
        maps[Batch.part_score_targets][0].shape
        == maps[Batch.part_score_weights][0].shape
    )
    assert (
        maps[Batch.part_score_targets][0].shape[2]
        == ma_dataset.cfg["num_joints"] + num_idchannel
    )
    assert maps[Batch.locref_targets][0].shape == maps[Batch.locref_mask][0].shape
    assert maps[Batch.locref_targets][0].shape[2] == 2 * ma_dataset.cfg["num_joints"]
    assert (
        maps[Batch.pairwise_targets][0].shape == maps[Batch.pairwise_targets][0].shape
    )
    assert maps[Batch.pairwise_targets][0].shape[2] == 2 * ma_dataset.cfg["num_limbs"]


def test_batching(ma_dataset):
    for _ in range(10):
        batch = ma_dataset.next_batch()


--- File: tests/test_inferenceutils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import os
import pickle
import pytest
from conftest import TEST_DATA_DIR
from copy import deepcopy
from deeplabcut.core import inferenceutils
from scipy.spatial.distance import squareform


def test_conv_square_to_condensed_indices():
    n = 5
    rows, cols = np.triu_indices(n, k=1)
    mat = np.zeros((n, n), dtype=int)
    mat[rows, cols] = mat[cols, rows] = np.arange(1, len(rows) + 1)
    vec = squareform(mat)
    vals = []
    for i, j in zip(rows, cols):
        ind = inferenceutils._conv_square_to_condensed_indices(i, j, n)
        vals.append(vec[ind])
    np.testing.assert_equal(vec, vals)


def test_calc_object_keypoint_similarity(real_assemblies):
    sigma = 0.01
    xy1 = real_assemblies[0][0].xy
    xy2 = real_assemblies[0][1].xy
    assert inferenceutils.calc_object_keypoint_similarity(xy1, xy1, sigma) == 1
    assert np.isclose(
        inferenceutils.calc_object_keypoint_similarity(xy1, xy2, sigma), 0
    )
    xy3 = xy1.copy()
    xy3[: len(xy3) // 2] = np.nan
    assert inferenceutils.calc_object_keypoint_similarity(xy3, xy1, sigma) == 0.5
    xy3[:] = np.nan
    assert inferenceutils.calc_object_keypoint_similarity(xy3, xy1, sigma) == 0
    assert np.isnan(inferenceutils.calc_object_keypoint_similarity(xy1, xy3, sigma))

    # Test flipped keypoints
    xy4 = xy1.copy()
    symmetric_pair = [0, 11]
    xy4[symmetric_pair] = xy4[symmetric_pair[::-1]]
    assert inferenceutils.calc_object_keypoint_similarity(xy1, xy4, sigma) != 1
    assert (
        inferenceutils.calc_object_keypoint_similarity(
            xy1, xy4, sigma, symmetric_kpts=[symmetric_pair]
        )
        == 1
    )


def test_match_assemblies(real_assemblies):
    assemblies = real_assemblies[0]
    num_gt, matches = inferenceutils.match_assemblies(
        assemblies, assemblies[::-1], 0.01
    )
    assert len(assemblies) == len(matches)
    for m in matches:
        assert m.prediction is m.ground_truth
        assert m.oks == 1

    num_gt, matches = inferenceutils.match_assemblies([], assemblies, 0.01)
    assert len(matches) == 0
    assert num_gt == len(assemblies)


def test_evaluate_assemblies(real_assemblies):
    assemblies = {i: real_assemblies[i] for i in range(3)}
    n_thresholds = 5
    thresholds = np.linspace(0.5, 0.95, n_thresholds)
    dict_ = inferenceutils.evaluate_assembly(
        assemblies, assemblies, oks_thresholds=thresholds
    )
    assert dict_["mAP"] == dict_["mAR"] == 1
    assert len(dict_["precisions"]) == len(dict_["recalls"]) == n_thresholds
    assert dict_["precisions"].shape[1] == 101
    np.testing.assert_allclose(dict_["precisions"], 1)

    dict_ = inferenceutils.evaluate_assembly(
        assemblies,
        assemblies,
        oks_thresholds=thresholds,
        symmetric_kpts=[(0, 5), (1, 4)],
    )
    assert dict_["mAP"] == dict_["mAR"] == 1
    assert len(dict_["precisions"]) == len(dict_["recalls"]) == n_thresholds
    assert dict_["precisions"].shape[1] == 101
    np.testing.assert_allclose(dict_["precisions"], 1)


def test_link():
    pos1 = 1, 1
    idx1 = 0
    pos2 = 10, 10
    idx2 = 1
    conf = 0.5
    j1 = inferenceutils.Joint(pos1, conf, idx=idx1)
    j2 = inferenceutils.Joint(pos2, conf, idx=idx2)
    link = inferenceutils.Link(j1, j2)
    assert link.confidence == conf**2
    assert link.idx == (idx1, idx2)
    assert link.to_vector() == [*pos1, *pos2]


def test_assembly():
    ass = inferenceutils.Assembly(3)
    assert len(ass) == 0

    j1 = inferenceutils.Joint((1, 1), label=0)
    j2 = inferenceutils.Joint((1, 1), label=1)
    assert ass.add_link(inferenceutils.Link(j1, j2), store_dict=True)
    assert len(ass) == 2
    assert ass.data[j2.label, 0] == 1
    assert ass.data[j2.label, -1] == -1
    assert ass.area == 0
    assert ass.intersection_with(ass) == 1.0
    # Original (cached) coordinates must have remained empty
    assert np.all(np.isnan(ass._dict["data"][:, :2]))

    ass.remove_joint(j2)
    assert len(ass) == 1
    assert np.all(np.isnan(ass.data[j2.label]))

    ass2 = inferenceutils.Assembly(2)
    ass2.add_link(inferenceutils.Link(j1, j2))
    with pytest.raises(ValueError):
        _ = ass + ass2
    ass2.remove_joint(j1)
    assert ass2 not in ass
    ass3 = ass + ass2
    assert len(ass3) == 2


def test_assembler(tmpdir_factory, real_assemblies):
    with open(os.path.join(TEST_DATA_DIR, "trimouse_full.pickle"), "rb") as file:
        data = pickle.load(file)
    with pytest.warns(UserWarning):
        ass = inferenceutils.Assembler(
            data,
            max_n_individuals=3,
            n_multibodyparts=12,
            identity_only=True,  # Test whether warning is properly raised
        )
    assert len(ass.metadata["imnames"]) == 50
    assert ass.n_keypoints == 12
    assert len(ass.graph) == len(ass.paf_inds) == 66
    # Assemble based on the smallest graph to speed up testing
    naive_graph = [
        [0, 1],
        [7, 8],
        [6, 7],
        [10, 11],
        [4, 5],
        [5, 6],
        [8, 9],
        [9, 10],
        [0, 3],
        [3, 4],
        [0, 2],
    ]
    ass.paf_inds = [ass.graph.index(edge) for edge in naive_graph]
    ass.assemble()
    assert not ass.unique
    assert len(ass.assemblies) == len(real_assemblies)
    assert sum(1 for a in ass.assemblies.values() for _ in a) == sum(
        1 for a in real_assemblies.values() for _ in a
    )

    output_dir = tmpdir_factory.mktemp("data")
    ass.to_h5(output_dir.join("fake.h5"))
    ass.to_pickle(output_dir.join("fake.pickle"))


def test_assembler_with_single_bodypart(real_assemblies):
    with open(os.path.join(TEST_DATA_DIR, "trimouse_full.pickle"), "rb") as file:
        temp = pickle.load(file)
    data = {"metadata": temp.pop("metadata")}
    for k, dict_ in temp.items():
        data[k] = {
            "coordinates": (dict_["coordinates"][0][:1],),
            "confidence": dict_["confidence"][:1],
        }
    ass = inferenceutils.Assembler(
        data,
        max_n_individuals=3,
        n_multibodyparts=1,
    )
    ass.metadata["joint_names"] = ass.metadata["joint_names"][:1]
    ass.metadata["num_joints"] = 1
    ass.metadata["paf_graph"] = []
    ass.metadata["paf"] = []
    ass.metadata["bpts"] = [0]
    ass.metadata["ibpts"] = [0]
    ass.assemble(chunk_size=0)
    assert not ass.unique
    assert len(ass.assemblies) == len(real_assemblies)
    assert all(len(a) == 3 for a in ass.assemblies.values())


def test_assembler_with_unique_bodypart(real_assemblies_montblanc):
    with open(os.path.join(TEST_DATA_DIR, "montblanc_full.pickle"), "rb") as file:
        data = pickle.load(file)
    ass = inferenceutils.Assembler(
        data,
        max_n_individuals=3,
        n_multibodyparts=4,
        pcutoff=0.1,
        min_affinity=0.1,
    )
    assert len(ass.metadata["imnames"]) == 180
    assert ass.n_keypoints == 5
    assert len(ass.graph) == len(ass.paf_inds) == 6
    ass.assemble(chunk_size=0)
    assert len(ass.assemblies) == len(real_assemblies_montblanc[0])
    assert len(ass.unique) == len(real_assemblies_montblanc[1])
    assemblies = np.concatenate(
        [ass.xy for assemblies in ass.assemblies.values() for ass in assemblies]
    )
    assemblies_gt = np.concatenate(
        [
            ass.xy
            for assemblies in real_assemblies_montblanc[0].values()
            for ass in assemblies
        ]
    )
    np.testing.assert_equal(assemblies, assemblies_gt)


def test_assembler_with_identity(tmpdir_factory, real_assemblies):
    with open(os.path.join(TEST_DATA_DIR, "trimouse_full.pickle"), "rb") as file:
        data = pickle.load(file)

    # Generate fake identity predictions
    for k, v in data.items():
        if k != "metadata":
            conf = v["confidence"]
            ids = [np.random.rand(c.shape[0], 3) for c in conf]
            v["identity"] = ids

    ass = inferenceutils.Assembler(data, max_n_individuals=3, n_multibodyparts=12)
    assert ass._has_identity
    assert len(ass.metadata["imnames"]) == 50
    assert ass.n_keypoints == 12
    assert len(ass.graph) == len(ass.paf_inds) == 66
    # Assemble based on the smallest graph to speed up testing
    naive_graph = [
        [0, 1],
        [7, 8],
        [6, 7],
        [10, 11],
        [4, 5],
        [5, 6],
        [8, 9],
        [9, 10],
        [0, 3],
        [3, 4],
        [0, 2],
    ]
    ass.paf_inds = [ass.graph.index(edge) for edge in naive_graph]
    ass.assemble()
    assert not ass.unique
    assert len(ass.assemblies) == len(real_assemblies)
    assert sum(1 for a in ass.assemblies.values() for _ in a) == sum(
        1 for a in real_assemblies.values() for _ in a
    )
    assert all(np.all(_.data[:, -1] != -1) for a in ass.assemblies.values() for _ in a)

    # Test now with identity only and ensure assemblies
    # contain only parts of a single group ID.
    ass.identity_only = True
    ass.assemble()
    assert len(ass.assemblies) == len(real_assemblies)
    eq = []
    for a in ass.assemblies.values():
        for _ in a:
            ids = _.data[:, -1]
            ids = ids[~np.isnan(ids)]
            eq.append(np.all(ids == ids[0]))
    assert all(eq)

    output_dir = tmpdir_factory.mktemp("data")
    ass.to_h5(output_dir.join("fake.h5"))
    ass.to_pickle(output_dir.join("fake.pickle"))


def test_assembler_calibration(real_assemblies):
    with open(os.path.join(TEST_DATA_DIR, "trimouse_full.pickle"), "rb") as file:
        data = pickle.load(file)
    ass = inferenceutils.Assembler(data, max_n_individuals=3, n_multibodyparts=12)
    ass.calibrate(os.path.join(TEST_DATA_DIR, "trimouse_calib.h5"))
    assert ass._kde is not None
    assert ass.safe_edge

    assembly = real_assemblies[0][0]
    mahal, proba = ass.calc_assembly_mahalanobis_dist(assembly, return_proba=True)
    assert np.isclose(mahal, 19.541, atol=1e-3)
    assert np.isclose(proba, 1, atol=1e-3)

    j1 = inferenceutils.Joint(tuple(assembly.xy[0]), label=0)
    j2 = inferenceutils.Joint(tuple(assembly.xy[1]), label=1)
    link = inferenceutils.Link(j1, j2)
    p = ass.calc_link_probability(link)
    assert np.isclose(p, 0.993, atol=1e-3)

    # Test empty assembly
    assembly_ = deepcopy(assembly)
    assembly_.data[:, :2] = np.nan
    mahal, proba = ass.calc_assembly_mahalanobis_dist(assembly_, return_proba=True)
    assert np.isinf(mahal)
    assert proba == 0


def test_find_outlier_assemblies(real_assemblies):
    assert len(inferenceutils.find_outlier_assemblies(real_assemblies)) == 13


--- File: tests/test_trainingsetmanipulation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import os
import pandas as pd
import pytest
from typing import List

from conftest import TEST_DATA_DIR
from deeplabcut.generate_training_dataset import (
    read_image_shape_fast,
    SplitTrials,
    format_training_data,
    format_multianimal_training_data,
    trainingsetmanipulation,
    multiple_individuals_trainingsetmanipulation,
    parse_video_filenames,
)

from deeplabcut.utils.auxfun_videos import imread
from deeplabcut.utils.conversioncode import guarantee_multiindex_rows
from skimage import color, io


def test_read_image_shape_fast(tmp_path):
    path_rgb_image = os.path.join(TEST_DATA_DIR, "image.png")
    img = imread(path_rgb_image, mode="skimage")
    shape = img.shape
    assert read_image_shape_fast(path_rgb_image) == (shape[2], shape[0], shape[1])
    path_gray_image = str(tmp_path / "gray.png")
    io.imsave(path_gray_image, color.rgb2gray(img).astype(np.uint8))
    assert read_image_shape_fast(path_gray_image) == (1, shape[0], shape[1])


def test_split_trials():
    n_rows = 123
    train_fractions = np.arange(50, 96) / 100
    for frac in train_fractions:
        train_inds, test_inds = SplitTrials(
            range(n_rows),
            frac,
            enforce_train_fraction=True,
        )
        assert (len(train_inds) / (len(train_inds) + len(test_inds))) == frac
        train_inds = train_inds[train_inds != -1]
        test_inds = test_inds[test_inds != -1]
        assert (len(train_inds) + len(test_inds)) == n_rows


def test_format_training_data(monkeypatch):
    fake_shape = 3, 480, 640
    monkeypatch.setattr(
        trainingsetmanipulation,
        "read_image_shape_fast",
        lambda _: fake_shape,
    )
    df = pd.read_hdf(os.path.join(TEST_DATA_DIR, "trimouse_calib.h5")).xs(
        "mus1", level="individuals", axis=1
    )
    guarantee_multiindex_rows(df)
    train_inds = list(range(10))
    _, data = format_training_data(df, train_inds, 12, "")
    assert len(data) == len(train_inds)
    # Check data comprise path, shape, and xy coordinates
    assert all(len(d) == 3 for d in data)
    assert all(
        (d[0].size == 3 and d[0].dtype.char == "U" and d[0][0, -1].endswith(".png"))
        for d in data
    )
    assert all(np.all(d[1] == np.array(fake_shape)[None]) for d in data)
    assert all(
        (d[2][0, 0].shape[1] == 3 and d[2][0, 0].dtype == np.int64) for d in data
    )


def test_format_multianimal_training_data(monkeypatch):
    fake_shape = 3, 480, 640
    monkeypatch.setattr(
        multiple_individuals_trainingsetmanipulation,
        "read_image_shape_fast",
        lambda _: fake_shape,
    )
    df = pd.read_hdf(os.path.join(TEST_DATA_DIR, "trimouse_calib.h5"))
    guarantee_multiindex_rows(df)
    train_inds = list(range(10))
    n_decimals = 1
    data = format_multianimal_training_data(df, train_inds, "", n_decimals)
    assert len(data) == len(train_inds)
    assert all(isinstance(d, dict) for d in data)
    assert all(len(d["image"]) == 3 for d in data)
    assert all(np.all(d["size"] == np.array(fake_shape)) for d in data)
    assert all(
        (xy.shape[1] == 3 and np.isfinite(xy).all())
        for d in data
        for xy in d["joints"].values()
    )


@pytest.mark.parametrize(
    "videos, expected_filenames",
    [
        ([], []),
        (["/data/my-video.mov"], ["my-video"]),
        (["/data/my-video.mp4", "/data2/my-video.mov"], ["my-video"]),
        (["/data/my-video.mov", "/data/video2.mov"], ["my-video", "video2"]),
        (["/a/v1.mov", "/a/v2.mp4", "/b/v1.mov"], ["v1", "v2"]),
        (["v1.mov", "v2.mov", "v1.mov"], ["v1", "v2"]),
        (["/a/v1.mp4", "/a/v2.mov", "/b/v2.mov"], ["v1", "v2"]),
        (["/a/v1.mp4", "/a/v2.mov", "/b/v2.mov", "/b/v3.mp4"], ["v1", "v2", "v3"]),
    ],
)
def test_parse_video_filenames(videos: List[str], expected_filenames: List[str]):
    filenames = parse_video_filenames(videos)
    assert filenames == expected_filenames


--- File: tests/test_conversioncode.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import pandas as pd
from conftest import TEST_DATA_DIR
from deeplabcut.utils import conversioncode


def test_guarantee_multiindex_rows():
    df_unix = pd.read_hdf(os.path.join(TEST_DATA_DIR, "trimouse_calib.h5"))
    df_posix = df_unix.copy()
    df_posix.index = df_posix.index.str.replace("/", "\\")
    nrows = len(df_unix)
    for df in (df_unix, df_posix):
        conversioncode.guarantee_multiindex_rows(df)
        assert isinstance(df.index, pd.MultiIndex)
        assert len(df) == nrows
        assert df.index.nlevels == 3
        assert all(df.index.get_level_values(0) == "labeled-data")
        assert all(img.endswith(".png") for img in df.index.get_level_values(2))


--- File: tests/test_triangulation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import pandas as pd
import pytest
from deeplabcut.pose_estimation_3d import triangulation


@pytest.fixture(scope="session")
def stereo_params():
    params = dict()
    for i in range(1, 3):
        params[f"cameraMatrix{i}"] = np.random.rand(3, 3)
        params[f"distCoeffs{i}"] = np.random.rand(1, 5)
        params[f"P{i}"] = np.random.rand(3, 4)
        params[f"R{i}"] = np.eye(3)
    return params


def test_undistort_points(stereo_params):
    points = np.random.rand(100, 20 * 3)
    points_undistorted = triangulation._undistort_points(
        points,
        stereo_params["cameraMatrix1"],
        stereo_params["distCoeffs1"],
        stereo_params["P1"],
        stereo_params["R1"],
    )
    # Test that shape was preserved after vectorization
    assert np.shape(points_undistorted) == np.shape(points)


@pytest.mark.parametrize(
    "n_view_pairs, is_multi",
    [(i, flag) for i in range(1, 7, 2) for flag in (False, True)],
)
def test_undistort_views(n_view_pairs, is_multi, stereo_params):
    df = pd.read_hdf("tests/data/montblanc_tracks.h5")
    if not is_multi:
        df = df.xs("bird1", level="individuals", axis=1)

    view_pairs = [(df, df) for _ in range(n_view_pairs)]
    cam_params = {
        f"camera-1-camera-{i}": stereo_params for i in range(2, n_view_pairs + 2)
    }
    dfs = triangulation._undistort_views(view_pairs, cam_params)
    assert len(dfs) == n_view_pairs
    assert all(len(pair) == 2 for pair in dfs)
    assert len(dfs[0][0].columns.levels) == (4 if is_multi else 3)


--- File: tests/test_predict_supermodel.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import pytest
from deeplabcut.pose_estimation_tensorflow.modelzoo.api import superanimal_inference


def test_get_multi_scale_frames():
    fake_img = (255 * np.random.rand(600, 800, 3)).astype(np.uint8)
    ar = fake_img.shape[1] / fake_img.shape[0]
    heights = list(range(100, 1000, 100))
    frames, shapes = superanimal_inference.get_multi_scale_frames(
        fake_img,
        heights,
    )
    assert len(frames) == len(shapes) == len(heights)
    assert all(shape[0] == h for shape, h in zip(shapes, heights))
    assert all(round(shape[0] * ar) == shape[1] for shape in shapes)


@pytest.mark.parametrize("scale", [0.7, 1.5, 2])
def test_project_pred_to_original_size(scale):
    old_shape = 400, 600, 3
    new_shape = old_shape[0] // scale, old_shape[1] // scale, 3
    xs = [10, 25, 50, 100]
    conf = [[1] for _ in range(len(xs))]
    coords = [[np.array([[x, x]]) for x in xs]]
    preds = {
        "coordinates": coords,
        "confidence": conf,
    }
    preds_orig = superanimal_inference._project_pred_to_original_size(
        preds,
        old_shape,
        new_shape,
    )
    coords_orig = preds_orig["coordinates"][0]
    assert len(coords_orig) == len(xs)
    assert all([round(x * scale) == round(xy[0]) for xy, x in zip(coords_orig, xs)])


--- File: tests/test_trackingutils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import pytest
from deeplabcut.core import trackingutils


@pytest.fixture()
def ellipse():
    params = {"x": 0, "y": 0, "width": 2, "height": 4, "theta": np.pi / 2}
    return trackingutils.Ellipse(**params)


def test_ellipse(ellipse):
    assert ellipse.aspect_ratio == 2
    np.testing.assert_equal(
        ellipse.contains_points(np.asarray([[0, 0], [10, 10]])), [True, False]
    )


def test_ellipse_similarity(ellipse):
    assert ellipse.calc_similarity_with(ellipse) == 1


def test_ellipse_fitter():
    fitter = trackingutils.EllipseFitter()
    assert fitter.fit(np.random.rand(2, 2)) is None
    xy = np.asarray([[-2, 0], [2, 0], [0, 1], [0, -1]], dtype=float)
    assert fitter.fit(xy) is not None
    fitter.sd = 0
    el = fitter.fit(xy)
    assert np.isclose(el.parameters, [0, 0, 4, 2, 0]).all()


def test_ellipse_tracker(ellipse):
    tracker1 = trackingutils.EllipseTracker(ellipse.parameters)
    tracker2 = trackingutils.EllipseTracker(ellipse.parameters)
    assert tracker1.id != tracker2.id
    tracker1.update(ellipse.parameters)
    assert tracker1.hit_streak == 1
    state = tracker1.predict()
    np.testing.assert_equal(ellipse.parameters, state)
    _ = tracker1.predict()
    assert tracker1.hit_streak == 0


def test_sort_ellipse():
    tracklets = dict()
    mot_tracker = trackingutils.SORTEllipse(1, 1, 0.6)
    poses = np.random.rand(2, 10, 3)
    trackers = mot_tracker.track(poses[..., :2])
    assert trackers.shape == (2, 7)
    trackingutils.fill_tracklets(tracklets, trackers, poses, imname=0)
    assert all(id_ in tracklets for id_ in trackers[:, -2])
    assert all(np.array_equal(tracklets[n][0], pose) for n, pose in enumerate(poses))


def test_tracking_ellipse(real_assemblies, real_tracklets):
    tracklets_ref = real_tracklets.copy()
    _ = tracklets_ref.pop("header", None)
    tracklets = dict()
    mot_tracker = trackingutils.SORTEllipse(1, 1, 0.6)
    for ind, assemblies in real_assemblies.items():
        animals = np.stack([ass.data for ass in assemblies])
        trackers = mot_tracker.track(animals[..., :2])
        trackingutils.fill_tracklets(tracklets, trackers, animals, ind)
    assert len(tracklets) == len(tracklets_ref)
    assert [len(tracklet) for tracklet in tracklets.values()] == [
        len(tracklet) for tracklet in tracklets_ref.values()
    ]
    assert all(
        t.shape[1] == 4 for tracklet in tracklets.values() for t in tracklet.values()
    )


def test_box_tracker():
    bbox = 0, 0, 100, 100
    tracker1 = trackingutils.BoxTracker(bbox)
    tracker2 = trackingutils.BoxTracker(bbox)
    assert tracker1.id != tracker2.id
    tracker1.update(bbox)
    assert tracker1.hit_streak == 1
    state = tracker1.predict()
    np.testing.assert_equal(bbox, state)
    _ = tracker1.predict()
    assert tracker1.hit_streak == 0


def test_tracking_box(real_assemblies, real_tracklets):
    tracklets_ref = real_tracklets.copy()
    _ = tracklets_ref.pop("header", None)
    tracklets = dict()
    mot_tracker = trackingutils.SORTBox(1, 1, 0.1)
    for ind, assemblies in real_assemblies.items():
        animals = np.stack([ass.data for ass in assemblies])
        bboxes = trackingutils.calc_bboxes_from_keypoints(animals)
        trackers = mot_tracker.track(bboxes)
        trackingutils.fill_tracklets(tracklets, trackers, animals, ind)
    assert len(tracklets) == len(tracklets_ref)
    assert [len(tracklet) for tracklet in tracklets.values()] == [
        len(tracklet) for tracklet in tracklets_ref.values()
    ]
    assert all(
        t.shape[1] == 4 for tracklet in tracklets.values() for t in tracklet.values()
    )


def test_tracking_montblanc(
    real_assemblies_montblanc,
    real_tracklets_montblanc,
):
    tracklets_ref = real_tracklets_montblanc.copy()
    _ = tracklets_ref.pop("header", None)
    tracklets = dict()
    tracklets["single"] = real_assemblies_montblanc[1]
    mot_tracker = trackingutils.SORTEllipse(1, 1, 0.6)
    for ind, assemblies in real_assemblies_montblanc[0].items():
        animals = np.stack([ass.data for ass in assemblies])
        trackers = mot_tracker.track(animals[..., :2])
        trackingutils.fill_tracklets(tracklets, trackers, animals, ind)
    assert len(tracklets) == len(tracklets_ref)
    assert [len(tracklet) for tracklet in tracklets.values()] == [
        len(tracklet) for tracklet in tracklets_ref.values()
    ]
    for k, assemblies in tracklets.items():
        ref = tracklets_ref[k]
        for ind, data in assemblies.items():
            frame = f"frame{str(ind).zfill(3)}" if k != "single" else ind
            np.testing.assert_equal(data, ref[frame])


def test_calc_bboxes_from_keypoints():
    # Test bounding box from a single keypoint
    xy = np.asarray([[[0, 0, 1]]])
    np.testing.assert_equal(
        trackingutils.calc_bboxes_from_keypoints(xy, 10), [[-10, -10, 10, 10, 1]]
    )
    np.testing.assert_equal(
        trackingutils.calc_bboxes_from_keypoints(xy, 20, 10), [[-10, -20, 30, 20, 1]]
    )

    width = 200
    height = width * 2
    xyp = np.zeros((1, 2, 3))
    xyp[:, 1, :2] = width, height
    xyp[:, 1, 2] = 1
    with pytest.raises(ValueError):
        _ = trackingutils.calc_bboxes_from_keypoints(xyp[..., :2])

    bboxes = trackingutils.calc_bboxes_from_keypoints(xyp)
    np.testing.assert_equal(bboxes, [[0, 0, width, height, 0.5]])

    slack = 20
    bboxes = trackingutils.calc_bboxes_from_keypoints(xyp, slack=slack)
    np.testing.assert_equal(
        bboxes, [[-slack, -slack, width + slack, height + slack, 0.5]]
    )

    offset = 50
    bboxes = trackingutils.calc_bboxes_from_keypoints(xyp, offset=offset)
    np.testing.assert_equal(bboxes, [[offset, 0, width + offset, height, 0.5]])


--- File: tests/generate_training_dataset/test_trainset_metadata.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests for deeplabcut/generate_training_dataset/metadata.py"""
from __future__ import annotations
import pickle

import pytest
from ruamel.yaml import YAML

import deeplabcut.generate_training_dataset.metadata as metadata
from deeplabcut.core.engine import Engine
from deeplabcut.utils import auxiliaryfunctions

SHUFFLE_DATA = [
    {"name": "pJun17-t50s1", "index": 1, "train_fraction": 0.5, "split": 1, "engine": "torch"},
    {"name": "pJun17-t50s2", "index": 2, "train_fraction": 0.5, "split": 1, "engine": "tf"},
    {"name": "pJun17-t60s1", "index": 1, "train_fraction": 0.6, "split": 2, "engine": "torch"},
    {"name": "pJun17-t60s2", "index": 2, "train_fraction": 0.6, "split": 3, "engine": "torch"},
]
SPLITS_DATA = {
    1: {"train": [0, 1], "test": [2, 3]},
    2: {"train": [0, 1, 2], "test": [3, 4]},
    3: {"train": [4, 3, 2], "test": [1, 0]},
}

BASE_SPLIT = metadata.DataSplit(train_indices=(1, 2), test_indices=(3, 4))
# Splits that should be equal to the base
EQ_SPLIT = metadata.DataSplit(train_indices=(1, 2), test_indices=(3, 4))
# Splits that should not be equal to the base
ADD_SPLIT = metadata.DataSplit(train_indices=(1, 2, 5), test_indices=(3, 4))
ADD_SPLIT2 = metadata.DataSplit(train_indices=(1, 2), test_indices=(3, 4, 5))
SUBS_SPLIT = metadata.DataSplit(train_indices=(1, 3), test_indices=(2, 4))
DEL_SPLIT = metadata.DataSplit(train_indices=(1,), test_indices=(3, 4))
DEL_SPLIT2 = metadata.DataSplit(train_indices=(1, 2), test_indices=(3,))

SHUFFLES = {
    1: metadata.ShuffleMetadata("pJun17-t50s1", 0.5, 1, Engine.PYTORCH, BASE_SPLIT),
    2: metadata.ShuffleMetadata("pJun17-t50s2", 0.5, 2, Engine.PYTORCH, ADD_SPLIT),
    3: metadata.ShuffleMetadata("pJun17-t50s3", 0.5, 3, Engine.TF, BASE_SPLIT),
    4: metadata.ShuffleMetadata("pJun17-t50s4", 0.5, 4, Engine.PYTORCH, DEL_SPLIT),
}


@pytest.mark.parametrize(
    "data",
    [
        {
            "shuffles": {SHUFFLE_DATA[idx]["name"]: SHUFFLE_DATA[idx] for idx in [0, 1, 2]},
            "splits": {idx: SPLITS_DATA[idx] for idx in [1, 2]},
        },
        {
            "shuffles": {SHUFFLE_DATA[idx]["name"]: SHUFFLE_DATA[idx] for idx in [0]},
            "splits": {idx: SPLITS_DATA[idx] for idx in [1, 2]},
        },
    ],
)
@pytest.mark.parametrize("load_splits", [True, False])
def test_load_metadata(tmpdir, data: dict, load_splits: bool):
    """Tests that loading the metadata from files doesn't fail"""
    # write data to tmp file
    cfg, cfg_path, trainset_dir, meta_path = _create_project_with_config(tmpdir)
    with open(meta_path, "w") as f:
        YAML().dump(data, f)

    print(cfg_path)
    print(meta_path)
    print(data["shuffles"])
    print(data["splits"])
    print()

    for name, s in data["shuffles"].items():
        split = data["splits"][s["split"]]
        train, test = split["train"], split["test"]
        _create_doc_data(
            cfg, trainset_dir, s["train_fraction"], s["index"], train, test
        )

    trainset_meta = metadata.TrainingDatasetMetadata.load(
        str(cfg_path), load_splits=load_splits
    )
    for s in trainset_meta.shuffles:
        print(s)

    assert len(data["shuffles"]) == len(trainset_meta.shuffles)

    for s in trainset_meta.shuffles:
        shuffle_in = data["shuffles"][s.name]
        split_idx = data["splits"][shuffle_in["split"]]
        assert s.train_fraction == shuffle_in["train_fraction"]
        assert s.engine == Engine(shuffle_in["engine"])
        if load_splits:
            assert s.split is not None
            assert s.split.train_indices == tuple(split_idx["train"])
            assert s.split.test_indices == tuple(split_idx["test"])
        else:
            assert s.split is None
            s_with_split = s.load_split(cfg, trainset_dir)
            assert s_with_split.split.train_indices == tuple(split_idx["train"])
            assert s_with_split.split.test_indices == tuple(split_idx["test"])


@pytest.mark.parametrize("data", [
    {
        "task": "ch",
        "date": "Aug1",
        "shuffles": (SHUFFLES[1], ),
        "expected": {
            "shuffles": {
                SHUFFLES[1].name: {
                    "index": 1, "train_fraction": 0.5, "split": 1, "engine": "pytorch"
                }
            },
        }
    },
    {
        "task": "t",
        "date": "Jan1",
        "shuffles": (SHUFFLES[1], SHUFFLES[3]),
        "expected": {
            "shuffles": {
                SHUFFLES[1].name: {
                    "index": 1, "train_fraction": 0.5, "split": 1, "engine": "pytorch"
                },
                SHUFFLES[3].name: {
                    "index": 3,
                    "train_fraction": 0.5,
                    "split": 1,
                    "engine": "tensorflow",
                },
            },
        }
    },
    {
        "task": "t",
        "date": "Jan1",
        "shuffles": (SHUFFLES[1], SHUFFLES[2]),
        "expected": {
            "shuffles": {
                SHUFFLES[1].name: {
                    "index": 1, "train_fraction": 0.5, "split": 1, "engine": "pytorch"
                },
                SHUFFLES[2].name: {
                    "index": 2, "train_fraction": 0.5, "split": 2, "engine": "pytorch"
                },
            },
        },
    },
    {
        "shuffles": (SHUFFLES[1], SHUFFLES[2], SHUFFLES[3]),
        "expected": {
            "shuffles": {
                SHUFFLES[1].name: {
                    "index": 1, "train_fraction": 0.5, "split": 1, "engine": "pytorch"
                },
                SHUFFLES[2].name: {
                    "index": 2, "train_fraction": 0.5, "split": 2, "engine": "pytorch"
                },
                SHUFFLES[3].name: {
                    "index": 3,
                    "train_fraction": 0.5,
                    "split": 1,
                    "engine": "tensorflow",
                },
            },
        },
    },
])
def test_save_metadata_simple(tmpdir, data):
    """Tests that saving the metadata creates the expected file"""
    cfg, cfg_path, trainset_dir, meta_path = _create_project_with_config(tmpdir)
    trainset_meta = metadata.TrainingDatasetMetadata(cfg, data["shuffles"])
    print(trainset_meta)

    trainset_meta.save()
    with open(meta_path, "r") as f:
        meta = YAML().load(f)
    print(data)
    print(meta)
    assert data["expected"] == meta


@pytest.mark.parametrize("shuffles", [
    [SHUFFLES[i] for i in indices]
    for indices in [[1], [1, 2], [1, 2, 3], [1, 2, 4], [1, 3, 4], [1, 2, 3, 4]]
])
def test_save_metadata(tmpdir, shuffles):
    """Tests that saving the metadata and reloading it leads to the same instance"""
    cfg, cfg_path, trainset_dir, meta_path = _create_project_with_config(tmpdir)
    for s in shuffles:
        train, test = s.split.train_indices, s.split.test_indices,
        _create_doc_data(cfg, trainset_dir, s.train_fraction, s.index, train, test)

    trainset_meta = metadata.TrainingDatasetMetadata(cfg, tuple(shuffles))
    print(trainset_meta)
    trainset_meta.save()
    reloaded = metadata.TrainingDatasetMetadata.load(cfg)
    print(reloaded)
    print()

    for s in trainset_meta.shuffles:
        print(s)
    print()
    for s in reloaded.shuffles:
        print(s)
    print()
    reloaded_with_splits = [s.load_split(cfg, trainset_dir) for s in reloaded.shuffles]
    assert len(reloaded.shuffles) == len(trainset_meta.shuffles)
    assert len(reloaded_with_splits) == len(trainset_meta.shuffles)
    assert tuple(reloaded_with_splits) == trainset_meta.shuffles


def test_add_shuffle(tmpdir):
    """Tests that a shuffle can be added correctlt"""
    cfg, cfg_path, trainset_dir, meta_path = _create_project_with_config(tmpdir)
    trainset_meta = metadata.TrainingDatasetMetadata(cfg, (SHUFFLES[1], ))
    trainset_meta_added = trainset_meta.add(SHUFFLES[2])
    assert len(trainset_meta.shuffles) == 1
    assert len(trainset_meta_added.shuffles) == 2
    assert trainset_meta_added.shuffles == (SHUFFLES[1], SHUFFLES[2])


def test_add_shuffle_twice(tmpdir):
    """Tests that a shuffle can be added correctlt"""
    cfg, cfg_path, trainset_dir, meta_path = _create_project_with_config(tmpdir)
    trainset_meta = metadata.TrainingDatasetMetadata(cfg, (SHUFFLES[1], ))
    trainset_meta_added = trainset_meta.add(SHUFFLES[2])
    trainset_meta_added_2 = trainset_meta.add(SHUFFLES[2])
    assert len(trainset_meta.shuffles) == 1
    assert trainset_meta.shuffles == (SHUFFLES[1], )
    assert len(trainset_meta_added.shuffles) == len(trainset_meta_added_2.shuffles)
    assert trainset_meta_added.shuffles == trainset_meta_added_2.shuffles


def test_add_shuffle_sorts_to_correct_order(tmpdir):
    """Tests that a shuffle can be added correctlt"""
    cfg, cfg_path, trainset_dir, meta_path = _create_project_with_config(tmpdir)
    trainset_meta = metadata.TrainingDatasetMetadata(cfg, (SHUFFLES[1], SHUFFLES[3]))
    trainset_meta_added = trainset_meta.add(SHUFFLES[2])
    assert len(trainset_meta.shuffles) == 2
    assert len(trainset_meta_added.shuffles) == 3
    assert trainset_meta_added.shuffles == (SHUFFLES[1], SHUFFLES[2], SHUFFLES[3])


@pytest.mark.parametrize("shuffles", [
    indices for indices in [[1], [1, 2], [1, 2, 3], [1, 2, 4], [1, 3, 4], [1, 2, 3, 4]]
])
@pytest.mark.parametrize("shuffle_to_add", [1, 2, 3, 4])
def test_add_shuffle(tmpdir, shuffles, shuffle_to_add):
    """Tests """
    cfg, cfg_path, trainset_dir, meta_path = _create_project_with_config(tmpdir)
    trainset_meta = metadata.TrainingDatasetMetadata(
        cfg, tuple([SHUFFLES[i] for i in shuffles])
    )
    if shuffle_to_add in shuffles:
        with pytest.raises(RuntimeError):
            trainset_meta_added = trainset_meta.add(
                SHUFFLES[shuffle_to_add], overwrite=False
            )

        trainset_meta_added = trainset_meta.add(
            SHUFFLES[shuffle_to_add], overwrite=True
        )
        assert len(trainset_meta_added.shuffles) == len(shuffles)
        assert [s.index for s in trainset_meta_added.shuffles] == shuffles
    else:
        trainset_meta_added = trainset_meta.add(
            SHUFFLES[shuffle_to_add], overwrite=False
        )
        indices = [s.index for s in trainset_meta_added.shuffles]
        assert len(trainset_meta_added.shuffles) == len(shuffles) + 1
        assert indices == list(sorted(shuffles + [shuffle_to_add]))


@pytest.mark.parametrize(
    "split1, split2, equal",
    [
        (BASE_SPLIT, EQ_SPLIT, True),
        (BASE_SPLIT, ADD_SPLIT, False),
        (BASE_SPLIT, ADD_SPLIT2, False),
        (BASE_SPLIT, SUBS_SPLIT, False),
        (BASE_SPLIT, DEL_SPLIT, False),
        (BASE_SPLIT, DEL_SPLIT2, False),
    ],
)
def test_data_split_equality(split1, split2, equal):
    """Tests that equality functions as expected for DataSplits"""
    print(split1)
    print(split2)
    print(equal)
    assert (split1 == split2) == equal


@pytest.mark.parametrize("split_idx", [1, 4, 20, 1000])
@pytest.mark.parametrize("indices", [(2, 1), (10, 1), (1, 21, 20), (1, 2, 4, 3)])
@pytest.mark.parametrize("sorted_indices", [(1, 2), (10, 12), (3, 4), (1, 1000, 1200)])
def test_data_split_requires_sorted(
    split_idx: int, indices: tuple[int], sorted_indices: tuple[int]
):
    """Tests that equality functions as expected for DataSplits"""
    with pytest.raises(RuntimeError):
        metadata.DataSplit(
            train_indices=tuple(indices), test_indices=tuple(sorted_indices)
        )

    with pytest.raises(RuntimeError):
        metadata.DataSplit(
            train_indices=tuple(sorted_indices), test_indices=tuple(indices)
        )

    with pytest.raises(RuntimeError):
        metadata.DataSplit(
            train_indices=tuple(indices), test_indices=tuple(indices)
        )

    metadata.DataSplit(
        train_indices=tuple(sorted_indices), test_indices=tuple(sorted_indices)
    )


@pytest.mark.parametrize("shuffles", [
    (
        {"idx": 3, "train": [1], "test": [2], "train_fraction": 0.5},
    ),
    (
        {"idx": 1, "train": [1], "test": [2], "train_fraction": 0.5},
        {"idx": 5, "train": [1, 2, 3], "test": [4, 5], "train_fraction": 0.6},
        {"idx": 4, "train": [1, 3], "test": [2], "train_fraction": 0.66},
    ),
])
def test_create_metadata_from_shuffles(tmpdir, shuffles):
    """Tests that equality functions as expected for DataSplits"""
    cfg, cfg_path, trainset_dir, meta_path = _create_project_with_config(tmpdir)
    print(trainset_dir)
    for s in shuffles:
        doc = f"Documentation_data-ex_{s['train_fraction']}shuffle{s['idx']}.pickle"
        doc_path = trainset_dir.join(doc)
        with open(doc_path, "wb") as f:
            pickle.dump(
                [[], s["train"], s["test"], s['train_fraction']], f,
                pickle.HIGHEST_PROTOCOL
            )

    trainset_metadata = metadata.TrainingDatasetMetadata.create(cfg)
    print()
    print(trainset_metadata)
    assert len(trainset_metadata.shuffles) == len(shuffles)

    for shuffle_data, shuffle in zip(shuffles, trainset_metadata.shuffles):
        print(shuffle.index)
        assert shuffle_data["idx"] == shuffle.index
        assert shuffle_data["train_fraction"] == shuffle.train_fraction
        assert tuple(shuffle_data["train"]) == shuffle.split.train_indices
        assert tuple(shuffle_data["test"]) == shuffle.split.test_indices
    print()


def _create_project_with_config(
    tmp,
    task: str = "example",
    date: str = "Feb21",
    scorer: str = "wayneRooney",
    iteration: int = 0,
    engine: str | None = None,
):
    project_dir = tmp.mkdir("ex-ample-2024-02-21")
    cfg = {
        "Task": task,
        "date": date,
        "scorer": scorer,
        "iteration": iteration,
        "project_path": str(project_dir),
    }
    if engine is not None:
        cfg["engine"] = engine

    cfg_path = project_dir.join("config.yaml")
    with open(cfg_path, "w") as file:
        YAML().dump(cfg, file)

    it = f"iteration-{iteration}"
    dir_name = "UnaugmentedDataSet_" + task + date
    trainset_dir = project_dir.mkdir("training-datasets").mkdir(it).mkdir(dir_name)

    meta_path = trainset_dir.join("metadata.yaml")
    return cfg, cfg_path, trainset_dir, meta_path


def _create_doc_data(
    cfg,
    trainset_dir,
    train_frac,
    shuffle,
    train_indices,
    test_indices,
) -> None:
    _, doc_path = auxiliaryfunctions.get_data_and_metadata_filenames(
        trainset_dir, train_frac, shuffle, cfg
    )
    auxiliaryfunctions.save_metadata(
        doc_path, {}, list(train_indices), list(test_indices), train_frac
    )


--- File: tests/generate_training_dataset/test_trainingset_manipulation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests for deeplabcut/generate_training_dataset/metadata.py"""
from __future__ import annotations

import pytest

import deeplabcut.generate_training_dataset.trainingsetmanipulation as trainingsetmanipulation


@pytest.mark.parametrize(
    "train_fraction", [1, 2, 5, 17, 24, 29, 34, 47, 50, 53, 61, 68, 75, 90, 95, 97, 99]
)
@pytest.mark.parametrize("n_train", [1, 2, 3, 5, 7, 11, 37, 62, 153])
@pytest.mark.parametrize("n_test", [1, 2, 3, 5, 7, 13, 19, 85, 112])
def test_compute_padding(train_fraction: int, n_train: int, n_test: int) -> None:
    """
    More complete tests can be run with:
        "train_fraction": list(range(1, 100))
        "n_train": list(range(1, 200))
        "n_test": list(range(1, 200))

    This was done locally, but as it's many many tests to run a subset was selected here
    """
    train_frac = train_fraction / 100
    train_pad, test_pad = trainingsetmanipulation._compute_padding(
        train_frac, n_train, n_test
    )
    print()
    print(train_fraction, n_train, n_test, train_pad, test_pad)
    frac = round((n_train + train_pad)/(n_train + n_test + train_pad + test_pad), 2)
    assert train_frac == frac


--- File: tests/core/metrics/test_metrics_api.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""General tests for the metrics API"""
import numpy as np
import pytest
from numpy.testing import assert_almost_equal

import deeplabcut.core.metrics as metrics


def _get_gt_and_pred_with_constant_err(
    num_idv: int, num_bpt: int, error: float
) -> tuple[np.ndarray, np.ndarray]:
    gt = np.arange(num_idv * num_bpt * 3).astype(float).reshape((num_idv, num_bpt, 3))
    gt[..., 2] = 2
    predictions = gt.copy()
    predictions[..., 2] = 0.9
    predictions[..., :2] += error
    return gt, predictions


def test_computing_metrics_with_no_predictions():
    gt = np.arange(5 * 6 * 3).astype(float).reshape((5, 6, 3))
    gt[..., 2] = 2
    metrics.compute_metrics(
        ground_truth={"image": gt},
        predictions={"image": np.zeros((0, 12, 3))},
        unique_bodypart_gt=None,
        unique_bodypart_poses=None,
    )


@pytest.mark.parametrize("error", [0.5, 1, 2])
def test_computing_metrics_with_constant_error(error):
    # only works for small errors: otherwise another matching can be found
    gt, predictions = _get_gt_and_pred_with_constant_err(5, 6, error)
    results = metrics.compute_metrics(
        ground_truth={"image": gt},
        predictions={"image": predictions},
        unique_bodypart_gt=None,
        unique_bodypart_poses=None,
    )
    assert_almost_equal(results["rmse"], np.sqrt(2) * error)
    assert_almost_equal(results["rmse_pcutoff"], np.sqrt(2) * error)


@pytest.mark.parametrize("error", [0.5, 1, 2])
def test_metrics_with_unique_with_constant_error(error):
    # only works for small errors: otherwise another matching can be found
    gt, predictions = _get_gt_and_pred_with_constant_err(5, 6, error)
    gt_unique, pred_unique = _get_gt_and_pred_with_constant_err(1, 8, error)
    results = metrics.compute_metrics(
        ground_truth={"image": gt},
        predictions={"image": predictions},
        unique_bodypart_gt={"image": gt_unique},
        unique_bodypart_poses={"image": pred_unique},
    )
    assert_almost_equal(results["rmse"], np.sqrt(2) * error)
    assert_almost_equal(results["rmse_pcutoff"], np.sqrt(2) * error)


@pytest.mark.parametrize("error", [0.5, 1, 2])
def test_metrics_per_bpt_with_unique_with_constant_error(error):
    # only works for small errors: otherwise another matching can be found
    gt, predictions = _get_gt_and_pred_with_constant_err(5, 6, error)
    gt_unique, pred_unique = _get_gt_and_pred_with_constant_err(1, 8, error)
    results = metrics.compute_metrics(
        ground_truth={"image": gt},
        predictions={"image": predictions},
        unique_bodypart_gt={"image": gt_unique},
        unique_bodypart_poses={"image": pred_unique},
        per_keypoint_rmse=True,
    )
    assert_almost_equal(results["rmse"], np.sqrt(2) * error)
    assert_almost_equal(results["rmse_pcutoff"], np.sqrt(2) * error)

    for bpt_idx in range(gt.shape[1]):
        key = f"rmse_keypoint_{bpt_idx}"
        assert key in results
        assert_almost_equal(results[key], np.sqrt(2) * error)
    for bpt_idx in range(gt_unique.shape[1]):
        key = f"rmse_unique_keypoint_{bpt_idx}"
        assert key in results
        assert_almost_equal(results[key], np.sqrt(2) * error)


@pytest.mark.parametrize("error", [0.5, 1, 2])
def test_computing_metrics_single_animal(error):
    # only works for small errors: otherwise another matching can be found
    gt = np.arange(6 * 3).astype(float).reshape((1, 6, 3))
    gt[..., 2] = 2
    predictions = gt.copy()
    predictions[..., 2] = 0.9
    predictions[..., :2] += error
    results = metrics.compute_metrics(
        ground_truth={"image": gt},
        predictions={"image": predictions},
        single_animal=True,
        unique_bodypart_gt=None,
        unique_bodypart_poses=None,
    )
    assert_almost_equal(results["rmse"], np.sqrt(2) * error)
    assert_almost_equal(results["rmse_pcutoff"], np.sqrt(2) * error)



--- File: tests/core/metrics/test_metrics_map_computation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests that mAP computation is correct"""

from __future__ import annotations

import numpy as np
import pytest
from numpy.testing import assert_almost_equal

from deeplabcut.core.metrics.api import prepare_evaluation_data
from deeplabcut.core.metrics.distance_metrics import compute_oks
from deeplabcut.pose_estimation_pytorch.data.utils import bbox_from_keypoints


@pytest.mark.parametrize(
    "ground_truth",
    [
        {
            "img0": [
                [
                    [100.0, 10.0, 2],
                    [150.0, 15.0, 2],
                    [202.0, 20.0, 2],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 2],
                    [140.0, 17.0, 2],
                    [192.0, 22.0, 2],
                ],
            ],
        },
    ],
)
@pytest.mark.parametrize(
    "predictions",
    [
        {
            "img0": [
                [
                    [100.0, 10.0, 0.9],
                    [150.0, 15.0, 0.7],
                    [202.0, 20.0, 0.8],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 0.9],
                    [140.0, 17.0, 0.7],
                    [192.0, 22.0, 0.8],
                ],
                [
                    [97.0, 11.0, 0.5],
                    [148.0, 14.0, 0.2],
                    [202.0, 21.0, 0.3],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 0.9],
                    [np.nan, np.nan, 0.0],
                    [192.0, 22.0, 0.8],
                ],
                [
                    [97.0, 11.0, 0.5],
                    [148.0, 14.0, 0.2],
                    [202.0, 21.0, 0.3],
                ],
            ],
        },
    ],
)
def test_map_single_image_simple(ground_truth: dict, predictions: dict):
    gt = {k: np.array(v) for k, v in ground_truth.items()}
    pred = {k: np.array(v) for k, v in predictions.items()}
    _evaluate(gt, pred)


@pytest.mark.parametrize(
    "ground_truth",
    [
        {
            "img0": [
                [
                    [100.0, 10.0, 2],
                    [150.0, 15.0, 2],
                    [202.0, 20.0, 2],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 2],
                    [140.0, 17.0, 2],
                    [192.0, 22.0, 2],
                ],
                [
                    [726.0, 325.0, 2],
                    [326.0, 236.0, 2],
                    [457.0, 832.0, 2],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 2],
                    [140.0, 17.0, 2],
                    [192.0, 22.0, 2],
                ],
                [
                    [726.0, 325.0, 2],
                    [0.0, 0.0, 0],
                    [457.0, 832.0, 2],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 2],
                    [140.0, 17.0, 2],
                    [192.0, 22.0, 2],
                ],
                [
                    [726.0, 325.0, 2],
                    [0, 0, 0],
                    [457.0, 832.0, 2],
                ],
                [
                    [452.0, 321.0, 2],
                    [213.0, 387.0, 2],
                    [213.0, 832.0, 2],
                ],
                [
                    [253.0, 238.0, 2],
                    [213.0, 238.0, 2],
                    [457.0, 832.0, 2],
                ],
            ],
        },
    ],
)
def test_map_single_image_random_errors(ground_truth: dict):
    rng = np.random.default_rng(seed=0)

    gt = {k: np.array(v) for k, v in ground_truth.items()}
    pred = {}
    for k, gt_kpts in gt.items():
        num_idv, num_bpt = gt_kpts.shape[:2]

        error = rng.integers(low=-30, high=30, size=(num_idv, num_bpt, 2))
        scores = rng.random(size=(num_idv, num_bpt))

        pred[k] = np.zeros(shape=(num_idv, num_bpt, 3))
        pred[k][..., :2] = np.clip(gt_kpts[..., :2] + error, 0, 1024)
        pred[k][..., 2] = scores

    _evaluate(gt, pred)


@pytest.mark.parametrize("num_images", [1, 2, 5, 10])
@pytest.mark.parametrize("num_joints", [2, 5, 8, 20])
@pytest.mark.parametrize("max_error", [1, 2, 5, 20, 40])
def test_random_map_computation(num_images, num_joints, max_error):
    rng = np.random.default_rng(seed=0)

    num_individuals = rng.integers(low=0, high=20, size=(num_images, 2))

    gt, pred = {}, {}
    for i, (gt_idv, pred_idv) in enumerate(num_individuals):
        gt_kpts = 2 * np.ones((gt_idv, num_joints, 3))
        gt_kpts[..., :2] = rng.integers(low=0, high=1024, size=(gt_idv, num_joints, 2))
        gt[f"img_{i}"] = gt_kpts

        # create predictions array
        pred_kpts = np.zeros((pred_idv, num_joints, 3))
        # set scores
        pred_kpts[..., 2] = rng.random(size=(pred_idv, num_joints))

        # predictions that are ground truth + error
        matched = min(gt_idv, pred_idv)
        if matched > 0:
            error = rng.integers(
                low=-max_error, high=max_error, size=(matched, num_joints, 2)
            )
            matched_pred = gt_kpts[:matched, :, :2] + error
            pred_kpts[:matched, :, :2] = np.clip(matched_pred, 0, 1024)

        # random predictions
        unmatched = pred_idv - matched
        if unmatched > 0:
            pred_kpts[matched:, :, :2] = rng.integers(
                low=0, high=1024, size=(unmatched, num_joints, 2)
            )

        pred[f"img_{i}"] = pred_kpts

    _evaluate(gt, pred)


@pytest.mark.parametrize("num_images", [1, 2, 5, 10])
@pytest.mark.parametrize("num_joints", [2, 5, 8, 20])
@pytest.mark.parametrize("max_error", [1, 2, 5, 20, 40])
def test_random_map_computation_with_missing_kpts(num_images, num_joints, max_error):
    rng = np.random.default_rng(seed=0)
    num_individuals = rng.integers(low=0, high=20, size=(num_images, 2))

    gt, pred = {}, {}
    for i, (gt_idv, pred_idv) in enumerate(num_individuals):
        gt_kpts = 2 * np.ones((gt_idv, num_joints, 3))
        gt_kpts[..., :2] = rng.integers(low=0, high=1024, size=(gt_idv, num_joints, 2))
        gt[f"img_{i}"] = gt_kpts

        # drop some ground truth keypoints
        gt_vis_mask = rng.random(size=(gt_idv, num_joints)) < 0.2
        gt_kpts[gt_vis_mask, 2] = 0

        # generate predicted keypoints
        pred_kpts = np.zeros((pred_idv, num_joints, 3))
        pred_kpts[:pred_idv, :, 2] = rng.random(size=(pred_idv, num_joints))

        # predictions that are ground truth + error
        matched = min(gt_idv, pred_idv)
        if matched > 0:
            error = rng.integers(
                low=-max_error, high=max_error, size=(matched, num_joints, 2)
            )
            matched_pred = gt_kpts[:matched, :, :2] + error
            pred_kpts[:matched, :, :2] = np.clip(matched_pred, 0, 1024)

        # random predictions
        unmatched = pred_idv - matched
        if unmatched > 0:
            pred_kpts[matched:, :, :2] = rng.integers(
                low=0, high=1024, size=(unmatched, num_joints, 2)
            )

        pred[f"img_{i}"] = pred_kpts

    _evaluate(gt, pred)


def _evaluate(gt: dict[str, np.ndarray], pred: dict[str, np.ndarray]):
    for k, v in gt.items():
        print(20 * "-")
        print(k)
        print("GT")
        print(v)
        print("PR")
        print(pred[k])

    data = prepare_evaluation_data(gt, pred)
    oks = compute_oks(data, oks_bbox_margin=0)

    num_joints = gt[list(gt.keys())[0]].shape[1]
    coco_gt = _to_coco_ground_truth(gt, num_joints, bbox_margin=0)
    coco_pred = _to_coco_predictions(coco_gt, pred, bbox_margin=0)
    coco_oks = eval_coco(coco_gt, coco_pred, num_joints)
    print(20 * "-")
    print(f"dlc mAP:")
    for k, v in oks.items():
        print(k)
        print(v)
    print(20 * "-")
    print(f"pycocotools mAP: {coco_oks}")
    print()
    dlc_map = oks["mAP"] / 100
    assert_almost_equal(dlc_map, coco_oks)


def _to_coco_ground_truth(
    data: dict[str, np.ndarray],
    num_joints: int,
    bbox_margin: int = 0,
    image_size: tuple[int, int] = (1024, 1024),
) -> dict[str, list[dict]]:
    w, h = image_size
    anns, images = [], []
    for path, image_keypoints in data.items():
        id_ = len(images) + 1
        images.append(dict(id=id_, file_name=path, width=w, height=h))

        assert image_keypoints.shape[1] == num_joints
        for idv_id, kpts in enumerate(image_keypoints):
            visible = kpts[:, 2] > 0
            num_keypoints = visible.sum()

            if num_keypoints > 1:
                bbox = bbox_from_keypoints(
                    keypoints=kpts,
                    image_h=h,
                    image_w=w,
                    margin=bbox_margin,
                )
                area = bbox[2].item() * bbox[3].item()
                anns.append(
                    {
                        "id": len(anns) + 1,
                        "image_id": id_,
                        "category_id": 1,
                        "area": area,
                        "bbox": bbox.tolist(),
                        "keypoints": kpts.reshape(-1).tolist(),
                        "iscrowd": 0,
                        "num_keypoints": num_keypoints,
                    }
                )

    keypoints = [f"bpt{i}" for i in range(num_joints)]
    category = dict(id=1, name="animal", supercategory="animal", keypoints=keypoints)
    return {"annotations": anns, "categories": [category], "images": images}


def _to_coco_predictions(
    ground_truth: dict,
    predictions: dict[str, np.ndarray],
    bbox_margin: int = 0,
    image_size: tuple[int, int] = (1024, 1024),
) -> list[dict]:
    w, h = image_size
    num_joints = len(ground_truth["categories"][0]["keypoints"])
    path_to_id = {img["file_name"]: img["id"] for img in ground_truth["images"]}

    coco_predictions = []
    for path, image_keypoints in predictions.items():
        assert image_keypoints.shape[1] == num_joints

        img_id = path_to_id[path]
        valid_predictions = [
            kpt for kpt in image_keypoints if np.any(np.all(~np.isnan(kpt), axis=-1))
        ]
        for kpts in valid_predictions:
            score = float(np.nanmean(kpts[:, 2]).item())
            kpts = kpts.copy()
            kpts[:, 2] = 2

            # NaN predictions to infinity
            kpts[np.isnan(kpts)] = np.inf

            bbox = bbox_from_keypoints(
                keypoints=kpts,
                image_h=h,
                image_w=w,
                margin=bbox_margin,
            )
            area = bbox[2].item() * bbox[3].item()
            coco_predictions.append(
                {
                    "image_id": img_id,
                    "category_id": 1,
                    "keypoints": kpts.reshape(-1).tolist(),
                    "bbox": bbox.tolist(),
                    "area": area,
                    "score": score,
                }
            )

    return coco_predictions


def eval_coco(
    ground_truth: dict,
    predictions: list[dict],
    num_joints: int,
) -> float | None:
    try:
        from pycocotools.coco import COCO
        from pycocotools.cocoeval import COCOeval

        coco = COCO()
        coco.dataset["annotations"] = ground_truth["annotations"]
        coco.dataset["categories"] = ground_truth["categories"]
        coco.dataset["images"] = ground_truth["images"]
        coco.createIndex()

        coco_det = coco.loadRes(predictions)
        coco_eval = COCOeval(coco, coco_det, iouType="keypoints")
        coco_eval.params.kpt_oks_sigmas = np.array(num_joints * [0.1])
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()
        return float(coco_eval.stats[0])

    except ModuleNotFoundError as err:
        print(f"pycocotools is not installed")


--- File: tests/core/metrics/test_metrics_rmse_computation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests RMSE computation"""
import numpy as np
import pytest
from numpy.testing import assert_almost_equal

from deeplabcut.core.metrics.distance_metrics import (
    compute_detection_rmse,
    compute_rmse,
)


@pytest.mark.parametrize(
    "gt, pred, result",
    [
        (
            [  # ground truth pose
                [[100.0, 10.0, 2], [150.0, 15.0, 2], [200.0, 20.0, 2]],
            ],
            [  # predicted pose
                [[100.0, 10.0, 0.9], [150.0, 15.0, 0.8], [200.0, 20.0, 0.8]],
            ],
            (0, 0),
        ),
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [10.0, 10.0, 2], [10.0, 10.0, 2]],
                [[20.0, 20.0, 2], [20.0, 20.0, 2], [20.0, 20.0, 2]],
            ],
            [  # predicted pose
                [[12.0, 10.0, 0.9], [12.0, 10.0, 0.9], [12.0, 10.0, 0.9]],
                [[22.0, 20.0, 0.9], [22.0, 20.0, 0.9], [22.0, 20.0, 0.9]],
            ],
            (2, 2),
        ),
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [10.0, 10.0, 2], [10.0, 10.0, 2]],
                [[20.0, 20.0, 2], [20.0, 20.0, 2], [20.0, 20.0, 2]],
            ],
            [  # predicted pose
                [[10.0, 12.0, 0.9], [10.0, 12.0, 0.9], [10.0, 12.0, 0.9]],
                [[20.0, 22.0, 0.9], [20.0, 22.0, 0.9], [20.0, 22.0, 0.9]],
            ],
            (2, 2),
        ),
    ],
)
def test_rmse_single_image(gt: list, pred: list, result: tuple[float, float]):
    data = [(np.asarray(gt), np.asarray(pred))]
    computed_results = compute_rmse(data, False, pcutoff=0.6, oks_bbox_margin=10.0)
    rmse, rmse_cutoff = computed_results["rmse"], computed_results["rmse_pcutoff"]
    expected_rmse, expected_rmse_cutoff = result
    assert_almost_equal(rmse, expected_rmse)
    assert_almost_equal(rmse_cutoff, expected_rmse_cutoff)


@pytest.mark.parametrize(
    "gt, pred, result",
    [
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [10.0, 10.0, 2], [10.0, 10.0, 2]],
                [[20.0, 20.0, 2], [20.0, 20.0, 2], [20.0, 20.0, 2]],
            ],
            [  # predicted pose
                [[10.0, 10.0, 0.9], [10.0, 10.0, 0.9], [10.0, 10.0, 0.9]],
                [[20.0, 22.0, 0.2], [20.0, 22.0, 0.2], [20.0, 22.0, 0.2]],
            ],
            (1, 0),  # 2 pixel error on half of keypoints, 0 on the other half
        ),
    ],
)
def test_rmse_pcutoff(gt: list, pred: list, result: tuple[float, float]):
    data = [(np.asarray(gt), np.asarray(pred))]
    expected_rmse, expected_rmse_cutoff = result

    computed_results = compute_rmse(data, False, pcutoff=0.6, oks_bbox_margin=10.0)
    rmse, rmse_cutoff = computed_results["rmse"], computed_results["rmse_pcutoff"]
    assert_almost_equal(rmse, expected_rmse)
    assert_almost_equal(rmse_cutoff, expected_rmse_cutoff)


@pytest.mark.parametrize(
    "gt, pred, result",
    [
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [float("nan"), float("nan"), 0], [10.0, 10.0, 2]],
            ],
            [  # predicted pose
                [[12.0, 10.0, 0.9], [10.0, 10.0, 0.4], [10.0, 10.0, 0.9]],
            ],
            (1, 1),  # only 2 valid ground truth bodyparts
        ),
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [10.0, 10.0, 2], [float("nan"), float("nan"), 0]],
                [[float("nan"), float("nan"), 0], [20.0, 20.0, 2], [20.0, 20.0, 2]],
            ],
            [  # predicted pose, swapped prediction order
                [[20.0, 20.0, 0.9], [21.0, 20.0, 0.9], [21.0, 20.0, 0.9]],
                [[15.0, 10.0, 0.4], [15.0, 10.0, 0.4], [10.0, 10.0, 0.9]],
            ],
            (3, 1),  # only 2 valid GT bodyparts
        ),
    ],
)
def test_rmse_with_nans(gt: list, pred: list, result: tuple[float, float]):
    data = [(np.asarray(gt), np.asarray(pred))]
    expected_rmse, expected_rmse_cutoff = result

    results = compute_rmse(data, False, pcutoff=0.6, oks_bbox_margin=10.0)
    rmse, rmse_cutoff = results["rmse"], results["rmse_pcutoff"]
    assert_almost_equal(rmse, expected_rmse)
    assert_almost_equal(rmse_cutoff, expected_rmse_cutoff)


@pytest.mark.parametrize(
    "gt, pred, data_unique, result",
    [
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [np.nan, np.nan, 0], [10.0, 10.0, 2]],
            ],
            [  # predicted pose
                [[12.0, 10.0, 0.9], [10.0, 10.0, 0.4], [10.0, 10.0, 0.9]],
            ],
            None, # unique data
            (1, 1),  # error 2 on one, 0 on the other; only 2 valid GT
        ),
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [20.0, 20.0, 2], [30.0, 30.0, 2]],
                [[40.0, 40.0, 2], [50.0, 50.0, 2], [60.0, 60.0, 2]],
            ],
            [  # predicted pose, perfect detections but mis-assembled
                [[10.0, 10.0, 0.9], [50.0, 50.0, 0.9], [30.0, 30.0, 0.9]],
                [[40.0, 40.0, 0.9], [20.0, 20.0, 0.4], [60.0, 60.0, 0.9]],
            ],
            None, # unique data
            (0, 0),  # all pose perfect
        ),
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [20.0, 20.0, 2], [30.0, 30.0, 2]],
                [[40.0, 40.0, 2], [50.0, 50.0, 2], [60.0, 60.0, 2]],
            ],
            [  # predicted pose, small error in pose and mis-assembled
                [[12.0, 10.0, 0.9], [52.0, 50.0, 0.9], [32.0, 30.0, 0.9]],
                [[42.0, 40.0, 0.9], [18.0, 20.0, 0.4], [62.0, 60.0, 0.9]],
            ],
            None, # unique data
            (2, 2),  # pixel error of 2 on x-axis for all predictions
        ),
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [20.0, 20.0, 2], [30.0, 30.0, 2]],
                [[40.0, 40.0, 2], [50.0, 50.0, 2], [60.0, 60.0, 2]],
            ],
            [  # predicted pose, small error in low-conf pose and mis-assembled
                [[12.0, 10.0, 0.4], [50.0, 50.0, 0.9], [30.0, 30.0, 0.9]],
                [[40.0, 40.0, 0.9], [22.0, 20.0, 0.4], [62.0, 60.0, 0.4]],
            ],
            None, # unique data
            (1, 0),  # error of 2 on half, 0 on the other half (with good conf)
        ),
        (  # more ground truth than detections
            [  # ground truth pose
                [[10.0, 10.0, 2], [20.0, 20.0, 2], [30.0, 30.0, 2]],
                [[40.0, 40.0, 2], [50.0, 50.0, 2], [60.0, 60.0, 2]],
                [[70.0, 70.0, 2], [80.0, 80.0, 2], [90.0, 90.0, 2]],
            ],
            [  # predicted pose, no error
                [[70.0, 70.0, 2], [80.0, 80.0, 2], [90.0, 90.0, 2]],
                [[40.0, 40.0, 2], [50.0, 50.0, 2], [60.0, 60.0, 2]],
            ],
            None, # unique data
            (0, 0),
        ),
        (  # more detections than GT
            [  # ground truth pose
                [[70.0, 70.0, 2], [80.0, 80.0, 2], [90.0, 90.0, 2]],
                [[40.0, 40.0, 2], [50.0, 50.0, 2], [60.0, 60.0, 2]],
            ],
            [  # predicted pose, no error
                [[10.0, 10.0, 2], [20.0, 20.0, 2], [30.0, 30.0, 2]],
                [[40.0, 40.0, 2], [50.0, 50.0, 2], [60.0, 60.0, 2]],
                [[70.0, 70.0, 2], [80.0, 80.0, 2], [90.0, 90.0, 2]],
            ],
            None, # unique data
            (0, 0),
        ),
        (
                [  # ground truth pose
                    [[10.0, 10.0, 2], [np.nan, np.nan, 0], [10.0, 10.0, 2]],
                ],
                [  # predicted pose
                    [[12.0, 10.0, 0.9], [10.0, 10.0, 0.4], [10.0, 10.0, 0.9]],
                ],
                (  # unique data
                        [[[20, 20, 2], [22, 23, 2]]],
                        [[[20, 20, 0.8], [22, 23, 0.7]]]
                ),
                (0.5, 0.5),  # error 2 on one, 0 on the other; only 2 valid GT
        ),
        (
                [  # ground truth pose
                    [[10.0, 10.0, 2], [20.0, 20.0, 2], [30.0, 30.0, 2]],
                    [[40.0, 40.0, 2], [50.0, 50.0, 2], [60.0, 60.0, 2]],
                ],
                [  # predicted pose, perfect detections but mis-assembled
                    [[10.0, 10.0, 0.9], [50.0, 50.0, 0.9], [30.0, 30.0, 0.9]],
                    [[40.0, 40.0, 0.9], [20.0, 20.0, 0.4], [60.0, 60.0, 0.9]],
                ],
                (  # unique data
                        [], # missing ground truth for unique bodyparts
                        [[[20, 20, 0.8], [22, 23, 0.7]]]
                ),
                (0, 0),  # all pose perfect
        ),
    ],
)
def test_detection_rmse(gt: list, pred: list, data_unique:tuple[list, list]|None, result: tuple[float, float]):
    data = [(np.asarray(gt), np.asarray(pred))]
    data_unique = [(np.asarray(data_unique[0]), np.asarray(data_unique[1]))] if data_unique else None
    expected_rmse, expected_rmse_cutoff = result
    rmse, rmse_cutoff = compute_detection_rmse(data, pcutoff=0.6, data_unique=data_unique)
    assert_almost_equal(rmse, expected_rmse)
    assert_almost_equal(rmse_cutoff, expected_rmse_cutoff)


@pytest.mark.parametrize(
    "gt, pred, unique_gt, unique_pred, result",
    [
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [10.0, 10.0, 2], [10.0, 10.0, 2]],
                [[20.0, 20.0, 2], [20.0, 20.0, 2], [20.0, 20.0, 2]],
            ],
            [  # predicted pose
                [[10.0, 10.0, 0.9], [10.0, 10.0, 0.9], [10.0, 10.0, 0.9]],
                [[20.0, 24.0, 0.2], [20.0, 24.0, 0.2], [20.0, 20.0, 0.2]],
            ],
            [  # Unique GT
                [[10.0, 10.0, 2], [10.0, 10.0, 2]],
            ],
            [  # Unique Pred
                [[10.0, 10.0, 0.9], [10.0, 10.0, 0.9]],
            ],
            # 4 pixel error on 2 keypoints, 0 error on 5 keypoints
            (1.0, 0.0),
        ),
        (
            [np.zeros((0, 3, 2))],  # no GT pose
            [  # predicted pose
                [[10.0, 10.0, 0.9], [10.0, 10.0, 0.9], [10.0, 10.0, 0.9]],
            ],
            [  # Unique GT
                [[10.0, 10.0, 2], [10.0, 10.0, 2]],
            ],
            [  # Unique Pred
                [[15.0, 10.0, 0.5], [11.0, 10.0, 0.9]],
            ],
            # 5 pixel error on 1 keypoint, 1 pixel error on the other
            (3.0, 1.0),
        ),
    ],
)
def test_rmse_with_unique(
    gt: list,
    pred: list,
    unique_gt: list,
    unique_pred: list,
    result: tuple[float, float]
) -> None:
    data = [(np.asarray(gt), np.asarray(pred))]
    data_unique = [(np.asarray(unique_gt), np.asarray(unique_pred))]
    expected_rmse, expected_rmse_cutoff = result

    results = compute_rmse(
        data, False, pcutoff=0.6, data_unique=data_unique, oks_bbox_margin=10.0,
    )
    rmse, rmse_cutoff = results["rmse"], results["rmse_pcutoff"]
    assert_almost_equal(rmse, expected_rmse)
    assert_almost_equal(rmse_cutoff, expected_rmse_cutoff)


@pytest.mark.parametrize(
    "gt, pred, unique_gt, unique_pred, result",
    [
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [10.0, 10.0, 2], [10.0, 10.0, 2]],
                [[20.0, 20.0, 2], [20.0, 20.0, 2], [20.0, 20.0, 2]],
            ],
            [  # predicted pose
                [[10.0, 10.0, 0.9], [10.0, 10.0, 0.9], [10.0, 10.0, 0.9]],
                [[20.0, 24.0, 0.2], [20.0, 24.0, 0.2], [20.0, 20.0, 0.2]],
            ],
            [  # Unique GT
                [[10.0, 10.0, 2], [10.0, 10.0, 2]],
            ],
            [  # Unique Pred
                [[10.0, 10.0, 0.9], [10.0, 10.0, 0.9]],
            ],
            # 4 pixel error on 2 keypoints, 0 error on 5 keypoints
            [
                (1.0, 0.0),
                [2.0, 2.0, 0.0],
                [0.0, 0.0]
            ],
        ),
        (
            [  # ground truth pose
                [[10.0, 10.0, 2], [10.0, 10.0, 2], [10.0, 10.0, 2]],
                [[20.0, 20.0, 2], [20.0, 20.0, 2], [20.0, 20.0, 2]],
            ],
            [  # predicted pose
                [[10.0, 12.0, 0.9], [10.0, 10.0, 0.9], [10.0, 10.0, 0.9]],
                [[20.0, 24.0, 0.7], [20.0, 24.0, 0.6], [20.0, 20.0, 0.8]],
            ],
            [  # Unique GT
                [[10.0, 10.0, 2], [10.0, 10.0, 2]],
            ],
            [  # Unique Pred
                [[12.0, 10.0, 0.9], [11.0, 10.0, 0.9]],
            ],
            [  # errors: 3 with 0px, 1 with 1px, 2 with 2px, 2 with 4px => 13/8
                (1.625, 1.625),
                [3.0, 2.0, 0.0],
                [2.0, 1.0]
            ],
        ),
    ],
)
def test_rmse_per_bodypart_with_unique(
    gt: list,
    pred: list,
    unique_gt: list,
    unique_pred: list,
    result: tuple[tuple[float, float], list[float], list[float]]
) -> None:
    data = [(np.asarray(gt), np.asarray(pred))]
    data_unique = [(np.asarray(unique_gt), np.asarray(unique_pred))]
    expected_rmse, expected_rmse_cutoff = result[0]
    bodypart_rmse = result[1]
    unique_rmse = result[2]

    results = compute_rmse(
        data,
        single_animal=False,
        pcutoff=0.6,
        data_unique=data_unique,
        per_keypoint_results=True,
        oks_bbox_margin=10.0,
    )
    assert_almost_equal(results["rmse"], expected_rmse)
    assert_almost_equal(results["rmse_pcutoff"], expected_rmse_cutoff)
    for bpt_index, bpt_rmse in enumerate(bodypart_rmse):
        key = f"rmse_keypoint_{bpt_index}"
        assert key in results
        assert_almost_equal(results[key], bpt_rmse)

    for bpt_index, bpt_rmse in enumerate(unique_rmse):
        key = f"rmse_unique_keypoint_{bpt_index}"
        assert key in results
        assert_almost_equal(results[key], bpt_rmse)


--- File: tests/core/metrics/test_meitrcs_identity_accuracy.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests for the scoring methods"""
import numpy as np
import pytest

import deeplabcut.core.metrics.identity


@pytest.mark.parametrize(
    "data",
    [
        {
            "individuals": ["i1", "i2"],
            "bodyparts": ["arm"],
            "predictions": {
                "img0.png": [  # (num_assemblies, num_bodyparts, 3)
                    [[2.0, 2.0, 0.8]],
                    [[1.0, 1.0, 0.7]],  # x, y, score
                ],
            },
            "identity_scores": {
                "img0.png": [  # (num_assemblies, num_bodyparts, num_individuals)
                    [[0.8, 0.5]],
                    [[0.51, 0.49]],
                ],
            },
            "ground_truth": {
                "img0.png": [  # (num_individuals, num_bodyparts, 3)
                    [[1.0, 1.0, 2]],
                    [[0, 0, 0]],  # x, y, visibility
                ]
            },
            "accuracy": {
                "arm_accuracy": 1.0,
            },
        },
        {
            "individuals": ["i1", "i2"],
            "bodyparts": ["arm"],
            "predictions": {
                "img0.png": [  # (num_assemblies, num_bodyparts, 3)
                    [[1.0, 1.0, 0.7]],
                    [[2.0, 2.0, 0.7]],  # x, y, score
                ],
            },
            "identity_scores": {
                "img0.png": [  # (num_assemblies, num_bodyparts, num_individuals)
                    [[0.4, 0.6]],
                    [[0.6, 0.4]],
                ],
            },
            "ground_truth": {
                "img0.png": [  # (num_individuals, num_bodyparts, 3)
                    [[2.0, 2.0, 2]],
                    [[1.0, 1.0, 2]],  # x, y, visibility
                ]
            },
            "accuracy": {
                "arm_accuracy": 1.0,
            },
        },
        {
            "individuals": ["i1", "i2"],
            "bodyparts": ["arm"],
            "predictions": {
                "img0.png": [  # (num_assemblies, num_bodyparts, 3)
                    [[1.0, 1.0, 0.7]],
                    [[2.0, 2.0, 0.7]],  # x, y, score
                ],
            },
            "identity_scores": {
                "img0.png": [  # (num_assemblies, num_bodyparts, num_individuals)
                    [[0.6, 0.4]],
                    [[0.6, 0.4]],  # both assemblies assigned to idv 1
                ],
            },
            "ground_truth": {
                "img0.png": [  # (num_individuals, num_bodyparts, 3)
                    [[2.0, 2.0, 2]],
                    [[1.0, 1.0, 2]],  # x, y, visibility
                ]
            },
            "accuracy": {
                "arm_accuracy": 0.5,
            },
        },
        {
            "individuals": ["i1", "i2"],
            "bodyparts": ["arm"],
            "predictions": {
                "img0.png": [  # (num_assemblies, num_bodyparts, 3)
                    [[1.0, 1.0, 0.7]],
                    [[2.0, 2.0, 0.7]],  # x, y, score
                ],
            },
            "identity_scores": {
                "img0.png": [  # (num_assemblies, num_bodyparts, num_individuals)
                    [[0.6, 0.4]],
                    [[0.4, 0.6]],  # both assigned to wrong ID
                ],
            },
            "ground_truth": {
                "img0.png": [  # (num_individuals, num_bodyparts, 3)
                    [[2.0, 2.0, 2]],  # x, y, visibility
                    [[1.0, 1.0, 2]],
                ]
            },
            "accuracy": {
                "arm_accuracy": 0.0,
            },
        },
        {
            "individuals": ["i1", "i2"],
            "bodyparts": ["arm", "leg"],
            "predictions": {
                "img0.png": [  # (num_assemblies, num_bodyparts, 3)
                    [[1.0, 1.0, 0.7], [10.0, 10.0, 0.9]],
                    [[100.0, 100.0, 0.9], [90.0, 90.9, 0.8]],
                ],
            },
            "identity_scores": {
                "img0.png": [  # (num_assemblies, num_bodyparts, num_individuals)
                    [[0.7, 0.3], [0.6, 0.2]],
                    [[0.6, 0.3], [0.6, 0.2]],  # should not matter, not assigned to GT
                ],
            },
            "ground_truth": {
                "img0.png": [  # (num_individuals, num_bodyparts, 3)
                    [[2.0, 2.0, 2], [8.0, 8.0, 2]],  # x, y, visibility
                    [[-1, -1, 0.0], [-1, -1, 0.0]],  # not visible
                ]
            },
            "accuracy": {
                "arm_accuracy": 1.0,
                "leg_accuracy": 1.0,
            },
        },
        {
            "individuals": ["i1", "i2", "i3"],
            "bodyparts": ["arm", "leg"],
            "predictions": {
                "img0.png": [  # (num_assemblies, num_bodyparts, 3)
                    [[1.0, 1.0, 0.7], [10.0, 10.0, 0.9]],
                    [[100.0, 100.0, 0.9], [90.0, 90.9, 0.8]],
                    [[110.0, 110.0, 0.9], [98.0, 91.9, 0.8]],
                ],
            },
            "identity_scores": {
                "img0.png": [  # (num_assemblies, num_bodyparts, num_individuals)
                    [[0.7, 0.3], [0.6, 0.2]],  # assigned to correct ID
                    [[0.6, 0.3], [0.6, 0.2]],  # should not matter, not assigned to GT
                    [[0.6, 0.3], [0.6, 0.2]],  # should not matter, not assigned to GT
                ],
            },
            "ground_truth": {
                "img0.png": [  # (num_individuals, num_bodyparts, 3)
                    [[2.0, 2.0, 2], [8.0, 8.0, 2]],  # x, y, visibility
                    [[-1, -1, 0.0], [-1, -1, 0.0]],  # not visible
                    [[-1, -1, 0.0], [-1, -1, 0.0]],  # not visible
                ]
            },
            "accuracy": {
                "arm_accuracy": 1.0,
                "leg_accuracy": 1.0,
            },
        },
        {
            "individuals": ["i1", "i2", "i3"],
            "bodyparts": ["arm", "leg"],
            "predictions": {
                "img0.png": [  # (num_assemblies, num_bodyparts, 3)
                    [[1.0, 1.0, 0.7], [10.0, 10.0, 0.9]],
                    [[100.0, 100.0, 0.9], [90.0, 90.9, 0.8]],
                    [[110.0, 110.0, 0.9], [98.0, 91.9, 0.8]],
                ],
            },
            "identity_scores": {
                "img0.png": [  # (num_assemblies, num_bodyparts, num_individuals)
                    [[0.7, 0.3, 0.1], [0.6, 0.2, 0.1]],  # assigned to correct ID
                    [[0.1, 0.2, 0.7], [0.4, 0.3, 0.2]],  # 1st correct, 2nd wrong
                    [
                        [0.6, 0.3, 0.5],
                        [0.6, 0.2, 0.4],
                    ],  # should not matter, not assigned to GT
                ],
            },
            "ground_truth": {
                "img0.png": [  # (num_individuals, num_bodyparts, 3)
                    [[2.0, 2.0, 2], [8.0, 8.0, 2]],  # x, y, visibility
                    [[-1, -1, 0.0], [-1, -1, 0.0]],  # not visible
                    [[90.0, 90, 2], [80, 80, 2.0]],  # x, y, visibility
                ]
            },
            "accuracy": {
                "arm_accuracy": 1.0,
                "leg_accuracy": 0.5,
            },
        },
    ],
)
def test_id_accuracy(data) -> None:
    scores = deeplabcut.core.metrics.identity.compute_identity_scores(
        individuals=data["individuals"],
        bodyparts=data["bodyparts"],
        predictions={k: np.array(v) for k, v in data["predictions"].items()},
        identity_scores={k: np.array(v) for k, v in data["identity_scores"].items()},
        ground_truth={k: np.array(v) for k, v in data["ground_truth"].items()},
    )
    assert scores == data["accuracy"]


--- File: tests/core/inferenceutils/test_map_computation.py ---
"""Tests mAP computation from inferenceutils"""

from __future__ import annotations

import numpy as np
import pytest

from deeplabcut.core import inferenceutils
from deeplabcut.pose_estimation_pytorch.data.utils import bbox_from_keypoints


@pytest.mark.parametrize(
    "ground_truth",
    [
        {
            "img0": [
                [
                    [100.0, 10.0, 2],
                    [150.0, 15.0, 2],
                    [202.0, 20.0, 2],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 2],
                    [140.0, 17.0, 2],
                    [192.0, 22.0, 2],
                ],
            ],
        },
    ],
)
@pytest.mark.parametrize(
    "predictions",
    [
        {
            "img0": [
                [
                    [100.0, 10.0, 0.9],
                    [150.0, 15.0, 0.7],
                    [202.0, 20.0, 0.8],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 0.9],
                    [140.0, 17.0, 0.7],
                    [192.0, 22.0, 0.8],
                ],
                [
                    [97.0, 11.0, 0.5],
                    [148.0, 14.0, 0.2],
                    [202.0, 21.0, 0.3],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 0.9],
                    [np.nan, np.nan, 0.0],
                    [192.0, 22.0, 0.8],
                ],
                [
                    [97.0, 11.0, 0.5],
                    [148.0, 14.0, 0.2],
                    [202.0, 21.0, 0.3],
                ],
            ],
        },
    ],
)
def test_map_single_image_simple(ground_truth: dict, predictions: dict):
    gt = {k: np.array(v) for k, v in ground_truth.items()}
    pred = {k: np.array(v) for k, v in predictions.items()}
    _evaluate(gt, pred)


@pytest.mark.parametrize(
    "ground_truth",
    [
        {
            "img0": [
                [
                    [100.0, 10.0, 2],
                    [150.0, 15.0, 2],
                    [202.0, 20.0, 2],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 2],
                    [140.0, 17.0, 2],
                    [192.0, 22.0, 2],
                ],
                [
                    [726.0, 325.0, 2],
                    [326.0, 236.0, 2],
                    [457.0, 832.0, 2],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 2],
                    [140.0, 17.0, 2],
                    [192.0, 22.0, 2],
                ],
                [
                    [726.0, 325.0, 2],
                    [0.0, 0.0, 0],
                    [457.0, 832.0, 2],
                ],
            ],
        },
        {
            "img0": [
                [
                    [90.0, 12.0, 2],
                    [140.0, 17.0, 2],
                    [192.0, 22.0, 2],
                ],
                [
                    [726.0, 325.0, 2],
                    [0, 0, 0],
                    [457.0, 832.0, 2],
                ],
                [
                    [452.0, 321.0, 2],
                    [213.0, 387.0, 2],
                    [213.0, 832.0, 2],
                ],
                [
                    [253.0, 238.0, 2],
                    [213.0, 238.0, 2],
                    [457.0, 832.0, 2],
                ],
            ],
        },
    ],
)
def test_map_single_image_random_errors(ground_truth: dict):
    rng = np.random.default_rng(seed=0)

    gt = {k: np.array(v) for k, v in ground_truth.items()}
    pred = {}
    for k, gt_kpts in gt.items():
        num_idv, num_bpt = gt_kpts.shape[:2]

        error = rng.integers(low=-30, high=30, size=(num_idv, num_bpt, 2))
        scores = rng.random(size=(num_idv, num_bpt))

        pred[k] = np.zeros(shape=(num_idv, num_bpt, 3))
        pred[k][..., :2] = np.clip(gt_kpts[..., :2] + error, 0, 1024)
        pred[k][..., 2] = scores

    _evaluate(gt, pred)


@pytest.mark.parametrize("num_images", [1, 2, 5, 10])
@pytest.mark.parametrize("num_joints", [2, 5, 8, 20])
@pytest.mark.parametrize("max_error", [1, 2, 5, 20, 40])
def test_random_map_computation(num_images, num_joints, max_error):
    rng = np.random.default_rng(seed=0)

    num_individuals = rng.integers(low=0, high=20, size=(num_images, 2))
    max_idv = num_individuals.max(initial=0)

    gt = {}
    pred = {}
    for i, (gt_idv, pred_idv) in enumerate(num_individuals):
        # padding needed as we then stack
        gt_kpts = np.zeros((max_idv, num_joints, 3))
        pred_kpts = -np.ones((max_idv, num_joints, 3))

        gt_kpts[:gt_idv] = 2 * np.ones((gt_idv, num_joints, 3))
        gt_kpts[:gt_idv, :, :2] = rng.integers(
            low=0, high=1024, size=(gt_idv, num_joints, 2)
        )
        gt[f"img_{i}"] = gt_kpts

        # set scores
        pred_kpts[:pred_idv, :, 2] = rng.random(size=(pred_idv, num_joints))

        # predictions that are ground truth + error
        matched = min(gt_idv, pred_idv)
        if matched > 0:
            error = rng.integers(
                low=-max_error, high=max_error, size=(matched, num_joints, 2)
            )
            matched_pred = gt_kpts[:matched, :, :2] + error
            pred_kpts[:matched, :, :2] = np.clip(matched_pred, 0, 1024)

        # random predictions
        unmatched = pred_idv - matched
        if unmatched > 0:
            pred_kpts[matched:pred_idv, :, :2] = rng.integers(
                low=0, high=1024, size=(unmatched, num_joints, 2)
            )

        pred[f"img_{i}"] = pred_kpts

    _evaluate(gt, pred)


@pytest.mark.parametrize("num_images", [1, 2, 5, 10])
@pytest.mark.parametrize("num_joints", [2, 5, 8, 20])
@pytest.mark.parametrize("max_error", [1, 2, 5, 20, 40])
def test_random_map_computation_with_missing_kpts(num_images, num_joints, max_error):
    rng = np.random.default_rng(seed=0)

    num_individuals = rng.integers(low=0, high=20, size=(num_images, 2))
    max_idv = num_individuals.max(initial=0)

    gt = {}
    pred = {}
    for i, (gt_idv, pred_idv) in enumerate(num_individuals):
        # padding needed as we then stack
        gt_kpts = np.zeros((max_idv, num_joints, 3))
        pred_kpts = -np.ones((max_idv, num_joints, 3))

        gt_kpts[:gt_idv] = 2 * np.ones((gt_idv, num_joints, 3))
        gt_kpts[:gt_idv, :, :2] = rng.integers(
            low=0, high=1024, size=(gt_idv, num_joints, 2)
        )
        gt[f"img_{i}"] = gt_kpts

        # drop some ground truth keypoints
        gt_vis_mask = rng.random(size=(max_idv, num_joints)) < 0.2
        gt_kpts[gt_vis_mask, 2] = 0

        # set scores
        pred_kpts[:pred_idv, :, 2] = rng.random(size=(pred_idv, num_joints))

        # predictions that are ground truth + error
        matched = min(gt_idv, pred_idv)
        if matched > 0:
            error = rng.integers(
                low=-max_error, high=max_error, size=(matched, num_joints, 2)
            )
            matched_pred = gt_kpts[:matched, :, :2] + error
            pred_kpts[:matched, :, :2] = np.clip(matched_pred, 0, 1024)

        # random predictions
        unmatched = pred_idv - matched
        if unmatched > 0:
            pred_kpts[matched:pred_idv, :, :2] = rng.integers(
                low=0, high=1024, size=(unmatched, num_joints, 2)
            )

        pred[f"img_{i}"] = pred_kpts

    _evaluate(gt, pred)


def _evaluate(gt: dict[str, np.ndarray], pred: dict[str, np.ndarray]):
    for k, v in gt.items():
        print(20 * "-")
        print(k)
        print("GT")
        print(v)
        print("PR")
        print(pred[k])

    gt_assemblies = _to_assemblies(gt, ground_truth=True)
    pred_assemblies = _to_assemblies(pred, ground_truth=False)
    oks = inferenceutils.evaluate_assembly_greedy(
        assemblies_gt=gt_assemblies,
        assemblies_pred=pred_assemblies,
        oks_sigma=0.1,
        oks_thresholds=np.linspace(0.5, 0.95, 10),
        margin=0.0,
        symmetric_kpts=None,
    )

    num_joints = gt[list(gt.keys())[0]].shape[1]
    coco_gt = _to_coco_ground_truth(gt, num_joints, bbox_margin=0)
    coco_pred = _to_coco_predictions(coco_gt, pred, bbox_margin=0)
    coco_oks = eval_coco(coco_gt, coco_pred, num_joints)
    print(20 * "-")
    print(f"dlc mAP:")
    for k, v in oks.items():
        print(k)
        print(v)
        print()
    print(20 * "-")
    print(f"pycocotools mAP: {coco_oks}")
    print()
    assert oks["mAP"] == coco_oks


def _to_assemblies(
    data: dict[str, np.ndarray], ground_truth: bool,
) -> dict[str, list[inferenceutils.Assembly]]:
    images = list(data.keys())
    raw_data = np.stack([data[i] for i in images], axis=0)

    # mask not visible entries
    mask = raw_data[..., 2] <= 0
    raw_data[mask] = np.nan

    # set the "score" to 1 for ground truth
    if ground_truth:
        raw_data[~mask, 2] = 1

    return {
        images[i]: assembly
        for i, assembly in inferenceutils._parse_ground_truth_data(raw_data).items()
    }


def _to_coco_ground_truth(
    data: dict[str, np.ndarray],
    num_joints: int,
    bbox_margin: int = 0,
    image_size: tuple[int, int] = (1024, 1024),
) -> dict[str, list[dict]]:
    w, h = image_size
    anns, images = [], []
    for path, image_keypoints in data.items():
        id_ = len(images) + 1
        images.append(dict(id=id_, file_name=path, width=w, height=h))

        assert image_keypoints.shape[1] == num_joints
        for idv_id, kpts in enumerate(image_keypoints):
            visible = kpts[:, 2] > 0
            num_keypoints = visible.sum()

            if num_keypoints > 1:
                bbox = bbox_from_keypoints(
                    keypoints=kpts,
                    image_h=h,
                    image_w=w,
                    margin=bbox_margin,
                )
                area = bbox[2].item() * bbox[3].item()
                anns.append(
                    {
                        "id": len(anns) + 1,
                        "image_id": id_,
                        "category_id": 1,
                        "area": area,
                        "bbox": bbox.tolist(),
                        "keypoints": kpts.reshape(-1).tolist(),
                        "iscrowd": 0,
                        "num_keypoints": num_keypoints,
                    }
                )

    keypoints = [f"bpt{i}" for i in range(num_joints)]
    category = dict(id=1, name="animal", supercategory="animal", keypoints=keypoints)
    return {"annotations": anns, "categories": [category], "images": images}


def _to_coco_predictions(
    ground_truth: dict,
    predictions: dict[str, np.ndarray],
    bbox_margin: int = 0,
    image_size: tuple[int, int] = (1024, 1024),
) -> list[dict]:
    w, h = image_size
    num_joints = len(ground_truth["categories"][0]["keypoints"])
    path_to_id = {img["file_name"]: img["id"] for img in ground_truth["images"]}

    coco_predictions = []
    for path, image_keypoints in predictions.items():
        assert image_keypoints.shape[1] == num_joints

        img_id = path_to_id[path]
        valid_predictions = [
            kpt for kpt in image_keypoints  if np.any(np.all(~np.isnan(kpt), axis=-1))
        ]
        for kpts in valid_predictions:
            score = float(np.nanmean(kpts[:, 2]).item())
            kpts = kpts.copy()
            kpts[:, 2] = 2

            # NaN predictions to infinity
            kpts[np.isnan(kpts)] = np.inf

            bbox = bbox_from_keypoints(
                keypoints=kpts,
                image_h=h,
                image_w=w,
                margin=bbox_margin,
            )
            area = bbox[2].item() * bbox[3].item()
            coco_predictions.append(
                {
                    "image_id": img_id,
                    "category_id": 1,
                    "keypoints": kpts.reshape(-1).tolist(),
                    "bbox": bbox.tolist(),
                    "area": area,
                    "score": score,
                }
            )

    return coco_predictions


def eval_coco(
    ground_truth: dict,
    predictions: list[dict],
    num_joints: int,
) -> float | None:
    try:
        from pycocotools.coco import COCO
        from pycocotools.cocoeval import COCOeval

        coco = COCO()
        coco.dataset["annotations"] = ground_truth["annotations"]
        coco.dataset["categories"] = ground_truth["categories"]
        coco.dataset["images"] = ground_truth["images"]
        coco.createIndex()

        coco_det = coco.loadRes(predictions)
        coco_eval = COCOeval(coco, coco_det, iouType="keypoints")
        coco_eval.params.kpt_oks_sigmas = np.array(num_joints * [0.1])
        coco_eval.evaluate()
        coco_eval.accumulate()
        coco_eval.summarize()
        return float(coco_eval.stats[0])

    except ModuleNotFoundError as err:
        print(f"pycocotools is not installed")


--- File: tests/pose_estimation_pytorch/post_processing/test_identity.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
""" Tests identity matching """
import numpy as np
import pytest

from deeplabcut.pose_estimation_pytorch.post_processing.identity import assign_identity


@pytest.mark.parametrize(
    "prediction, identity_scores, output_order",
    [
        (
            [
                [[0, 0, 1.0], [0, 0, 1.0]],  # assembly 1
                [[5, 5, 1.0], [5, 5, 1.0]],  # assembly 2
                [[9, 9, 1.0], [9, 9, 1.0]],  # assembly 3
            ],
            [  # a0 -> idv1, a1 -> idv2, a2 -> idv0
                [[0.1, 0.8, 0.3], [0.1, 0.7, 0.3]],  # assembly 1 ID scores
                [[0.2, 0.1, 0.6], [0.3, 0.1, 0.5]],  # assembly 2 ID scores
                [[0.7, 0.1, 0.1], [0.6, 0.2, 0.2]],  # assembly 3 ID scores
            ],
            [2, 0, 1],
        ),
        (
            [
                [[0, 0, 1.0], [0, 0, 1.0]],  # assembly 1
                [[1, 1, 1.0], [5, 5, 1.0]],  # assembly 2
                [[0, 0, 1.0], [9, 9, 1.0]],  # assembly 3
            ],
            [  # a0 -> idv0, a1 -> idv1, a2 -> idv2
                [[0.4, 0.4, 0.3], [0.5, 0.3, 0.3]],  # assembly 1 ID scores
                [[0.4, 0.4, 0.3], [0.3, 0.5, 0.4]],  # assembly 2 ID scores
                [[0.2, 0.2, 0.4], [0.2, 0.2, 0.3]],  # assembly 3 ID scores
            ],
            [0, 1, 2],
        ),
    ],
)
def test_single_identity_assignment(prediction, identity_scores, output_order):
    predictions = np.array(prediction)
    identity_scores = np.array(identity_scores)
    new_order = assign_identity(predictions, identity_scores)
    predictions_with_id = predictions[new_order]

    print()
    print(predictions.shape)
    print(identity_scores.shape)
    np.testing.assert_equal(predictions[output_order], predictions_with_id)


--- File: tests/pose_estimation_pytorch/apis/test_create_tracking_dataset.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests method to create the tracking dataset in PyTorch"""
from pathlib import Path

import torch

import deeplabcut.pose_estimation_pytorch as dlc_torch
import deeplabcut.pose_estimation_pytorch.apis.tracking_dataset as tracking_dataset
import deeplabcut.pose_estimation_pytorch.models as models


class MockLoader(dlc_torch.Loader):
    """Mock loader for data"""

    def __init__(self, tmp_folder: Path, bodyparts: list[str] | None = None):
        if bodyparts is None:
            bodyparts = ["nose", "left_eye", "right_eye", "tail_base"]
        self.bodyparts = bodyparts

        model_config_path = tmp_folder / "pytorch_config.yaml"
        dlc_torch.config.make_pytorch_pose_config(
            project_config=dlc_torch.config.make_basic_project_config(
                dataset_path=str(tmp_folder),
                bodyparts=self.bodyparts,
                max_individuals=3,
            ),
            pose_config_path=tmp_folder / "pytorch_config.yaml",
            net_type="resnet_50",
            save=True,
        )
        super().__init__(model_config_path)

    def load_data(self, mode: str = "train") -> dict[str, list[dict]]:
        return {
            "annotations": [],
            "categories": [],
            "images": [],
        }

    def get_dataset_parameters(self) -> dlc_torch.PoseDatasetParameters:
        return dlc_torch.PoseDatasetParameters(
            bodyparts=self.bodyparts,
            unique_bpts=[],
            individuals=self.model_cfg["metadata"]["individuals"],
        )


def test_build_feature_extraction_runner(tmp_path_factory):
    tmp_folder = Path(tmp_path_factory.mktemp("tmp-project"))

    loader = MockLoader(tmp_folder=tmp_folder)
    model = models.PoseModel.build(loader.model_cfg["model"])
    snapshot_path = loader.model_folder / "snapshot.pt"
    torch.save(dict(model=model.state_dict()), snapshot_path)
    _ = tracking_dataset.build_feature_extraction_runner(
        loader=loader,
        snapshot_path=snapshot_path,
        device="cpu",
        batch_size=1,
    )





--- File: tests/pose_estimation_pytorch/apis/test_apis_export.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests exporting models"""
import copy
import shutil
from pathlib import Path
from unittest.mock import Mock, patch

import pytest
import torch

import deeplabcut.pose_estimation_pytorch.apis.export as export
import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.pose_estimation_pytorch import Task
from deeplabcut.pose_estimation_pytorch.runners.snapshots import Snapshot


@pytest.fixture()
def project_dir(tmp_path_factory) -> Path:
    project_dir = tmp_path_factory.mktemp("tmp-project")
    print(f"\nTemporary project directory:")
    print(str(project_dir))
    print("---")
    yield project_dir
    shutil.rmtree(str(project_dir))


def _mock_multianimal_project(project_dir: Path):
    video_dir = project_dir / "videos"
    video_dir.mkdir(exist_ok=True)

    cfg_file, yaml_file = af.create_config_template(multianimal=True)
    cfg_file["Task"] = "mock"
    cfg_file["scorer"] = "mock"
    cfg_file["video_sets"] = {str(video_dir / "vid.mp4"): dict(crop="0, 640, 0, 480")}
    cfg_file["project_path"] = str(project_dir)
    cfg_file["individuals"] = ["a", "b"]
    cfg_file["uniquebodyparts"] = []
    cfg_file["multianimalbodyparts"] = ["k1", "k2", "k3"]
    cfg_file["bodyparts"] = "MULTI!"

    with open(project_dir / "config.yaml", "w") as f:
        yaml_file.dump(cfg_file, f)


def _make_mock_loader(
    project_path: Path,
    project_task: str,
    project_iteration: int,
    model_folder: Path,
    net_type: str,
    pose_task: Task,
    default_snapshot_index: int | str,
    default_detector_snapshot_index: int | str,
) -> Mock:
    loader = Mock()
    loader.project_path = project_path
    loader.model_folder = model_folder
    loader.pose_task = pose_task
    loader.shuffle = 0

    loader.project_cfg = dict(
        project_path=str(project_path),
        Task=project_task,
        date="Jan12",
        TrainingFraction=[0.95],
        snapshotindex=default_snapshot_index,
        detector_snapshotindex=default_detector_snapshot_index,
        iteration=project_iteration,
    )
    loader.model_cfg = dict(
        net_type=net_type,
        metadata=dict(
            project_path=str(project_path),
            pose_config_path=str(loader.model_folder / "pytorch_config.yaml"),
        ),
        weight_init=None,
        resume_training_from=None,
    )
    if pose_task == Task.TOP_DOWN:
        loader.model_cfg["detector"] = dict(resume_training_from=None)

    return loader


def _get_export_model_data(
    project_dir: Path,
    num_snapshots: int,
    task: Task,
    project_iteration: int = 0,
):
    _mock_multianimal_project(project_dir)

    model_dir = Path(project_dir) / f"iteration-{project_iteration}" / "fake-shuffle-0"
    model_dir.mkdir(exist_ok=True, parents=True)
    snapshots = []
    snapshot_data = []
    for i in range(num_snapshots):
        snapshot = dict(model=dict(idx=i))
        snapshot_path = model_dir / f"snapshot-{i:03}.pt"
        torch.save(snapshot, snapshot_path)
        snapshots.append(Snapshot(best=False, epochs=i, path=snapshot_path))
        snapshot_data.append(snapshot)

    detector_snapshots = []
    detector_data = []
    if task == Task.TOP_DOWN:
        for i in range(num_snapshots):
            snapshot = dict(model=dict(idx=i))
            snapshot_path = model_dir / f"snapshot-detector-{i:03}.pt"
            torch.save(snapshot, snapshot_path)
            detector_data.append(snapshot)
            detector_snapshots.append(
                Snapshot(best=False, epochs=i, path=snapshot_path)
            )

    mock_loader = _make_mock_loader(
        project_path=project_dir,
        project_task="mock",
        project_iteration=project_iteration,
        model_folder=model_dir,
        net_type="fake-net",
        pose_task=task,
        default_snapshot_index=-1,
        default_detector_snapshot_index=-1,
    )
    return mock_loader, snapshots, snapshot_data, detector_snapshots, detector_data


@pytest.mark.parametrize(
    "task, num_snapshots, idx, detector_idx",
    [
        (Task.BOTTOM_UP, 10, 0, None),
        (Task.BOTTOM_UP, 10, 5, None),
        (Task.BOTTOM_UP, 10, -1, None),
        (Task.TOP_DOWN, 10, 0, 0),
        (Task.TOP_DOWN, 10, -1, 0),
        (Task.TOP_DOWN, 10, -1, 5),
        (Task.TOP_DOWN, 10, -1, -1),
    ],
)
def test_export_model(
    project_dir,
    task: Task,
    num_snapshots: int,
    idx: int,
    detector_idx: int | None,
):
    test_data = _get_export_model_data(project_dir, num_snapshots, task)
    mock_loader, snapshots, snapshot_data, detector_snapshots, detector_data = test_data

    def get_mock_loader(*args, **kwargs):
        return mock_loader

    with patch(
        "deeplabcut.pose_estimation_pytorch.apis.export.dlc3_data.DLCLoader",
        get_mock_loader,
    ):
        # export the model
        export.export_model(
            project_dir / "config.yaml",
            snapshotindex=idx,
            detector_snapshot_index=detector_idx,
        )

        # check that the correct snapshot was exported
        snapshot = snapshots[idx]
        detector = None
        if task == Task.TOP_DOWN:
            detector = detector_snapshots[detector_idx]

        dir_name = export.get_export_folder_name(mock_loader)
        filename = export.get_export_filename(mock_loader, snapshot, detector)
        expected_export = project_dir / "exported-models-pytorch" / dir_name / filename
        assert expected_export.exists()

        # check that content of the exports are correct
        exported_data = torch.load(expected_export, weights_only=True)
        assert isinstance(exported_data, dict)
        assert "config" in exported_data
        assert exported_data["config"] == mock_loader.model_cfg

        assert "pose" in exported_data
        assert exported_data["pose"] == snapshot_data[idx]["model"]

        if task == Task.TOP_DOWN:
            assert "detector" in exported_data
            assert exported_data["detector"] == detector_data[detector_idx]["model"]


@patch("deeplabcut.pose_estimation_pytorch.apis.export.wipe_paths_from_model_config")
@pytest.mark.parametrize("task", [Task.BOTTOM_UP, Task.TOP_DOWN])
def test_export_model_clear_paths(mock_wipe: Mock, project_dir, task: Task):
    test_data = _get_export_model_data(project_dir, 1, task)
    mock_loader, snapshots, snapshot_data, detector_snapshots, detector_data = test_data

    def get_mock_loader(*args, **kwargs):
        return mock_loader

    with patch(
        "deeplabcut.pose_estimation_pytorch.apis.export.dlc3_data.DLCLoader",
        get_mock_loader,
    ):
        export.export_model(project_dir / "config.yaml", wipe_paths=True)

        # check that wipe_paths_from_model_config was called
        assert mock_wipe.call_count == 1


@pytest.mark.parametrize("task", [Task.BOTTOM_UP, Task.TOP_DOWN])
@pytest.mark.parametrize("overwrite", [True, False])
def test_export_overwrite(project_dir, task: Task, overwrite: bool):
    test_data = _get_export_model_data(project_dir, 1, task)
    mock_loader, snapshots, snapshot_data, detector_snapshots, detector_data = test_data
    snapshot = snapshots[0]
    detector = None if task == Task.BOTTOM_UP else detector_snapshots[0]

    def get_mock_loader(*args, **kwargs):
        return mock_loader

    with patch(
        "deeplabcut.pose_estimation_pytorch.apis.export.dlc3_data.DLCLoader",
        get_mock_loader,
    ):
        dir_name = export.get_export_folder_name(mock_loader)
        filename = export.get_export_filename(mock_loader, snapshot, detector)
        expected_export = project_dir / "exported-models-pytorch" / dir_name / filename
        expected_export.parent.mkdir(exist_ok=False, parents=True)

        # add existing data
        assert not expected_export.exists()
        existing_data = dict()
        torch.save(existing_data, expected_export)

        # export data
        export.export_model(project_dir / "config.yaml", overwrite=overwrite)

        exported_data = torch.load(expected_export, weights_only=True)

        if overwrite:
            assert existing_data != exported_data
        else:
            assert existing_data == exported_data


@pytest.mark.parametrize("task", [Task.BOTTOM_UP, Task.TOP_DOWN])
@pytest.mark.parametrize("iteration", [5, 12])
def test_export_change_iteration(project_dir, task: Task, iteration: int):
    test_data = _get_export_model_data(
        project_dir,
        1,
        task,
        project_iteration=0,
    )
    mock_loader, snapshots, snapshot_data, detector_snapshots, detector_data = test_data
    snapshot = snapshots[0]
    detector = None if task == Task.BOTTOM_UP else detector_snapshots[0]

    loader_diff_iter = _get_export_model_data(
        project_dir, 1, task, project_iteration=iteration
    )[0]

    def get_mock_loader(config, *args, **kwargs):
        _loader = copy.deepcopy(mock_loader)
        if isinstance(config, dict):
            _loader = copy.deepcopy(mock_loader)
            _loader.project_cfg = config
        return _loader

    def read_mock_config(*args, **kwargs):
        return copy.deepcopy(mock_loader.project_cfg)

    # patch the DLCLoader but also read_config
    with patch(
        "deeplabcut.pose_estimation_pytorch.apis.export.dlc3_data.DLCLoader",
        get_mock_loader,
    ):
        with patch(
            "deeplabcut.pose_estimation_pytorch.apis.export.af.read_config",
            read_mock_config,
        ):
            # check no exports exist yet
            for loader in [mock_loader, loader_diff_iter]:
                dir_name = export.get_export_folder_name(loader)
                filename = export.get_export_filename(loader, snapshot, detector)
                assert not (
                    project_dir / "exported-models-pytorch" / dir_name / filename
                ).exists()

            # export data
            export.export_model(project_dir / "config.yaml", iteration=iteration)

            # check the export exists for the correct iteration
            for loader, file_should_exist in [
                (mock_loader, False),
                (loader_diff_iter, True),
            ]:
                dir_name = export.get_export_folder_name(loader)
                filename = export.get_export_filename(loader, snapshot, detector)
                expected = project_dir / "exported-models-pytorch" / dir_name / filename
                expected_exists = expected.exists()
                assert expected_exists == file_should_exist


--- File: tests/pose_estimation_pytorch/apis/test_apis_evaluate.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from dataclasses import dataclass
from unittest.mock import Mock, patch

import numpy as np
import pytest

import deeplabcut.pose_estimation_pytorch.apis as apis
import deeplabcut.pose_estimation_pytorch.data as data


PREDICT = Mock()


@patch("deeplabcut.pose_estimation_pytorch.apis.evaluation.predict", PREDICT)
@pytest.mark.parametrize("num_individuals", [1, 2, 5])
@pytest.mark.parametrize(
    "bodyparts, error",
    [
        (["nose", "left_ear"], [5, 10]),
        (["nose", "left_ear", "right_ear"], [2, 3, 4]),
    ]
)
def test_evaluate_basic(
    num_individuals: int,
    bodyparts: list[str],
    error: list[float],
) -> None:
    print()
    gt, pred = generate_data(1, num_individuals, len(bodyparts), error)

    pose_runner = Mock()

    PREDICT.return_value = {img: {"bodyparts": pose} for img, pose in pred.items()}
    loader = build_mock_loader(gt, num_individuals, bodyparts)
    results, preds = apis.evaluate(pose_runner, loader, mode="test")
    print("results", results)
    np.testing.assert_almost_equal(results["rmse"], np.mean(error))


@patch("deeplabcut.pose_estimation_pytorch.apis.evaluation.predict", PREDICT)
@pytest.mark.parametrize("num_individuals", [1, 2, 5])
@pytest.mark.parametrize(
    "bodyparts, error",
    [
        (["nose", "left_ear"], [5, 10]),
        (["nose", "left_ear", "right_ear"], [2, 3, 4]),
    ]
)
@pytest.mark.parametrize(
    "unique_bodyparts, unique_error",
    [
        (["top_left"], [2]),
        (["top_left", "bottom_right"], [2, 3]),
    ]
)
def test_evaluate_with_unique_bodyparts(
    num_individuals: int,
    bodyparts: list[str],
    error: list[float],
    unique_bodyparts: list[str],
    unique_error: list[float],
) -> None:
    print()
    num_images = 5
    gt, pred = generate_data(num_images, num_individuals, len(bodyparts), error)
    gt_unique, pred_unique = generate_data(
        num_images, 1, len(unique_bodyparts), unique_error
    )

    pose_runner = Mock()
    PREDICT.return_value = {
        img: {"bodyparts": pose, "unique_bodyparts": pred_unique[img]}
        for img, pose in pred.items()
    }
    loader = build_mock_loader(
        gt, num_individuals, bodyparts, gt_unique=gt_unique, unique=unique_bodyparts
    )
    results, preds = apis.evaluate(pose_runner, loader, mode="test")
    idv_errors = np.tile(error, (num_individuals, 1)).reshape(-1)
    expected_rmse = np.mean(np.concatenate([idv_errors, unique_error]))
    print(num_individuals)
    print(error)
    print(idv_errors)
    print(unique_error)
    print(np.concatenate([idv_errors, unique_error]))
    print(expected_rmse)
    print("results", results)
    np.testing.assert_almost_equal(results["rmse"], expected_rmse)


@dataclass
class CompTestConfig:
    num_individuals: int = 1
    bodyparts: tuple[str, ...] = ("nose", "left_ear")
    error: tuple[float, ...] = (5, 10)
    unique_bodyparts: tuple[str, ...] = ("top_left", )
    unique_error: tuple[float, ...] = (2, )
    comparison_bodyparts: str | list[str] | None = None
    expected_error: float = (2 + 5 + 10) / 3

    def num_bpt(self) -> int:
        return len(self.bodyparts)

    def num_unique(self) -> int:
        return len(self.unique_bodyparts)


@patch("deeplabcut.pose_estimation_pytorch.apis.evaluation.predict", PREDICT)
@pytest.mark.parametrize(
    "cfg",
    [
        CompTestConfig(comparison_bodyparts=None),
        CompTestConfig(comparison_bodyparts="all"),
        CompTestConfig(comparison_bodyparts=["nose", "left_ear", "top_left"]),
        CompTestConfig(num_individuals=2, expected_error=(2 + 5 + 5 + 10 + 10) / 5),
        CompTestConfig(comparison_bodyparts="nose", expected_error=5),
        CompTestConfig(comparison_bodyparts=["nose"], expected_error=5),
        CompTestConfig(comparison_bodyparts=["left_ear"], expected_error=10),
        CompTestConfig(comparison_bodyparts=["nose", "left_ear"], expected_error=7.5),
        CompTestConfig(comparison_bodyparts="top_left", expected_error=2),
        CompTestConfig(comparison_bodyparts=["top_left"], expected_error=2),
        CompTestConfig(
            unique_bodyparts=("a", "b", "c"),
            unique_error=(3.0, 4.0, 5.0),
            comparison_bodyparts=["a", "b", "c"],
            expected_error=4,
        ),
        CompTestConfig(
            num_individuals=1,
            unique_bodyparts=("a", "b", "c"),
            unique_error=(3.0, 4.0, 5.0),
            comparison_bodyparts=["nose", "a", "b", "c"],
            expected_error=(5.0 + 3.0 + 4.0 + 5.0) / 4,
        ),
        CompTestConfig(
            num_individuals=7,
            unique_bodyparts=("a", "b", "c"),
            unique_error=(3.0, 4.0, 5.0),
            comparison_bodyparts=["nose", "left_ear", "a", "b"],
            expected_error=((7 * 5) + (7 * 10) + 3.0 + 4.0) / (7 + 7 + 2),
        ),
    ]
)
def test_evaluate_with_comparison_bodyparts(cfg: CompTestConfig) -> None:
    print()
    num_images = 5
    gt, pred = generate_data(num_images, cfg.num_individuals, cfg.num_bpt(), cfg.error)
    gt_unique, pred_unique = generate_data(num_images, 1, cfg.num_unique(), cfg.unique_error)

    pose_runner = Mock()
    PREDICT.return_value = {
        img: {"bodyparts": pose, "unique_bodyparts": pred_unique[img]}
        for img, pose in pred.items()
    }
    loader = build_mock_loader(
        gt,
        cfg.num_individuals,
        cfg.bodyparts,
        gt_unique=gt_unique,
        unique=cfg.unique_bodyparts,
    )
    results, preds = apis.evaluate(
        pose_runner, loader, mode="test", comparison_bodyparts=cfg.comparison_bodyparts,
    )
    print(cfg)
    print("results", results)
    np.testing.assert_almost_equal(results["rmse"], cfg.expected_error)


@dataclass
class KeypointData:
    img: int
    idv: int
    bodypart: str
    gt: tuple[float, float]
    pred: tuple[float, float]
    score: float

    def image(self) -> str:
        return f"image_{self.img:04d}.png"

    def error(self) -> float:
        return np.linalg.norm(
            np.asarray(self.gt, dtype=float) - np.asarray(self.pred, dtype=float)
        ).item()


@patch("deeplabcut.pose_estimation_pytorch.apis.evaluation.predict", PREDICT)
@pytest.mark.parametrize(
    "pcutoff", [0.4, 0.6, 0.8, [0.3, 0.5, 0.7]],
)
@pytest.mark.parametrize(
    "keypoints", [
        [
            KeypointData(img=0, idv=0, bodypart="a", gt=(10, 10), pred=(11, 10), score=0.7),
            KeypointData(img=0, idv=0, bodypart="b", gt=(20, 20), pred=(21, 20), score=0.7),
            KeypointData(img=0, idv=0, bodypart="c", gt=(20, 20), pred=(20, 22), score=0.5),
        ],
        [
            KeypointData(img=0, idv=0, bodypart="a", gt=(10, 10), pred=(11, 10), score=0.7),
            KeypointData(img=0, idv=0, bodypart="b", gt=(20, 20), pred=(21, 20), score=0.5),
            KeypointData(img=0, idv=0, bodypart="c", gt=(30, 30), pred=(30, 32), score=0.2),
            KeypointData(img=0, idv=1, bodypart="a", gt=(40, 10), pred=(41, 10), score=0.7),
            KeypointData(img=0, idv=1, bodypart="b", gt=(50, 20), pred=(49, 20), score=0.5),
            KeypointData(img=0, idv=1, bodypart="c", gt=(60, 20), pred=(58, 20), score=0.2),
        ],
    ]
)
def test_evaluate_with_pcutoff(
    pcutoff: float | list[float],
    keypoints: list[KeypointData],
) -> None:
    print()

    images = {d.image() for d in keypoints}
    individuals = list({d.idv for d in keypoints if d.idv != -1})
    bodyparts = list({d.bodypart for d in keypoints if d.idv != -1})
    unique_bodyparts = list({d.bodypart for d in keypoints if d.idv == -1})

    num_idv = len(individuals)
    num_bodyparts = len(bodyparts)
    num_unique = len(unique_bodyparts)

    gt, pred = {}, {}
    for img in images:
        gt[img] = np.zeros((num_idv, num_bodyparts, 3))
        pred[img] = np.zeros((num_idv, num_bodyparts, 3))

    errors = []
    errors_cutoff = []
    for kpt in keypoints:
        img = kpt.image()
        bpt = bodyparts.index(kpt.bodypart)

        gt[img][kpt.idv, bpt, :2] = kpt.gt
        gt[img][kpt.idv, bpt, 2] = 2
        pred[img][kpt.idv, bpt, :2] = kpt.pred
        pred[img][kpt.idv, bpt, 2] = kpt.score

        if isinstance(pcutoff, list):
            bpt_cutoff = pcutoff[bpt]
        else:
            bpt_cutoff = pcutoff

        errors.append(kpt.error())
        if kpt.score >= bpt_cutoff:
            errors_cutoff.append(kpt.error())

    print(errors)
    print(errors_cutoff)

    pose_runner = Mock()
    PREDICT.return_value = {img: {"bodyparts": pose} for img, pose in pred.items()}
    loader = build_mock_loader(gt, num_idv, bodyparts)
    results, preds = apis.evaluate(pose_runner, loader, mode="test", pcutoff=pcutoff)
    print("results", results)
    np.testing.assert_almost_equal(results["rmse"], np.mean(errors))
    np.testing.assert_almost_equal(results["rmse_pcutoff"], np.mean(errors_cutoff))
    if "rmse_detections" in results:
        np.testing.assert_almost_equal(
            results["rmse_detections"], np.mean(errors)
        )
        np.testing.assert_almost_equal(
            results["rmse_detections_pcutoff"], np.mean(errors_cutoff)
        )


@patch("deeplabcut.pose_estimation_pytorch.apis.evaluation.predict", PREDICT)
@pytest.mark.parametrize(
    "pcutoff", [
        0.4,
        0.6,
        0.8,
        [0.3, 0.5, 0.7, 0.4, 0.6],
        [0.25, 0.43, 0.61, 0.46, 0.92],
        [0.12, 0.15, 0.92, 0.97, 0.85],
        [0.92, 0.97, 0.85, 0.12, 0.15],
    ],
)
@pytest.mark.parametrize(
    "keypoints", [
        [
            KeypointData(img=0, idv=0, bodypart="a", gt=(10, 10), pred=(11, 10), score=0.7),
            KeypointData(img=0, idv=0, bodypart="b", gt=(20, 20), pred=(21, 20), score=0.7),
            KeypointData(img=0, idv=0, bodypart="c", gt=(20, 20), pred=(20, 22), score=0.5),
            KeypointData(img=0, idv=-1, bodypart="u1", gt=(20, 20), pred=(20, 22), score=0.5),
            KeypointData(img=0, idv=-1, bodypart="u2", gt=(20, 20), pred=(20, 22), score=0.3),
        ],
        [
            KeypointData(img=0, idv=0, bodypart="a", gt=(10, 10), pred=(11, 10), score=0.7),
            KeypointData(img=0, idv=0, bodypart="b", gt=(20, 20), pred=(21, 20), score=0.5),
            KeypointData(img=0, idv=0, bodypart="c", gt=(30, 30), pred=(30, 32), score=0.2),
            KeypointData(img=0, idv=1, bodypart="a", gt=(40, 10), pred=(41, 10), score=0.7),
            KeypointData(img=0, idv=1, bodypart="b", gt=(50, 20), pred=(49, 20), score=0.5),
            KeypointData(img=0, idv=1, bodypart="c", gt=(60, 20), pred=(58, 20), score=0.2),
            KeypointData(img=0, idv=-1, bodypart="u1", gt=(2, 3), pred=(3, 3), score=0.7),
            KeypointData(img=0, idv=-1, bodypart="u2", gt=(20, 20), pred=(20, 22), score=0.9),
        ],
        [
            KeypointData(img=0, idv=0, bodypart="a", gt=(8, 13), pred=(11, 10), score=0.7),
            KeypointData(img=0, idv=0, bodypart="b", gt=(20, 27), pred=(21, 20), score=0.5),
            KeypointData(img=0, idv=0, bodypart="c", gt=(30, 36), pred=(30, 32), score=0.2),
            KeypointData(img=0, idv=-1, bodypart="u1", gt=(2, 3), pred=(3, 3), score=0.7),
            KeypointData(img=0, idv=-1, bodypart="u2", gt=(20, 20), pred=(20, 22), score=0.9),
            KeypointData(img=1, idv=0, bodypart="a", gt=(15, 20), pred=(41, 10), score=0.7),
            KeypointData(img=1, idv=0, bodypart="b", gt=(20, 12), pred=(49, 20), score=0.5),
            KeypointData(img=1, idv=0, bodypart="c", gt=(17, 32), pred=(58, 20), score=0.2),
            KeypointData(img=1, idv=-1, bodypart="u1", gt=(37, 4), pred=(3, 3), score=0.7),
            KeypointData(img=1, idv=-1, bodypart="u2", gt=(12, 6), pred=(20, 22), score=0.9),
        ],
        [
            KeypointData(img=0, idv=0, bodypart="a", gt=(8, 13), pred=(11, 10), score=0.7),
            KeypointData(img=0, idv=0, bodypart="b", gt=(20, 27), pred=(21, 20), score=0.5),
            KeypointData(img=0, idv=-1, bodypart="u1", gt=(30, 36), pred=(30, 32), score=0.2),
            KeypointData(img=0, idv=-1, bodypart="u2", gt=(2, 3), pred=(3, 3), score=0.7),
            KeypointData(img=0, idv=-1, bodypart="u3", gt=(20, 20), pred=(20, 22), score=0.9),
            KeypointData(img=1, idv=0, bodypart="a", gt=(15, 20), pred=(41, 10), score=0.7),
            KeypointData(img=1, idv=0, bodypart="b", gt=(20, 12), pred=(49, 20), score=0.5),
            KeypointData(img=1, idv=-1, bodypart="u1", gt=(17, 32), pred=(58, 20), score=0.2),
            KeypointData(img=1, idv=-1, bodypart="u2", gt=(37, 4), pred=(3, 3), score=0.7),
            KeypointData(img=1, idv=-1, bodypart="u3", gt=(12, 6), pred=(20, 22), score=0.9),
        ]
    ]
)
def test_evaluate_with_pcutoff_and_unique_bodyparts(
    pcutoff: float | list[float],
    keypoints: list[KeypointData],
) -> None:
    print()

    images = {d.image() for d in keypoints}
    individuals = list({d.idv for d in keypoints if d.idv != -1})
    bodyparts = list({d.bodypart for d in keypoints if d.idv != -1})
    unique_bodyparts = list({d.bodypart for d in keypoints if d.idv == -1})

    num_idv = len(individuals)
    num_bodyparts = len(bodyparts)
    num_unique = len(unique_bodyparts)

    gt, pred, gt_unique, pred_unique = {}, {}, {}, {}
    for img in images:
        gt[img] = np.zeros((num_idv, num_bodyparts, 3))
        pred[img] = np.zeros((num_idv, num_bodyparts, 3))
        gt_unique[img] = np.zeros((1, num_unique, 3))
        pred_unique[img] = np.zeros((1, num_unique, 3))

    errors, errors_cutoff = [], []
    for kpt in keypoints:
        img = kpt.image()
        if kpt.idv == -1:
            idv, bpt = 0, unique_bodyparts.index(kpt.bodypart)
            pcutoff_idx = bpt + len(bodyparts)  # offset by number of bodyparts
            gt_data, pred_data = gt_unique[img], pred_unique[img]
        else:
            idv, bpt = kpt.idv, bodyparts.index(kpt.bodypart)
            pcutoff_idx = bpt
            gt_data, pred_data = gt[img], pred[img]

        gt_data[idv, bpt, :2] = kpt.gt
        gt_data[idv, bpt, 2] = 2
        pred_data[idv, bpt, :2] = kpt.pred
        pred_data[idv, bpt, 2] = kpt.score

        if isinstance(pcutoff, list):
            bpt_cutoff = pcutoff[pcutoff_idx]
        else:
            bpt_cutoff = pcutoff

        errors.append(kpt.error())
        if kpt.score >= bpt_cutoff:
            errors_cutoff.append(kpt.error())

    print(errors)
    print(errors_cutoff)

    pose_runner = Mock()
    PREDICT.return_value = {
        img: {"bodyparts": pose, "unique_bodyparts": pred_unique[img]}
        for img, pose in pred.items()
    }
    loader = build_mock_loader(gt, num_idv, bodyparts, gt_unique, unique_bodyparts)
    results, preds = apis.evaluate(pose_runner, loader, mode="test", pcutoff=pcutoff)

    print("results", results)
    np.testing.assert_almost_equal(results["rmse"], np.mean(errors))
    np.testing.assert_almost_equal(results["rmse_pcutoff"], np.mean(errors_cutoff))
    if "rmse_detections" in results:
        np.testing.assert_almost_equal(
            results["rmse_detections"], np.mean(errors)
        )
        np.testing.assert_almost_equal(
            results["rmse_detections_pcutoff"], np.mean(errors_cutoff)
        )


def generate_data(
    num_images: int,
    num_individuals: int,
    num_bodyparts: int,
    error: list[float] | tuple[float, ...] | np.ndarray,
    cutoffs: list[float] | tuple[float, ...] | np.ndarray | None = None,
    error_cutoff: list[float] | tuple[float, ...] | np.ndarray | None = None,
) -> tuple[dict[str, np.ndarray], dict[str, np.ndarray]]:
    num_elems = num_individuals * num_bodyparts
    shape = num_individuals, num_bodyparts, 3
    error = np.asarray(error)
    coord_error = (np.sqrt(2) / 2) * error

    gt, pred = {}, {}
    for img in range(num_images):
        gt_pose = 100 * np.arange(3 * num_elems, dtype=float).reshape(shape)
        gt_pose[..., 2] = 2
        gt[f"img_{img:04d}.png"] = gt_pose

        pred_pose = np.ones(shape, dtype=float)
        pred_pose[..., :2] = gt_pose[..., :2]
        pred_pose[:, :, 0] += coord_error
        pred_pose[:, :, 1] += coord_error
        pred[f"img_{img:04d}.png"] = pred_pose

    if error_cutoff is not None and cutoffs is not None:
        for img in range(num_images):
            gt_pose = 100 * np.arange(3 * num_elems, dtype=float).reshape(shape)
            gt_pose[..., 2] = 2
            gt[f"img_{num_images + img:04d}.png"] = gt_pose

            pred_pose = np.ones(shape, dtype=float)
            pred_pose[..., :2] = gt_pose[..., :2]
            pred_pose[..., 2] = cutoffs
            pred_pose[:, :, 0] += coord_error
            pred_pose[:, :, 1] += coord_error
            pred[f"img_{num_images + img:04d}.png"] = pred_pose

    return gt, pred


def build_mock_loader(
    gt: dict[str, np.ndarray],
    num_individuals: int,
    bodyparts: list[str] | tuple[str, ...],
    gt_unique: dict[str, np.ndarray] | None = None,
    unique: list[str] | tuple[str, ...] | None = None,
) -> Mock:
    if unique is None:
        unique = []

    def _gt(mode: str, unique_bodypart: bool = False) -> dict[str, np.ndarray]:
        if unique_bodypart:
            print("LOADING UNIQUE GT")
            return gt_unique
        print("LOADING GT")
        return gt

    individuals = [f"animal_{i:03d}" for i in range(num_individuals)]
    loader = Mock()
    loader.get_dataset_parameters.return_value = data.PoseDatasetParameters(
        bodyparts=bodyparts,
        unique_bpts=unique,
        individuals=individuals,
    )
    loader.ground_truth_keypoints = _gt
    loader.model_cfg = {
        "metadata": {
            "bodyparts": bodyparts,
            "unique_bodyparts": unique,
            "individuals": individuals,
            "with_identity": False,
        },
        "train_settings": {},
    }
    return loader


--- File: tests/pose_estimation_pytorch/apis/test_tracklets.py ---
import numpy as np
import pandas as pd
import pytest

from deeplabcut.pose_estimation_pytorch.apis.tracklets import build_tracklets


@pytest.mark.parametrize(
    "assemblies_data, inference_cfg, joints, scorer, num_frames, unique_bodyparts",
    [
        (
            # assemblies_data
            {
                "single": {
                    0: np.array([[1, 2, 0.9]]),
                    1: np.array([[1, 3, 0.7]]),
                    2: np.array([[0, 1, 0.9]]),
                },
                0: [
                    np.array([[10, 20, 0.9, -1], [30, 40, 0.8, -1]]),
                    np.array([[13, 23, 0.9, -1], [33, 43, 0.8, -1]]),
                ],
                1: [
                    np.array([[9, 19, 0.9, -1], [29, 41, 0.8, -1]]),
                    np.array([[15, 21, 0.9, -1], [35, 45, 0.8, -1]]),
                ],
                2: [
                    np.array([[13, 23, 0.9, -1], [33, 43, 0.8, -1]]),
                    np.array([[10, 20, 0.9, -1], [30, 40, 0.8, -1]]),
                ],
            },
            # inference_cfg
            {"max_age": 3, "min_hits": 1, "topktoretain": 1, "pcutoff": 0.5},
            # joints
            ["nose", "ear"],
            # scorer
            "DLC",
            # num_frames
            3,
            # unique_bodyparts
            ["led"],
        ),
        (
            # assemblies_data
            {
                0: [
                    np.array([[10, 20, 0.9, -1], [30, 40, 0.8, -1]]),
                    np.array([[13, 23, 0.9, -1], [33, 43, 0.8, -1]]),
                ],
                1: [
                    np.array([[9, 19, 0.9, -1], [29, 41, 0.8, -1]]),
                    np.array([[15, 21, 0.9, -1], [35, 45, 0.8, -1]]),
                ],
                2: [
                    np.array([[13, 23, 0.9, -1], [33, 43, 0.8, -1]]),
                    np.array([[10, 20, 0.9, -1], [30, 40, 0.8, -1]]),
                ],
            },
            # inference_cfg
            {"max_age": 3, "min_hits": 1, "topktoretain": 1, "pcutoff": 0.5},
            # joints
            ["nose", "ear"],
            # scorer
            "DLC",
            # num_frames
            3,
            # unique_bodyparts
            None,
        ),
    ],
)
def test_build_tracklets(
    assemblies_data: dict,
    inference_cfg: dict,
    joints: list,
    scorer: str,
    num_frames: int,
    unique_bodyparts: list,
):
    # Run the function
    tracklets = build_tracklets(
        assemblies_data=assemblies_data,
        track_method="box",
        inference_cfg=inference_cfg,
        joints=joints,
        scorer=scorer,
        num_frames=num_frames,
        unique_bodyparts=unique_bodyparts,
        identity_only=False,
    )

    # # Assertions
    assert "header" in tracklets
    assert isinstance(tracklets["header"], pd.MultiIndex)
    if unique_bodyparts:
        assert "single" in tracklets
    else:
        assert not "single" in tracklets

    assert isinstance(tracklets, dict)


--- File: tests/pose_estimation_pytorch/config/test_make_pose_config.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests the pre-processors"""
import pytest

import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.core.config import pretty_print
from deeplabcut.pose_estimation_pytorch.config.make_pose_config import (
    make_basic_project_config,
    make_pytorch_pose_config,
)
from deeplabcut.pose_estimation_pytorch.config.utils import (
    update_config,
    update_config_by_dotpath,
)


@pytest.mark.parametrize("bodyparts", [["nose"], ["nose", "ear", "eye"]])
@pytest.mark.parametrize(
    "net_type", ["resnet_50", "resnet_101", "hrnet_w18", "hrnet_w32", "hrnet_w48"]
)
def test_make_single_animal_config(bodyparts: list[str], net_type: str):
    # Single animal projects can't have unique bodyparts
    project_config = _make_project_config(
        project_path="my/little/project",
        multianimal=False,
        identity=False,
        individuals=[],
        bodyparts=bodyparts,
        unique_bodyparts=[],
    )
    pytorch_pose_config = make_pytorch_pose_config(
        project_config,
        "pytorch_config.yaml",
        net_type=net_type,
    )
    pretty_print(pytorch_pose_config)

    # check heads are there
    assert "bodypart" in pytorch_pose_config["model"]["heads"].keys()
    # check that the bodypart head has locref and heatmaps and the correct output shapes
    bodypart_head = pytorch_pose_config["model"]["heads"]["bodypart"]

    outputs = [("heatmap_config", len(bodyparts))]
    if bodypart_head["predictor"]["location_refinement"]:
        outputs += [("locref_config", 2 * len(bodyparts))]

    for name, output_channels in outputs:
        head = bodypart_head[name]
        if "final_conv" in head:
            actual_output_channels = head["final_conv"]["out_channels"]
        else:
            actual_output_channels = head["channels"][-1]
        assert name in bodypart_head
        assert actual_output_channels == output_channels


@pytest.mark.parametrize("multianimal", [True])
@pytest.mark.parametrize("individuals", [["single"], ["bugs", "daffy"]])
@pytest.mark.parametrize("bodyparts", [["nose"], ["nose", "ear", "eye"]])
@pytest.mark.parametrize("identity", [False, True])
@pytest.mark.parametrize("unique_bodyparts", [[], ["tail"]])
@pytest.mark.parametrize(
    "net_type", ["resnet_50", "resnet_101", "hrnet_w18", "hrnet_w32", "hrnet_w48"]
)
def test_backbone_plus_paf_config(
    multianimal: bool,
    individuals: list[str],
    bodyparts: list[str],
    identity: bool,
    unique_bodyparts: list[str],
    net_type: str,
):
    # Single animal projects can't have unique bodyparts
    project_config = _make_project_config(
        project_path="my/little/project",
        multianimal=multianimal,
        identity=identity,
        individuals=individuals,
        bodyparts=bodyparts,
        unique_bodyparts=unique_bodyparts,
    )
    pytorch_pose_config = make_pytorch_pose_config(
        project_config,
        "pytorch_config.yaml",
        net_type=net_type,
    )
    pretty_print(pytorch_pose_config)

    graph = [
        [i, j] for i in range(len(bodyparts)) for j in range(i + 1, len(bodyparts))
    ]
    num_limbs = len(graph) * 2

    # check heads are there
    assert "bodypart" in pytorch_pose_config["model"]["heads"].keys()
    bodypart_head = pytorch_pose_config["model"]["heads"]["bodypart"]

    # check PAF head
    assert bodypart_head["type"] == "DLCRNetHead"
    assert bodypart_head["predictor"]["type"] == "PartAffinityFieldPredictor"

    for name, output_channels in [
        ("heatmap_config", len(bodyparts)),
        ("locref_config", len(bodyparts) * 2),
        ("paf_config", num_limbs),
    ]:
        print(name, bodypart_head[name]["channels"])
        assert name in bodypart_head
        assert bodypart_head[name]["channels"][-1] == output_channels

    if len(unique_bodyparts) > 0:
        assert "unique_bodypart" in pytorch_pose_config["model"]["heads"].keys()
        unique_bodypart_head = pytorch_pose_config["model"]["heads"]["unique_bodypart"]
        for name, output_channels in [
            ("heatmap_config", len(unique_bodyparts)),
            ("locref_config", 2 * len(unique_bodyparts)),
        ]:
            assert name in unique_bodypart_head
            assert unique_bodypart_head[name]["channels"][-1] == output_channels
        assert unique_bodypart_head["target_generator"]["heatmap_mode"] == "KEYPOINT"

    if identity:
        assert "identity" in pytorch_pose_config["model"]["heads"].keys()
        id_head = pytorch_pose_config["model"]["heads"]["identity"]
        assert "heatmap_config" in id_head
        assert id_head["heatmap_config"]["channels"][-1] == len(individuals)
        assert "locref_config" not in id_head
        assert id_head["target_generator"]["heatmap_mode"] == "INDIVIDUAL"


@pytest.mark.parametrize(
    "detector",
    [
        (None, "SSDLite"),
        ("ssdlite", "SSDLite"),
        ("fasterrcnn_mobilenet_v3_large_fpn", "FasterRCNN"),
        ("fasterrcnn_resnet50_fpn_v2", "FasterRCNN"),
    ],
)
@pytest.mark.parametrize("individuals", [["single"], ["bugs", "daffy"]])
@pytest.mark.parametrize("bodyparts", [["nose"], ["nose", "ear", "eye"]])
@pytest.mark.parametrize(
    "net_type", ["resnet_50", "resnet_101", "hrnet_w18", "hrnet_w32", "hrnet_w48"]
)
def test_top_down_config(
    detector: tuple[str, str],
    individuals: list[str],
    bodyparts: list[str],
    net_type: str,
):
    # Single animal projects can't have unique bodyparts
    detector_type, expected_detector_type = detector
    project_config = _make_project_config(
        project_path="my/little/project",
        multianimal=True,
        identity=False,
        individuals=individuals,
        bodyparts=bodyparts,
        unique_bodyparts=[],
    )
    pytorch_pose_config = make_pytorch_pose_config(
        project_config,
        "pytorch_config.yaml",
        net_type=net_type,
        top_down=True,
        detector_type=detector_type,
    )
    pretty_print(pytorch_pose_config)

    # check no collate function
    collate = pytorch_pose_config["data"]["train"].get("collate")
    print(f"Collate: {collate}")
    assert not collate

    # check heads are there
    assert "bodypart" in pytorch_pose_config["model"]["heads"].keys()
    bodypart_head = pytorch_pose_config["model"]["heads"]["bodypart"]

    # check detector is there
    assert "detector" in pytorch_pose_config.keys()
    assert pytorch_pose_config["detector"]["model"]["type"] == expected_detector_type

    for name, output_channels in [
        ("heatmap_config", len(bodyparts)),
    ]:
        print(name, bodypart_head[name]["channels"])
        assert name in bodypart_head
        assert bodypart_head[name]["final_conv"]["out_channels"] == output_channels


@pytest.mark.parametrize("multianimal", [True])
@pytest.mark.parametrize("individuals", [["single"], ["bugs", "daffy"]])
@pytest.mark.parametrize("bodyparts", [["nose"], ["nose", "ear", "eye"]])
@pytest.mark.parametrize("identity", [False, True])
@pytest.mark.parametrize("unique_bodyparts", [[], ["tail"]])
@pytest.mark.parametrize("net_type", ["dekr_w18", "dekr_w32", "dekr_w48"])
def test_make_dekr_config(
    multianimal: bool,
    individuals: list[str],
    bodyparts: list[str],
    identity: bool,
    unique_bodyparts: list[str],
    net_type: str,
):
    project_config = _make_project_config(
        project_path="my/little/project",
        multianimal=multianimal,
        identity=identity,
        individuals=individuals,
        bodyparts=bodyparts,
        unique_bodyparts=unique_bodyparts,
    )
    pytorch_pose_config = make_pytorch_pose_config(
        project_config,
        "pytorch_config.yaml",
        net_type=net_type,
    )
    pretty_print(pytorch_pose_config)

    # check heads are there
    assert "bodypart" in pytorch_pose_config["model"]["heads"].keys()
    bodypart_head = pytorch_pose_config["model"]["heads"]["bodypart"]
    for name, output_channels in [
        ("heatmap_config", len(bodyparts) + 1),
        ("offset_config", len(bodyparts)),
    ]:
        print(name, bodypart_head[name]["channels"])
        assert name in bodypart_head
        assert bodypart_head[name]["channels"][-1] == output_channels

    if len(unique_bodyparts) > 0:
        assert "unique_bodypart" in pytorch_pose_config["model"]["heads"].keys()
        unique_bodypart_head = pytorch_pose_config["model"]["heads"]["unique_bodypart"]
        for name, output_channels in [
            ("heatmap_config", len(unique_bodyparts)),
            ("locref_config", 2 * len(unique_bodyparts)),
        ]:
            assert name in unique_bodypart_head
            assert unique_bodypart_head[name]["channels"][-1] == output_channels
        assert unique_bodypart_head["target_generator"]["heatmap_mode"] == "KEYPOINT"

    if identity:
        assert "identity" in pytorch_pose_config["model"]["heads"].keys()
        id_head = pytorch_pose_config["model"]["heads"]["identity"]
        assert "heatmap_config" in id_head
        assert id_head["heatmap_config"]["channels"][-1] == len(individuals)
        assert "locref_config" not in id_head
        assert id_head["target_generator"]["heatmap_mode"] == "INDIVIDUAL"


@pytest.mark.parametrize("multianimal", [True])
@pytest.mark.parametrize("individuals", [["single"], ["bugs", "daffy"]])
@pytest.mark.parametrize("bodyparts", [["nose", "ears"], ["nose", "ear", "eye"]])
@pytest.mark.parametrize("identity", [False, True])
@pytest.mark.parametrize("unique_bodyparts", [[], ["tail"]])
@pytest.mark.parametrize("net_type", ["dlcrnet_stride16_ms5", "dlcrnet_stride32_ms5"])
def test_make_dlcrnet_config(
    multianimal: bool,
    individuals: list[str],
    bodyparts: list[str],
    identity: bool,
    unique_bodyparts: list[str],
    net_type: str,
):
    project_config = _make_project_config(
        project_path="my/little/project",
        multianimal=multianimal,
        identity=identity,
        individuals=individuals,
        bodyparts=bodyparts,
        unique_bodyparts=unique_bodyparts,
    )
    pytorch_pose_config = make_pytorch_pose_config(
        project_config,
        "pytorch_config.yaml",
        net_type=net_type,
    )
    pretty_print(pytorch_pose_config)
    paf_graph = [
        [i, j] for i in range(len(bodyparts)) for j in range(i + 1, len(bodyparts))
    ]
    num_limbs = len(paf_graph)

    # check heads are there
    assert "bodypart" in pytorch_pose_config["model"]["heads"].keys()
    bodypart_head = pytorch_pose_config["model"]["heads"]["bodypart"]
    for name, output_channels in [
        ("heatmap_config", len(bodyparts)),
        ("locref_config", 2 * len(bodyparts)),
        ("paf_config", 2 * num_limbs),
    ]:
        print(name, bodypart_head[name]["channels"])
        assert name in bodypart_head
        assert bodypart_head[name]["channels"][-1] == output_channels

    if len(unique_bodyparts) > 0:
        assert "unique_bodypart" in pytorch_pose_config["model"]["heads"].keys()
        unique_bodypart_head = pytorch_pose_config["model"]["heads"]["unique_bodypart"]
        for name, output_channels in [
            ("heatmap_config", len(unique_bodyparts)),
            ("locref_config", 2 * len(unique_bodyparts)),
        ]:
            assert name in unique_bodypart_head
            assert unique_bodypart_head[name]["channels"][-1] == output_channels
        assert unique_bodypart_head["target_generator"]["heatmap_mode"] == "KEYPOINT"

    if identity:
        assert "identity" in pytorch_pose_config["model"]["heads"].keys()
        id_head = pytorch_pose_config["model"]["heads"]["identity"]
        assert "heatmap_config" in id_head
        assert id_head["heatmap_config"]["channels"][-1] == len(individuals)
        assert "locref_config" not in id_head
        assert id_head["target_generator"]["heatmap_mode"] == "INDIVIDUAL"


@pytest.mark.parametrize("individuals", [["single"], ["bugs", "daffy"]])
@pytest.mark.parametrize("bodyparts", [["nose", "eyes"], ["nose", "ear", "eye"]])
@pytest.mark.parametrize("identity", [False, True])
@pytest.mark.parametrize("unique_bodyparts", [[], ["tail"]])
@pytest.mark.parametrize("net_type", ["animaltokenpose_base"])
def test_make_tokenpose_config(
    individuals: list[str],
    bodyparts: list[str],
    identity: bool,
    unique_bodyparts: list[str],
    net_type: str,
):
    project_config = _make_project_config(
        project_path="my/little/project",
        multianimal=True,
        identity=identity,
        individuals=individuals,
        bodyparts=bodyparts,
        unique_bodyparts=unique_bodyparts,
    )

    if identity or len(unique_bodyparts) > 0:
        with pytest.raises(ValueError) as err_info:
            # Not yet implemented!
            _ = make_pytorch_pose_config(
                project_config,
                "pytorch_config.yaml",
                net_type=net_type,
            )
    else:
        pytorch_pose_config = make_pytorch_pose_config(
            project_config,
            "pytorch_config.yaml",
            net_type=net_type,
        )
        pretty_print(pytorch_pose_config)

        # check no collate function
        collate = pytorch_pose_config["data"]["train"].get("collate")
        print(f"Collate: {collate}")
        assert not collate

        # check detector is there
        assert "detector" in pytorch_pose_config
        assert "data" in pytorch_pose_config["detector"]


@pytest.mark.parametrize(
    "data",
    [
        {
            "config": {"a": 0, "b": 0},
            "updates": {"b": 1},
            "expected_result": {"a": 0, "b": 1},
        },
        {
            "config": {"a": 0, "b": {"i0": 1, "i1": 2}},
            "updates": {"b": 1},
            "expected_result": {"a": 0, "b": 1},
        },
        {
            "config": {"a": 0, "b": {"i0": 1, "i1": 2}},
            "updates": {"b": {"i0": [1, 2, 3]}},
            "expected_result": {"a": 0, "b": {"i0": [1, 2, 3], "i1": 2}},
        },
        {
            "config": {"detector": {"batch_size": 1, "epochs": 10, "save_epochs": 5}},
            "updates": {
                "batch_size": 1,
                "detector": {"batch_size": 8, "save_epochs": 1},
            },
            "expected_result": {
                "batch_size": 1,
                "detector": {"batch_size": 8, "epochs": 10, "save_epochs": 1},
            },
        },
    ],
)
def test_update_config(data: dict):
    result = update_config(config=data["config"], updates=data["updates"])
    print("\nResult")
    pretty_print(result)
    assert result == data["expected_result"]


@pytest.mark.parametrize(
    "data",
    [
        {
            "config": {"a": 0, "b": 0},
            "updates": {"b": 1},
            "expected_result": {"a": 0, "b": 1},
        },
        {
            "config": {"a": 0, "b": {"i0": 1, "i1": 2}},
            "updates": {"b": 1},
            "expected_result": {"a": 0, "b": 1},
        },
        {
            "config": {"a": 0, "b": {"i0": 1, "i1": 2}},
            "updates": {"b.i0": [1, 2, 3]},
            "expected_result": {"a": 0, "b": {"i0": [1, 2, 3], "i1": 2}},
        },
        {
            "config": {"detector": {"batch_size": 1, "epochs": 10, "save_epochs": 5}},
            "updates": {
                "batch_size": 1,
                "detector.batch_size": 8,
                "detector.save_epochs": 1,
            },
            "expected_result": {
                "batch_size": 1,
                "detector": {"batch_size": 8, "epochs": 10, "save_epochs": 1},
            },
        },
    ],
)
def test_update_config_by_dotpath(data: dict):
    result = update_config_by_dotpath(config=data["config"], updates=data["updates"])
    print("\nResult")
    pretty_print(result)
    assert result == data["expected_result"]


def _make_project_config(
    project_path: str,
    multianimal: bool,
    identity: bool,
    individuals: list[str],
    bodyparts: list[str],
    unique_bodyparts: list[str],
) -> dict:
    project_config = {
        "project_path": project_path,
        "multianimalproject": multianimal,
        "identity": identity,
        "uniquebodyparts": unique_bodyparts,
    }

    if multianimal:
        project_config["multianimalbodyparts"] = bodyparts
        project_config["bodyparts"] = "MULTI!"
        project_config["individuals"] = individuals
    else:
        project_config["bodyparts"] = bodyparts

    return project_config


@pytest.mark.parametrize("bodyparts", [["nose"], ["nose", "ear", "eye"]])
@pytest.mark.parametrize("max_idv", [1, 12, 20])
@pytest.mark.parametrize("multi", [True, False])
def test_make_basic_project_config(bodyparts: list[str], max_idv: int, multi: bool):
    if not multi and max_idv > 1:
        return

    project_config = make_basic_project_config(
        dataset_path="path/dataset",
        bodyparts=bodyparts,
        max_individuals=max_idv,
        multi_animal=multi,
    )

    bpts = af.get_bodyparts(project_config)
    assert bodyparts == bpts

    individuals = project_config["individuals"]
    assert len(individuals) == max_idv
    assert len(set(individuals)) == max_idv


--- File: tests/pose_estimation_pytorch/config/test_config_utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Test util functions for config creation"""
import pytest

import deeplabcut.pose_estimation_pytorch.config.utils as utils


@pytest.mark.parametrize(
    "data",
    [
        dict(
            config={},
            num_bodyparts=None,
            num_individuals=None,
            backbone_output_channels=None,
            output_config={},
        ),
        dict(
            config={
                "a": "num_bodyparts",
                "b": ["num_bodyparts // 2", "num_bodyparts // 3"],
                "c": "num_bodyparts x 2",
                "d": "num_bodyparts + 2",
            },
            num_bodyparts=10,
            num_individuals=None,
            backbone_output_channels=None,
            output_config={
                "a": 10,
                "b": [5, 3],
                "c": 20,
                "d": 12,
            },
        ),
        dict(
            config={
                "a": [{"b": "num_individuals x 3"}],
                "b": [[{"b": "num_bodyparts x 3"}]],
            },
            num_bodyparts=10,
            num_individuals=1,
            backbone_output_channels=None,
            output_config={
                "a": [{"b": 3}],
                "b": [[{"b": 30}]],
            },
        )
    ],
)
def test_replace_default_values_no_extras(data: dict):
    output_config = utils.replace_default_values(
        config=data["config"],
        num_bodyparts=data["num_bodyparts"],
        num_individuals=data["num_individuals"],
        backbone_output_channels=data["backbone_output_channels"],
    )
    assert output_config == data["output_config"]


--- File: tests/pose_estimation_pytorch/other/test_data_helper.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import os
from unittest.mock import patch, Mock
from zipfile import Path

import numpy as np
import pytest

from deeplabcut.pose_estimation_pytorch.data.dlcloader import DLCLoader
from deeplabcut.pose_estimation_pytorch.data.utils import merge_list_of_dicts
from deeplabcut.generate_training_dataset import create_training_dataset


def mock_aux() -> Mock:
    aux_functions = Mock()
    aux_functions.read_plainconfig = Mock()
    aux_functions.read_plainconfig.return_value = {}
    return aux_functions


@patch("deeplabcut.pose_estimation_pytorch.data.base.auxiliaryfunctions", mock_aux())
def _get_loader(project_root):
    if not (Path(project_root) / "training-datasets").exists():
        create_training_dataset(config=str(Path(project_root) / "config.yaml"))
    return DLCLoader(Path(project_root) / "config.yaml", shuffle=1)


@pytest.mark.skip
@pytest.mark.parametrize("repo_path", ["/home/anastasiia/DLCdev"])
def test_propertymeta_project(repo_path):
    project_root = os.path.join(repo_path, "examples", "openfield-Pranav-2018-10-30")
    dlc_loader = _get_loader(project_root)

    for prop in dlc_loader.properties:
        print(prop, getattr(dlc_loader, prop))


@pytest.mark.skip
@pytest.mark.parametrize(
    "repo_path, mode",
    [("/home/anastasiia/DLCdev", "train"), ("/home/anastasiia/DLCdev", "test")],
)
def test_propertymeta_dataset(repo_path, mode):
    repo_path = "/home/anastasiia/DLCdev"
    mode = "train"
    project_root = os.path.join(repo_path, "examples", "openfield-Pranav-2018-10-30")
    dlc_loader = _get_loader(project_root)
    dataset = dlc_loader.create_dataset(transform=None, mode=mode)

    for prop in dataset.properties:
        print(prop, getattr(dataset, prop))


@pytest.mark.parametrize(
    "list_dicts, keys_to_include",
    [
        ([{"a": 1, "b": 2}, {"a": 3, "b": 4}], ["a"]),
        (
            [
                *[
                    {
                        "keypoints": np.random.randn(27, 3),
                        "images": np.random.randn(256, 192),
                    }
                ]
                * 10
            ],
            [*["keypoints", "images"] * 10],
        ),
    ],
)
def test_merge_list_of_dicts(list_dicts, keys_to_include):
    result_dict = merge_list_of_dicts(list_dicts, keys_to_include)
    expected_result_dict = {}
    for dictionary in list_dicts:
        for key in dictionary:
            if key not in keys_to_include:
                continue
            else:
                if key not in expected_result_dict:
                    expected_result_dict[key] = []
                expected_result_dict[key].append(dictionary[key])
    assert result_dict == expected_result_dict


--- File: tests/pose_estimation_pytorch/other/test_helper.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import torch


def test_train_valid_call():
    tmp_model = torch.nn.Linear(3, 10)
    to_train_mode = getattr(tmp_model, "train")
    to_train_mode()
    assert tmp_model.training == True
    to_valid_mode = getattr(tmp_model, "eval")
    to_valid_mode()
    assert tmp_model.training == False


--- File: tests/pose_estimation_pytorch/other/test_match_predictions_to_gt.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import numpy as np
import pytest

import deeplabcut.pose_estimation_pytorch.post_processing.match_predictions_to_gt as deeplabcut_torch_match_predictions_gt


@pytest.fixture
def animals_and_keypoints_invalid():
    """Summary:
    Fixture with invalid pred_kpts and gt_kpts shapes that will raise ValueErrors.

    Returns:
        tuple containing:
             predicted keypoints(pred_kpts), of shape num_animals, num_keypoints, (x,y,score)
             ground truth keypoints (gt_kpts), of shape num_animals, num_keypoints, (x,y)
             individual names (indv_names)
    """
    gt_kpts = 2 * np.ones((6, 6, 3))  # num animals, num keypoints, (x,y,vis)
    gt_kpts[:, :, :2] = np.random.rand(6, 6, 2)
    pred_kpts = np.random.rand(6, 8, 3)  # num animals, num keypoints, (x,y,score)
    indv_names = ["indv1", "indv2"]
    return pred_kpts, gt_kpts, indv_names


@pytest.fixture
def animals_and_keypoints():
    """Summary:
    Fixture with pred_kpts, gt_kpts shapes and indv_names.

    Returns:
        tuple containing:
             predicted keypoints(pred_kpts), of shape num_animals, num_keypoints, (x,y,score)
             ground truth keypoints (gt_kpts), of shape num_animals, num_keypoints, (x,y)
             individual names (indv_names)
    """
    gt_kpts = 2 * np.ones((6, 6, 3))  # num animals, num keypoints, (x,y,vis)
    gt_kpts[:, :, :2] = np.random.rand(6, 6, 2)

    # adding score value because the shape of pred_kpts should be (6,6,3)
    score = np.full((gt_kpts.shape[0], gt_kpts.shape[1], 1), 0.5)
    pred_kpts = np.concatenate((gt_kpts, score), axis=2)
    np.random.shuffle(pred_kpts)  # shuffle predicted keypoints

    indv_names = ["indv1", "indv2"]
    return pred_kpts, gt_kpts, indv_names


def test_invalid_rmse(animals_and_keypoints_invalid: tuple) -> None:
    """Summary:
    Tets if an invalid output really returns a ValueError in the rmse function.

    Args:
        animals_and_keypoints_invalid (tuple): containing predicted keypoints (pred_kpts),
        ground truth keypoints (gt_kpts) and individual names (indv_names).
    """
    pred_kpts, gt_kpts, indv_names = animals_and_keypoints_invalid

    with pytest.raises(ValueError):
        deeplabcut_torch_match_predictions_gt.rmse_match_prediction_to_gt(
            pred_kpts, gt_kpts
        )


def test_invalid_oks(animals_and_keypoints_invalid: tuple) -> None:
    """Summary:
    Test if an invalid output really returns a ValueError in the oks function.

    Args:
        animals_and_keypoints_invalid   (tuple): containing predicted keypoints (pred_kpts), ground truth keypoints (gt_kpts)
                and individual names (indv_names)
    """
    pred_kpts, gt_kpts, indv_names = animals_and_keypoints_invalid

    with pytest.raises(ValueError):
        deeplabcut_torch_match_predictions_gt.oks_match_prediction_to_gt(
            pred_kpts, gt_kpts, indv_names
        )


def test_rmse_match_predictions_to_gt(
    animals_and_keypoints: tuple, num_animals: int = 6
) -> None:
    """Summary:
    Test if rmse_match_prediction_to_gt function returns the expected shape output.

    Args:
        animals_and_keypoints (tuple): containing predicted keypoints (pred_kpts), ground truth keypoints (gt_kpts)
                and individual names (indv_names)
    """
    pred_kpts, gt_kpts, indv_names = animals_and_keypoints

    col_ind = deeplabcut_torch_match_predictions_gt.rmse_match_prediction_to_gt(
        pred_kpts, gt_kpts
    )
    assert isinstance(col_ind, np.ndarray)
    assert col_ind.shape == (num_animals,)


def test_oks_match_predictions_to_gt(
    animals_and_keypoints: tuple, num_animals: int = 6
) -> None:
    """Summary:
    Test if oks_match_predictions_to_gt function returns the expected shape output.

    Args:
        animals_and_keypoints (tuple): containing predicted keypoints (pred_kpts), ground truth keypoints (gt_kpts)
                and individual names (indv_names)
    """
    pred_kpts, gt_kpts, indv_names = animals_and_keypoints

    col_ind = deeplabcut_torch_match_predictions_gt.rmse_match_prediction_to_gt(
        pred_kpts, gt_kpts
    )
    assert isinstance(col_ind, np.ndarray)
    assert col_ind.shape == (num_animals,)


def test_extend_col_ind(animals_and_keypoints: tuple, num_animals: int = 6) -> None:
    """Summary:
    Test if the column indices have the expected shape.

    Args:
        animals_and_keypoints (tuple): containing predicted keypoints (pred_kpts), ground truth keypoints (gt_kpts)
                and individual names (indv_names)
    """
    pred_kpts, gt_kpts, indv_names = animals_and_keypoints

    col_ind = deeplabcut_torch_match_predictions_gt.rmse_match_prediction_to_gt(
        pred_kpts, gt_kpts
    )
    extended_array = deeplabcut_torch_match_predictions_gt.extend_col_ind(
        col_ind, num_animals
    )
    assert extended_array.shape == (num_animals,)


--- File: tests/pose_estimation_pytorch/other/test_gaussian_targets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import pytest
import torch

from deeplabcut.pose_estimation_pytorch.models.target_generators import HeatmapGaussianGenerator


@pytest.mark.parametrize(
    "batch_size, num_keypoints, image_size",
    [(2, 2, (64, 64)), (1, 5, (48, 64)), (15, 50, (64, 48))],
)
def test_gaussian_target_generation(
    batch_size: int, num_keypoints: int, image_size: tuple, num_animals=1
):
    # generate annotations
    labels = {
        "keypoints": torch.randint(
            1, min(image_size), (batch_size, num_animals, num_keypoints, 2)
        )
    }  # batch size, num animals, num keypoints, 2 for x,y
    # generate predictions
    stride = 1
    prediction = {
        "heatmap": torch.rand((batch_size, num_keypoints, *image_size[:2])),
        "locref": torch.rand((batch_size, 2 * num_keypoints, *image_size[:2])),
    }

    # generate heatmap
    output = HeatmapGaussianGenerator(
        num_heatmaps=num_keypoints,
        pos_dist_thresh=17,
        locref_std=5.0,
    )
    output = output(stride, prediction, labels)["heatmap"]["target"].reshape(
        batch_size, num_keypoints, image_size[0] * image_size[1]
    )

    # get coords of max value of the heatmap
    gaus_max = torch.argmax(output, dim=2)

    # get unraveled coords
    x = gaus_max % image_size[1]
    y = gaus_max // image_size[1]

    # get heatmap center tensor
    predict_kp = torch.stack((x, y), dim=-1)
    # Remove num_animals dimension - only one animal is supported
    labels["keypoints"] = torch.squeeze(labels["keypoints"], dim=1)

    # compare heatmap center to annotation
    assert torch.eq(labels["keypoints"], predict_kp).all().item()


--- File: tests/pose_estimation_pytorch/other/test_api_utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import random

import numpy as np
import pytest

import deeplabcut.pose_estimation_pytorch.data.transforms as transforms

transform_dicts = [
    {"auto_padding": {"pad_height_divisor": 64, "pad_width_divisor": 27}},
    {"resize": {"height": 512, "width": 256, "keep_ration": True}},
    {
        "covering": True,
        "gaussian_noise": 12.75,
        "hist_eq": True,
        "motion_blur": True,
        "normalize_images": True,
        "rotation": 30,
        "scale_jitter": [0.5, 1.25],
        "auto_padding": {"pad_width_divisor": 64, "pad_height_divisor": 27},
    },
    {
        "covering": True,
        "gaussian_noise": 100,
        "hist_eq": True,
        "motion_blur": True,
        "normalize_images": True,
        "rotation": 180,
        "scale_jitter": [0.03, 20],
        "auto_padding": {"pad_width_divisor": 64, "pad_height_divisor": 27},
    },
]


def _get_random_params(transform_idx):
    return (
        transform_dicts[transform_idx],
        (random.randint(100, 1000), random.randint(100, 1000)),
        random.randint(1, 100),
        random.randint(1, 100),
    )


@pytest.mark.parametrize(
    "transform_dict, size_image, num_keypoints, num_animals",
    [_get_random_params(i) for i in range(4)],
)
def test_build_transforms(transform_dict, size_image, num_keypoints, num_animals):
    transform_bbox_aug = transforms.build_transforms(transform_dict)
    w, h = size_image
    for i in range(10):
        test_image = np.random.randint(0, 255, (h, w, 3), dtype=np.uint8)
        bboxes = np.random.randint(0, min(w - 1, h - 1), (num_animals, 4))
        bboxes[:, 2] = w - bboxes[:, 0]
        bboxes[:, 3] = h - bboxes[:, 1]
        keypoints = np.random.randint(0, min(w, h), (num_keypoints, 2))

        with pytest.raises(Exception):
            transformed = transform_bbox_aug(image=test_image)
            transformed = transform_bbox_aug(image=test_image, bboxes=bboxes.copy())
            transformed = transform_bbox_aug(
                image=test_image, keypoints=keypoints.copy(), bboxes=bboxes.copy()
            )

        transformed_with_bbox = transform_bbox_aug(
            image=test_image,
            keypoints=keypoints.copy(),
            bboxes=bboxes.copy(),
            bbox_labels=np.arange(num_animals),
            class_labels=[0 for _ in range(len(keypoints))]
        )

        if "resize" in transform_dict.keys():
            assert transformed_with_bbox["image"].shape[:2] == (
                transform_dict["resize"]["height"],
                transform_dict["resize"]["width"],
            )

        if "auto_padding" in transform_dict.keys():
            modh, modw = (
                transform_dict["auto_padding"]["pad_height_divisor"],
                transform_dict["auto_padding"]["pad_width_divisor"],
            )
            assert transformed_with_bbox["image"].shape[0] % modh == 0
            assert transformed_with_bbox["image"].shape[1] % modw == 0

        assert len(transformed_with_bbox["keypoints"]) == len(keypoints)


--- File: tests/pose_estimation_pytorch/other/test_modelzoo.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os

import pytest

from deeplabcut.modelzoo.video_inference import video_inference_superanimal
from deeplabcut.utils import auxiliaryfunctions

examples_folder = os.path.join(
    auxiliaryfunctions.get_deeplabcut_path(),
    "modelzoo",
    "examples",
)

# requires videos to be in the examples folder
@pytest.mark.skip
@pytest.mark.parametrize(
    "video_paths, superanimal_name",
    [
        (f"{examples_folder}/black_dog.mp4", "superanimal_quadruped"),
        (f"{examples_folder}/black_dog.mp4", "superanimal_quadruped_hrnetw32"),
        (f"{examples_folder}/swear_mouse_tiny.mp4", "superanimal_topviewmouse"),
        (
            f"{examples_folder}/swear_mouse_tiny.mp4",
            "superanimal_topviewmouse_hrnetw32",
        ),
    ],
)
def test_video_inference_saves_file(video_paths, superanimal_name):
    video_inference_superanimal(
        video_paths,
        superanimal_name=superanimal_name,
    )
    if isinstance(video_paths, str):
        video_paths = [video_paths]
    for video_path in video_paths:
        output_path = video_path.replace(".mp4", f"_labeled.mp4")
        assert os.path.exists(output_path), "Output video file does not exist"

        assert os.stat(output_path).st_size > 0, "Output video file is empty"


--- File: tests/pose_estimation_pytorch/other/test_paf_targets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import pytest
import torch

from deeplabcut.pose_estimation_pytorch.models.target_generators import pafs_targets


@pytest.mark.parametrize(
    "batch_size, num_keypoints, image_size",
    [(2, 2, (64, 64)), (1, 5, (48, 64)), (8, 50, (64, 48))],
)
def test_paf_target_generation(
    batch_size: int, num_keypoints: int, image_size: tuple, num_animals=2
):
    labels = {
        "keypoints": torch.randint(
            1, min(image_size), (batch_size, num_animals, num_keypoints, 2)
        )
    }  # 2 for x,y coords
    graph = [(i, j) for i in range(num_keypoints) for j in range(i + 1, num_keypoints)]
    prediction = {
        "heatmap": torch.rand((batch_size, num_keypoints, image_size[0], image_size[1])),
        "paf": torch.rand((batch_size, len(graph) * 2, image_size[0], image_size[1])),
    }
    generator = pafs_targets.PartAffinityFieldGenerator(graph=graph, width=20)
    targets_output = generator(1, prediction, labels)
    assert targets_output["paf"]["target"].shape == (
        batch_size,
        len(graph) * 2,
        image_size[0],
        image_size[1],
    )


--- File: tests/pose_estimation_pytorch/other/test_dataset.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import random
from pathlib import Path
from unittest.mock import Mock, patch

import albumentations as A
import pytest
from torch.utils.data import DataLoader

import deeplabcut.pose_estimation_pytorch as dlc
import deeplabcut.utils.auxiliaryfunctions as dlc_auxfun
from deeplabcut.core.engine import Engine
from deeplabcut.generate_training_dataset import create_training_dataset


def mock_config() -> Mock:
    aux_functions = Mock()
    aux_functions.read_config_as_dict = Mock()
    aux_functions.read_config_as_dict.return_value = {
        "data": {"train": {}, "inference": {}},
        "metadata": {
            "project_path": "",
            "pose_config_path": "",
            "bodyparts": ["snout", "leftear", "rightear", "tailbase"],
            "unique_bodyparts": [],
            "individuals": ["animal"],
            "with_identity": False,
        },
        "method": "bu",
    }
    return aux_functions


@patch("deeplabcut.pose_estimation_pytorch.data.base.config", mock_config())
def _get_dataset(path, transform, mode="train"):
    project_root = Path(path)
    if not (project_root / "training-datasets").exists():
        print(str(project_root / "config.yaml"))
        create_training_dataset(
            config=str(project_root / "config.yaml"),
            net_type="resnet_50",
            engine=Engine.PYTORCH,
        )

    loader = dlc.DLCLoader(Path(project_root) / "config.yaml", shuffle=1)
    dataset = loader.create_dataset(transform=transform, mode=mode)
    return dataset


def _get_openfield_dataset(transform=None):
    dlc_path = dlc_auxfun.get_deeplabcut_path()
    repo_path = os.path.dirname(dlc_path)
    openfield_path = os.path.join(repo_path, "examples", "openfield-Pranav-2018-10-30")

    return _get_dataset(openfield_path, transform=transform)


key_set = {
    "offsets",
    "path",
    "scales",
    "image",
    "original_size",
    "annotations",
    "image_id",
}
anno_key_set = {
    "keypoints",
    "keypoints_unique",
    "with_center_keypoints",
    "area",
    "boxes",
    "is_crowd",
    "labels",
    "individual_ids",
}


@pytest.mark.parametrize("batch_size", [1, 2, random.randint(2, 20)])
def test_iter_all_dataset_no_transform(batch_size):
    if batch_size > 1:  # if batched, all images need to be the same size
        transform = A.Compose(
            [A.Resize(512, 512)],
            keypoint_params=A.KeypointParams(format="xy"),
            bbox_params=A.BboxParams(format="coco", label_fields=["bbox_labels"]),
        )
    else:
        transform = A.Compose(
            [A.Normalize()],
            keypoint_params=A.KeypointParams(format="xy"),
            bbox_params=A.BboxParams(format="coco", label_fields=["bbox_labels"]),
        )
    dataset = _get_openfield_dataset(transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size)
    max_num_animals = dataset.parameters.max_num_animals
    num_keypoints = dataset.parameters.num_joints
    for i, item in enumerate(dataloader):
        is_last_batch = i == (len(dataloader) - 1)
        assert (
            set(item.keys()) == key_set
        ), "the key returned don't match the required ones"

        anno = item["annotations"]
        assert (
            set(anno.keys()) == anno_key_set
        ), "the annotation keys returned don't match the required ones"

        assert (len(item["image"].shape) == 4) and (
            (item["image"].shape[:2] == (batch_size, 3)) or is_last_batch
        ), "image shape is not (batch_size, 3, h, w)"

        b, _, h, w = item["image"].shape
        kpts, bboxes = anno["keypoints"], anno["boxes"]
        assert (
            kpts.shape == (batch_size, max_num_animals, num_keypoints, 3)
            or is_last_batch
        ), "keypoints have the wrong shape"
        assert (
            bboxes.shape == (batch_size, max_num_animals, 4) or is_last_batch
        ), "boxes have the wrong shape"
        assert ((bboxes[:, :, 0] + bboxes[:, :, 2]) <= w).all() and (
            (bboxes[:, :, 1] + bboxes[:, :, 3]) <= h
        ).all(), "boxes don't seem to be un the format (x, y, w, h)"


def _generate_random_test_values_aug(min_exa):
    batch_size = random.randint(1, 20)
    x_size = random.randint(50, 600)
    y_size = random.randint(50, 600)
    exaggeration = random.randint(min_exa, 99)

    return batch_size, x_size, y_size, exaggeration


@pytest.mark.parametrize(
    "batch_size, x_size, y_size, exaggeration",
    [
        (1, 512, 512, 1),
        _generate_random_test_values_aug(1),
        _generate_random_test_values_aug(50),
    ],
)
def test_iter_all_augmented_dataset(batch_size, x_size, y_size, exaggeration):
    transform = A.Compose(
        [
            A.Affine(
                scale=(1 - exaggeration * 0.01, 1 + exaggeration),
                rotate=(-exaggeration * 2, exaggeration * 2),
                translate_px=(-exaggeration * 10, exaggeration * 10),
            ),
            A.Resize(y_size, x_size),
        ],
        keypoint_params=A.KeypointParams(format="xy", remove_invisible=False),
        bbox_params=A.BboxParams(format="coco", label_fields=["bbox_labels"]),
    )
    dataset = _get_openfield_dataset(transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size)
    max_num_animals = dataset.parameters.max_num_animals
    num_keypoints = dataset.parameters.num_joints
    for i, item in enumerate(dataloader):
        is_last_batch = i == (len(dataloader) - 1)
        assert (
            set(item.keys()) == key_set
        ), "the key returned don't match the required ones"

        anno = item["annotations"]
        assert (
            set(anno.keys()) == anno_key_set
        ), "the annotation keys returned don't match the required ones"

        assert (len(item["image"].shape) == 4) and (
            (item["image"].shape[:2] == (batch_size, 3)) or is_last_batch
        ), "image shape is not (batch_size, 3, h, w)"

        kpts, bboxes = anno["keypoints"], anno["boxes"]
        b, _, h, w = item["image"].shape
        assert (h == y_size) and (w == x_size)
        assert (
            kpts.shape == (batch_size, max_num_animals, num_keypoints, 3)
            or is_last_batch
        ), "keypoints have the wrong shape"
        assert (
            bboxes.shape == (batch_size, max_num_animals, 4) or is_last_batch
        ), "boxes have the wrong shape"
        assert ((bboxes[:, :, 0] + bboxes[:, :, 2]) <= w).all() and (
            (bboxes[:, :, 1] + bboxes[:, :, 3]) <= h
        ).all()


--- File: tests/pose_estimation_pytorch/other/test_custom_transforms.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import pytest

from deeplabcut.pose_estimation_pytorch.data import transforms


@pytest.mark.parametrize("width, height", [(200, 200), (300, 300), (400, 400)])
def test_keypoint_aware_cropping(width, height):
    fake_image = np.empty((600, 600, 3))
    fake_keypoints = [(i * 100, i * 100, 0, 0) for i in range(1, 6)]
    aug = transforms.KeypointAwareCrop(
        width=width, height=height, crop_sampling="density"
    )
    transformed = aug(image=fake_image, keypoints=fake_keypoints)
    assert transformed["image"].shape[:2] == (height, width)
    # Ensure at least a keypoint is visible in each crop
    assert len(transformed["keypoints"])


def test_grayscale():
    fake_image = np.ones((600, 600, 3))
    fake_image *= np.random.uniform(0, 255, size=fake_image.shape)
    fake_image = fake_image.astype(np.uint8)
    gray = transforms.Grayscale(alpha=1, p=1)
    aug_image = gray(image=fake_image)["image"]
    assert aug_image.shape == fake_image.shape

    gray = transforms.Grayscale(alpha=0, p=1)
    aug_image = gray(image=fake_image)["image"]
    assert np.allclose(fake_image, aug_image)

    with pytest.warns(UserWarning, match="clipped"):
        gray = transforms.Grayscale(alpha=1.5)
    assert gray.alpha == 1


def test_coarse_dropout():
    fake_image = np.ones((300, 300, 3))
    fake_image *= np.random.uniform(0, 255, size=fake_image.shape)
    fake_image = fake_image.astype(np.uint8)
    cd = transforms.CoarseDropout(max_height=0.9999, max_width=0.9999, p=1)
    kpts = np.random.rand(10, 2) * 300
    aug_kpts = cd(image=fake_image, keypoints=kpts)["keypoints"]
    assert len(aug_kpts) == kpts.shape[0]
    assert np.isnan([c for kpt in aug_kpts for c in kpt]).all()


--- File: tests/pose_estimation_pytorch/other/test_seq_targets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from itertools import combinations

import torch

from deeplabcut.pose_estimation_pytorch.models.target_generators import (
    TARGET_GENERATORS,
)


def test_sequential_generator():
    batch_size = 4
    image_size = 256, 256
    num_keypoints = 12
    num_animals = 2
    graph = [list(edge) for edge in combinations(range(num_keypoints), 2)]
    num_limbs = len(graph)
    cfg = {
        "type": "SequentialGenerator",
        "generators": [
            {
                "type": "HeatmapPlateauGenerator",
                "num_heatmaps": num_keypoints,
                "pos_dist_thresh": 17,
                "generate_locref": True,
                "locref_std": 7.2801,
            },
            {"type": "PartAffinityFieldGenerator", "graph": graph, "width": 20},
        ],
    }
    gen = TARGET_GENERATORS.build(cfg)

    annotations = {
        "keypoints": torch.randint(
            1, min(image_size), (batch_size, num_animals, num_keypoints, 2)
        )
    }
    head_outputs = {
        "heatmap": torch.rand(batch_size, num_keypoints, 32, 32),
        "locref": torch.rand(batch_size, num_keypoints * 2, 32, 32),
        "paf": torch.rand(batch_size, num_limbs * 2, 32, 32),
    }
    out = gen(stride=1, outputs=head_outputs, labels=annotations)
    assert all(s in out for s in list(head_outputs))
    for k, v in head_outputs.items():
        assert out[k]["target"].shape == v.shape


--- File: tests/pose_estimation_pytorch/other/test_pose_model.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import copy
import random

import pytest
import torch

import deeplabcut.pose_estimation_pytorch.models as dlc_models
from deeplabcut.pose_estimation_pytorch.models import CRITERIONS, TARGET_GENERATORS, PREDICTORS
from deeplabcut.pose_estimation_pytorch.models.criterions import LOSS_AGGREGATORS
from deeplabcut.pose_estimation_pytorch.models.modules import AdaptBlock, BasicBlock

backbones_dicts = [
    {
        "type": "HRNet",
        "model_name": "hrnet_w32",
        "output_channels": 480,
        "stride": 4,
        "interpolate_branches": True,
    },
    {
        "type": "HRNet",
        "model_name": "hrnet_w18",
        "output_channels": 270,
        "stride": 4,
        "interpolate_branches": True,
    },
    {
        "type": "HRNet",
        "model_name": "hrnet_w48",
        "output_channels": 720,
        "stride": 4,
        "interpolate_branches": True,
    },
    {
        "type": "HRNet",
        "model_name": "hrnet_w32",
        "output_channels": 32,
        "interpolate_branches": False,
        "increased_channel_count": False,
        "stride": 4,
    },
    {
        "type": "HRNet",
        "model_name": "hrnet_w18",
        "output_channels": 18,
        "interpolate_branches": False,
        "increased_channel_count": False,
        "stride": 4,
    },
    {
        "type": "HRNet",
        "model_name": "hrnet_w48",
        "output_channels": 48,
        "interpolate_branches": False,
        "increased_channel_count": False,
        "stride": 4,
    },
    {"type": "ResNet", "model_name": "resnet50_gn", "output_channels": 2048, "stride": 32},
]

heads_dicts = [
    {
        "type": "HeatmapHead",
        "predictor": {
            "type": "HeatmapPredictor",
            "location_refinement": True,
            "locref_std": 7.2801,
        },
        "target_generator": {
            "type": "HeatmapPlateauGenerator",
            "num_heatmaps": "num_bodyparts",
            "pos_dist_thresh": 17,
            "heatmap_mode": "KEYPOINT",
            "generate_locref": True,
            "locref_std": 7.2801,
        },
        "criterion": {
            "heatmap": {
                "type": "WeightedBCECriterion",
                "weight": 1.0,
            },
            "locref": {
                "type": "WeightedHuberCriterion",
                "weight": 0.05,
            },
        },
        "heatmap_config": {
            "channels": [2048, 1024, -1],
            "kernel_size": [2, 2],
            "strides": [2, 2],
        },
        "locref_config": {
            "channels": [2048, 1024, -1],
            "kernel_size": [2, 2],
            "strides": [2, 2],
        },
        "output_channels": -1,
        "input_channels": 2048,
        "total_stride": 4,
    },
    {
        "type": "TransformerHead",
        "predictor": {
            "type": "HeatmapPredictor",
            "location_refinement": False,
        },
        "target_generator": {
            "type": "HeatmapPlateauGenerator",
            "num_heatmaps": "num_bodyparts",
            "pos_dist_thresh": 17,
            "heatmap_mode": "KEYPOINT",
            "generate_locref": False,
        },
        "criterion": {"type": "WeightedBCECriterion"},
        "dim": 192,
        "hidden_heatmap_dim": 384,
        "heatmap_dim": -1,
        "apply_multi": True,
        "heatmap_size": [-1, -1],
        "apply_init": True,
        "total_stride": 1,
        "input_channels": -1,
        "output_channels": -1,
        "head_stride": 1,
    },
    {
        "type": "DEKRHead",
        "predictor": {
            "type": "DEKRPredictor",
            "num_animals": 1,
            "keypoint_score_type": "heatmap",
            "max_absorb_distance": 75,
        },
        "target_generator": {
            "type": "DEKRGenerator",
            "num_joints": "num_bodyparts",
            "pos_dist_thresh": 17,
            "bg_weight": 0.1,
        },
        "criterion": {
            "heatmap": {
                "type": "WeightedBCECriterion",
                "weight": 1.0,
            },
            "offset": {
                "type": "WeightedHuberCriterion",
                "weight": 0.03,
            },
        },
        "heatmap_config": {
            "channels": [480, 64, -1],
            "num_blocks": 1,
            "dilation_rate": 1,
            "final_conv_kernel": 1,
            "block": BasicBlock,
        },
        "offset_config": {
            "channels": [480, -1, -1],
            "num_offset_per_kpt": 15,
            "num_blocks": 1,
            "dilation_rate": 1,
            "final_conv_kernel": 1,
            "block": AdaptBlock,
        },
        "total_stride": 1,
        "input_channels": 480,
        "output_channels": -1,
    },
]


def _generate_random_backbone_inputs(i):
    # Returns sizes that are divisible by 64to be able to predict consistently output size
    # (and be able to do the forward pass of HRNet)
    x_size_tmp, y_size_tmp = random.randint(100, 1000), random.randint(100, 1000)
    return (
        backbones_dicts[i],
        (x_size_tmp - x_size_tmp % 64, y_size_tmp - y_size_tmp % 64),
    )


@pytest.mark.parametrize(
    "backbone_dict, input_size",
    [_generate_random_backbone_inputs(i) for i in range(len(backbones_dicts))],
)
def test_backbone(backbone_dict, input_size):
    input_tensor = torch.Tensor(1, 3, input_size[1], input_size[0])

    stride = backbone_dict.pop("stride")
    output_channels = backbone_dict.pop("output_channels")
    backbone = dlc_models.BACKBONES.build(backbone_dict)

    features = backbone(input_tensor)
    _, c, h, w = features.shape
    assert c == output_channels
    assert h == input_size[1] // stride
    assert w == input_size[0] // stride


def _generate_random_head_inputs(i):
    # Returns sizes that are divisible by 64to be able to predict consistently output size
    # (and be able to do the forward pass of HRNet)
    x_size_tmp, y_size_tmp = random.randint(8, 500), random.randint(8, 500)
    num_kpts = random.randint(2, 50)
    return (
        heads_dicts[i],
        (x_size_tmp - x_size_tmp % 4, y_size_tmp - y_size_tmp % 4),
        num_kpts,
    )


@pytest.mark.parametrize(
    "head_dict, input_shape, num_keypoints",
    [_generate_random_head_inputs(i) for i in range(len(heads_dicts))],
)
def test_head(head_dict, input_shape, num_keypoints):
    w, h = input_shape
    head_dict = copy.deepcopy(head_dict)

    head_type = head_dict["type"]
    input_channels = head_dict.pop("input_channels")
    output_channels = head_dict.pop("output_channels")
    total_stride = head_dict.pop("total_stride")
    if head_type == "HeatmapHead":
        output_channels = num_keypoints
        head_dict["heatmap_config"]["channels"][2] = output_channels
        head_dict["locref_config"]["channels"][2] = 2 * output_channels
        head_dict["target_generator"]["num_heatmaps"] = output_channels
        input_tensor = torch.zeros((1, input_channels, h, w))

    elif head_type == "TransformerHead":
        output_channels = num_keypoints
        input_channels = num_keypoints
        head_dict["heatmap_dim"] = h * w
        head_dict["heatmap_size"] = [h, w]
        head_dict["target_generator"]["num_heatmaps"] = output_channels
        input_tensor = torch.zeros((1, input_channels, head_dict["dim"] * 3))

    elif head_type == "DEKRHead":
        output_channels = num_keypoints + 1
        head_dict["target_generator"]["num_joints"] = num_keypoints
        head_dict["heatmap_config"]["channels"][2] = num_keypoints + 1
        head_dict["offset_config"]["channels"][1] = (
            num_keypoints * head_dict["offset_config"]["num_offset_per_kpt"]
        )
        head_dict["offset_config"]["channels"][2] = num_keypoints
        input_tensor = torch.zeros((1, input_channels, h, w))

    if "type" in head_dict["criterion"]:
        head_dict["criterion"] = CRITERIONS.build(head_dict["criterion"])
    else:
        weights = {}
        criterions = {}
        for loss_name, criterion_cfg in head_dict["criterion"].items():
            weights[loss_name] = criterion_cfg.get("weight", 1.0)
            criterion_cfg = {
                k: v for k, v in criterion_cfg.items() if k != "weight"
            }
            criterions[loss_name] = CRITERIONS.build(criterion_cfg)

        aggregator_cfg = {"type": "WeightedLossAggregator", "weights": weights}
        head_dict["aggregator"] = LOSS_AGGREGATORS.build(aggregator_cfg)
        head_dict["criterion"] = criterions

    head_dict["target_generator"] = TARGET_GENERATORS.build(
        head_dict["target_generator"]
    )
    head_dict["predictor"] = PREDICTORS.build(head_dict["predictor"])
    head = dlc_models.HEADS.build(head_dict)

    output = head(input_tensor)["heatmap"]
    _, c_out, h_out, w_out = output.shape
    assert (h_out == h * total_stride) and (w_out == w * total_stride)
    assert c_out == output_channels


def test_msa_hrnet():
    # TODO: build microsoft asia hrnet and check dimension of output
    # TODO: check if hyperparameters are loaded correctly (from the config file)
    pass


def test_msa_tokenpose():
    # TODO: build microsoft asia hrnet and check dimension of output
    # TODO: check if hyperparameters are loaded correctly (from the config file)
    # cf https://github.com/amathislab/BUCTDdev/blob/main/lib/models/transpose_h.py#L1
    pass


def test_msa_hrnetCOAM():
    # TODO: build BUCTD COAM hrnet and check dimension of output
    # TODO: check if hyperparameters are loaded correctly (from the config file)
    pass


# TODO: add other model variants our pipeline can build ;)


--- File: tests/pose_estimation_pytorch/other/test_heatmap_plateau_targets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from typing import Tuple

import pytest
import torch

from deeplabcut.pose_estimation_pytorch.models.target_generators import HeatmapPlateauGenerator


def get_target(
    batch_size: int,
    num_animals: int,
    num_joints: int,
    image_size: Tuple[int, int],
    locref_std: float,
    pos_dist_thresh: int,
):
    """Summary
    Getting the target generator for certain annotations, predictions and image size.

    Args:
        batch_size (int): number of images
        num_animals (int): number of animals
        num_joints (int): number of bodyparts
        image_size (tuple): image size in pixels
        locref_std (float): scaling factor
        pos_dist_thresh (int): radius plateau on the heatmap

    Returns:
        target_output (dict): containing the heatmaps, locref_maps and locref_masks.
        annotations (dict): containing input keypoint annotations.

    Examples:
        input:
            batch_size = 1
            num_animals = 1
            num_joints = 6
            image_size = (256,256)
            locref_stdev = 7.2801
            pos_dist_thresh = 17
        output:

    """
    labels = {
        "keypoints": torch.randint(
            1, min(image_size), (batch_size, num_animals, num_joints, 2)
        )
    }  # 2 for x,y coords
    stride = 1
    prediction = {
        "heatmap": torch.rand((batch_size, num_joints, image_size[0], image_size[1])),
        "locref": torch.rand((batch_size, 2 * num_joints, image_size[0], image_size[1])),
    }
    generator = HeatmapPlateauGenerator(
        num_heatmaps=num_joints,
        pos_dist_thresh=pos_dist_thresh,
        locref_std=locref_std,
        generate_locref=True,
    )

    targets_output = generator(stride, prediction, labels)
    return targets_output, labels


data = [(1, 1, 10, (256, 256), 7.2801, 17)]


@pytest.mark.parametrize(
    "batch_size, num_animals, num_joints, image_size, locref_stdev, pos_dist_thresh",
    data,
)
def test_expected_output(
    batch_size: int,
    num_animals: int,
    num_joints: int,
    image_size: Tuple[int, int],
    locref_stdev: float,
    pos_dist_thresh: int,
):
    """Summary:
    Testing if plateau targets return the expected output. We take a target generator from
    get_target function. Given a sequence of random numbers for batch_size, num_animals etc., we assert if
    it returns the expected heatmaps and locrefmaps, as well as checking if the output has the expected shape.

    Args:
        batch_size (int): number of images
        num_animals (int): number of animals
        num_joints (int): number of bodyparts
        image_size (tuple): image size in pixels
        locref_stdev (float): scaling factor
        pos_dist_thresh (int): radius plateau on heatmap

    Returns:
        None

    Examples:
        input:
            batch_size = 1
            num_animals = 1
            num_joints = 6
            image_size = (256,256)
            locref_stdev = 7.2801
            pos_dist_thresh = 17
    """
    targets_output, annotations = get_target(
        batch_size, num_animals, num_joints, image_size, locref_stdev, pos_dist_thresh
    )

    assert "heatmap" in targets_output
    assert "locref" in targets_output
    assert targets_output["heatmap"]["target"].shape == (
        batch_size,
        num_joints,
        image_size[0],
        image_size[1],
    )  # heatmaps score output
    assert targets_output["locref"]["weights"].shape == (
        batch_size,
        num_joints * 2,
        image_size[0],
        image_size[1],
    )
    assert targets_output["locref"]["target"].shape == (
        batch_size,
        num_joints * 2,
        image_size[0],
        image_size[1],
    )


data = [(1, 1, 10, (256, 256), 7.2801, 17)]


@pytest.mark.parametrize(
    "batch_size, num_animals, num_joints, image_size, locref_stdev, pos_dist_thresh",
    data,
)
def test_single_animal(
    batch_size: int,
    num_animals: int,
    num_joints: int,
    image_size: Tuple[int, int],
    locref_stdev: float,
    pos_dist_thresh: int,
):
    """Summary
    Testing, for single animals experiments (num_animals=1) if the distance between the expected keypoints
    and the annotations keypoints is smaller than the radius plateau.

    'argmax' function returns the indices of the max values of all elements in the input tensor.
    If there are multiple maximal values, such as in our case because it's a plateau, then the
    indices of the first maximal value are returned. From this tensor we exctact x,y coords
    and then concatenate these new tensors along a new dimension. Then, we assert if the distance between
    each x,y element in annotations and predicted keypoints is smaller or equal to the 'pos_dist_thresh',
    which represents the radius of the plateau heatmap.

    Args:
        batch_size (int): number of images
        num_animals (int): number of animals
        num_joints (int): number of bodyparts
        image_size (tuple): image size in pixels
        locref_stdev (float): scaling factor
        pos_dist_thresh (int): radius plateau on heatmap

    Returns:
        None

    Examples:
        input:
            batch_size = 1
            num_animals = 1
            num_joints = 6
            image_size = (256,256)
            locref_stdev = 7.2801
            pos_dist_thresh = 17
    """
    targets_output, annotations = get_target(
        batch_size, num_animals, num_joints, image_size, locref_stdev, pos_dist_thresh
    )

    targets_output = torch.tensor(
        targets_output["heatmap"]["target"].reshape(1, 10, image_size[0] * image_size[1])
    )  # converting from dict to tensor. 'argmax' works on tensors.

    plt_max = torch.argmax(targets_output, dim=2)
    # get unraveled coords
    x = plt_max % image_size[1]
    y = plt_max // image_size[1]

    predict_kp = torch.stack((x, y), dim=-1)

    predict_kp = predict_kp.float()

    annotations["keypoints"] = torch.squeeze(annotations["keypoints"], dim=1)
    annotations["keypoints"] = annotations["keypoints"].float()

    dist = torch.norm(annotations["keypoints"] - predict_kp, p=2, dim=-1)
    assert (dist <= pos_dist_thresh).all()


--- File: tests/pose_estimation_pytorch/other/test_configs/config.yaml ---
    # Project definitions (do not edit)
Task: openfield
scorer: Pranav
date: Aug20
multianimalproject: false
identity:

    # Project path (change when moving around)
project_path: /home/quentin/datasets/Openfield_pytorch

    # Annotation data set configuration (and individual video cropping parameters)
video_sets:
  /Data/openfield-Pranav-2018-08-20/videos/m1s1.mp4:
    crop: 0, 640, 0, 480
  /Data/openfield-Pranav-2018-08-20/videos/m1s2.mp4:
    crop: 0, 640, 0, 480
  /Data/openfield-Pranav-2018-08-20/videos/m2s1.mp4:
    crop: 0, 640, 0, 480
  /Data/openfield-Pranav-2018-08-20/videos/m3s1.mp4:
    crop: 0, 640, 0, 480
  /Data/openfield-Pranav-2018-08-20/videos/m3s2.mp4:
    crop: 0, 640, 0, 480
  /Data/openfield-Pranav-2018-08-20/videos/m4s1.mp4:
    crop: 0, 640, 0, 480
  /Data/openfield-Pranav-2018-08-20/videos/m5s1.mp4:
    crop: 0, 800, 0, 800
  /Data/openfield-Pranav-2018-08-20/videos/m6s1.mp4:
    crop: 0, 800, 0, 800
  /Data/openfield-Pranav-2018-08-20/videos/m6s2.mp4:
    crop: 0, 800, 0, 800
  /Data/openfield-Pranav-2018-08-20/videos/m7s1.mp4:
    crop: 0, 800, 0, 800
  /Data/openfield-Pranav-2018-08-20/videos/m7s2.mp4:
    crop: 0, 800, 0, 800
  /Data/openfield-Pranav-2018-08-20/videos/m7s3.mp4:
    crop: 0, 800, 0, 800
  /Data/openfield-Pranav-2018-08-20/videos/m8s1.mp4:
    crop: 0, 800, 0, 800

  /Users/mwmathis/Downloads/ARCricket1.avi:
    crop: 0, 720, 0, 540
bodyparts:
- snout
- leftear
- rightear
- tailbase

flipped_keypoints:
- 0
- 2
- 1
- 3


    # Fraction of video to start/stop when extracting frames for labeling/refinement

    # Fraction of video to start/stop when extracting frames for labeling/refinement

    # Fraction of video to start/stop when extracting frames for labeling/refinement

    # Fraction of video to start/stop when extracting frames for labeling/refinement

    # Fraction of video to start/stop when extracting frames for labeling/refinement

    # Fraction of video to start/stop when extracting frames for labeling/refinement

    # Fraction of video to start/stop when extracting frames for labeling/refinement

    # Fraction of video to start/stop when extracting frames for labeling/refinement

    # Fraction of video to start/stop when extracting frames for labeling/refinement
start: 0
stop: 1
numframes2pick: 20

    # Plotting configuration
skeleton: []
skeleton_color: black
pcutoff: 0.4
dotsize: 8
alphavalue: 0.7
colormap: jet

    # Training,Evaluation and Analysis configuration
TrainingFraction:
- 0.95
iteration: 1
default_net_type: resnet_50
default_augmenter: default
snapshotindex: -1
batch_size: 1

    # Cropping Parameters (for analysis and outlier frame detection)
cropping: false
    #if cropping is true for analysis, then set the values here:
x1: 0
x2: 640
y1: 277
y2: 624

    # Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
- 50
- 50
move2corner: true
croppedtraining:


--- File: tests/pose_estimation_pytorch/other/test_configs/pytorch_config.yaml ---
project_root: /home/quentin/datasets/Openfield_pytorch
pose_cfg_path: /home/quentin/datasets/Openfield_pytorch/dlc-models/iteration-1/openfieldAug20-trainset95shuffle1/train/pose_cfg.yaml
cfg_path: /home/quentin/datasets/Openfield_pytorch/config.yaml

seed: 42
device: 'cuda:2' #needs to be updated dynamically; some users might have CPUs
model:
  backbone:
    type: 'ResNet'
    pretrained: 'https://download.pytorch.org/models/resnet50-19c8e357.pth'
  heatmap_head:
    type: 'SimpleHead'
    channels: [ 2048, 1024, 4 ]
    kernel_size: [ 2, 2 ]
    strides: [ 2, 2 ]
  locref_head:
    type: 'SimpleHead'
    channels: [ 2048, 1024, 8 ]
    kernel_size: [ 2, 2 ]
    strides: [ 2, 2 ]
  pose_model:
    stride: 8
    heatmap_type: 'plateau'
optimizer:
  type: 'SGD'
  params:
    lr: 0.005
scheduler:
  type: "LRListScheduler"
  params:
    milestones : [10, 430]
    lr_list : [[0.02], [0.002]]
criterion:
  type: 'PoseLoss'
  loss_weight_locref: 0.1
  locref_huber_loss: True
#logger:
#  type: 'WandbLogger'
#  project_name: 'deeplabcut'
#  run_name: 'tmp'
solver:
  type: 'BottomUpSingleAnimalSolver'
pos_dist_thresh : 17
batch_size: 1
epochs: 600


--- File: tests/pose_estimation_pytorch/other/test_configs/pose_cfg.yaml ---
    # Project definitions (do not edit)
Task:
scorer:
date:
multianimalproject:
identity:

    # Project path (change when moving around)
project_path: /home/quentin/datasets/Openfield_pytorch/dlc-models/iteration-1/openfieldAug20-trainset95shuffle1/train

    # Annotation data set configuration (and individual video cropping parameters)
video_sets:
bodyparts:

    # Fraction of video to start/stop when extracting frames for labeling/refinement
start:
stop:
numframes2pick:

    # Plotting configuration
skeleton: []
skeleton_color: black
pcutoff:
dotsize:
alphavalue:
colormap:

    # Training,Evaluation and Analysis configuration
TrainingFraction:
iteration:
default_net_type:
default_augmenter:
snapshotindex:
batch_size: 1

    # Cropping Parameters (for analysis and outlier frame detection)
cropping:
    #if cropping is true for analysis, then set the values here:
x1:
x2:
y1:
y2:

    # Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
move2corner:
all_joints:
- - 0
- - 1
- - 2
- - 3
all_joints_names:
- snout
- leftear
- rightear
- tailbase
alpha_r: 0.02
apply_prob: 0.5
contrast:
  clahe: true
  claheratio: 0.1
  histeq: true
  histeqratio: 0.1
convolution:
  edge: false
  emboss:
    alpha:
    - 0.0
    - 1.0
    strength:
    - 0.5
    - 1.5
  embossratio: 0.1
  sharpen: false
  sharpenratio: 0.3
cropratio: 0.4
dataset: training-datasets/iteration-1/UnaugmentedDataSet_openfieldAug20/openfield_Pranav95shuffle1.mat
dataset_type: default
decay_steps: 30000
display_iters: 1000
global_scale: 0.8
init_weights: /home/quentin/miniconda/envs/DEEPLABCUT/lib/python3.8/site-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt
intermediate_supervision: false
intermediate_supervision_layer: 12
location_refinement: true
locref_huber_loss: true
locref_loss_weight: 0.05
locref_stdev: 7.2801
lr_init: 0.0005
max_input_size: 1500
metadataset: training-datasets/iteration-1/UnaugmentedDataSet_openfieldAug20/Documentation_data-openfield_95shuffle1.pickle
min_input_size: 64
mirror: false
multi_stage: false
multi_step:
- - 0.005
  - 10000
- - 0.02
  - 430000
- - 0.002
  - 730000
- - 0.001
  - 1030000
net_type: resnet_50
num_joints: 4
pairwise_huber_loss: false
pairwise_predict: false
partaffinityfield_predict: false
pos_dist_thresh: 17
rotation: 25
rotratio: 0.4
save_iters: 50000
scale_jitter_lo: 0.5
scale_jitter_up: 1.25
scmap_type: plateau


--- File: tests/pose_estimation_pytorch/models/target_generators/test_heatmap_targets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests the heatmap target generators (plateau and gaussian)"""
import numpy as np
import torch
import pytest

from deeplabcut.pose_estimation_pytorch.models.target_generators.heatmap_targets import (
    HeatmapGaussianGenerator,
)

@pytest.mark.parametrize(
    "data",
    [
        {
            "dist_thresh": 3,
            "num_heatmaps": 1,
            "in_shape": (3, 3),
            "out_shape": (3, 3),
            "centers": [(1, 1)],
            "expected_output": [
                [0.7788, 0.8825, 0.7788],
                [0.8825, 1.0000, 0.8825],
                [0.7788, 0.8825, 0.7788],
            ],
        },
        {
            "dist_thresh": 3,
            "num_heatmaps": 1,
            "in_shape": (5, 5),
            "out_shape": (5, 5),
            "centers": [[1, 1], [2, 2]],
            "expected_output": [
                [0.7788, 0.8825, 0.7788, 0.5353, 0.3679],
                [0.8825, 1.0000, 0.8825, 0.7788, 0.5353],
                [0.7788, 0.8825, 1.0000, 0.8825, 0.6065],
                [0.5353, 0.7788, 0.8825, 0.7788, 0.5353],
                [0.3679, 0.5353, 0.6065, 0.5353, 0.3679],
            ],
        },
        {
            "dist_thresh": 1,
            "num_heatmaps": 1,
            "in_shape": (4, 4),
            "out_shape": (4, 4),
            "centers": [[1, 1]],
            "expected_output": [
                [0.1054, 0.3247, 0.1054, 0.0036],
                [0.3247, 1.0, 0.3247, 0.0111],
                [0.1054, 0.3247, 0.1054, 0.0036],
                [0.0036, 0.0111, 0.0036, 0.0001]
            ],
        },
    ],
)
def test_gaussian_heatmap_generation_single_keypoint(data):
    dist_thresh = data["dist_thresh"]
    generator = HeatmapGaussianGenerator(
        num_heatmaps=data["num_heatmaps"],
        pos_dist_thresh=dist_thresh,
        heatmap_mode=HeatmapGaussianGenerator.Mode.KEYPOINT,
        generate_locref=False,
    )
    stride = data["in_shape"][0] / data["out_shape"][0]
    outputs = torch.zeros((1, data["num_heatmaps"], *data["out_shape"]))
    ann_shape = (1, len(data["centers"]), data["num_heatmaps"], 2)
    annotations = {
        "keypoints": torch.tensor(data["centers"]).reshape(ann_shape)  # x, y
    }
    targets = generator(stride, {"heatmap": outputs}, annotations)

    print("Targets")
    print(targets["heatmap"]["target"])
    print()
    np.testing.assert_almost_equal(
        targets["heatmap"]["target"].cpu().numpy().reshape(data["out_shape"]),
        np.array(data["expected_output"]),
        decimal=3,
    )


@pytest.mark.parametrize(
    "batch_size, num_keypoints, image_size",
    [(2, 2, (64, 64)), (1, 5, (48, 64)), (15, 50, (64, 48))],
)
def test_random_gaussian_target_generation(
    batch_size: int, num_keypoints: int, image_size: tuple, num_animals=1
):
    # generate annotations
    annotations = {
        "keypoints": torch.randint(
            1, min(image_size), (batch_size, num_animals, num_keypoints, 2)
        )
    }  # batch size, num animals, num keypoints, 2 for x,y

    # model stride 1
    stride = 1

    # generate predictions
    predicted_heatmaps = {
        "heatmap": torch.zeros((batch_size, num_keypoints, *image_size))
    }

    # generate heatmap
    generator = HeatmapGaussianGenerator(
        num_heatmaps=num_keypoints,
        pos_dist_thresh=17,
        heatmap_mode=HeatmapGaussianGenerator.Mode.KEYPOINT,
        generate_locref=False,
    )
    targets = generator(stride, predicted_heatmaps, annotations)
    target_heatmap = targets["heatmap"]["target"].reshape(
        batch_size, num_keypoints, image_size[0] * image_size[1]
    )

    # get coords of max value of the heatmap
    gaus_max = torch.argmax(target_heatmap, dim=2)

    # get unraveled coords
    x = gaus_max % image_size[1]
    y = gaus_max // image_size[1]

    # get heatmap center tensor
    predict_kp = torch.stack((x, y), dim=-1)
    # Remove num_animals dimension - only one animal is supported
    annotations["keypoints"] = torch.squeeze(annotations["keypoints"], dim=1)

    # compare heatmap center to annotation
    assert torch.eq(annotations["keypoints"], predict_kp).all().item()


--- File: tests/pose_estimation_pytorch/models/target_generators/test_plateau_targets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests the heatmap target generators (plateau and gaussian)"""
import numpy as np
import torch
import pytest

from deeplabcut.pose_estimation_pytorch.models.target_generators.heatmap_targets import (
    HeatmapGenerator,
    HeatmapPlateauGenerator,
)


@pytest.mark.parametrize(
    "data",
    [
        {
            "dist_thresh": 1,
            "num_heatmaps": 1,
            "in_shape": (3, 3),
            "out_shape": (3, 3),
            "centers": [(1, 1)],
            "expected_output": [
                [0., 1., 0.],
                [1., 1., 1.],
                [0., 1., 0.],
            ],
        },
        {
            "dist_thresh": 2,
            "num_heatmaps": 1,
            "in_shape": (5, 5),
            "out_shape": (5, 5),
            "centers": [[1, 1], [2, 2]],
            "expected_output": [
                [1., 1., 1., 0., 0.],
                [1., 1., 1., 1., 0.],
                [1., 1., 1., 1., 1.],
                [0., 1., 1., 1., 0.],
                [0., 0., 1., 0., 0.],
            ],
        },
        {
            "dist_thresh": 2,
            "num_heatmaps": 1,
            "in_shape": (4, 4),
            "out_shape": (4, 4),
            "centers": [[1, 1]],
            "expected_output": [
                [1., 1., 1., 0.],
                [1., 1., 1., 1.],
                [1., 1., 1., 0.],
                [0., 1., 0., 0.],
            ],
        },
    ],
)
def test_plateau_heatmap_generation_single_keypoint(data):
    dist_thresh = data["dist_thresh"]
    generator = HeatmapPlateauGenerator(
        num_heatmaps=data["num_heatmaps"],
        pos_dist_thresh=dist_thresh,
        heatmap_mode=HeatmapGenerator.Mode.KEYPOINT,
        generate_locref=False,
    )
    stride = data["in_shape"][0] / data["out_shape"][0]
    outputs = torch.zeros((1, data["num_heatmaps"], *data["out_shape"]))
    ann_shape = (1, len(data["centers"]), data["num_heatmaps"], 2)
    annotations = {
        "keypoints": torch.tensor(data["centers"]).reshape(ann_shape)  # x, y
    }
    targets = generator(stride, {"heatmap": outputs}, annotations)

    print("Targets")
    print(targets["heatmap"]["target"])
    print()
    np.testing.assert_almost_equal(
        targets["heatmap"]["target"].cpu().numpy().reshape(data["out_shape"]),
        np.array(data["expected_output"]),
        decimal=3,
    )


--- File: tests/pose_estimation_pytorch/modelzoo/test_download.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os

import dlclibrary
import pytest
from dlclibrary.dlcmodelzoo.modelzoo_download import MODELOPTIONS


def test_download_huggingface_model(tmp_path_factory, model="full_cat"):
    folder = tmp_path_factory.mktemp("temp")
    dlclibrary.download_huggingface_model(model, str(folder))

    assert os.path.exists(folder / "pose_cfg.yaml")
    assert any(f.startswith("snapshot-") for f in os.listdir(folder))
    # Verify that the Hugging Face folder was removed
    assert not any(f.startswith("models--") for f in os.listdir(folder))


def test_download_huggingface_wrong_model():
    with pytest.raises(ValueError):
        dlclibrary.download_huggingface_model("wrong_model_name")


@pytest.mark.skip(reason="slow")
@pytest.mark.parametrize("model", MODELOPTIONS)
def test_download_all_models(tmp_path_factory, model):
    test_download_huggingface_model(tmp_path_factory, model)


--- File: tests/pose_estimation_pytorch/modelzoo/test_load_superanimal_models.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import dlclibrary
import pytest
import torch

from deeplabcut.pose_estimation_pytorch.modelzoo import get_super_animal_snapshot_path


@pytest.mark.skip(reason="require-models")
def test_load_superanimal_models_weights_only():
    super_animal_names = dlclibrary.get_available_datasets()
    for super_animal in super_animal_names:
        print(f"\nTesting {super_animal}")
        for detector in dlclibrary.get_available_detectors(super_animal):
            print(super_animal, detector)
            path = get_super_animal_snapshot_path(super_animal, detector)
            snapshot = torch.load(path, map_location="cpu", weights_only=True)

        for pose_model in dlclibrary.get_available_models(super_animal):
            print(super_animal, pose_model)
            path = get_super_animal_snapshot_path(super_animal, pose_model)
            snapshot = torch.load(path, map_location="cpu", weights_only=True)


--- File: tests/pose_estimation_pytorch/modelzoo/test_modelzoo_utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import pytest

import deeplabcut.pose_estimation_pytorch.modelzoo as modelzoo


@pytest.mark.parametrize(
    "super_animal", ["superanimal_quadruped", "superanimal_topviewmouse"]
)
@pytest.mark.parametrize("model_name", ["hrnet_w32"])
@pytest.mark.parametrize("detector_name", [None, "fasterrcnn_resnet50_fpn_v2"])
def test_get_config_model_paths(super_animal, model_name, detector_name):
    model_config = modelzoo.load_super_animal_config(
        super_animal=super_animal,
        model_name=model_name,
        detector_name=detector_name,
    )

    assert isinstance(model_config, dict)
    if detector_name is None:
        assert model_config["method"].lower() == "bu"
        assert "detector" not in model_config
    else:
        assert model_config["method"].lower() == "td"
        assert "detector" in model_config


--- File: tests/pose_estimation_pytorch/modelzoo/test_webapp.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os

import cv2
import numpy as np
import pytest

from deeplabcut.modelzoo.webapp.inference import SuperanimalPyTorchInference
from deeplabcut.utils import auxiliaryfunctions


@pytest.mark.parametrize("max_individuals", [1, 3])
@pytest.mark.parametrize(
    "project_name", ["superanimal_quadruped", "superanimal_topviewmouse"]
)
@pytest.mark.parametrize("pose_model_type", ["hrnet_w32"])
def test_class_init(project_name, pose_model_type, max_individuals):
    inference_pipeline = SuperanimalPyTorchInference(
        project_name, pose_model_type, max_individuals=max_individuals
    )

    assert isinstance(inference_pipeline.config, dict)
    assert inference_pipeline.config["metadata"]["bodyparts"]
    assert len(inference_pipeline.config["metadata"]["bodyparts"]) > 0


@pytest.mark.skip(reason="require-models")
@pytest.mark.parametrize(
    "project_name", ["superanimal_quadruped", "superanimal_topviewmouse"]
)
@pytest.mark.parametrize("pose_model_type", ["hrnet_w32"])
def test_runner_init(project_name, pose_model_type):
    inference_pipeline = SuperanimalPyTorchInference(
        project_name, pose_model_type, max_individuals=1
    )
    weight_folder = f"{auxiliaryfunctions.get_deeplabcut_path()}/modelzoo/checkpoints"
    snapshot_path = f"{weight_folder}/{project_name}_{pose_model_type}.pth"
    detector_path = f"{weight_folder}/{project_name}_fasterrcnn.pt"

    inference_pipeline.initialize_models(snapshot_path, detector_path)

    assert inference_pipeline.models.pose_runner
    assert inference_pipeline.models.detector_runner


@pytest.mark.skip(reason="require-models")
@pytest.mark.parametrize("max_individuals", [10, 4, 1])
@pytest.mark.parametrize(
    "project_name", ["superanimal_quadruped", "superanimal_topviewmouse"]
)
@pytest.mark.parametrize("pose_model_type", ["hrnet_w32"])
def test_predict(project_name, pose_model_type, max_individuals):
    inference_pipeline = SuperanimalPyTorchInference(
        project_name, pose_model_type, max_individuals=max_individuals
    )
    image_path = "img0001.png"
    weight_folder = f"{auxiliaryfunctions.get_deeplabcut_path()}/modelzoo/checkpoints"
    snapshot_path = f"{weight_folder}/{project_name}_{pose_model_type}.pth"
    detector_path = f"{weight_folder}/{project_name}_fasterrcnn.pt"

    inference_pipeline.initialize_models(snapshot_path, detector_path)
    frame = {image_path: np.random.rand(100, 100, 3)}
    response = inference_pipeline.predict(frame)

    assert isinstance(response, dict)
    assert response["joint_names"] == inference_pipeline.config["bodyparts"]
    assert response["predictions"][0]["markers"].shape == (
        max_individuals,
        len(inference_pipeline.config["bodyparts"]),
        3,
    )
    assert response["predictions"][0]["image_path"] == image_path


--- File: tests/pose_estimation_pytorch/runners/test_schedulers.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests building schedulers from config"""
import random
from dataclasses import dataclass

import numpy as np
import pytest
import torch
import torch.nn as nn

import deeplabcut.pose_estimation_pytorch.runners.schedulers as schedulers


def generate_random_lr_list(num_floats: int):
    """Generate list of lists including random numbers.

    Args:
        num_floats: number of floats we want to include in our list

    Returns:
        ran_list: random list of sorted numbers, being first number bigger than the last
    """
    ran_list = []
    for i in range(num_floats):
        random_floats = [random.random()]
        ran_list.append(random_floats)
    return sorted(ran_list, reverse=True)


@pytest.mark.parametrize(
    "milestones, lr_list",
    [
        ([10, 430], [[0.05], [0.005]]),
        (list(sorted(random.sample(range(0, 999), 2))), generate_random_lr_list(2))
    ]
)
def test_scheduler(milestones, lr_list):
    """Testing schedulers.py.

    Given a list of milestones and a list of learning rates, this function tests
    if the length of each list is the same. Furthermore, it will assess if
    the current learning rate (output from the function we are testing) is a float
    and corresponds to the expected learning rate given the milestones.

    Args:
        milestones: list of epochs indices (number of epochs)
        lr_list: learning rates list

    Returns:
        None

    Examples:
        input:
            milestones = [10,25,50]
            lr_list = [[0.00001],[0.000005],[0.000001]]
    """

    assert len(milestones) == len(lr_list)

    optimizer = torch.optim.SGD([torch.randn(2, 2)], lr=0.01)
    s = schedulers.LRListScheduler(optimizer, milestones=milestones, lr_list=lr_list)

    index_rng = range(milestones[0], milestones[1])
    for i in range((milestones[-1]) + 1):
        if i < milestones[0]:
            expected_lr = [0.01]
        elif i in index_rng:
            expected_lr = lr_list[0]
        else:
            expected_lr = lr_list[1]

        current_lr = s.get_lr()[0]
        assert s.get_lr() == expected_lr
        assert isinstance(current_lr, float)
        optimizer.step()
        s.step()


@dataclass
class SchedulerTestConfig:
    cfg: dict
    init_lr: float
    expected_lrs: list[float]


TEST_SCHEDULERS = [
    SchedulerTestConfig(
        cfg=dict(
            type="LRListScheduler",
            params=dict(milestones=[2, 5], lr_list=[[0.5], [0.1]])
        ),
        init_lr=1.0,
        expected_lrs=[1.0, 1.0, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1],
    ),
    SchedulerTestConfig(
        cfg=dict(type="LRListScheduler", params=dict(milestones=[1], lr_list=[[0.1]])),
        init_lr=0.1,
        expected_lrs=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
    ),
    SchedulerTestConfig(
        cfg=dict(type="LRListScheduler", params=dict(milestones=[1], lr_list=[[0.5]])),
        init_lr=0.1,
        expected_lrs=[0.1, 0.5, 0.5, 0.5],
    ),
    SchedulerTestConfig(
        cfg=dict(type="StepLR", params=dict(step_size=3, gamma=0.1)),
        init_lr=1.0,
        expected_lrs=[1.0, 1.0, 1.0, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.001],
    ),
]


@pytest.mark.parametrize("test_cfg", TEST_SCHEDULERS)
def test_build_scheduler(test_cfg: SchedulerTestConfig) -> None:
    optimizer = torch.optim.SGD([torch.randn(2, 2)], lr=test_cfg.init_lr)
    s = schedulers.build_scheduler(test_cfg.cfg, optimizer)
    print()
    print(f"Scheduler: {s}")
    num_epochs = len(test_cfg.expected_lrs)
    for e in range(num_epochs):
        _assert_learning_rates_match(e, optimizer, test_cfg.expected_lrs[e])
        optimizer.step()
        s.step()


@pytest.mark.parametrize("test_cfg", TEST_SCHEDULERS)
def test_resume_scheduler_after_each_epoch(test_cfg: SchedulerTestConfig) -> None:
    optimizer = torch.optim.SGD([torch.randn(2, 2)], lr=test_cfg.init_lr)
    s = schedulers.build_scheduler(test_cfg.cfg, optimizer)
    print()
    print(f"Scheduler: {s}")
    num_epochs = len(test_cfg.expected_lrs)
    for e in range(num_epochs):
        _assert_learning_rates_match(e, optimizer, test_cfg.expected_lrs[e])
        optimizer.step()
        s.step()

        optimizer = torch.optim.SGD([torch.randn(2, 2)], lr=test_cfg.init_lr)
        new_scheduler = schedulers.build_scheduler(test_cfg.cfg, optimizer)
        schedulers.load_scheduler_state(new_scheduler, s.state_dict())
        s = new_scheduler


@pytest.mark.parametrize(
    "test_cfg, middle_epoch",
    [
        (TEST_SCHEDULERS[0], 3),
        (TEST_SCHEDULERS[1], 5),
        (TEST_SCHEDULERS[2], 2),
        (TEST_SCHEDULERS[3], 2),
        (TEST_SCHEDULERS[3], 3),
        (TEST_SCHEDULERS[3], 4),
    ],
)
def test_two_stage_training(test_cfg: SchedulerTestConfig, middle_epoch: int) -> None:
    num_epochs = len(test_cfg.expected_lrs)
    optimizer = torch.optim.SGD([torch.randn(2, 2)], lr=test_cfg.init_lr)
    s = schedulers.build_scheduler(test_cfg.cfg, optimizer)

    print()
    print(f"Scheduler: {s}")
    for e in range(middle_epoch):
        _assert_learning_rates_match(e, optimizer, test_cfg.expected_lrs[e])
        optimizer.step()
        s.step()

    optimizer = torch.optim.SGD([torch.randn(2, 2)], lr=test_cfg.init_lr)
    new_scheduler = schedulers.build_scheduler(test_cfg.cfg, optimizer)
    schedulers.load_scheduler_state(new_scheduler, s.state_dict())
    s = new_scheduler
    for e in range(middle_epoch, num_epochs):
        _assert_learning_rates_match(e, optimizer, test_cfg.expected_lrs[e])
        s.step()


@pytest.mark.parametrize(
    "data",
    [
        dict(  # example with 3 warm-up epochs
            config=dict(
                dict(
                    type="ConstantLR",
                    params=dict(factor=0.1, total_iters=3),
                ),
            ),
            start_lr=1.0,
            expected_lrs=[[0.1], [0.1], [0.1], [1.0], [1.0]],
        ),
        dict(  # example from torch.optim.lr_scheduler.SequentialLR
            config=dict(
                type="SequentialLR",
                params=dict(
                    schedulers=[
                        dict(
                            type="ConstantLR",
                            params=dict(factor=0.1, total_iters=2),
                        ),
                        dict(type="ExponentialLR", params=dict(gamma=0.9)),
                    ],
                    milestones=[2],
                ),
            ),
            start_lr=1.0,
            expected_lrs=[[0.1], [0.1], [1.0], [0.9], [0.81], [0.729]],
        ),
        dict(  # example from torch.optim.lr_scheduler.SequentialLR
            config=dict(
                type="SequentialLR",
                params=dict(
                    schedulers=[
                        dict(
                            type="ConstantLR",
                            params=dict(factor=0.1, total_iters=2),
                        ),
                        dict(type="StepLR", params=dict(step_size=2, gamma=0.1)),
                    ],
                    milestones=[5],
                ),
            ),
            start_lr=1.0,
            expected_lrs=[
                [0.1], [0.1], [1.0], [1.0], [1.0],  # ConstantLR
                [1.0], [1.0], [0.1], [0.1], [0.01],  # StepLR
            ],
        ),
    ],
)
def test_build_sequential_lr(data):
    print("\nTESTING")
    start_lr = data["start_lr"]
    print(f"Start LR: {start_lr}")
    model = nn.Linear(in_features=1, out_features=1)
    optimizer = torch.optim.SGD(params=model.parameters(), lr=start_lr)

    print("BUILDING")
    scheduler = schedulers.build_scheduler(data["config"], optimizer)

    print("RUNNING")
    lrs = []
    for epoch in range(len(data["expected_lrs"])):
        lrs.append(scheduler.get_last_lr())
        print(scheduler.get_last_lr())
        scheduler.step()

    print(f"Expected: {data['expected_lrs']}")
    print(f"Actual: {lrs}")
    np.testing.assert_allclose(
        np.asarray(data["expected_lrs"]),
        np.asarray(lrs),
        atol=1e-10,
    )


def _assert_learning_rates_match(e, optimizer, expected):
    current_lrs = [g["lr"] for g in optimizer.param_groups]
    print(f"Epoch {e}: LR={current_lrs}, expected={expected}")
    for lr in current_lrs:
        assert isinstance(lr, float)
        np.testing.assert_almost_equal(lr, expected)


--- File: tests/pose_estimation_pytorch/runners/test_task.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
""" Tests the Task enum """
import pytest

from deeplabcut.pose_estimation_pytorch.task import Task


@pytest.mark.parametrize(
    "task, task_strings",
    [
        (Task.BOTTOM_UP, ["bu", "BU", "bU", "Bu"]),
        (Task.TOP_DOWN, ["TD", "tD"]),
        (Task.DETECT, ["dt", "DT"]),
    ],
)
def test_build_task(task: Task, task_strings: list[str]):
    for s in task_strings:
        assert task == Task(s)


--- File: tests/pose_estimation_pytorch/runners/test_runners_inference.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests inference runners"""
from unittest.mock import Mock, patch

import numpy as np
import pytest
import torch

import deeplabcut.pose_estimation_pytorch.data.postprocessor as post
import deeplabcut.pose_estimation_pytorch.data.preprocessor as prep
import deeplabcut.pose_estimation_pytorch.runners.inference as inference
from deeplabcut.pose_estimation_pytorch import get_load_weights_only
from deeplabcut.pose_estimation_pytorch.task import Task


@patch("deeplabcut.pose_estimation_pytorch.runners.train.build_optimizer", Mock())
@pytest.mark.parametrize("task", [Task.DETECT, Task.TOP_DOWN, Task.BOTTOM_UP])
@pytest.mark.parametrize("weights_only", [None, True, False])
def test_load_weights_only_with_build_training_runner(task: Task, weights_only: bool):
    with patch("deeplabcut.pose_estimation_pytorch.runners.base.torch.load") as load:
        snapshot = "snapshot.pt"
        runner = inference.build_inference_runner(
            task=task,
            model=Mock(),
            device="cpu",
            snapshot_path=snapshot,
            load_weights_only=weights_only,
        )
        if weights_only is None:
            weights_only = get_load_weights_only()
        load.assert_called_once_with(
            snapshot, map_location="cpu", weights_only=weights_only
        )


class MockInferenceRunner(inference.InferenceRunner):
    """Mocks the predict function for an inference runner"""

    def __init__(
        self,
        batch_size: int = 1,
        preprocessor: prep.Preprocessor | None = None,
        postprocessor: post.Postprocessor | None = None,
    ) -> None:
        super().__init__(
            model=Mock(),
            batch_size=batch_size,
            preprocessor=preprocessor,
            postprocessor=postprocessor,
        )
        self.batch_shapes = []

    def predict(self, inputs: torch.Tensor) -> list[dict[str, dict[str, np.ndarray]]]:
        self.batch_shapes.append(tuple(inputs.shape))
        return [  # return first elem of input
            {"mock": {"index": i[0, 0, 0].detach().numpy()}} for i in inputs
        ]


@pytest.mark.parametrize("batch_size", [1, 2, 4, 8])
def test_mock_bottom_up(batch_size):
    h, w = 640, 480
    images = [i * np.ones((1, 3, h, w)) for i in range(10)]

    runner = MockInferenceRunner(batch_size=batch_size)
    predictions = runner.inference(images)

    print()
    print(f"Num images: {len(predictions)}")
    print(f"Num predictions: {len(predictions)}")
    print(f"Batch shapes: {runner.batch_shapes}")
    print(80 * "-")
    for i in images:
        print(i[0, 0, 0, 0])
        print("----")
    print(80 * "-")
    for p in predictions:
        print(p)
        print("----")

    _check_batch_shapes(batch_size, h, w, runner.batch_shapes)
    assert len(images) == len(predictions)
    for i, p in zip(images, predictions):
        assert len(p) == 1  # only 1 output per image
        assert i[0, 0, 0, 0] == p[0]["mock"]["index"]


@pytest.mark.parametrize("batch_size", [1, 2, 4, 8])
@pytest.mark.parametrize(
    "detections_per_image",
    [
        [1, 1, 1, 1, 1],
        [0, 1, 0, 1, 1],  # some frames might not have predictions
        [0, 0, 0, 5, 2],
        [1, 2, 3, 4],
        [3, 4, 2, 1, 4],
        [4, 23, 5, 20, 64, 100],
    ],
)
def test_mock_top_down(batch_size, detections_per_image):
    h, w = 8, 8
    images = []
    for index, num_detections in enumerate(detections_per_image):
        if num_detections == 0:
            detections = np.zeros((0, 3, 1, 1))  # random shape when no detections
        else:
            detections = np.concatenate(
                [
                    (1_000_000 * (index + 1) + i) * np.ones((1, 3, h, w))
                    for i in range(num_detections)
                ],
                axis=0,
            )

        images.append(detections)

    runner = MockInferenceRunner(batch_size=batch_size)
    predictions = runner.inference(images)

    print()
    print(f"Num images: {len(predictions)}")
    print(f"Num predictions: {len(predictions)}")
    print(80 * "-")
    for i in images:
        for i_det in i:
            print(i_det.shape)
            print(i_det[0, 0, 0])
        print("----")

    print(80 * "-")
    for p in predictions:
        print(p)
        print("----")

    _check_batch_shapes(batch_size, h, w, runner.batch_shapes)

    assert len(images) == len(predictions)
    for i, p in zip(images, predictions):
        assert len(p) == len(i)  # one prediction per input
        for i_det, p_det in zip(i, p):
            print(i_det.shape)
            print(p_det["mock"]["index"])
            assert i_det[0, 0, 0] == p_det["mock"]["index"]


def test_dynamic_pose_inference_calls_dynamic():
    pose_batch = [Mock()]
    image_crop = Mock()
    image_crop.__len__ = Mock(return_value=1)

    model = Mock()
    model.get_predictions = Mock()
    model.get_predictions.return_value = dict(bodypart=dict(poses=pose_batch))

    dynamic = Mock()
    dynamic.crop = Mock()
    dynamic.crop.return_value = image_crop
    dynamic.update = Mock()

    runner = inference.PoseInferenceRunner(
        model=model,
        dynamic=dynamic,
        batch_size=1,
    )
    image = torch.Tensor((1, 3, 64, 64))
    _ = runner.predict(image)
    dynamic.crop.assert_called_once_with(image)
    dynamic.update.assert_called_once_with(pose_batch)


def _check_batch_shapes(batch_size, h, w, batch_shapes) -> None:
    for b in batch_shapes[:-1]:
        assert b[0] == batch_size
        assert b[1] == 3
        assert b[2] == h
        assert b[3] == w

    assert batch_shapes[-1][0] <= batch_size
    assert batch_shapes[-1][1] <= 3
    assert batch_shapes[-1][2] <= h
    assert batch_shapes[-1][3] <= w


--- File: tests/pose_estimation_pytorch/runners/test_runners_train.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from dataclasses import dataclass
from unittest.mock import Mock, patch

import numpy as np
import pytest
import torch

import deeplabcut.pose_estimation_pytorch.runners.schedulers as schedulers
import deeplabcut.pose_estimation_pytorch.runners.train as train_runners
from deeplabcut.pose_estimation_pytorch.models import PoseModel
from deeplabcut.pose_estimation_pytorch.models.backbones import ResNet
from deeplabcut.pose_estimation_pytorch.models.heads import HeatmapHead
from deeplabcut.pose_estimation_pytorch.task import Task


@patch("deeplabcut.pose_estimation_pytorch.runners.train.build_optimizer", Mock())
@patch("deeplabcut.pose_estimation_pytorch.runners.train.CSVLogger", Mock())
@pytest.mark.parametrize("task", [Task.DETECT, Task.TOP_DOWN, Task.BOTTOM_UP])
@pytest.mark.parametrize("weights_only", [True, False])
def test_load_weights_only_with_build_training_runner(task: Task, weights_only: bool):
    runner_config = dict(
        optimizer=dict(),
        snapshots=dict(max_snapshots=1, save_epochs=5, save_optimizer_state=False),
        load_weights_only=weights_only,
    )
    with patch("deeplabcut.pose_estimation_pytorch.runners.base.torch.load") as load:
        train_runners.build_training_runner(
            runner_config=runner_config,
            model_folder=Mock(),
            task=task,
            model=Mock(),
            device="cpu",
            snapshot_path="snapshot.pt",
        )
        load.assert_called_once_with(
            "snapshot.pt", map_location="cpu", weights_only=weights_only
        )


@dataclass
class SchedulerTestConfig:
    cfg: dict
    init_lr: float
    expected_lrs: list[float]


TEST_SCHEDULERS = [
    SchedulerTestConfig(
        cfg=dict(
            type="LRListScheduler",
            params=dict(milestones=[2, 5], lr_list=[[0.5], [0.1]]),
        ),
        init_lr=1.0,
        expected_lrs=[1.0, 1.0, 0.5, 0.5, 0.5, 0.1, 0.1, 0.1],
    ),
    SchedulerTestConfig(
        cfg=dict(type="LRListScheduler", params=dict(milestones=[1], lr_list=[[0.1]])),
        init_lr=0.1,
        expected_lrs=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],
    ),
    SchedulerTestConfig(
        cfg=dict(type="LRListScheduler", params=dict(milestones=[1], lr_list=[[0.5]])),
        init_lr=0.1,
        expected_lrs=[0.1, 0.5, 0.5, 0.5],
    ),
    SchedulerTestConfig(
        cfg=dict(type="StepLR", params=dict(step_size=3, gamma=0.1)),
        init_lr=1.0,
        expected_lrs=[1.0, 1.0, 1.0, 0.1, 0.1, 0.1, 0.01, 0.01, 0.01, 0.001],
    ),
]


@pytest.mark.parametrize("load_head_weights", [True, False])
def test_load_head_weights(tmp_path_factory, load_head_weights):
    model_folder = tmp_path_factory.mktemp("model_folder")
    runner_config = dict(
        optimizer=dict(type="SGD", params=dict(lr=1)),
        snapshots=dict(max_snapshots=1, save_epochs=1, save_optimizer_state=False),
    )

    model = PoseModel(
        cfg=dict(),
        backbone=ResNet(),
        heads=dict(
            bodyparts=HeatmapHead(
                predictor=Mock(),
                target_generator=Mock(),
                criterion=Mock(),
                aggregator=None,
                heatmap_config=dict(channels=[2048, 10], kernel_size=[3], strides=[2]),
            ),
        ),
    )

    original_state_dict = model.state_dict()
    zero_state_dict = {
        k: torch.zeros_like(v) for k, v in original_state_dict.items()
    }

    load = Mock()
    load.return_value = dict(model=zero_state_dict)

    with patch("deeplabcut.pose_estimation_pytorch.runners.train.torch.load", load):
        r = train_runners.build_training_runner(
            runner_config,
            model_folder=model_folder,
            task=Task.BOTTOM_UP,
            model=model,
            device="cpu",
            snapshot_path=model_folder / "snapshot.pt",
            load_head_weights=load_head_weights,
        )
        loaded_state_dict = r.model.state_dict()
        for k, v in loaded_state_dict.items():
            if load_head_weights or k.startswith("backbone."):
                assert torch.equal(v, zero_state_dict[k])
            else:
                assert torch.equal(v, original_state_dict[k])


@pytest.mark.parametrize("load_head_weights", [True, False])
def test_mocked_load_head_weights(tmp_path_factory, load_head_weights):
    model_folder = tmp_path_factory.mktemp("model_folder")
    snapshot_manager = Mock()
    snapshot_manager.model_folder = model_folder

    model = Mock()
    model.backbone = Mock()
    state_dict = {"backbone.test": 0, "head.test": 1}
    state_dict_backbone = {"test": 0}
    load = Mock()
    load.return_value = dict(model=state_dict)

    with patch("deeplabcut.pose_estimation_pytorch.runners.train.torch.load", load):
        _ = train_runners.PoseTrainingRunner(
            model=model,
            optimizer=Mock(),
            snapshot_manager=snapshot_manager,
            device="cpu",
            snapshot_path="snapshot.pt",
            load_head_weights=load_head_weights,
        )
        if load_head_weights:
            model.load_state_dict.assert_called_once_with(state_dict)
        else:
            model.backbone.load_state_dict.assert_called_once_with(state_dict_backbone)


@patch("deeplabcut.pose_estimation_pytorch.runners.train.CSVLogger", Mock())
@pytest.mark.parametrize(
    "runner_cls",
    [
        train_runners.PoseTrainingRunner,
        train_runners.DetectorTrainingRunner,
    ],
)
@pytest.mark.parametrize("test_cfg", TEST_SCHEDULERS)
def test_training_with_scheduler(runner_cls, test_cfg: SchedulerTestConfig) -> None:
    runner = _fit_runner_and_check_lrs(
        runner_cls,
        test_cfg.init_lr,
        test_cfg.cfg,
        test_cfg.expected_lrs,
    )
    assert runner.current_epoch == len(test_cfg.expected_lrs)


@patch("deeplabcut.pose_estimation_pytorch.runners.train.CSVLogger", Mock())
@pytest.mark.parametrize(
    "runner_cls",
    [
        train_runners.PoseTrainingRunner,
        train_runners.DetectorTrainingRunner,
    ],
)
@pytest.mark.parametrize("test_cfg", TEST_SCHEDULERS)
def test_resuming_training_scheduler_every_epoch(
    runner_cls,
    test_cfg: SchedulerTestConfig,
):
    snapshot_to_load = None
    for epoch, expected_lr in enumerate(test_cfg.expected_lrs):
        runner = _fit_runner_and_check_lrs(
            runner_cls,
            test_cfg.init_lr,
            test_cfg.cfg,
            [expected_lr],  # trains for 1 epoch
            snapshot_to_load=snapshot_to_load,
        )
        snapshot_to_load = dict(
            metadata=dict(epoch=epoch + 1), scheduler=runner.scheduler.state_dict()
        )


@patch("deeplabcut.pose_estimation_pytorch.runners.train.CSVLogger", Mock())
@pytest.mark.parametrize(
    "runner_cls",
    [
        train_runners.PoseTrainingRunner,
        train_runners.DetectorTrainingRunner,
    ],
)
@pytest.mark.parametrize(
    "test_cfg, resume_epoch",
    [
        (
            SchedulerTestConfig(
                cfg=dict(
                    type="LRListScheduler",
                    params=dict(milestones=[2, 5], lr_list=[[0.5], [0.1]]),
                ),
                init_lr=1.0,
                expected_lrs=[1.0, 1.0, 0.5, 1.0, 1.0, 0.1, 0.1, 0.1],
            ),
            3,  # cut after the 3rd epoch - restart at LR=1 until epoch 5
        ),
        (
            SchedulerTestConfig(
                cfg=dict(type="StepLR", params=dict(step_size=4, gamma=0.1)),
                init_lr=1.0,
                expected_lrs=(4 * [1.0]) + (4 * [0.1]) + (4 * [0.01]) + (4 * [0.001]),
            ),
            3,  # cut after the 3rd epoch - restart at LR=1 and update at 4 correctly
        ),
        (
            SchedulerTestConfig(
                cfg=dict(type="StepLR", params=dict(step_size=4, gamma=0.1)),
                init_lr=1.0,
                expected_lrs=(4 * [1.0]) + [0.1, 1, 1, 1] + (4 * [0.1]),
            ),
            5,  # cut after the 5th epoch - restart at LR=1 and update again at 8
        ),
    ],
)
def test_resuming_training_with_no_scheduler_state(
    runner_cls, test_cfg: SchedulerTestConfig, resume_epoch: int
):
    """
    Without a scheduler config, there is no way to set the initial LR. All we can do is
    set the last_epoch value, and adjust correctly at milestones going forward.
    """
    runner = _fit_runner_and_check_lrs(
        runner_cls,
        test_cfg.init_lr,
        test_cfg.cfg,
        test_cfg.expected_lrs[:resume_epoch],
    )
    assert runner.current_epoch == resume_epoch

    runner = _fit_runner_and_check_lrs(
        runner_cls,
        test_cfg.init_lr,
        test_cfg.cfg,
        expected_lrs=test_cfg.expected_lrs[resume_epoch:],
        snapshot_to_load=dict(metadata=dict(epoch=resume_epoch)),
    )
    assert runner.current_epoch == len(test_cfg.expected_lrs)


def _fit_runner_and_check_lrs(
    runner_cls,
    init_lr: float,
    scheduler_cfg: dict,
    expected_lrs: list[float],
    snapshot_to_load: dict | None = None,
) -> train_runners.TrainingRunner:
    runner_kwargs = dict(device="cpu", eval_interval=1_000_000)
    optimizer = torch.optim.SGD([torch.randn(2, 2)], lr=init_lr)
    scheduler = schedulers.build_scheduler(scheduler_cfg, optimizer)
    num_epochs = len(expected_lrs)

    base_path = "deeplabcut.pose_estimation_pytorch.runners"
    with patch(f"{base_path}.base.Runner.load_snapshot") as base_mock_load:
        with patch(f"{base_path}.train.PoseTrainingRunner.load_snapshot") as mock_load:
            snapshot_path = None
            base_mock_load.return_value = dict()
            mock_load.return_value = dict()
            if snapshot_to_load is not None:
                snapshot_path = "fake_snapshot.pt"
                base_mock_load.return_value = snapshot_to_load
                mock_load.return_value = snapshot_to_load

            print()
            print(f"Scheduler: {scheduler}")
            print(f"Starting training for {num_epochs} epochs")
            runner = runner_cls(
                model=Mock(),
                optimizer=optimizer,
                snapshot_manager=Mock(),
                scheduler=scheduler,
                snapshot_path=snapshot_path,
                **runner_kwargs,
            )

            # Mock the step call; check that the learning rate is correct for the epoch
            def step(*args, **kwargs):
                # the current_epoch value is indexed at 1
                total_epoch = runner.current_epoch - 1
                epoch = total_epoch - runner.starting_epoch
                _assert_learning_rates_match(total_epoch, optimizer, expected_lrs[epoch])
                optimizer.step()
                return dict(total_loss=0)

            train_loader, val_loader = [Mock()], [Mock()]
            runner.step = step
            runner.fit(train_loader, val_loader, epochs=num_epochs, display_iters=1000)

    return runner


def _assert_learning_rates_match(e, optimizer, expected):
    current_lrs = [g["lr"] for g in optimizer.param_groups]
    print(f"Epoch {e}: LR={current_lrs}, expected={expected}")
    for lr in current_lrs:
        assert isinstance(lr, float)
        np.testing.assert_almost_equal(lr, expected)


--- File: tests/pose_estimation_pytorch/runners/bottum_up.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
""" Tests for the bottom-up pytorch runner """
from pathlib import Path
from typing import Dict, Any

import pytest
import torch
from deeplabcut.pose_estimation_pytorch.config import make_pytorch_pose_config

from deeplabcut.pose_estimation_pytorch.models import PoseModel, LOSSES, PREDICTORS
from deeplabcut.pose_estimation_pytorch.models.criterion import WeightedAggregateLoss
from deeplabcut.pose_estimation_pytorch.runners import RUNNERS
from deeplabcut.pose_estimation_pytorch.runners.schedulers import LRListScheduler
from deeplabcut.utils import auxiliaryfunctions


SINGLE_ANIMAL_NETS = ["resnet_50"]
MULTI_ANIMAL_NETS = ["dekr_w18"]
NETS = [(n, False) for n in SINGLE_ANIMAL_NETS] + [(n, True) for n in MULTI_ANIMAL_NETS]


def print_dict(data: Dict, indent: int = 0):
    for k, v in data.items():
        if isinstance(v, dict):
            print_dict(v, indent=indent + 2)
        else:
            print(f"{indent * ' '}{k}: {v}")


@pytest.mark.parametrize("net_type, multianimal", NETS)
def test_build_bottom_up_runner(
    net_type: str,
    multianimal: bool,
) -> None:
    project_cfg: Dict[str, Any] = {"multianimalproject": multianimal}
    if multianimal:
        project_cfg["bodyparts"] = "MULTI!"
        project_cfg["multianimalbodyparts"] = ["head", "shoulder", "knee", "toe"]
        project_cfg["uniquebodyparts"] = []
        project_cfg["individuals"] = ["tom", "jerry"]
    else:
        project_cfg["bodyparts"] = ["head", "shoulder", "knee", "toe"]
        project_cfg["uniquebodyparts"] = []
        project_cfg["individuals"] = ["tom"]

    root_path = Path(auxiliaryfunctions.get_deeplabcut_path())
    template_path = root_path / "pose_estimation_pytorch" / "apis" / "pytorch_config.yaml"
    template = auxiliaryfunctions.read_plainconfig(str(template_path))
    pytorch_cfg = make_pytorch_pose_config(project_cfg, str(template_path), net_type)
    print_dict(pytorch_cfg)

    pose_model = PoseModel.build(pytorch_cfg["model"])

    head_criterions = []
    for head_cfg in pytorch_cfg["model"]["heads"]:
        crit_cfg = head_cfg["criterion"]
        criterion_weight = crit_cfg.get("weight", 1)
        criterion = LOSSES.build({k: v for k, v in crit_cfg.items() if k != "weight"})
        head_criterions.append((criterion_weight, criterion))
    criterion = WeightedAggregateLoss(head_criterions)

    get_optimizer = getattr(torch.optim, pytorch_cfg["optimizer"]["type"])
    optimizer = get_optimizer(
        params=pose_model.parameters(), **pytorch_cfg["optimizer"]["params"]
    )

    predictor = PREDICTORS.build(dict(pytorch_cfg["model"]["predictor"]))

    if pytorch_cfg.get("scheduler"):
        if pytorch_cfg["scheduler"]["type"] == "LRListScheduler":
            _scheduler = LRListScheduler
        else:
            _scheduler = getattr(
                torch.optim.lr_scheduler, pytorch_cfg["scheduler"]["type"]
            )
        scheduler = _scheduler(
            optimizer=optimizer, **pytorch_cfg["scheduler"]["params"]
        )
    else:
        scheduler = None

    logger = None
    runner = RUNNERS.build(
        dict(
            **pytorch_cfg["solver"],
            model=pose_model,
            criterion=criterion,
            optimizer=optimizer,
            predictor=predictor,
            cfg=pytorch_cfg,
            device=pytorch_cfg["device"],
            scheduler=scheduler,
            logger=logger,
        )
    )


--- File: tests/pose_estimation_pytorch/runners/test_runners.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import pickle
from pathlib import Path
from unittest.mock import Mock

import numpy as np
import pytest
import torch

import deeplabcut.pose_estimation_pytorch.runners as runners


@pytest.mark.parametrize("value", [True, False])
def test_set_load_weights_only(value: bool):
    print(f"\nget_load_weights_only: {runners.get_load_weights_only()}")
    print(f"setting value to {value}")
    runners.set_load_weights_only(value)
    print(f"get_load_weights_only: {runners.get_load_weights_only()}\n")
    assert runners.get_load_weights_only() == value


def test_load_snapshot_weights_only_error(tmpdir_factory):
    snapshot_dir = Path(tmpdir_factory.mktemp("snapshot-dir"))
    snapshot_path = snapshot_dir / "snapshot.pt"
    torch.save(dict(content=np.zeros(10)), str(snapshot_path))

    runners.set_load_weights_only(False)
    with pytest.raises(pickle.UnpicklingError):
        runners.Runner.load_snapshot(
            snapshot_path, device="cpu", model=Mock(), weights_only=True
        )


--- File: tests/pose_estimation_pytorch/runners/test_dynamic_cropper.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests the dynamic cropper"""
import pytest

import numpy as np
import torch

from deeplabcut.pose_estimation_pytorch.runners.dynamic_cropping import DynamicCropper


@pytest.mark.parametrize("dynamic", [(False, 0.5, 10)])
def test_build_dynamic_cropper(dynamic: tuple[bool, float, int]):
    cropper = DynamicCropper.build(*dynamic)
    should_be_built, threshold, margin = dynamic
    if should_be_built:
        assert isinstance(cropper, DynamicCropper)
        assert cropper.threshold == threshold
        assert cropper.margin == margin
    else:
        assert cropper is None


@pytest.mark.parametrize("batch_size", [0, 2, 8])
def test_dynamic_fails_with_image_batch(batch_size: int):
    cropper = DynamicCropper(threshold=0.6, margin=10)
    with pytest.raises(RuntimeError):
        cropper.crop(torch.zeros(batch_size, 3, 128, 128))


def test_dynamic_fails_with_variable_frame_size():
    cropper = DynamicCropper(threshold=0.6, margin=10)
    cropper.crop(torch.zeros(1, 3, 64, 64))
    with pytest.raises(RuntimeError):
        cropper.crop(torch.zeros(1, 3, 128, 128))


def test_dynamic_fails_with_update_before_crop():
    cropper = DynamicCropper(threshold=0.6, margin=10)
    with pytest.raises(RuntimeError):
        cropper.update(torch.ones(5, 17, 3))


@pytest.mark.parametrize("threshold", [0.25, 0.5, 0.8])
def test_dynamic_cropper_does_nothing_with_low_quality(threshold: float):
    cropper = DynamicCropper(threshold=threshold, margin=10)
    image_in = torch.ones((1, 3, 32, 32))
    cropper.crop(image_in)
    for i in range(10):
        pose = _generate_random_pose(
            (32, 64),
            min_score=0.0,
            max_score=threshold - 0.001,
            seed=i,
        )
        cropper.update(pose)
        image_out = cropper.crop(image_in)
        assert torch.equal(image_in, image_out)


@pytest.mark.parametrize(
    "pose, threshold, margin, expected_crop",
    [
        ([[float("nan"), float("nan"), float("nan")]], 0.1, 10, [0, 0, 64, 64]),
        ([[float("nan"), 30, 0.0]], 0.5, 10, [0, 0, 64, 64]),
        ([[20, 30, 0.0]], 0.5, 10, [0, 0, 64, 64]),
        ([[20, 30, 0.49]], 0.5, 10, [0, 0, 64, 64]),
        ([[20, 30, 0.8]], 0.5, 10, [10, 20, 30, 40]),
        ([[20, 30, 0.8], [float("nan"), float("nan"), 0.2]], 0.5, 15, [5, 15, 35, 45]),
        ([[20, 30, 0.8], [5, 5, 0.2]], 0.5, 15, [0, 0, 35, 45]),
        ([[20, 30, 0.8], [35, 30, 0.79]], 0.8, 5, [15, 25, 40, 35]),
        ([[40, 10, 0.2], [35, 15, 0.79]], 0.3, 8, [27, 2, 48, 23]),
        (
            [
                [[float("nan"), float("nan"), float("nan")]],
                [[float("nan"), float("nan"), float("nan")]],
            ],
            0.15, 10, [0, 0, 64, 64]
        ),
        (
            [
                [[20, 30, 0.8], [5, 12, 0.2]],
                [[40, 10, 0.2], [35, 15, 0.79]],
            ],
            0.15, 5, [0, 5, 45, 35]
        ),
    ],
)
def test_dynamic_cropper_basic_crop(
    pose: list[list[float]],
    threshold: float,
    margin: int,
    expected_crop: tuple[int, int, int, int]
) -> None:
    x0, y0, x1, y1 = expected_crop
    crop_w, crop_h = x1 - x0, y1 - y0

    image_in = torch.zeros((1, 3, 64, 64))
    image_in[:, :, y0:y1, x0:x1] = 1
    expected_image_out = torch.ones((1, 3, crop_h, crop_w))

    cropper = DynamicCropper(threshold=threshold, margin=margin)
    image_out = cropper.crop(image_in)
    assert torch.equal(image_out, image_in)

    cropper.update(torch.tensor(pose))
    image_out = cropper.crop(image_in)
    assert image_out.shape == expected_image_out.shape
    assert torch.equal(image_out, expected_image_out)

    pose_out = torch.tensor(pose)
    print("\nPose in")
    print(pose_out.numpy())
    pose_out[..., 0] -= x0
    pose_out[..., 1] -= y0
    print("Pose out before update")
    print(pose_out.numpy())
    cropper.update(pose_out)
    print("Pose out after update")
    print(pose_out.numpy())
    np.testing.assert_allclose(pose_out.numpy(), np.array(pose))


def _generate_random_pose(
    image_shape: tuple[int, int],
    min_score: float,
    max_score: float,
    num_animals: int = 3,
    num_keypoints: int = 7,
    seed: int = 0,
) -> torch.Tensor:
    gen = np.random.default_rng(seed)
    pose = gen.random((num_animals, num_keypoints, 3))
    pose[..., 0] *= image_shape[0]
    pose[..., 1] *= image_shape[1]
    pose[..., 2] = (pose[..., 2] * (max_score - min_score)) + min_score
    return torch.from_numpy(pose)


--- File: tests/pose_estimation_pytorch/runners/test_logger.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests loggers"""
from typing import Any

import pytest
import torch

import deeplabcut.pose_estimation_pytorch.runners.logger as logging


class MockImageLogger(logging.ImageLoggerMixin):
    """Mock image logger"""

    def log_images(
        self,
        inputs: dict[str, Any],
        outputs: dict[str, torch.Tensor],
        targets: dict[str, dict[str, torch.Tensor]],
        step: int,
    ) -> None:
        pass


@pytest.mark.parametrize(
    "keypoints",
    [
        [
            [[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]],
        ],
        [
            [[float("nan"), float("nan")], [float("nan"), float("nan")]],
        ],
        [
            [[0.0, 0.0], [1, 1], [2, 2]],
        ],
        [[[float("nan"), 0.0], [1, 1], [2, 2]]],
        [[[-1.0, -1.0], [1, 1], [2, 2]]],
        [
            [[-1.0, -1.0], [-1.0, -1.0]],
        ],
        [
            [[-1.0, -1.0], [-1.0, -1.0]],
            [[1.0, 1.0], [1.0, 1.0]],
        ],
    ],
)
@pytest.mark.parametrize("denormalize", [True, False])
def test_prepare_image(keypoints: list[list[float]], denormalize: bool) -> None:
    image = torch.ones((3, 256, 256))
    keypoints = torch.tensor(keypoints)

    print()
    print(f"IMAGE: {image.shape}")
    print(f"KEYPOINTS: {keypoints.shape}")
    for k in keypoints:
        print(k)
    print()
    print()

    logger = MockImageLogger()
    logger._prepare_image(
        image=image,
        denormalize=denormalize,
        keypoints=keypoints,
        bboxes=None,
    )


--- File: tests/pose_estimation_pytorch/data/test_postprocessor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests the pre-processors"""
import numpy as np
import pytest

from deeplabcut.pose_estimation_pytorch.data.postprocessor import (
    PredictKeypointIdentities,
    PrepareBackboneFeatures,
    RescaleAndOffset,
    TrimOutputs,
)


@pytest.mark.parametrize(
    "data",
    [
        {
            "predictions": [[[0, 0, 0.95], [20, 30, 0.5]]],
            "offsets": [(0, 0)],
            "scales": [(1, 1)],
            "rescaled": [[[0, 0, 0.95], [20, 30, 0.5]]],
        },
        {
            "predictions": [
                [[0, 0, 0.12], [1000, 0, 0.5]],  # individual 1
                [[18, 2, 0.24], [0, 1000, 0.6]],  # individual 2
            ],
            "offsets": [(0, 0), (0, 0)],
            "scales": [(1, 1), (0.5, 1.0)],
            "rescaled": [
                [[0, 0, 0.12], [1000, 0, 0.5]],  # individual 1
                [[9, 2, 0.24], [0, 1000, 0.6]],  # individual 2
            ],
        },
        {
            "predictions": [
                [[0, 0, 0.95], [20, 30, 0.5]],  # individual 1
                [[110, 5, 0.95], [60, 1200, 0.5]],  # individual 2
            ],
            "offsets": [(12, 5), (27, 10)],
            "scales": [(0.5, 0.5), (0.2, 0.2)],
            "rescaled": [
                [[12, 5, 0.95], [22, 20, 0.5]],  # individual 1
                [[49, 11, 0.95], [39, 250, 0.5]],  # individual 2
            ],
        },
    ],
)
def test_rescale_topdown(data):
    """expects x_processed = x * scale + offset"""
    postprocessor = RescaleAndOffset(
        keys_to_rescale=["bodyparts"],
        mode=RescaleAndOffset.Mode.KEYPOINT_TD,
    )
    context = {"scales": data["scales"], "offsets": data["offsets"]}
    predictions = {"bodyparts": np.array(data["predictions"])}
    predictions, context = postprocessor(predictions, context=context)
    print(predictions["bodyparts"].tolist())
    print(data["rescaled"])
    np.testing.assert_array_equal(predictions["bodyparts"], np.array(data["rescaled"]))


@pytest.mark.parametrize(
    "data",
    [
        {
            "bboxes": [[0, 0, 0, 0], [1, 1, 1, 1]],
            "bbox_scores": [0, 0],
            "max_individuals": {"bboxes": 1, "bbox_scores": 1},
        },
        {
            "bboxes": [[0, 0, 0, 0], [1, 1, 1, 1]],
            "bbox_scores": [0, 0],
            "max_individuals": {"bboxes": 2, "bbox_scores": 2},
        },
    ],
)
def test_trim_outputs(data):
    """expects x_processed = x * scale + offset"""
    postprocessor = TrimOutputs(max_individuals=data["max_individuals"])
    context = {}
    predictions = {"bboxes": np.array(data["bboxes"]), "bbox_scores": np.array(data["bbox_scores"])}
    predictions, context = postprocessor(predictions, context=context)
    print(predictions["bboxes"].tolist())
    print(predictions["bbox_scores"].tolist())
    assert len(predictions["bboxes"]) == data["max_individuals"]["bboxes"]
    assert len(predictions["bbox_scores"]) == data["max_individuals"]["bbox_scores"]


@pytest.mark.parametrize(
    "data",
    [
        {
            "predictions": [[[0, 0, 0.95], [20, 30, 0.5]]],
            "offsets": (0, 0),
            "scales": (1, 1),
            "rescaled": [[[0, 0, 0.95], [20, 30, 0.5]]],
        },
        {
            "predictions": [
                [[0, 0, 0.12], [10, 0, 0.5]],  # individual 1
                [[1000, 500, 0.24], [50, 250, 0.6]],  # individual 2
            ],
            "offsets": (5, 7),
            "scales": (0.2, 0.5),
            "rescaled": [
                [[5, 7, 0.12], [7, 7, 0.5]],  # individual 1
                [[205, 257, 0.24], [15, 132, 0.6]],  # individual 2
            ],
        },
    ],
)
def test_rescale_bottom_up(data):
    """expects x_processed = x * scale + offset"""
    postprocessor = RescaleAndOffset(
        keys_to_rescale=["bodyparts"],
        mode=RescaleAndOffset.Mode.KEYPOINT,
    )
    context = {"scales": data["scales"], "offsets": data["offsets"]}
    predictions = {"bodyparts": np.array(data["predictions"])}
    predictions, context = postprocessor(predictions, context=context)
    print(predictions["bodyparts"].tolist())
    print(data["rescaled"])
    np.testing.assert_array_equal(predictions["bodyparts"], np.array(data["rescaled"]))


@pytest.mark.parametrize(
    "data",
    [
        {
            "bboxes": [[222.0, 562.0, 721.0, 637.0]],
            "offsets": (0, 0),
            "scales": (1, 1),
            "rescaled": [[222.0, 562.0, 721.0, 637.0]],
        },
        {
            "bboxes": [[386.71875, 219.53125, 281.640625, 248.828125]],
            "offsets": (-768, 0),
            "scales": (2.56, 2.56),
            "rescaled": [[222.0, 562.0, 721.0, 637.0]],
        },
        {
            "bboxes": [
                [0, 0, 100, 100],
                [5, 10, 100, 100],
                [5, 10, 10, 20],
            ],
            "offsets": (3, 7),
            "scales": (2, 0.5),
            "rescaled": [
                [3, 7, 200, 50],
                [13, 12, 200, 50],
                [13, 12, 20, 10],
            ],
        },
    ],
)
def test_rescale_detector(data):
    """expects x_processed = x * scale + offset"""
    postprocessor = RescaleAndOffset(
        keys_to_rescale=["bboxes"],
        mode=RescaleAndOffset.Mode.BBOX_XYWH,
    )
    context = {"scales": data["scales"], "offsets": data["offsets"]}
    predictions = {"bboxes": np.array(data["bboxes"])}
    predictions, context = postprocessor(predictions, context=context)
    print(predictions["bboxes"].tolist())
    print(data["rescaled"])
    np.testing.assert_array_equal(predictions["bboxes"], np.array(data["rescaled"]))


@pytest.mark.parametrize(
    "data",
    [
        {
            "bodyparts": [
                [[3.1, 1, 0.8], [1, 0, 0.9]],  # assembly 1  (x, y, score)
                [[2.2, 1.6, 0.5], [3, 3, 0.4]],  # assembly 2  (x, y, score)
            ],
            "id_heatmap": [  # id1, id2 score for each pixel
                [[0.1, 0.1], [0.2, 0.1], [0.3, 0.1], [0.4, 0.1]],
                [[0.1, 0.2], [0.2, 0.2], [0.3, 0.2], [0.4, 0.2]],
                [[0.1, 0.3], [0.2, 0.3], [0.3, 0.3], [0.4, 0.3]],
                [[0.1, 0.4], [0.2, 0.4], [0.3, 0.4], [0.4, 0.4]],
            ],
            "id_scores": [  # id1, id2 score for each bodypart
                [[0.4, 0.2], [0.2, 0.1]],  # assembly 1 (id_1 proba, id_2 proba)
                [[0.3, 0.3], [0.4, 0.4]],  # assembly 2 (id_1 proba, id_2 proba)
            ],
        },
    ],
)
def test_assign_id_scores(data):
    p = PredictKeypointIdentities(
        identity_key="keypoint_identity",
        identity_map_key="identity_map",
        pose_key="bodyparts",
        keep_id_maps=True,
    )
    bodyparts = np.array(data["bodyparts"])
    id_heatmap = np.array(data["id_heatmap"])
    expected_ids = np.array(data["id_scores"])
    print()
    print(bodyparts.shape)
    print(id_heatmap.shape)
    print(expected_ids.shape)
    predictions_in = {"bodyparts": bodyparts, "identity_map": id_heatmap}
    predictions, _ = p(predictions_in, {})
    np.testing.assert_array_equal(
        predictions["keypoint_identity"],
        expected_ids,
    )


def test_prepare_backbone_features():
    p = PrepareBackboneFeatures(top_down=False)

    img_w, img_h = 256, 128
    features = np.zeros((1, img_h, img_w))

    features[0, 15, 10] = 1
    features[0, 25, 20] = 2
    features[0, 35, 30] = 3

    pose = np.array([
        [
            [10.1, 15.1, 0.95],
            [20.1, 25.1, 0.95],
            [29.9, 34.9, 0.95],
        ],
    ])

    predictions = [dict(backbone=dict(features=features), bodypart=dict(poses=pose))]
    context = dict(image_size=(img_w, img_h))
    predictions_out, context_out = p(predictions, context)

    assert len(predictions_out) == 1
    assert len(context_out) == 1
    preds = predictions_out[0]

    assert "backbone" in preds
    assert "bodypart_features" in preds["backbone"]
    bodypart_features = preds["backbone"]["bodypart_features"]
    print(f"Bodypart features: {bodypart_features.shape}")
    print(bodypart_features)
    assert bodypart_features.shape == (1, 3, 1)
    assert bodypart_features.reshape(-1).tolist() == [1, 2, 3]


def test_prepare_top_down_backbone_features():
    p = PrepareBackboneFeatures(top_down=True)

    img_w, img_h = 256, 256

    features = np.zeros((2, 1, img_h, img_w))
    features[0, 0, 15, 10] = 1
    features[0, 0, 25, 20] = 2
    features[0, 0, 35, 30] = 3
    features[1, 0, 95, 10] = 11
    features[1, 0, 85, 20] = 12
    features[1, 0, 75, 30] = 13

    pose_idv0 = np.array([
        [
            [10.1, 15.1, 0.95],
            [20.1, 25.1, 0.95],
            [29.9, 34.9, 0.95],
        ],
    ])
    pose_idv1 = np.array(
        [
            [
                [10.1, 95.1, 0.95],
                [20.1, 85.1, 0.95],
                [29.9, 74.9, 0.95],
            ],
        ]
    )

    predictions = [
        dict(backbone=dict(features=features[0]), bodypart=dict(poses=pose_idv0)),
        dict(backbone=dict(features=features[1]), bodypart=dict(poses=pose_idv1)),
    ]
    context = dict(top_down_crop_size=(img_w, img_h))
    predictions_out, context_out = p(predictions, context)

    assert len(predictions_out) == 2
    assert len(context_out) == 1
    for preds, expected in zip(predictions_out, [[1, 2, 3], [11, 12, 13]]):
        assert "backbone" in preds
        assert "bodypart_features" in preds["backbone"]
        bodypart_features = preds["backbone"]["bodypart_features"]
        print(f"Bodypart features: {bodypart_features.shape}")
        print(bodypart_features)
        assert bodypart_features.shape == (1, 3, 1)
        assert bodypart_features.reshape(-1).tolist() == expected


--- File: tests/pose_estimation_pytorch/data/test_utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests data utils"""
import numpy as np
import pytest

import deeplabcut.pose_estimation_pytorch.data.utils as utils


@pytest.mark.parametrize(
    "keypoints, expected_bboxes, params",
    [
        (
            [[0, 0, 2], [10, 5, 2]],
            [0, 0, 10, 5],
            dict(image_w=1024, image_h=1024, margin=0),
        ),
        (
            [[-1, -1, 2], [3, 4, 2]],
            [0, 0, 3, 4],
            dict(image_w=1024, image_h=1024, margin=0),
        ),
        (
            [[0, 0, 2], [10, 5, 2]],
            [0, 0, 5, 3],
            dict(image_w=5, image_h=3, margin=0),
        ),
        (
            [[0, 0, 2], [10, 5, 2]],
            [0, 0, 5, 3],
            dict(image_w=5, image_h=3, margin=10),
        ),
        (
            [[[0, 0, 2], [10, 5, 2]]],
            [[0, 0, 10, 5]],
            dict(image_w=1024, image_h=1024, margin=0),
        ),
        (
            [
                [[4, 1, 2], [10, 5, 2], [3, 12, 0]],
                [[7, 3, 2], [2, 0, -1], [1, 12, 2]],
            ],
            [
                [4, 1, 6, 4],
                [1, 3, 6, 9],
            ],
            dict(image_w=1024, image_h=1024, margin=0),
        ),
        (
            [
                [[4, 1, 2], [10, 5, 2], [3, 12, 0]],
                [[7, 3, 2], [2, 0, -1], [1, 12, 2]],
            ],
            [
                [2, 0, 10, 7],
                [0, 1, 9, 13],
            ],
            dict(image_w=1024, image_h=1024, margin=2),
        ),
        (
            [
                [[4, 1, 2], [10, 5, 2], [3, 12, 0]],
                [[7, 3, 2], [2, 0, -1], [1, 12, 2]],
            ],
            [
                [2, 0, 8, 7],
                [0, 1, 9, 9],
            ],
            dict(image_w=10, image_h=10, margin=2),
        ),
        (
            [
                [[4, 1, 2], [10, 5, 2], [3, 12, 0]],
                [[7, 3, 0], [2, 0, -1], [1, 12, 0]],
            ],
            [
                [2, 0, 8, 7],
                [0, 0, 0, 0],
            ],
            dict(image_w=10, image_h=10, margin=2),
        ),
    ],
)
def test_bbox_from_keypoints(keypoints, expected_bboxes, params):
    keypoints = np.asarray(keypoints, dtype=float)
    bboxes = utils.bbox_from_keypoints(keypoints, **params)
    expected_bboxes = np.asarray(expected_bboxes, dtype=float)
    np.testing.assert_array_almost_equal(bboxes, expected_bboxes)


--- File: tests/pose_estimation_pytorch/data/test_preprocessor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests the pre-processors"""
import albumentations as A
import numpy as np
import pytest

from deeplabcut.pose_estimation_pytorch.data.transforms import build_resize_transforms
from deeplabcut.pose_estimation_pytorch.data.preprocessor import AugmentImage


@pytest.mark.parametrize(
    "data",
    [
        {
            "image_shape": (2, 4, 4),
            "resize_transform": {"height": 5, "width": 4, "keep_ratio": True},
            "output_shape": (2, 4, 4),
            "padded_shape": (5, 4, 4),  # single offset as not a batch
            "output_context": {"offsets": (0, 0), "scales": (1, 1)}
        },
        {
            "image_shape": (1, 2, 4, 4),  # as batch
            "resize_transform": {"height": 10, "width": 4, "keep_ratio": True},
            "output_shape": (1, 2, 4, 4),
            "padded_shape": (1, 10, 4, 4),
            "output_context": {"offsets": [(0, 0)], "scales": [(1, 1)]}
        },
        {
            "image_shape": (2, 4, 3),
            "resize_transform": {"height": 10, "width": 8, "keep_ratio": True},
            "output_shape": (4, 8, 3),
            "padded_shape": (10, 8, 3),
            "output_context": {"offsets": (0, 0), "scales": (0.5, 0.5)}
        },
    ],
)
def test_augment_image_rescaling(data):
    resize_transform = build_resize_transforms(data["resize_transform"])
    transform = A.Compose(
        resize_transform,
        keypoint_params=A.KeypointParams("xy", remove_invisible=False),
        bbox_params=A.BboxParams(format="coco", label_fields=["bbox_labels"]),
    )
    preprocessor = AugmentImage(transform)
    img = np.ones(data["image_shape"])
    transformed_image, context = preprocessor(img, context={})
    print()
    print(transformed_image[:, :, 0])  # first channel
    print(context)
    assert np.sum(transformed_image) == np.sum(np.ones(data["output_shape"]))
    assert context == data["output_context"]
    assert transformed_image.shape == data["padded_shape"]


--- File: tests/pose_estimation_pytorch/data/test_transforms.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Tests the custom transforms"""
import random

import albumentations as A
import numpy as np
import pytest

from deeplabcut.pose_estimation_pytorch.data import transforms


@pytest.mark.parametrize(
    "height, width, image_shapes",
    [
        (200, 200, [(300, 300, 3), (1000, 1000, 3), (1024, 1024, 1)]),
        (512, 512, [(1024, 1024, 3), (128, 128, 4), (300, 300, 1)]),
        (1024, 512, [(600, 300, 3), (4096, 2048, 3), (50, 25, 1)]),
        (800, 1300, [(80, 130, 3), (1600, 2600, 4), (1200, 1950, 1)]),
    ],
)
def test_dlc_resize_pad_good_aspect_ratio(height, width, image_shapes):
    aug = transforms.KeepAspectRatioResize(width=width, height=height, mode="pad")
    for image_shape in image_shapes:
        fake_image = np.zeros(image_shape)
        transformed = aug(image=fake_image, keypoints=[])
        assert transformed["image"].shape[:2] == (height, width)
        assert transformed["image"].shape[2] == fake_image.shape[2]


@pytest.mark.parametrize(
    "data",
    [
        {
            "height": 200,
            "width": 200,
            "in_shapes": [(100, 50, 3), (50, 400, 3)],
            "out_shapes": [(200, 100, 3), (25, 200, 3)],
        },
        {
            "height": 128,
            "width": 256,
            "in_shapes": [(100, 100, 3), (512, 256, 3)],
            "out_shapes": [(128, 128, 3), (128, 64, 3)],
        },
    ],
)
def test_dlc_resize_pad_bad_aspect_ratio(data):
    aug = transforms.KeepAspectRatioResize(width=data["width"], height=data["height"], mode="pad")
    for in_shape, out_shape in zip(data["in_shapes"], data["out_shapes"]):
        fake_image = np.zeros(in_shape)
        transformed = aug(image=fake_image, keypoints=[])
        assert transformed["image"].shape == out_shape


@pytest.mark.parametrize(
    "data",
    [
        {
            "height": 200,
            "width": 200,
            "in_shape": (100, 50, 3),
            "out_shape": (200, 100, 3),
            "in_keypoints": [(50.0, 50.0), (25.0, 10.0)],
            "out_keypoints": [(100.0, 100.0), (50.0, 20.0)],
        },
        {
            "height": 512,
            "width": 256,
            "in_shape": (1024, 1024, 3),
            "out_shape": (256, 256, 3),
            "in_keypoints": [(512.0, 512.0), (100.0, 10.0)],
            "out_keypoints": [(128.0, 128.0), (25.0, 2.5)],
        },
    ],
)
def test_dlc_resize_pad_bad_aspect_ratio_with_keypoints(data):
    aug = transforms.KeepAspectRatioResize(width=data["width"], height=data["height"], mode="pad")
    transform = A.Compose(
        [aug],
        keypoint_params=A.KeypointParams("xy", remove_invisible=False),
    )
    fake_image = np.zeros(data["in_shape"])
    transformed = transform(image=fake_image, keypoints=data["in_keypoints"])
    assert transformed["image"].shape == data["out_shape"]
    assert transformed["keypoints"] == data["out_keypoints"]


def test_coarse_dropout():
    aug = transforms.CoarseDropout(
        max_holes=10,
        max_height=0.05,
        min_height=0.01,
        max_width=0.05,
        min_width=0.01,
        p=0.5,
    )


@pytest.mark.parametrize(
    "data",
    [
        {
            "image_shape": [480, 640, 3],
            "transform_config": dict(
                shift_factor=10.0,
                shift_prob=0.0,
                scale_factor=[0.1, 2.0],
                scale_prob=0.0,
            ),
        },
        {
            "image_shape": [480, 640, 3],
            "transform_config": dict(
                shift_factor=0.0,
                shift_prob=1.0,
                scale_factor=[1.0, 1.0],
                scale_prob=1.0,
                sampling="uniform",  # truncnorm throws an error if delta is 0
            ),
        },
    ],
)
def test_random_bbox_transform_does_not_modify_with_base_config(data: dict) -> None:
    _set_random_seed()
    h, w, c = data["image_shape"]

    # generate 100 bboxes
    bboxes = _gen_random_bboxes(np.random.default_rng(seed=0), 100, w, h)

    t = A.Compose(
        [transforms.RandomBBoxTransform(**data["transform_config"])],
        bbox_params=A.BboxParams(format="coco", label_fields=["bbox_labels"]),
    )
    output = t(
        image=np.zeros((h, w, c)), bboxes=bboxes, bbox_labels=np.zeros(len(bboxes)),
    )
    print("Output bounding boxes")
    for out_bbox in output["bboxes"]:
        print(out_bbox)
    print()
    bboxes_out = np.asarray(output["bboxes"])
    print("bboxes")
    print(bboxes_out)
    print()
    np.testing.assert_array_almost_equal(bboxes, bboxes_out)


@pytest.mark.parametrize(
    "data",
    [
        {
            "image_shape": [480, 640, 3],
            "transform_config": dict(
                shift_factor=0.0,
                shift_prob=0.0,
                scale_factor=[0.25, 0.5],
                scale_prob=1.0,
            ),
        },
        {
            "image_shape": [480, 640, 3],
            "transform_config": dict(
                shift_factor=0.0,
                shift_prob=0.0,
                scale_factor=[1.0, 1.5],
                scale_prob=1.0,
            ),
        },
        {
            "image_shape": [480, 640, 3],
            "transform_config": dict(
                shift_factor=0.0,
                shift_prob=0.0,
                scale_factor=[0.5, 1.25],
                scale_prob=1.0,
            ),
        },
        {
            "image_shape": [480, 640, 3],
            "transform_config": dict(
                shift_factor=0.0,
                shift_prob=0.0,
                scale_factor=[0.5, 1.5],
                scale_prob=0.5,
            ),
        },
    ],
)
def test_random_bbox_transform_scale(data: dict) -> None:
    _set_random_seed()
    h, w, c = data["image_shape"]

    # generate 100 bboxes
    bboxes = _gen_random_bboxes(np.random.default_rng(seed=0), 100, w, h)

    t = A.Compose(
        [transforms.RandomBBoxTransform(**data["transform_config"])],
        bbox_params=A.BboxParams(format="coco", label_fields=["bbox_labels"]),
    )
    output = t(
        image=np.zeros((h, w, c)), bboxes=bboxes, bbox_labels=np.zeros(len(bboxes)),
    )
    print("Output bounding boxes")
    for out_bbox in output["bboxes"]:
        print(out_bbox)
    print()

    bboxes_out = np.asarray(output["bboxes"])
    scale_low, scale_high = data["transform_config"]["scale_factor"]
    for bbox_in_wh, bbox_out_wh in zip(bboxes[:, 2:], bboxes_out[:, 2:]):
        print("bbox_in_wh", bbox_in_wh)
        w, h = bbox_in_wh[0].item(), bbox_in_wh[1].item()
        w_low, w_high = w * scale_low, w * scale_high
        h_low, h_high = h * scale_low, h * scale_high
        print("(w, w_low, w_high)", w, w_low, w_high)
        print("(h, h_low, h_high)", h, h_low, h_high)
        assert w_low <= bbox_out_wh[0].item() <= w_high
        assert h_low <= bbox_out_wh[1].item() <= h_high


@pytest.mark.parametrize(
    "data",
    [
        {
            "image_shape": [480, 640, 3],
            "transform_config": dict(
                shift_factor=0.1,
                shift_prob=1.0,
                scale_factor=[1.0, 1.0],
                scale_prob=0.0,
            ),
        },
    ],
)
def test_random_bbox_transform_shift(data: dict) -> None:
    _set_random_seed()
    h, w, c = data["image_shape"]

    # generate 100 bboxes
    bboxes = _gen_random_bboxes(np.random.default_rng(seed=0), 100, w, h)

    t = A.Compose(
        [transforms.RandomBBoxTransform(**data["transform_config"])],
        bbox_params=A.BboxParams(format="coco", label_fields=["bbox_labels"]),
    )
    output = t(
        image=np.zeros((h, w, c)), bboxes=bboxes, bbox_labels=np.zeros(len(bboxes)),
    )
    print("Output bounding boxes")
    for out_bbox in output["bboxes"]:
        print(out_bbox)
    print()

    bboxes_out = np.asarray(output["bboxes"])
    shift = data["transform_config"]["shift_factor"]
    for bbox_in, bbox_out in zip(bboxes, bboxes_out):
        print("bbox_in", bbox_in)
        x, y, w, h = bbox_in
        x_out, y_out, w_out, h_out = bbox_out
        max_shift_x, max_shift_y = w * shift, h * shift
        assert x - max_shift_x <= x_out <= x + max_shift_x
        assert y - max_shift_y <= y_out <= y + max_shift_y


def _set_random_seed():
    np.random.seed(0)
    random.seed(0)


def _gen_random_bboxes(
    gen: np.random.Generator, num_bboxes: int, w: int, h: int,
) -> np.ndarray:
    image_wh = np.array([w, h])
    bboxes = np.zeros((num_bboxes, 4))
    # sample x, y in the images
    bboxes[:, :2] = image_wh * gen.random((num_bboxes, 2))
    # sample w, h with the space remaining
    bboxes[:, 2:] = (image_wh - bboxes[:, :2]) * gen.random((num_bboxes, 2))

    print()
    print("Input bounding boxes")
    print(bboxes)
    return bboxes


--- File: tests/utils/test_multiprocessing.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import pytest
import time
from deeplabcut.utils.multiprocessing import call_with_timeout


def _succeeding_method(parameter):
    return parameter


def _failing_method():
    raise ValueError("Raise value error on purpose")


def _hanging_method():
    while True:
        time.sleep(5)


def test_call_with_timeout():
    parameter = (10, "Hello test")
    assert call_with_timeout(_succeeding_method, 30, parameter) == parameter

    with pytest.raises(ValueError):
        call_with_timeout(_failing_method, timeout=30)

    with pytest.raises(TimeoutError):
        call_with_timeout(_hanging_method, timeout=1)


--- File: docs/convert_maDLC.md ---
(convert-maDLC)=
# How to convert a pre-2.2 project for use with DeepLabCut 2.2 or later

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572296495650-Y4ZTJ2XP2Z9XF1AD74VW/ke17ZwdGBToddI8pDm48kMulEJPOrz9Y8HeI7oJuXxR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UZiU3J6AN9rgO1lHw9nGbkYQrCLTag1XBHRgOrY8YAdXW07ycm2Trb21kYhaLJjddA/DLC_logo_blk-01.png?format=1000w" width="150" title="DLC" alt="DLC!" align="right" vspace = "50">


If you have a pre-2.2 project (`labeled-data`) with a **single animal** that you want to use with a multianimal project
in DLC 2.2 or later, i.e. use your older data to now train the new multi-task deep neural network, here is what you
need to do.

(1) We recommend you make a back-up of your project folder.

(2) Open your `config.yaml` file (in any text editor, or python IDE such as PyCharm, Spyder, VScode, atom, etc).

<p align="center">
<img src= https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1587946828128-VQQRJYYF4I5Q4TK4R7NF/ke17ZwdGBToddI8pDm48kDUwYPb5NcTX7SbsUW3p69pZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpz5alHTAeHWMjsyxt20uNzeb3sgcN8_6mzgExgMZEG-xs3TaY24DmEIA6oEFne2xjs/Screen+Shot+2020-04-23+at+10.32.53+PM.png?format=750w width="80%">
 </p>

- After `task, scorer, date, project_path` please add the following (i.e. in the image above, you would start adding
below line 6) Note, the ordering isn't important but useful to keep consistent with the template:

```python
multianimalproject: true
individuals:
uniquebodyparts: []
multianimalbodyparts:
identity: false/true
```
- Now, please name the animal you have a new name under individuals, i.e.:
```python
individuals:
- mouse1
```

- `"uniquebodyparts: []` can stay blank, unless you have other items labeled you want to estimate (consider these as
similar to bodyparts in pre-2.2); i.e. corners of a box, etc. All unique bodyparts should not be connected to the
multianimal bodyparts in the skeleton you will eventually make. See "advanced option" below.

- Please move your "bodyparts:" to "multianimalbodyparts:" (bodypart names must stay the same!) These are the parts
that will always be interconnected fully!
```python
multianimalbodyparts:
- snout
- leftear
- rightear
- tailbase
```
then you can set `bodyparts: MULTI!`

(3) Save the config.yaml (be sure to double check for spacing or typos first!) and then run:
```python
deeplabcut.convert2_maDLC(path_config_file, userfeedback=True)
```

Now you will see that your data within `labeled-data` are converted to a new format, and the single animal format was
saved for you under a new file named `CollectedData_ ...singleanimal.h5` and `.csv` as a back-up!

(4) We strongly recommend to first run check_labels and verify that the conversion was as expected before creating a
multianimal training dataset. For instance, you can load this project `config.yaml` in the Project Manager GUI and
check labels then create a multi-animal training set with
```python
deeplabcut.create_multianimaltraining_dataset(path_config_file)
```
to begin training.

**Advanced option:** You can also assign former `bodyparts` to either `uniquebodyparts` or `multianimalbodyparts`
(you can even leave some unassigned, which means they will be dropped in the conversion).

Example: Imagine you had a project with the moon and a rocket with two parts labeled:
`bodyparts: [moon, rocket_tip,rocket_bottom]`

Now you want to use this former project (labeled-data) and work on a new dataset (videos) with one moon but multiple
(3) rockets. Then convert it as follows:
```
individuals: [rocket1, rocket2, rocket3]
uniquebodyparts: [moon]
multianimalbodyparts: [rocket_tip,rocket_bottom]
skeleton: [[[rocket_tip,rocket_bottom]]]
```
In the unusual case, that your data also has multiple moons (e.g. is now carried out around Jupiter), but one rocket:
```
individuals: [Io, Europa, Ganymede, Callisto]
uniquebodyparts: [rocket_tip,rocket_bottom]
multianimalbodyparts: [moon]
```
Note you can use the single object tracker for this situation. What if you have multiple moons and rockets?


--- File: docs/maDLC_UserGuide.md ---
(multi-animal-userguide)=
# DeepLabCut for Multi-Animal Projects

This document should serve as the user guide for maDLC,
and it is here to support the scientific advances presented in [Lauer et al. 2022](https://doi.org/10.1038/s41592-022-01443-0).

Note, we strongly encourage you to use the [Project Manager GUI](project-manager-gui) when you first start using multi-animal mode. Each tab is customized for multi-animal when you create or load a multi-animal project. As long as you follow the recommendations within the GUI, you should be good to go!

````{versionadded} 3.0.0
PyTorch is now available as a deep learning engine for pose estimation models, along 
with new model architectures! For more information about moving from TensorFlow to
PyTorch (if you're already familiar with DeepLabCut & the TensorFlow engine), 
check out [the PyTorch user guide](dlc3-user-guide). If you're just starting 
out with DeepLabCut, we suggest you use the PyTorch backend.
````

## How to think about using maDLC:

You should think of maDLC being **four** parts.
- (1) Curate annotation data that allows you to learn a model to track the objects/animals of interest.
- (2) Create a high-quality pose estimation model.
- (3) Track in space and time, i.e., assemble bodyparts to detected objects/animals and link across time. This step performs assembly and tracking (comprising first local tracking and then tracklet stitching by global reasoning).
- (4) Any and all post-processing you wish to do with the output data, either within DLC or outside of it.

Thus, you should always label, train, and evaluate the pose estimation performance first. If and when that performance is high, then you should go advance to the tracking step (and video analysis). There is a natural break point for this, as you will see below.

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1596370260800-SP2GWKDPJCOIR7LJ31VM/ke17ZwdGBToddI8pDm48kB4fL2ovSQh5dRlH2jCMtpoUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcSV94BuD0XUinmig_1P1RJNYVU597j3jgswapL4c_w92BJE9r6UgUperYhWQ2ubQ_/workflow.png?format=2500w" width="550" title="maDLC" alt="maDLC" align="center" vspace = "50">

## Install:

**Quick start:** If you are using DeepLabCut on the cloud, or otherwise cannot use the GUIs and you should install with: `pip install 'deeplabcut'`; if you need GUI support, please use: `pip install 'deeplabcut[gui]'`. Check the [installation page](how-to-install) for more information, including GPU support.

IF you want to use the bleeding edge version to make edits to the code, see [here on how to install it and test it](https://deeplabcut.github.io/DeepLabCut/docs/recipes/installTips.html#how-to-use-the-latest-updates-directly-from-github).

## Get started in the terminal or Project GUI:

**GUI:** simply launch your conda env, and type `python -m deeplabcut` in the terminal.
Then follow the tabs! It might be useful to read the following, however, so you understand what each command does.

**TERMINAL:** To begin, 🚨 (windows) navigate to anaconda prompt and right-click to "open as admin", or (unix/MacOS) simply launch "terminal" on your computer. We assume you have DeepLabCut installed (if not, [see installation instructions](how-to-install)). Next, launch your conda env (i.e., for example `conda activate DEEPLABCUT`).

```{Hint}
🚨 If you use Windows, please always open the terminal with administrator privileges! Right click, and "run as administrator".
```
 Please read more [here](https://deeplabcut.github.io/DeepLabCut/docs/docker.html), and in our Nature Protocols paper [here](https://www.nature.com/articles/s41596-019-0176-0). And, see our [troubleshooting wiki](https://github.com/DeepLabCut/DeepLabCut/wiki/Troubleshooting-Tips).

Open an ``ipython`` session and import the package by typing in the terminal:
```python
ipython
import deeplabcut
```

```{TIP}
for every function there is a associated help document that can be viewed by adding a **?** after the function name; i.e. ``deeplabcut.create_new_project?``. To exit this help screen, type ``:q``.
```

### (A) Create a New Project

```python
deeplabcut.create_new_project(
    "ProjectName",
    "YourName",
    ["/usr/FullPath/OfVideo1.avi", "/usr/FullPath/OfVideo2.avi", "/usr/FullPath/OfVideo1.avi"],
    copy_videos=True,
    multianimal=True,
)
```

Tip: if you want to place the project folder somewhere specific, please also pass : ``working_directory = "FullPathOftheworkingDirectory"``

- Note, if you are a linux/macOS user the path should look like: ``["/home/username/yourFolder/video1.mp4"]``; if you are a Windows user, it should look like: ``[r"C:\username\yourFolder\video1.mp4"]``
- Note, you can also put ``config_path = `` in front of the above line to create the path to the config.yaml that is used in the next step, i.e. ``config_path=deeplabcut.create_project(...)``)
    - If you do not, we recommend setting a variable so this can be easily used! Once you run this step, the config_path is printed for you once you run this line, so set a variable for ease of use, i.e. something like:
```python
config_path = '/thefulloutputpath/config.yaml'
```
 - just be mindful of the formatting for Windows vs. Unix, see above.

This set of arguments will create a project directory with the name **Name of the project+name of the experimenter+date of creation of the project** in the **Working directory** and creates the symbolic links to videos in the **videos** directory. The project directory will have subdirectories: **dlc-models**, **dlc-models-pytorch**, **labeled-data**, **training-datasets**, and **videos**.  All the outputs generated during the course of a project will be stored in one of these subdirectories, thus allowing each project to be curated in separation from other projects. The purpose of the subdirectories is as follows:

**dlc-models** and **dlc-models-pytorch** have a similar structure: the first contains 
files for the TensorFlow engine while the second contains files for the PyTorch engine.
At the top level in these directories, there are
directories referring to different iterations of labels refinement (see below): **iteration-0**, **iteration-1**, etc.
The refinement iterations directories store shuffle directories, each shuffle directory stores model data related to a
particular experiment: trained and tested on a particular training and testing sets, and with a particular model
architecture. Each shuffle directory contains the subdirectories *test* and *train*, each of which holds the meta
information with regard to the parameters of the feature detectors in configuration files. The configuration files are
YAML files, a common human-readable data serialization language. These files can be opened and edited with standard text
editors. The subdirectory *train* will store checkpoints (called snapshots) during training of the model. These
snapshots allow the user to reload the trained model without re-training it, or to pick-up training from a particular
saved checkpoint, in case the training was interrupted.

**labeled-data:** This directory will store the frames used to create the training dataset. Frames from different videos are stored in separate subdirectories. Each frame has a filename related to the temporal index within the corresponding video, which allows the user to trace every frame back to its origin.

**training-datasets:**  This directory will contain the training dataset used to train the network and metadata, which contains information about how the training dataset was created.

**videos:** Directory of video links or videos. When **copy\_videos** is set to ``False``, this directory contains symbolic links to the videos. If it is set to ``True`` then the videos will be copied to this directory. The default is ``False``. Additionally, if the user wants to add new videos to the project at any stage, the function **add\_new\_videos** can be used. This will update the list of videos in the project's configuration file. Note: you neither need to use this folder for videos, nor is it required for analyzing videos (they can be anywhere).

```python
deeplabcut.add_new_videos(
    "Full path of the project configuration file*",
    ["full path of video 4", "full path of video 5"],
    copy_videos=True/False,
)
```

*Please note, *Full path of the project configuration file* will be referenced as ``config_path`` throughout this protocol.

You can also use annotated data from single-animal projects, by converting those files.
There are docs for this: [convert single to multianimal annotation data](convert-maDLC)

![Box 1 - Multi Animal Project Configuration File Glossary](images/box1-multi.png)

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_new_project.rst
```
````

### (B) Configure the Project

Next, open the **config.yaml** file, which was created during  **create\_new\_project**.
You can edit this file in any text editor. Familiarize yourself with the meaning of the
parameters (Box 1). You can edit various parameters, in particular you **must add the list of *individuals* and *bodyparts* (or points of interest)**.

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588892210304-EW7WD46PYAU43WWZS4QZ/ke17ZwdGBToddI8pDm48kAXtGtTuS2U1SVcl-tYMBOAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8PaoYXhp6HxIwZIk7-Mi3Tsic-L2IOPH3Dwrhl-Ne3Z2YjE9w60pqfeJxDohDRZk1jXSVCSSfcEA7WmgMAGpjTehHAH51QaxKq4KdVMVBxpG/1nktc1kdgq2.jpg?format=1000w" width="175" title="colormaps" alt="DLC Utils" align="right" vspace = "50">

You can also set the *colormap* here that is used for all downstream steps (can also be edited at anytime), like labeling GUIs, videos, etc. Here any [matplotlib colormaps](https://matplotlib.org/tutorials/colors/colormaps.html) will do!

An easy way to programmatically edit the config file at any time is to use the function **edit\_config**, which takes the full path of the config file to edit and a dictionary of key–value pairs to overwrite.

```python
import deeplabcut

config_path = "/path/to/project-dlc-2025-01-01/config.yaml"
edits = {
    "colormap": "summer",
    "individuals": ["mickey", "minnie", "bianca"],
    "skeleton": [["snout", "tailbase"], ["snout", "rightear"]]
}
deeplabcut.auxiliaryfunctions.edit_config(config_path, edits)
```

Please DO NOT have spaces in the names of bodyparts, uniquebodyparts, individuals, etc.

**ATTENTION:** You need to edit the config.yaml file to **modify the following items** which specify the animal ID, bodyparts, and any unique labels. Note, we also highly recommend that you use **more bodyparts** that you might be interested in for your experiment, i.e., labeling along the spine/tail for 8 bodyparts would be better than four. This will help the performance.

Modifying the `config.yaml` is crucial:

```python
individuals:
- m1
- m2
- m3

uniquebodyparts:
- topleftcornerofBox
- toprightcornerofBox

multianimalbodyparts:
- snout
- leftear
- rightear
- tailbase

identity: True/False
```

**Individuals:** are names of "individuals" in the annotation dataset. These should/can be generic (e.g. mouse1, mouse2, etc.). These individuals are comprised of the same bodyparts defined by `multianimalbodyparts`. For annotation in the GUI and training, it is important that all individuals in each frame are labeled. Thus, keep in mind that you need to set individuals to the maximum number in your labeled-data set, .i.e., if there is (even just one frame) with 17 animals then the list should be `- indv1` to `- indv17`. Note, once trained if you have a video with more or less animals, that is fine - you can have more or less animals during video analysis!

**Identity:** If you can tell the animals apart, i.e.,  one might have a collar, or a black marker on the tail of a mouse, then you should label these individuals consistently (i.e., always label the mouse with the black marker as "indv1", etc). If you have this scenario, please set `identity: True` in your `config.yaml` file. If you have 4 black mice, and you truly cannot tell them apart, then leave this as `false`.

**Multianimalbodyparts:** are the bodyparts of each individual (in the above list).

**Uniquebodyparts:** are points that you want to track, but that appear only once within each frame, i.e. they are "unique". Typically these are things like unique objects, landmarks, tools, etc. They can also be animals, e.g. in the case where one German shepherd is attending to many sheep the sheep bodyparts would be multianimalbodyparts, the shepherd parts would be uniquebodyparts and the individuals would be the list of sheep (e.g. Polly, Molly, Dolly, ...).

### (C) Select Frames to Label

**CRITICAL:** A good training dataset should consist of a sufficient number of frames that capture the breadth of the behavior. This ideally implies to select the frames from different (behavioral) sessions, different lighting and different animals, if those vary substantially (to train an invariant, robust feature detector). Thus for creating a robust network that you can reuse in the laboratory, a good training dataset should reflect the diversity of the behavior with respect to postures, luminance conditions, background conditions, animal identities, etc. of the data that will be analyzed. For the simple lab behaviors comprising mouse reaching, open-field behavior and fly behavior, 100−200 frames gave good results [Mathis et al, 2018](https://www.nature.com/articles/s41593-018-0209-y). However, depending on the required accuracy, the nature of behavior, the video quality (e.g. motion blur, bad lighting) and the context, more or less frames might be necessary to create a good network. Ultimately, in order to scale up the analysis to large collections of videos with perhaps unexpected conditions, one can also refine the data set in an adaptive way (see refinement below). **For maDLC, be sure you have labeled frames with closely interacting animals!**

The function `extract_frames` extracts frames from all the videos in the project configuration file in order to create a training dataset. The extracted frames from all the videos are stored in a separate subdirectory named after the video file’s name under the ‘labeled-data’. This function also has various parameters that might be useful based on the user’s need.

```python
deeplabcut.extract_frames(
    config_path,
    mode='automatic/manual',
    algo='uniform/kmeans',
    userfeedback=False,
    crop=True/False,
)
```

**CRITICAL POINT:** It is advisable to keep the frame size small, as large frames increase the training and
inference time, or you might not have a large enough GPU for this.
When running the function `extract_frames`, if the parameter crop=True, then you will be asked to draw a box within the GUI (and this is written to the config.yaml file).

`userfeedback` allows the user to check which videos they wish to extract frames from. In this way, if you added more videos to the config.yaml file it does not, by default, extract frames (again) from every video. If you wish to disable this question, set `userfeedback = True`.

The provided function either selects frames from the videos in a randomly and temporally uniformly distributed
way (uniform), by clustering based on visual appearance (k-means), or by manual selection. Random
selection of frames works best for behaviors where the postures vary across the whole video. However, some behaviors
might be sparse, as in the case of reaching where the reach and pull are very fast and the mouse is not moving much
between trials (thus, we have the default set to True, as this is best for most use-cases we encounter). In such a case, the function that allows selecting frames based on k-means derived quantization would
be useful. If the user chooses to use k-means as a method to cluster the frames, then this function downsamples the
video and clusters the frames using k-means, where each frame is treated as a vector. Frames from different clusters
are then selected. This procedure makes sure that the frames look different. However, on large and long videos, this
code is slow due to computational complexity.

**CRITICAL POINT:** It is advisable to extract frames from a period of the video that contains interesting
behaviors, and not extract the frames across the whole video. This can be achieved by using the start and stop
parameters in the config.yaml file. Also, the user can change the number of frames to extract from each video using
the numframes2extract in the config.yaml file.

```{TIP}
For maDLC,  **be sure you have labeled frames with closely interacting animals**! 
Therefore, manually selecting some frames is a good idea if interactions are not highly
frequent in the video.
```

However, picking frames is highly dependent on the data and the behavior being studied.
Therefore, it is hard to provide all purpose code that extracts frames to create a good
training dataset for every behavior and animal. If the user feels specific frames are
lacking, they can extract hand selected frames of interest using the interactive GUI
provided along with the toolbox. This can be launched by using:

```python
deeplabcut.extract_frames(config_path, 'manual')
```

// FIXME(niels) - add a napari frame extractor description.
The user can use the *Load Video* button to load one of the videos in the project
configuration file, use the scroll bar to navigate across the video and *Grab a Frame*. 
The user can also look at the extracted frames and e.g. delete frames (from the
directory) that are too similar before reloading the set and then manually annotating
them.

````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.extract_frames.rst
```
````

### (D) Label Frames

```python
deeplabcut.label_frames(config_path)
```

The toolbox provides a function **label_frames** which helps the user to easily label
all the extracted frames using an interactive graphical user interface (GUI). The user
should have already named the bodyparts to label (points of interest) in the 
project’s configuration file by providing a list. The following command invokes the 
napari-deeplabcut labelling GUI.

[🎥 DEMO](https://youtu.be/hsA9IB5r73E)

HOT KEYS IN THE Labeling GUI (also see "help" in GUI):

```
Ctrl + C: Copy labels from previous frame.
Keyboard arrows: advance frames.
Delete key: delete label.
```

![hot keys](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/192345a5-e411-4d56-b718-ef52f91e195e/Qwerty.png?format=2500w)

**CRITICAL POINT:** It is advisable to **consistently label similar spots** (e.g., on a
wrist that is very large, try to label the same location). In general, invisible or
occluded points should not be labeled by the user, unless you want to teach the network
to "guess" - this is possible, but could affect accuracy. If you don't want/or don't see
a bodypart, they can simply be skipped by not applying the label anywhere on the frame.

OPTIONAL: In the event of adding more labels to the existing labeled dataset, the user 
needs to append the new labels to the bodyparts in the config.yaml file. Thereafter, the
user can call the function **label_frames**. A box will pop up and ask the user if they
wish to display all parts, or only add in the new labels. Saving the labels after all
the images are labelled will append the new labels to the existing labeled dataset.

**maDeepLabCut CRITICAL POINT:** For multi-animal labeling, unless you can tell apart
the animals, you do not need to worry about the "ID" of each animal. For example: if you
have a white and black mouse label the white mouse as animal 1, and black as animal 2
across all frames. If two black mice, then the ID label 1 or 2 can switch between 
frames - no need for you to try to identify them (but always label consistently within a
frame). If you have 2 black mice but one always has an optical fiber (for example), then
DO label them consistently as animal1 and animal_fiber (for example). The point of 
multi-animal DLC is to train models that can first group the correct bodyparts to
individuals, then associate those points in a given video to a specific individual,
which then also uses temporal information to link across the video frames.

Note, we also highly recommend that you use more bodyparts that you might otherwise have
(see the example below).

For more information, checkout the [napari-deeplabcut docs](napari-gui) for 
more information about the labelling workflow.

### (E) Check Annotated Frames

Checking if the labels were created and stored correctly is beneficial for training, since labeling
is one of the most critical parts for creating the training dataset. The DeepLabCut toolbox provides a function
`check_labels` to do so. It is used as follows:

```python
deeplabcut.check_labels(config_path, visualizeindividuals=True/False)
 ```

**maDeepLabCut:** you can check and plot colors per individual or per body part, just set the flag `visualizeindividuals=True/False`. Note, you can run this twice in both states to see both images.

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1586203062876-D9ZL5Q7NZ464FUQN95NA/ke17ZwdGBToddI8pDm48kKmw982fUOZVIQXHUCR1F55Zw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpx7krGdD6VO1HGZR3BdeCbrijc_yIxzfnirMo-szZRSL5-VIQGAVcQr6HuuQP1evvE/img1068_individuals.png?format=750w" width="50%">
</p>

For each video directory in labeled-data this function creates a subdirectory with **labeled** as a suffix. Those directories contain the frames plotted with the annotated body parts. The user can double check if the body parts are labeled correctly. If they are not correct, the user can reload the frames (i.e. `deeplabcut.label_frames`), move them around, and click save again.

````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.check_labels.rst
```
````

### (F) Create Training Dataset

At this point, you'll need to select your neural network type.

For the **PyTorch engine**, please see [the PyTorch Model Architectures](
dlc3-architectures) for options.

For the **TensorFlow engine**, please see Lauer et al. 2021 for options. Multi-animal
models will use `imgaug`, ADAM optimization, our new DLCRNet, and batch training. We
suggest keeping these defaults at this time. At this step, the ImageNet pre-trained
networks (i.e. ResNet-50) weights will be downloaded. If they do not download (you will
see this downloading in the terminal, then you may not have permission to do so (
something we have seen with some Windows users - see the **[
WIKI troubleshooting for more help!](
https://github.com/DeepLabCut/DeepLabCut/wiki/Troubleshooting-Tips)**).

Then run:

```python
deeplabcut.create_training_dataset(config_path)
```

- The set of arguments in the function will shuffle the combined labeled dataset and split it to create train and test
sets. The subdirectory with suffix ``iteration#`` under the directory **training-datasets** stores the dataset and meta
information, where the ``#`` is the value of ``iteration`` variable stored in the project’s configuration file (this number
keeps track of how often the dataset was refined).

- OPTIONAL: If the user wishes to benchmark the performance of the DeepLabCut, they can create multiple
training datasets by specifying an integer value to the `num_shuffles`; see the docstring for more details.

- Each iteration of the creation of a training dataset will create several files, which
is used by the feature detectors, and a ``.pickle`` file that contains the meta
information about the training dataset. This also creates two subdirectories within
**dlc-models-pytorch** (**dlc-models** for the TensorFlow engine) called ``test`` and
``train``, and these each have a configuration file called pose_cfg.yaml. Specifically,
the user can edit the **pytorch_config.yaml** (**pose_cfg.yaml** for TensorFlow engine)
within the **train** subdirectory before starting the training. These configuration
files contain meta information with regard to the parameters of the feature detectors.
Key parameters are listed in Box 2.

**DATA AUGMENTATION:** At this stage you can also decide what type of augmentation to
use. Once you've called `create_training_dataset`, you can edit the 
[**pytorch_config.yaml**](dlc3-pytorch-config) file that was created (or for the
TensorFlow engine, the [**pose_cfg.yaml**](
https://github.com/DeepLabCut/DeepLabCut/blob/master/deeplabcut/pose_cfg.yaml) file).

- PyTorch Engine: [Albumentations](https://albumentations.ai/docs/) is used for data
augmentation. Look at the [**pytorch_config.yaml**](dlc3-pytorch-config) for more 
information about image augmentation options.
- TensorFlow Engine: The default augmentation works well for most tasks (as shown on
www.deeplabcut.org), but there are many options, more data augmentation, intermediate
supervision, etc. Only `imgaug` augmentation is available for multi-animal projects.

[A Primer on Motion Capture with Deep Learning: Principles, Pitfalls, and Perspectives](
https://www.cell.com/neuron/pdf/S0896-6273(20)30717-0.pdf), details the advantage of
augmentation for a worked example (see Fig 8). TL;DR: use imgaug and use the symmetries
of your data!

Importantly, image cropping as previously done with `deeplabcut.cropimagesandlabels` in multi-animal projects
is now part of the augmentation pipeline. In other words, image crops are no longer stored in labeled-data/..._cropped
folders. Crop size still defaults to (400, 400); if your images are very large (e.g. 2k, 4k pixels), consider increasing the crop size, but be aware unless you have a strong GPU (24 GB memory or more), you will hit memory errors. You can lower the batch size, but this may affect performance.

In addition, one can specify a crop sampling strategy: crop centers can either be taken at random over the image (`uniform`) or the annotated keypoints (`keypoints`); with a focus on regions of the scene with high body part density (`density`); last, combining `uniform` and `density` for a `hybrid` balanced strategy (this is the default strategy). Note that both parameters can be easily edited prior to training in the **pose_cfg.yaml** configuration file.
As a reminder, cropping images into smaller patches is a form of data augmentation that simultaneously
allows the use of batch processing even on small GPUs that could not otherwise accommodate larger images + larger batchsizes (this usually increases performance and decreasing training time).

**MODEL COMPARISON**: You can also test several models by creating the same train/test
split for different networks.
You can easily do this in the Project Manager GUI (by selecting the "Use an existing 
data split" option), which also lets you compare PyTorch and TensorFlow models.

````{versionadded} 3.0.0
You can now create new shuffles using the same train/test split as 
existing shuffles with `create_training_dataset_from_existing_split`. This allows you to
compare model performance (between different architectures or when using different
training hyper-parameters) as the shuffles were trained on the same data, and evaluated
on the same test data!

Example usage - creating 3 new shuffles (with indices 10, 11 and 12) for a ResNet 50
pose estimation model, using the same data split as was used for shuffle 0:

```python
deeplabcut.create_training_dataset_from_existing_split(
    config_path,
    from_shuffle=0,
    shuffles=[10, 11, 12],
    net_type="resnet_50",
)
```
````

````{admonition} Click the button to see API Docs for deeplabcut.create_training_dataset
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_training_dataset.rst
```
````

````{admonition} Click the button to see API Docs for deeplabcut.create_training_model_comparison
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_training_model_comparison.rst
```
````

````{admonition} Click the button to see API Docs for deeplabcut.create_training_dataset_from_existing_split
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_training_dataset_from_existing_split.rst
```
````

### (G) Train The Network

```python
deeplabcut.train_network(config_path, shuffle=1)
```

The set of arguments in the function starts training the network for the dataset created
for one specific shuffle. Note that you can change training parameters in the 
[**pytorch_config.yaml**](dlc3-pytorch-config) file (or **pose_cfg.yaml** for TensorFlow
models) of the model that you want to train (before you start training).

At user specified iterations during training checkpoints are stored in the subdirectory 
*train* under the respective iteration & shuffle directory.

````{admonition} Tips on training models with the PyTorch Engine
:class: dropdown

Example parameters that one can call:

```python
deeplabcut.train_network(
    config_path,
    shuffle=1,
    trainingsetindex=0,
    device="cuda:0",
    max_snapshots_to_keep=5,
    displayiters=100,
    save_epochs=5,
    epochs=200,
)
```

Pytorch models in DeepLabCut 3.0 are trained for a set number of epochs, instead of a
maximum number of iterations (which is what was used for TensorFlow models). An epoch
is a single pass through the training dataset, which means your model has seen each
training image exactly once. So if you have 64 training images for your network, an
epoch is 64 iterations with batch size 1 (or 32 iterations with batch size 2, 16 with
batch size 4, etc.).

By default, the pretrained networks are not in the DeepLabCut toolbox (as they can be 
more than 100MB), but they get downloaded automatically before you train.

If the user wishes to restart the training at a specific checkpoint they can specify the
full path of the checkpoint to the variable ``resume_training_from`` in the [
**pytorch_config.yaml**](
dlc3-pytorch-config) file (checkout the "Restarting Training at a Specific Checkpoint"
section of the docs) under the *train* subdirectory.

**CRITICAL POINT:** It is recommended to train the networks **until the loss plateaus** 
(depending on the dataset, model architecture and training hyper-parameters this happens
after 100 to 250 epochs of training).

The variables ``display_iters`` and ``save_epochs`` in the [**pytorch_config.yaml**](
dlc3-pytorch-config) file allows the user to alter how often the loss is displayed
and how often the weights are stored. We suggest saving every 5 to 25 epochs.
````

````{admonition} Tips on training models with the TensorFlow Engine 
:class: dropdown

Example parameters that one can call:

```python
deeplabcut.train_network(
    config_path,
    shuffle=1,
    trainingsetindex=0,
    gputouse=None,
    max_snapshots_to_keep=5,
    autotune=False,
    displayiters=100,
    saveiters=15000,
    maxiters=30000,
    allow_growth=True,
)
```

By default, the pretrained networks are not in the DeepLabCut toolbox (as they are 
around 100MB each), but they get downloaded before you train. However, if not previously
downloaded from the TensorFlow model weights, it will be downloaded and stored in a
subdirectory *pre-trained* under the subdirectory *models* in 
*Pose_Estimation_Tensorflow*. At user specified iterations during training checkpoints
are stored in the subdirectory *train* under the respective iteration directory.

If the user wishes to restart the training at a specific checkpoint they can specify the
full path of the checkpoint to the variable ``init_weights`` in the **pose_cfg.yaml**
file under the *train* subdirectory (see Box 2).

**CRITICAL POINT:** It is recommended to train the networks for thousands of iterations
until the loss plateaus (typically around **500,000**) if you use batch size 1, and
**50-100K** if you use batchsize 8 (the default).

If you use **maDeepLabCut** the recommended training iterations is **20K-100K** 
(it automatically stops at 200K!), as we use Adam and batchsize 8; if you have to reduce
 the batchsize for memory reasons then the number of iterations needs to be increased.

The variables ``display_iters`` and ``save_iters`` in the **pose_cfg.yaml** file allows
the user to alter how often the loss is displayed and how often the weights are stored.

**maDeepLabCut CRITICAL POINT:** For multi-animal projects we are using not only
different and new output layers, but also new data augmentation, optimization, learning
rates, and batch training defaults. Thus, please use a lower ``save_iters`` and
``maxiters``. I.e. we suggest saving every 10K-15K iterations, and only training until
50K-100K iterations. We recommend you look closely at the loss to not overfit on your
data. The bonus, training time is much less!!!
````

````{admonition} Click the button to see API Docs for train_network
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.train_network.rst
```
````

### (H) Evaluate the Trained Network

It is important to evaluate the performance of the trained network. This performance is 
measured by computing two metrics:

- **Average root mean square error** (RMSE) between the manual labels and the ones
predicted by your trained DeepLabCut model. The RMSE is proportional to the mean average
Euclidean error (MAE) between the manual labels and the ones predicted by DeepLabCut. 
The MAE is displayed for all pairs and only likely pairs (>p-cutoff). This helps to
exclude, for example, occluded body parts. One of the strengths of DeepLabCut is that
due to the probabilistic output of the scoremap, it can, if sufficiently trained, also 
reliably report if a body part is visible in a given frame. (see discussions of finger
tips in reaching and the Drosophila legs during 3D behavior in [Mathis et al, 2018]).
- **Mean Average Precision** (mAP) and **Mean Average Recall** (mAR) for the individuals
predicted by your trained DeepLabCut model. This metric describes the precision of your
model, based on a considered definition of what a correct detection of an individual is.
It isn't as useful for single-animal models, as RMSE does a great job of evaluating your
model in that case.

```{admonition} A more detailed description of mAP and mAR
:class: dropdown

For multi-animal pose estimation, multiple predictions can be made for each image.
We want to get some idea of the proportion of correct predictions among all predictions
that are made.
However, the notion of "correct prediction" for pose estimation is not straightforward:
is a prediction correct if all predicted keypoints are within 5 pixels of the ground
truth? Within 2 pixels of the ground truth? What if all pixels but one match the ground
truth perfectly, but the wrong prediction is 50 pixels away? Mean average precision (
and mean average recall) estimate the precision/recall of your models by setting 
different "thresholds of correctness" and averaging results. How "correct" a
prediction is can be evaluated through [object-keypoint similarity](
https://cocodataset.org/#keypoints-eval).

A good resource to get a deeper understanding of mAP is the [Stanford CS230 course](
https://cs230.stanford.edu/section/8/#object-detection-iou-ap-and-map). While it 
describes mAP for object detection (where bounding boxes are predicted instead of 
keypoints), the same metric can be computed for pose estimation, where similarity 
between predictions and ground truth is computed through [object-keypoint similarity](
https://cocodataset.org/#keypoints-eval) instead of intersection-over-union (IoU). 
```

It's also important to visually inspect predictions on individual frames to assess the
performance of your model. You can do this by setting `plotting=True` when you call
`evaluate_network`. The evaluation results are computed by typing:

```python
deeplabcut.evaluate_network(config_path, Shuffles=[1], plotting=True)
```

🎥 [VIDEO TUTORIAL AVAILABLE!](https://www.youtube.com/watch?v=bgfnz1wtlpo)

Setting ``plotting`` to True plots all the testing and training frames with the manual and predicted labels; these will
be colored by body part type by default. They can alternatively be colored by individual by passing `plotting="individual"`.
The user should visually check the labeled test (and training) images that are created in the ‘evaluation-results’ directory.
Ideally, DeepLabCut labeled unseen (test images) according to the user’s required accuracy, and the average train
and test errors are comparable (good generalization). What (numerically) comprises an acceptable MAE depends on
many factors (including the size of the tracked body parts, the labeling variability, etc.). Note that the test error can
also be larger than the training error due to human variability (in labeling, see Figure 2 in Mathis et al, Nature Neuroscience 2018).

The plots can be customized by editing the **config.yaml** file (i.e., the colormap, scale, marker size (dotsize), and
transparency of labels (alpha-value) can be modified). By default each body part is plotted in a different color
(governed by the colormap) and the plot labels indicate their source. Note that by default the human labels are
plotted as plus (‘+’), DeepLabCut’s predictions either as ‘.’ (for confident predictions with likelihood > `pcutoff`) and
’x’ for (likelihood <= `pcutoff`).

The evaluation results for each shuffle of the training dataset are stored in a unique
subdirectory in a newly created directory ‘evaluation-results-pytorch’ (or 
‘evaluation-results’ for TensorFlow models) in the project directory.
The user can visually inspect if the distance between the labeled and the predicted body
parts are acceptable. In the event of benchmarking with different shuffles of same training
dataset, the user can provide multiple shuffle indices to evaluate the corresponding 
network. If the generalization is not sufficient, the user might want to:

• check if the labels were imported correctly; i.e., invisible points are not labeled
and the points of interest are labeled accurately

• make sure that the loss has already converged

• consider labeling additional images and make another iteration of the training data set

````{admonition} Click the button to see API Docs for evaluate_network
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.evaluate_network.rst
```
````

**maDeepLabCut: (or on normal projects!)**

In multi-animal projects, model evaluation is crucial as this is when
the data-driven selection of the optimal skeleton is carried out. Skipping that step
causes video analysis to use the redundant skeleton by default, which is not only slow
but does not guarantee best performance.

You should also plot the scoremaps, locref layers, and PAFs to assess performance:

```python
deeplabcut.extract_save_all_maps(config_path, shuffle=shuffle, Indices=[0, 5])
```

You can drop "Indices" to run this on all training/testing images (this is very slow!)

### (I) Analyze new Videos

**-------------------- DECISION POINT -------------------**

**ATTENTION!**
**Pose estimation and tracking should be thought of as separate steps.** If you do not have good pose estimation evaluation metrics at this point, stop, check original labels, add more data, etc --> don't move forward with this model. If you think you have a good model, please test the "raw" pose estimation performance on a video to validate performance:

Please run:

```python
videos_to_analyze = ['/fullpath/project/videos/testVideo.mp4']
scorername = deeplabcut.analyze_videos(config_path, videos_to_analyze, videotype='.mp4')
deeplabcut.create_video_with_all_detections(config_path, videos_to_analyze, videotype='.mp4')
```

Please note that you do **not** get the .h5/csv file you might be used to getting (this
comes after tracking). You will get a `pickle` file that is used in
`create_video_with_all_detections`.

For models predicting part-affinity fields, another sanity check may be to 
examine the distributions of edge affinity costs using `deeplabcut.utils.plot_edge_affinity_distributions`. Easily separable distributions
indicate that the model has learned strong links to group keypoints into distinct
individuals — likely a necessary feature for the assembly stage (note that the amount of
overlap will also depend on the amount of interactions between your animals in the
dataset). All TensorFlow multi-animal models use part-affinity fields and PyTorch models
consisting of just a backbone name (e.g. `resnet_50`, `resnet_101`) use part-affinity
fields. If you're unsure whether your PyTorch model has a one, check 
the **pytorch_config.yaml** for a `DLCRNetHead`.

IF you have good clean out video, ending in `....full.mp4` (and the evaluation metrics
look good, scoremaps look good, plotted evaluation images, and affinity distributions
are far apart for most edges), then go forward!!!

If this does not look good, we recommend extracting and labeling more frames (even from more videos). Try to label close interactions of animals for best performance. Once you label more, you can create a new training set and train.

You can either:
1. extract more frames manually from existing or new videos and label as when initially building the training data set, or
2. let DeepLabCut find frames where keypoints were poorly detected and automatically extract those for you. All you need is
to run:

```python
deeplabcut.find_outliers_in_raw_data(config_path, pickle_file, video_file)
```

where pickle_file is the `_full.pickle` one obtains after video analysis.
Flagged frames will be added to your collection of images in the corresponding labeled-data folders for you to label.


### Animal Assembly and Tracking across frames

After pose estimation, now you perform assembly and tracking.

````{versionadded} v2.2.0
*NEW* in 2.2 is a novel data-driven way to set the optimal skeleton and assembly
metrics, so this no longer requires user input. The metrics, in case you do want to edit
them, can be found in the `inference_cfg.yaml` file.
````

### Optimized Animal Assembly + Video Analysis:
Please note that **novel videos DO NOT need to be added to the config.yaml file**. You
can simply have a folder elsewhere on your computer and pass the video folder (then it
will analyze all videos of the specified type (i.e. ``videotype='.mp4'``), or pass the
path to the **folder** or exact video(s) you wish to analyze:

```python
deeplabcut.analyze_videos(config_path, ['/fullpath/project/videos/'], videotype='.mp4', auto_track=True)
```

#### IF auto_track = True:

```{versionadded} v2.2.0.3
A new argument `auto_track=True`, was added to `deeplabcut.analyze_videos` chaining pose
estimation, tracking, and stitching in a single function call with defaults we found to
work well. Thus, you'll now get the `.h5` file you might be used to getting in standard
DLC. If `auto_track=False`, one must run `convert_detections2tracklets` and
`stitch_tracklets` manually (see below), granting more control over the last steps of
the workflow (ideal for advanced users).
```

#### IF auto_track = False:

You can validate the tracking parameters. Namely, you can iteratively change the
parameters, run `convert_detections2tracklets` then load them in the GUI 
(`refine_tracklets`) if you want to look at the performance. If you want to edit these,
you will need to open the `inference_cfg.yaml` file (or click button in GUI). The
options are:

```python
# Tracking:
#p/m pixels in width and height for increasing bounding boxes.
boundingboxslack : 0
# Intersection over Union (IoU) threshold for linking two bounding boxes
iou_threshold: .2
# maximum duration of a lost tracklet before it's considered a "new animal" (in frames)
max_age: 100
# minimum number of consecutive frames before a detection is tracked
min_hits: 3
```

  - **IMPORTANT POINT FOR SUPERVISED IDENTITY TRACKING**

    If the network has been trained to learn the animals' identities (i.e., you set `identity=True` in config.yaml before training) this information can be leveraged both during: (i) animal assembly, where body parts are grouped based on the animal they are predicted to belong to (affinity between pairs of keypoints is no longer considered in that case); and (ii) animal tracking, where identity only can be utilized in place of motion trackers to form tracklets.

To use this ID information, simply pass:
```python
deeplabcut.convert_detections2tracklets(..., identity_only=True)
```

- **Note:** If only one individual is to be assembled and tracked, assembly and tracking are skipped, and detections are treated as in single-animal projects; i.e., it is the keypoints with highest confidence that are kept and accumulated over frames to form a single, long tracklet. No action is required from users, this is done automatically.


**Animal assembly and tracking quality** can be assessed via `deeplabcut.utils.make_labeled_video.create_video_from_pickled_tracks`. This function provides an additional diagnostic tool before moving on to refining tracklets.


If animal assemblies do not look pretty, an alternative to the outlier search described above is to pass the
`_assemblies.pickle` to `find_outliers_in_raw_data` in place of the `_full.pickle`.
This will focus the outlier search on unusual assemblies (i.e., animal skeletons that were oddly reconstructed). This may be a bit more sensitive with crowded scenes or frames where animals interact closely.
Note though that at that stage it is likely preferable anyway to carry on with the remaining steps, and extract outliers
from the final h5 file as was customary in single animal projects.


**Next, tracklets are stitched to form complete tracks with:

```python
deeplabcut.stitch_tracklets(
    config_path,
    ['videofile_path'],
    videotype='mp4',
    shuffle=1,
    trainingsetindex=0,
)
```

Note that the base signature of the function is identical to `analyze_videos` and `convert_detections2tracklets`.
If the number of tracks to reconstruct is different from the number of individuals
originally defined in the config.yaml, `n_tracks` (i.e., the number of animals you have in your video)
can be directly specified as follows:

```python
deeplabcut.stitch_tracklets(..., n_tracks=n)
```

In such cases, file columns will default to dummy animal names (ind1, ind2, ..., up to indn).

#### API Docs

````{admonition} Click the button to see API Docs for analyze_videos
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.analyze_videos.rst
```
````

````{admonition} Click the button to see API Docs for convert_detections2tracklets
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.convert_detections2tracklets.rst
```
````

````{admonition} Click the button to see API Docs for stitch_tracklets
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.stitch_tracklets.rst
```
````

### Using Unsupervised Identity Tracking:

In Lauer et al. 2022 we introduced a new method to do unsupervised reID of animals.
Here, you can use the tracklets to learn the identity of animals to enhance your
tracking performance. To use the code:

```python
deeplabcut.transformer_reID(config, videos_to_analyze, n_tracks=None, videotype="mp4")
```

Note you should pass the n_tracks (number of animals) you expect to see in the video.

### Refine Tracklets:

You can also optionally **refine the tracklets**. You can fix both "major" ID swaps, i.e. perhaps when animals cross, and you can micro-refine the individual body points. You will load the `...trackertype.pickle` or `.h5'` file that was created above, and then you can launch a GUI to interactively refine the data. This also has several options, so please check out the docstring. Upon saving the refined tracks you get an `.h5` file (akin to what you might be used to from standard DLC. You can also load (1) filter this to take care of small jitters, and (2) load this `.h5` this to refine (again) in case you find another issue, etc!

```python
deeplabcut.refine_tracklets(config_path, pickle_or_h5_file, videofile_path, max_gap=0, min_swap_len=2, min_tracklet_len=2, trail_len=50)
```

If you use the GUI (or otherwise), here are some settings to consider:

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1619628014395-BQ09VLLTKCLQQGRB5T9A/ke17ZwdGBToddI8pDm48kLMj_XrWI9gi4tVeBdgcB8p7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0lt53wR20brczws2A6XSGt3kSTbW7uM0ncVKHWPvgHR4kN5Ka1TcK96ljy4ji9jPkQ/TrackletGUI.png?format=1000w" width="950" title="maDLCtrack" alt="maDLC" align="center" vspace = "50">

*note, setting `max_gap=0` can be used to fill in all frames across the video; otherwise, 1-n is the # of frames you want to fill in, i.e. maybe you want to fill in short gaps of 5 frames, but 15 frames indicates another issue, etc. You can test this in the GUI very easy by editing the value and then re-launch pop-up GUI.

If you fill in gaps, they will be associated to an ultra low probability, 0.01, so you are aware this is not the networks best estimate, this is the human-override! Thus, if you create a video, you need to set your pcutoff to 0 if you want to see these filled in frames.

[Read more here!](functionDetails.md#madeeplabcut-critical-point---assemble--refine-tracklets)

Short demo:
 <p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588690928000-90ZMRIM8SN6QE20ZOMNX/ke17ZwdGBToddI8pDm48kJ1oJoOIxBAgRD2ClXVCmKFZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpxBw7VlGKDQO2xTcc51Yv6DahHgScLwHgvMZoEtbzk_9vMJY_JknNFgVzVQ2g0FD_s/refineDEMO.gif?format=750w" width="70%">
</p>

### (J) Filter Pose Data

Firstly, Here are some tips for scaling up your video analysis, including looping over many folders for batch processing: https://github.com/DeepLabCut/DeepLabCut/wiki/Batch-Processing-your-Analysis

You can also filter the predicted bodyparts by:
```python
deeplabcut.filterpredictions(config_path,['/fullpath/project/videos/reachingvideo1.avi'])
```
Note, this creates a file with the ending filtered.h5 that you can use for further analysis. This filtering step has many parameters, so please see the full docstring by typing: ``deeplabcut.filterpredictions?``

````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.filterpredictions.rst
```
````

### (K) Plot Trajectories , (L) Create Labeled Videos

- **NOTE :bulb::mega::** Before you create a video, you should set what threshold to use for plotting. This is set in the `config.yaml` file as `pcutoff` - if you have a well trained network, this should be high, i.e. set it to `0.8` or higher! IF YOU FILLED IN GAPS, you need to set this to `0` to "see" the filled in parts.


- You can also determine a good `pcutoff` value by looking at the likelihood plot created during `plot_trajectories`:

Plot the outputs:
```python
  deeplabcut.plot_trajectories(config_path,['/fullpath/project/videos/reachingvideo1.avi'],filtered = True)
```

Create videos:
```python
  deeplabcut.create_labeled_video(config_path, [videos], videotype='avi', shuffle=1, trainingsetindex=0, filtered=False, fastmode=True, save_frames=False, keypoints_only=False, Frames2plot=None, displayedbodyparts='all', displayedindividuals='all', codec='mp4v', outputframerate=None, destfolder=None, draw_skeleton=False, trailpoints=0, displaycropped=False, color_by='bodypart', track_method='')
```
- **NOTE :bulb::mega::** You have a lot of options in terms of video plotting (quality, display type, etc). We recommend checking the docstring!

(more details [here](functionDetails.md#i-video-analysis-and-plotting-results))

````{admonition} Click the button to see API Docs for plot_trajectories
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.plot_trajectories.rst
```
````

````{admonition} Click the button to see API Docs for create_labeled_video
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_labeled_video.rst
```
````

### HELP:

In ipython/Jupyter notebook:

```
deeplabcut.nameofthefunction?
```

In python or pythonw:

```
help(deeplabcut.nameofthefunction)
```

## Tips for "daily" use:

<p align="center">
<img src= https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5ccc5abe0d9297405a428522/1556896461304/howtouseDLC-01.png?format=1000w width="80%">
 </p>

You can always exit an conda environment and easily jump back into a project by simply:

Linux/MacOS formatting example:
```
source activate yourdeeplabcutEnvName
ipython or pythonw
import deeplabcut
config_path ='/home/yourprojectfolder/config.yaml'
```
Windows formatting example:
```
activate yourdeeplabcutEnvName
ipython
import deeplabcut
config_path = r'C:\home\yourprojectfolder\config.yaml'
```

Now, you can run any of the functions described in this documentation.

# Getting help with maDLC:

- If you have a detailed question about how to use the code, or you hit errors that are not "bugs" but you want code assistance, please post on the [![Image.sc forum](https://img.shields.io/badge/dynamic/json.svg?label=forum&amp;url=https%3A%2F%2Fforum.image.sc%2Ftags%2Fdeeplabcut.json&amp;query=%24.topic_list.tags.0.topic_count&amp;colorB=brightgreen&amp;&amp;suffix=%20topics&amp;logo=data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA4AAAAOCAYAAAAfSC3RAAABPklEQVR42m3SyyqFURTA8Y2BER0TDyExZ+aSPIKUlPIITFzKeQWXwhBlQrmFgUzMMFLKZeguBu5y+//17dP3nc5vuPdee6299gohUYYaDGOyyACq4JmQVoFujOMR77hNfOAGM+hBOQqB9TjHD36xhAa04RCuuXeKOvwHVWIKL9jCK2bRiV284QgL8MwEjAneeo9VNOEaBhzALGtoRy02cIcWhE34jj5YxgW+E5Z4iTPkMYpPLCNY3hdOYEfNbKYdmNngZ1jyEzw7h7AIb3fRTQ95OAZ6yQpGYHMMtOTgouktYwxuXsHgWLLl+4x++Kx1FJrjLTagA77bTPvYgw1rRqY56e+w7GNYsqX6JfPwi7aR+Y5SA+BXtKIRfkfJAYgj14tpOF6+I46c4/cAM3UhM3JxyKsxiOIhH0IO6SH/A1Kb1WBeUjbkAAAAAElFTkSuQmCC)](https://forum.image.sc/tags/deeplabcut)

- If you have a quick, short question that fits a "chat" format:
[![Gitter](https://badges.gitter.im/DeepLabCut/community.svg)](https://gitter.im/DeepLabCut/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)

- If you want to share some results, or see others:
[![Twitter Follow](https://img.shields.io/twitter/follow/DeepLabCut.svg?label=DeepLabCut&style=social)](https://twitter.com/DeepLabCut)

- If you have a code bug report, please create an issue and show the minimal code to reproduce the error: https://github.com/DeepLabCut/DeepLabCut/issues

- if you are looking for resources to increase your understanding of the software and general guidelines, we have an open source, free course: http://DLCcourse.deeplabcut.org.

**Please note:** what we cannot do is provided support or help designing your experiments and data analysis. The number of requests for this is too great to sustain in our inbox. We are happy to answer such questions in the forum as a community, in a scalable way. We hope and believe we have given enough tools and resources to get started and to accelerate your research program, and this is backed by the >700 citations using DLC, 2 clinical trials by others, and countless applications. Thus, we believe this code works, is accessible, and with limited programming knowledge can be used. Please read our [Missions & Values statement](mission-and-values) to learn more about what we DO hope to provide you.


--- File: docs/standardDeepLabCut_UserGuide.md ---
(single-animal-userguide)=
# DeepLabCut User Guide (for single animal projects)

This document covers single/standard DeepLabCut use. If you have a complicated multi-animal scenario (i.e., they look
the same), then please see our [maDLC user guide](multi-animal-userguide).

To get started, you can use the GUI, or the terminal. See below.

## DeepLabCut Project Manager GUI (recommended for beginners)



**GUI:**

To begin, navigate to Anaconda Prompt Terminal and right-click to "open as admin "(Windows), or simply launch
"Terminal" (unix/MacOS) on your computer. We assume you have DeepLabCut installed (if not, see
[install docs](how-to-install)!). Next, launch your conda env (i.e., for example `conda activate DEEPLABCUT`). Then,
simply run `python -m deeplabcut`. The below functions are available to you in an easy-to-use graphical user interface.
While most functionality is available, advanced users might want the additional flexibility that command line interface
offers. Read more below.
```{Hint}
🚨 If you use Windows, please always open the terminal with administrator privileges! Right click, and "run as administrator".
```

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572824438905-QY9XQKZ8LAJZG6BLPWOQ/ke17ZwdGBToddI8pDm48kIIa76w436aRzIF_cdFnEbEUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcLthF_aOEGVRewCT7qiippiAuU5PSJ9SSYal26FEts0MmqyMIhpMOn8vJAUvOV4MI/guilaunch.jpg?format=1000w" width="60%">
</p>

As a reminder, the core functions are described in our
[Nature Protocols paper](https://www.nature.com/articles/s41596-019-0176-0) (published at the time of 2.0.6).
Additional functions and features are continually added to the package. Thus, we recommend you read over the protocol
and then please look at the following documentation and the doctrings. Thanks for using DeepLabCut!

## DeepLabCut in the Terminal/Command line interface:

To begin, navigate to Anaconda Prompt Terminal and right-click to "open as admin "(Windows), or simply launch
"Terminal" (unix/MacOS) on your computer. We assume you have DeepLabCut installed (if not, see Install docs!). Next,
launch your conda env (i.e., for example `conda activate DEEPLABCUT`) and then type `ipython`. Then type:
```python
import deeplabcut
```

```{Hint}
🚨 If you use Windows, please always open the terminal with administrator privileges! Right click, and "run as administrator".
```

### (A) Create a New Project

The function `create_new_project` creates a new project directory, required subdirectories, and a basic project
configuration file. Each project is identified by the name of the project (e.g. Reaching), name of the experimenter
(e.g. YourName), as well as the date at creation.

Thus, this function requires the user to input the name of the project, the name of the experimenter, and the full
path of the videos that are (initially) used to create the training dataset.

Optional arguments specify the working directory, where the project directory will be created, and if the user wants
to copy the videos (to the project directory). If the optional argument `working_directory` is unspecified, the
project directory is created in the current working directory, and if `copy_videos` is unspecified symbolic links
for the videos are created in the videos directory. Each symbolic link creates a reference to a video and thus
eliminates the need to copy the entire video to the video directory (if the videos remain at the original location).

```python
deeplabcut.create_new_project(
    "Name of the project",
    "Name of the experimenter",
    ["Full path of video 1", "Full path of video2", "Full path of video3"],
    working_directory="Full path of the working directory",
    copy_videos=True/False,
    multianimal=False
)
```

**Important path formatting note**

Windows users, you must input paths as: `r'C:\Users\computername\Videos\reachingvideo1.avi'` or
` 'C:\\Users\\computername\\Videos\\reachingvideo1.avi'`

TIP: you can also place `config_path` in front of `deeplabcut.create_new_project` to create a variable that holds
the path to the config.yaml file, i.e. `config_path=deeplabcut.create_new_project(...)`

This set of arguments will create a project directory with the name
**<Name of the project>+<name of the experimenter>+<date of creation of the project>** in the **Working directory** and
creates the symbolic links to videos in the **videos** directory. The project directory will have subdirectories:
**dlc-models**, **dlc-models-pytorch**, **labeled-data**, **training-datasets**, and **videos**.  All the outputs
generated during the course of a project will be stored in one of these subdirectories, thus allowing each project to be
curated in separation from other projects. The purpose of the subdirectories is as follows:

**dlc-models** and **dlc-models-pytorch** have a similar structure; the first contains files for the TensorFlow engine
while the second contains files for the PyTorch engine. At the top level in these directories, there are directories
referring to different iterations of label refinement (see below): **iteration-0**, **iteration-1**, etc.
The iteration directories store shuffle directories, where each shuffle directory stores model data related to a
particular experiment: trained and tested on a particular training and testing sets, and with a particular model
architecture. Each shuffle directory contains the subdirectories *test* and *train*, each of which holds the meta
information with regard to the parameters of the feature detectors in configuration files. The configuration files are
YAML files, a common human-readable data serialization language. These files can be opened and edited with standard text
editors. The subdirectory *train* will store checkpoints (called snapshots) during training of the model. These
snapshots allow the user to reload the trained model without re-training it, or to pick-up training from a particular
saved checkpoint, in case the training was interrupted.

**labeled-data:** This directory will store the frames used to create the training dataset. Frames from different videos
are stored in separate subdirectories. Each frame has a filename related to the temporal index within the corresponding
video, which allows the user to trace every frame back to its origin.

**training-datasets:**  This directory will contain the training dataset used to train the network and metadata, which
contains information about how the training dataset was created.

**videos:** Directory of video links or videos. When **copy\_videos** is set to `False`, this directory contains
symbolic links to the videos. If it is set to `True` then the videos will be copied to this directory. The default is
`False`. Additionally, if the user wants to add new videos to the project at any stage, the function
**add\_new\_videos** can be used. This will update the list of videos in the project's configuration file.

```python
deeplabcut.add_new_videos(
    "Full path of the project configuration file*",
    ["full path of video 4", "full path of video 5"],
    copy_videos=True/False
)
```

*Please note, *Full path of the project configuration file* will be referenced as `config_path` throughout this
protocol.

The project directory also contains the main configuration file called *config.yaml*. The *config.yaml* file contains
many important parameters of the project. A complete list of parameters including their description can be found in
Box1.

The `create_new_project` step writes the following parameters to the configuration file: *Task*, *scorer*, *date*,
*project\_path* as well as a list of videos *video\_sets*. The first three parameters should **not** be changed. The
list of videos can be changed by adding new videos or manually removing videos.

![Box 1 - Single Animal Project Configuration File Glossary](images/box1-single.png)

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_new_project.rst
```
````

### (B) Configure the Project

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588892210304-EW7WD46PYAU43WWZS4QZ/ke17ZwdGBToddI8pDm48kAXtGtTuS2U1SVcl-tYMBOAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8PaoYXhp6HxIwZIk7-Mi3Tsic-L2IOPH3Dwrhl-Ne3Z2YjE9w60pqfeJxDohDRZk1jXSVCSSfcEA7WmgMAGpjTehHAH51QaxKq4KdVMVBxpG/1nktc1kdgq2.jpg?format=1000w" width="175" title="colormaps" alt="DLC Utils" align="right" vspace = "50">

Next, open the **config.yaml** file, which was created during  **create\_new\_project**. You can edit this file in any
text editor.  Familiarize yourself with the meaning of the parameters (Box 1). You can edit various parameters, in
particular you **must add the list of *bodyparts* (or points of interest)** that you want to track. You can also set the
*colormap* here that is used for all downstream steps (can also be edited at anytime), like labeling GUIs, videos, etc.
Here any [matplotlib colormaps](https://matplotlib.org/tutorials/colors/colormaps.html) will do!
Please DO NOT have spaces in the names of bodyparts.

**bodyparts:** are the bodyparts of each individual (in the above list).


 ### (C) Select Frames to Label

**CRITICAL:** A good training dataset should consist of a sufficient number of frames that capture the breadth of the
behavior. This ideally implies to select the frames from different (behavioral) sessions, different lighting and
different animals, if those vary substantially (to train an invariant, robust feature detector). Thus for creating a
robust network that you can reuse in the laboratory, a good training dataset should reflect the diversity of the
behavior with respect to postures, luminance conditions, background conditions, animal identities,etc. of the data that
will be analyzed. For the simple lab behaviors comprising mouse reaching, open-field behavior and fly behavior, 100−200
frames gave good results [Mathis et al, 2018](https://www.nature.com/articles/s41593-018-0209-y). However, depending on
the required accuracy, the nature of behavior, the video quality (e.g. motion blur, bad lighting) and the context, more
or less frames might be necessary to create a good network. Ultimately, in order to scale up the analysis to large
collections of videos with perhaps unexpected conditions, one can also refine the data set in an adaptive way (see
refinement below).

The function `extract_frames` extracts frames from all the videos in the project configuration file in order to create
a training dataset. The extracted frames from all the videos are stored in a separate subdirectory named after the video
file’s name under the ‘labeled-data’. This function also has various parameters that might be useful based on the user’s
need.
```python
deeplabcut.extract_frames(
    config_path,
    mode="automatic/manual",
    algo="uniform/kmeans",
    crop=True/False,
    userfeedback=False
)
```
**CRITICAL POINT:** It is advisable to keep the frame size small, as large frames increase the training and
inference time. The cropping parameters for each video can be provided in the config.yaml file (and see below).
When running the function extract_frames, if the parameter crop=True, then you will be asked to draw a box within the
GUI (and this is written to the config.yaml file).

`userfeedback` allows the user to specify which videos they wish to extract frames from. When set to `"True"`, a dialog
will be initiated, where the user is asked for each video if (additional/any) frames from this video should be
extracted. Use this, e.g. if you have already labeled some folders and want to extract data for new videos.

The provided function either selects frames from the videos that are randomly sampled from a uniform distribution
(uniform), by clustering based on visual appearance (k-means), or by manual selection. Random uniform selection of
frames works best for behaviors where the postures vary across the whole video. However, some behaviors might be sparse,
as in the case of reaching where the reach and pull are very fast and the mouse is not moving much between trials. In
such a case, the function that allows selecting frames based on k-means derived quantization would be useful. If the
user chooses to use k-means as a method to cluster the frames, then this function downsamples the video and clusters the
frames using k-means, where each frame is treated as a vector. Frames from different clusters are then selected. This
procedure makes sure that the frames look different. However, on large and long videos, this code is slow due to
computational complexity.

**CRITICAL POINT:** It is advisable to extract frames from a period of the video that contains interesting
behaviors, and not extract the frames across the whole video. This can be achieved by using the start and stop
parameters in the config.yaml file. Also, the user can change the number of frames to extract from each video using
the numframes2extract in the config.yaml file.

However, picking frames is highly dependent on the data and the behavior being studied. Therefore, it is hard to
provide all purpose code that extracts frames to create a good training dataset for every behavior and animal. If the
user feels specific frames are lacking, they can extract hand selected frames of interest using the interactive GUI
provided along with the toolbox. This can be launched by using:
```python
deeplabcut.extract_frames(config_path, "manual")
```
The user can use the *Load Video* button to load one of the videos in the project configuration file, use the scroll
bar to navigate across the video and *Grab a Frame* (or a range of frames, as of version 2.0.5) to extract the frame(s).
The user can also look at the extracted frames and e.g. delete frames (from the directory) that are too similar before
reloading the set and then manually annotating them.

<p align="center">
<img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c71bfbc71c10b4a23d20567/1550958540700/cropMANUAL.gif?format=750w" width="70%">
</p>

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.extract_frames.rst
```
````

### (D) Label Frames

The toolbox provides a function **label_frames** which helps the user to easily label
all the extracted frames using an interactive graphical user interface (GUI). The user
should have already named the bodyparts to label (points of interest) in the
project’s configuration file by providing a list. The following command invokes the
napari-deeplabcut labelling GUI. Checkout the [napari-deeplabcut docs](napari-gui) for
more information about the labelling workflow.

```python
deeplabcut.label_frames(config_path)
```

[🎥 DEMO](https://youtu.be/hsA9IB5r73E)

HOT KEYS IN THE Labeling GUI (also see "help" in GUI):

```
Ctrl + C: Copy labels from previous frame.
Keyboard arrows: advance frames.
Delete key: delete label.
```

![hot keys](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/192345a5-e411-4d56-b718-ef52f91e195e/Qwerty.png?format=2500w)

**CRITICAL POINT:** It is advisable to **consistently label similar spots** (e.g., on a wrist that is very large, try
to label the same location). In general, invisible or occluded points should not be labeled by the user. They can
simply be skipped by not applying the label anywhere on the frame.

OPTIONAL: In the event of adding more labels to the existing labeled dataset, the user need to append the new
labels to the bodyparts in the config.yaml file. Thereafter, the user can call the function **label_frames**. As of
2.0.5+: then a box will pop up and ask the user if they wish to display all parts, or only add in the new labels.
Saving the labels after all the images are labelled will append the new labels to the existing labeled dataset.

For more information, checkout the [napari-deeplabcut docs](napari-gui) for 
more information about the labelling workflow.

### (E) Check Annotated Frames

OPTIONAL: Checking if the labels were created and stored correctly is beneficial for training, since labeling
is one of the most critical parts for creating the training dataset. The DeepLabCut toolbox provides a function
‘check_labels’ to do so. It is used as follows:
```python
deeplabcut.check_labels(config_path, visualizeindividuals=True/False)
 ```

For each video directory in labeled-data this function creates a subdirectory with **labeled** as a suffix. Those
directories contain the frames plotted with the annotated body parts. The user can double check if the body parts are
labeled correctly. If they are not correct, the user can reload the frames (i.e. `deeplabcut.label_frames`), move them
around, and click save again.

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.check_labels.rst
```
````

(create-training-dataset)=
### (F) Create Training Dataset

**CRITICAL POINT:** Only run this step **where** you are going to train the network. If you label on your laptop but
move your project folder to Google Colab or AWS, lab server, etc, then run the step below on that platform! If you
labeled on a Windows machine but train on Linux, this is fine as of 2.0.4 onwards it will be done automatically (it
saves file sets as both Linux and Windows for you).

- If you move your project folder, you must only change the `project_path` (which is done automatically) in the main
config.yaml file - that's it - no need to change the video paths, etc! Your project is fully portable.

- Be aware you select your neural network backbone at this stage. As of DLC3+ we support PyTorch (and TensorFlow, but
this will be phased out). 

**OVERVIEW:** This function combines the labeled datasets from all the videos and splits them to create train and test
datasets. The training data will be used to train the network, while the test data set will be used for evaluating the
network.

```python
deeplabcut.create_training_dataset(config_path)
```

- OPTIONAL: If the user wishes to benchmark the performance of the DeepLabCut, they can create multiple training
datasets by specifying an integer value to the `num_shuffles`; see the docstring for more details.

The function creates a new shuffle(s) directory in the **dlc-models-pytorch** directory
(**dlc-models** if using Tensorflow), in the current "iteration" directory.
The `train` and `test` directories each have a configuration file
(**pytorch_config.yaml** in **train** and **pose_cfg.yaml** in **test** for Pytorch models,
**pose_cfg.yaml** in **train** and **test** for Tensorflow models).
Specifically, the user can edit the **pytorch_config.yaml** (or **pose_cfg.yaml**) within the **train** subdirectory
before starting the training. These configuration files contain meta information with regard to the parameters
of the feature detectors. For more information about the **pytorch_config.yaml** file, see [here](dlc3-pytorch-config)
(for TensorFlow-based models, see key parameters
[here](https://github.com/DeepLabCut/DeepLabCut/blob/main/deeplabcut/pose_cfg.yaml)).

**CRITICAL POINT:** At this step, for **create_training_dataset** you select the network you want to use, and any
additional data augmentation (beyond our defaults). You can set `net_type`, `detector_type` (if using a detector)
and `augmenter_type` when you call the function.

- Networks: ImageNet pre-trained networks OR SuperAnimal pre-trained networks weights will be downloaded, as you
select. You can decide to do transfer-learning (recommended) or "fine-tune" both the backbone and the decoder head. We
suggest seeing our [dedicated documentation on models](dlc3-architectures) for more information (
or the [this page on selecting models](what-neural-network-should-i-use) for the TensorFlow engine).

```{Hint}
🚨 If they do not download (you will see this downloading in the terminal), then you may not have permission to do
so - be sure to open your terminal "as an admin" (This is only something we have seen with some Windows users - see
the **[docs for more help!](tf-training-tips-and-tricks)**).
```

**DATA AUGMENTATION:** At this stage you can also decide what type of augmentation to
use. Once you've called `create_training_dataset`, you can edit the
[**pytorch_config.yaml**](dlc3-pytorch-config) file that was created (or for the
TensorFlow engine, the [**pose_cfg.yaml**](
https://github.com/DeepLabCut/DeepLabCut/blob/main/deeplabcut/pose_cfg.yaml) file).

- PyTorch Engine: [Albumentations](https://albumentations.ai/docs/) is used for data
augmentation. Look at the [**pytorch_config.yaml**](dlc3-pytorch-config) for more
information about image augmentation options.
- TensorFlow Engine: The default augmentation works well for most tasks (as shown on
www.deeplabcut.org), but there are many options, more data augmentation, intermediate
supervision, etc. Here are the available loaders:
  - `imgaug`: a lot of augmentation possibilities, efficient code for target map creation & batch sizes >1 supported.
  You can set the parameters such as the `batch_size` in the `pose_cfg.yaml` file for the model you are training. This
  is the recommended default!
  - `crop_scale`: our standard DLC 2.0 introduced in Nature Protocols variant (scaling, auto-crop augmentation)
  - `tensorpack`: a lot of augmentation possibilities, multi CPU support for fast processing, target maps are created
  less efficiently than in imgaug, does not allow batch size>1
  - `deterministic`: only useful for testing, freezes numpy seed; otherwise like default.

**MODEL COMPARISON**: You can also test several models by creating the same train/test
split for different networks.
You can easily do this in the Project Manager GUI (by selecting the "Use an existing 
data split" option), which also lets you compare PyTorch and TensorFlow models.

````{versionadded} 3.0.0
You can now create new shuffles using the same train/test split as 
existing shuffles with `create_training_dataset_from_existing_split`. This allows you to
compare model performance (between different architectures or when using different
training hyper-parameters) as the shuffles were trained on the same data, and evaluated
on the same test data!

Example usage - creating 3 new shuffles (with indices 10, 11 and 12) for a ResNet 50
pose estimation model, using the same data split as was used for shuffle 0:

```python
deeplabcut.create_training_dataset_from_existing_split(
    config_path,
    from_shuffle=0,
    shuffles=[10, 11, 12],
    net_type="resnet_50",
)
```
````

````{admonition} Click the button to see API Docs for deeplabcut.create_training_dataset
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_training_dataset.rst
```
````

````{admonition} Click the button to see API Docs for deeplabcut.create_training_model_comparison
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_training_model_comparison.rst
```
````

````{admonition} Click the button to see API Docs for deeplabcut.create_training_dataset_from_existing_split
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_training_dataset_from_existing_split.rst
```
````

### (G) Train The Network

The function ‘train_network’ helps the user in training the network. It is used as follows:
```python
deeplabcut.train_network(config_path)
```
The set of arguments in the function starts training the network for the dataset created
for one specific shuffle. Note that you can change training parameters in the 
[**pytorch_config.yaml**](dlc3-pytorch-config) file (or **pose_cfg.yaml** for TensorFlow
models) of the model that you want to train (before you start training).

At user specified iterations during training checkpoints are stored in the subdirectory 
*train* under the respective iteration & shuffle directory.

````{admonition} Tips on training models with the PyTorch Engine
:class: dropdown

Example parameters that one can call:

```python
deeplabcut.train_network(
    config_path,
    shuffle=1,
    trainingsetindex=0,
    device="cuda:0",
    max_snapshots_to_keep=5,
    displayiters=100,
    save_epochs=5,
    epochs=200,
)
```

Pytorch models in DeepLabCut 3.0 are trained for a set number of epochs, instead of a
maximum number of iterations (which is what was used for TensorFlow models). An epoch
is a single pass through the training dataset, which means your model has seen each
training image exactly once. So if you have 64 training images for your network, an
epoch is 64 iterations with batch size 1 (or 32 iterations with batch size 2, 16 with
batch size 4, etc.).

By default, the pretrained networks are not in the DeepLabCut toolbox (as they can be 
more than 100MB), but they get downloaded automatically before you train.

If the user wishes to restart the training at a specific checkpoint they can specify the
full path of the checkpoint to the variable ``resume_training_from`` in the [
**pytorch_config.yaml**](
dlc3-pytorch-config) file (checkout the "Restarting Training at a Specific Checkpoint"
section of the docs) under the *train* subdirectory.

**CRITICAL POINT:** It is recommended to train the networks **until the loss plateaus** 
(depending on the dataset, model architecture and training hyper-parameters this happens
after 100 to 250 epochs of training).

The variables ``display_iters`` and ``save_epochs`` in the [**pytorch_config.yaml**](
dlc3-pytorch-config) file allows the user to alter how often the loss is displayed
and how often the weights are stored. We suggest saving every 5 to 25 epochs.
````

````{admonition} Tips on training models with the TensorFlow Engine 
:class: dropdown

Example parameters that one can call:

```python
deeplabcut.train_network(
    config_path,
    shuffle=1,
    trainingsetindex=0,
    gputouse=None,
    max_snapshots_to_keep=5,
    autotune=False,
    displayiters=100,
    saveiters=25000,
    maxiters=300000,
    allow_growth=True,
)
```

By default, the pretrained networks are not in the DeepLabCut toolbox (as they are 
around 100MB each), but they get downloaded before you train. However, if not previously
downloaded from the TensorFlow model weights, it will be downloaded and stored in a
subdirectory *pre-trained* under the subdirectory *models* in 
*Pose_Estimation_Tensorflow*. At user specified iterations during training checkpoints
are stored in the subdirectory *train* under the respective iteration directory.

If the user wishes to restart the training at a specific checkpoint they can specify the
full path of the checkpoint to the variable ``init_weights`` in the **pose_cfg.yaml**
file under the *train* subdirectory (see Box 2).

**CRITICAL POINT:** It is recommended to train the networks for thousands of iterations
until the loss plateaus (typically around **500,000**) if you use batch size 1. If you
want to batch train, we recommend using Adam,
[see more here](tf-custom-image-augmentation).

The variables ``display_iters`` and ``save_iters`` in the **pose_cfg.yaml** file allows
the user to alter how often the loss is displayed and how often the weights are stored.

**maDeepLabCut CRITICAL POINT:** For multi-animal projects we are using not only
different and new output layers, but also new data augmentation, optimization, learning
rates, and batch training defaults. Thus, please use a lower ``save_iters`` and
``maxiters``. I.e. we suggest saving every 10K-15K iterations, and only training until
50K-100K iterations. We recommend you look closely at the loss to not overfit on your
data. The bonus, training time is much less!!!
````

````{admonition} Click the button to see API Docs for train_network
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.train_network.rst
```
````

### (H) Evaluate the Trained Network

It is important to evaluate the performance of the trained network. This performance is measured by computing
the average root mean square error (RMSE) between the manual labels and the ones predicted by DeepLabCut.
The RMSE is saved as a comma separated file and displayed for all pairs and only likely pairs (>p-cutoff).
This helps to exclude, for example, occluded body parts. One of the strengths of DeepLabCut is that due to the
probabilistic output of the scoremap, it can, if sufficiently trained, also reliably report if a body part is visible
in a given frame. (see discussions of finger tips in reaching and the Drosophila legs during 3D behavior in
[Mathis et al, 2018]). The evaluation results are computed by typing:

```python
deeplabcut.evaluate_network(config_path, Shuffles=[1], plotting=True)
```

Setting `plotting` to true plots all the testing and training frames with the manual and predicted labels. The user
should visually check the labeled test (and training) images that are created in the ‘evaluation-results’ directory.
Ideally, DeepLabCut labeled unseen (test images) according to the user’s required accuracy, and the average train
and test errors are comparable (good generalization). What (numerically) comprises an acceptable RMSE depends on
many factors (including the size of the tracked body parts, the labeling variability, etc.). Note that the test error
can also be larger than the training error due to human variability (in labeling, see Figure 2 in Mathis et al, Nature
Neuroscience 2018).

**Optional parameters:**

- `Shuffles: list, optional` - List of integers specifying the shuffle indices of the training dataset.
The default is [1]

- `plotting: bool, optional` - Plots the predictions on the train and test images. The default is `False`;
if provided it must be either `True` or `False`

- `show_errors: bool, optional` - Display train and test errors. The default is `True`

- `comparisonbodyparts: list of bodyparts, Default is all` - The average error will be computed for those body parts
only (Has to be a subset of the body parts).

- `gputouse: int, optional` - Natural number indicating the number of your GPU (see number in nvidia-smi). If you do not
have a GPU, put None. See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

The plots can be customized by editing the **config.yaml** file (i.e., the colormap, scale, marker size (dotsize), and
transparency of labels (alphavalue) can be modified). By default each body part is plotted in a different color
(governed by the colormap) and the plot labels indicate their source. Note that by default the human labels are
plotted as plus (‘+’), DeepLabCut’s predictions either as ‘.’ (for confident predictions with likelihood > p-cutoff) and
’x’ for (likelihood <= `pcutoff`).

The evaluation results for each shuffle of the training dataset are stored in a unique subdirectory in a newly created
directory ‘evaluation-results-pytorch’ (‘evaluation-results’ for tensorflow models) in the project directory.
The user can visually inspect if the distance between the labeled and the predicted body parts are acceptable.
In the event of benchmarking with different shuffles of same training dataset, the user can provide multiple shuffle
indices to evaluate the corresponding network.
Note that with multi-animal projects additional distance statistics aggregated over animals or bodyparts are also stored
in that directory. This aims at providing a finer quantitative evaluation of multi-animal prediction performance
before animal tracking. If the generalization is not sufficient, the user might want to:

• check if the labels were imported correctly; i.e., invisible points are not labeled and the points of interest are
labeled accurately

• make sure that the loss has already converged

• consider labeling additional images and make another iteration of the training data set

**OPTIONAL:** You can also plot the scoremaps, locref layers, and PAFs:

```python
deeplabcut.extract_save_all_maps(config_path, shuffle=shuffle, Indices=[0, 5])
```
you can drop "Indices" to run this on all training/testing images (this is slow!)

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.evaluate_network.rst
```
````

### (I) Analyze new Videos

The trained network can be used to analyze new videos. Novel/new videos **DO NOT have to be in the config file!**.
You can analyze new videos anytime by simply using the following line of code:
```python
deeplabcut.analyze_videos(
    config_path, ["fullpath/analysis/project/videos/reachingvideo1.avi"],
    save_as_csv=True
)
```
There are several other optional inputs, such as:
```python
deeplabcut.analyze_videos(
    config_path,
    videos,
    videotype="avi",
    shuffle=1,
    trainingsetindex=0,
    gputouse=None,
    save_as_csv=False,
    destfolder=None,
    dynamic=(True, .5, 10)
)
```
The user can choose a checkpoint for analyzing the videos. For this, the user can enter the corresponding index of the
checkpoint to the variable snapshotindex in the config.yaml file. By default, the most recent checkpoint (i.e. last) is
used for analyzing the video.
The labels are stored in a MultiIndex [Pandas](http://pandas.pydata.org) Array, which contains the name of the network,
body part name, (x, y) label position in pixels, and the likelihood for each frame per body part. These arrays are
stored in an efficient Hierarchical Data Format (HDF) in the same directory, where the video is stored.
However, if the flag `save_as_csv` is set to `True`, the data can also be exported in comma-separated values format
(.csv), which in turn can be imported in many programs, such as MATLAB, R, Prism, etc.; This flag is set to `False`
by default. You can also set a destination folder (`destfolder`) for the output files by passing a path of the folder
you wish to write to.

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.analyze_videos.rst
```
````

### Novel Video Analysis: extra features

#### Dynamic-cropping of videos:

As of 2.1+ we have a dynamic cropping option. Namely, if you have large frames and the animal/object occupies a smaller
fraction, you can crop around your animal/object to make processing speeds faster. For example, if you have a large open
field experiment but only track the mouse, this will speed up your analysis (also helpful for real-time applications).
To use this simply add `dynamic=(True,.5,10)` when you call `analyze_videos`.

```python
dynamic: triple containing (state, detectiontreshold, margin)

    If the state is true, then dynamic cropping will be performed.
    That means that if an object is detected (i.e., any body part > detectiontreshold),
    then object boundaries are computed according to the smallest/largest x position and
    smallest/largest y position of all body parts. This window is expanded by the margin
    and from then on only the posture within this crop is analyzed (until the object is lost;
    i.e., < detectiontreshold). The current position is utilized for updating the crop window
    for the next frame (this is why the margin is important and should be set large enough
    given the movement of the animal).
```
### (J) Filter Pose Data

You can also filter the predictions with a median filter (default) or with a [SARIMAX model](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html), if you wish. This creates a new .h5 file with the ending *_filtered* that you can use in create_labeled_data and/or plot trajectories.
```python
deeplabcut.filterpredictions(
    config_path,
    ["fullpath/analysis/project/videos/reachingvideo1.avi"]
)
```
  An example call:
```python
deeplabcut.filterpredictions(
    config_path,
    ["fullpath/analysis/project/videos"],
    videotype=".mp4",
    filtertype="arima",
    ARdegree=5,
    MAdegree=2
)
```
  Here are parameters you can modify and pass:
```python
deeplabcut.filterpredictions(
    config_path,
    ["fullpath/analysis/project/videos/reachingvideo1.avi"],
    shuffle=1,
    trainingsetindex=0,
    filtertype="arima",
    p_bound=0.01,
    ARdegree=3,
    MAdegree=1,
    alpha=0.01
)
```
 Here is an example of how this can be applied to a video:

 <p align="center">
<img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5ccc8b8ae6e8df000100a995/1556908943893/filter_example-01.png?format=1000w" width="70%">
</p>

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.filterpredictions.rst
```
````

### (K) Plot Trajectories

The plotting components of this toolbox utilizes matplotlib. Therefore, these plots can easily be customized by
the end user. We also provide a function to plot the trajectory of the extracted poses across the analyzed video, which
can be called by typing:

```
deeplabcut.plot_trajectories(config_path, [‘fullpath/analysis/project/videos/reachingvideo1.avi’])
```

It creates a folder called `plot-poses` (in the directory of the video). The plots display the coordinates of body parts
vs. time, likelihoods vs time, the x- vs. y- coordinate of the body parts, as well as histograms of consecutive
coordinate differences. These plots help the user to quickly assess the tracking performance for a video. Ideally, the
likelihood stays high and the histogram of consecutive coordinate differences has values close to zero (i.e. no jumps in
body part detections across frames). Here are example plot outputs on a demo video (left):

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559946148685-WHDO5IG9MMCHU0T7RC62/ke17ZwdGBToddI8pDm48kEOb1vFO6oRDmR8SXh4iL21Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVG1gXK66ltnjKh4U2immgm7AVAdfOWODmXNLQLqbLRZ2DqWIIaSPh2v08GbKqpiV54/file0289.png?format=500w" height="240">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559939762886-CCB0R107I2HXAHZLHECP/ke17ZwdGBToddI8pDm48kNeA8e5AnyMqj80u4_mB0hV7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UcpboONgOQYHLzaUWEI1Ir9fXt7Ehyn7DSgU3GCReAA-ZDqXZYzu2fuaodM4POSZ4w/plot_poses-01.png?format=1000w" height="250">
</p>

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.plot_trajectories.rst
```
````

### (L) Create Labeled Videos

Additionally, the toolbox provides a function to create labeled videos based on the extracted poses by plotting the
labels on top of the frame and creating a video. There are two modes to create videos: FAST and SLOW (but higher
quality!). One can use the command as follows to create multiple labeled videos:
```python
deeplabcut.create_labeled_video(
    config_path,
    ["fullpath/analysis/project/videos/reachingvideo1.avi",
     "fullpath/analysis/project/videos/reachingvideo2.avi"],
    save_frames = True/False
)
```
 Optionally, if you want to use the filtered data for a video or directory of filtered videos pass `filtered=True`,
 i.e.:
```python
deeplabcut.create_labeled_video(
    config_path,
    ["fullpath/afolderofvideos"],
    videotype=".mp4",
    filtered=True
)
```
You can also optionally add a skeleton to connect points and/or add a history of points for visualization. To set the
"trailing points" you need to pass `trailpoints`:
```python
deeplabcut.create_labeled_video(
    config_path,
    ["fullpath/afolderofvideos"],
    videotype=".mp4",
    trailpoints=10
)
```
To draw a skeleton, you need to first define the pairs of connected nodes (in the `config.yaml` file) and set the
skeleton color (in the `config.yaml` file). There is also a GUI to help you do this, use by calling
`deeplabcut.SkeletonBuilder(configpath)`!

Here is how the `config.yaml` additions/edits should look (for example, on the Openfield demo data we provide):
```python
# Plotting configuration
skeleton:
  - ["snout", "leftear"]
  - ["snout", "rightear"]
  - ["leftear", "tailbase"]
  - ["leftear", "rightear"]
  - ["rightear", "tailbase"]
skeleton_color: white
pcutoff: 0.4
dotsize: 4
alphavalue: 0.5
colormap: jet
```
Then pass `draw_skeleton=True` with the command:
```python
deeplabcut.create_labeled_video(
    config_path,
    ["fullpath/afolderofvideos"],
    videotype=".mp4",
    draw_skeleton=True
)
```

**NEW** as of 2.2b8: You can create a video with only the "dots" plotted, i.e., in the
[style of Johansson](https://link.springer.com/article/10.1007/BF00309043), by passing `keypoints_only=True`:

```python
deeplabcut.create_labeled_video(
    config_path,["fullpath/afolderofvideos"],
    videotype=".mp4",
    keypoints_only=True
)
```

**PRO TIP:** that the **best quality videos** are created when `fastmode=False` is passed. Therefore, when
`trailpoints` and `draw_skeleton` are used, we **highly** recommend you also pass `fastmode=False`!

 <p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559935526258-KFYZC8BDHK01ZIDPNVIX/ke17ZwdGBToddI8pDm48kJbosy0LGK_KqcAZRQ_Qph1Zw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzkC6kmM1CbNgeHQVxASNv0wiXikHv274BIFe4LR7nd1rKmAka4uxYMJ9FupazBoaU/mouse_skel_trail.gif?format=750w" width="40%">
</p>

This function has various other parameters, in particular the user can set the `colormap`, the `dotsize`, and
`alphavalue` of the labels in **config.yaml** file.

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.create_labeled_video.rst
```
````

#### Extract "Skeleton" Features:

NEW, as of 2.0.7+: You can save the "skeleton" that was applied in `create_labeled_videos` for more computations.
Namely,  it extracts length and orientation of each "bone" of the skeleton as defined in the **config.yaml** file. You
can use the function by:

```python
deeplabcut.analyzeskeleton(
    config,
    video,
    videotype="avi",
    shuffle=1,
    trainingsetindex=0,
    save_as_csv=False,
    destfolder=None
)
```

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.analyzeskeleton.rst
```
````

(active-learning)=
### (M) Optional Active Learning -> Network Refinement: Extract Outlier Frames

While DeepLabCut typically generalizes well across datasets, one might want to optimize its performance in various,
perhaps unexpected, situations. For generalization to large data sets, images with insufficient labeling performance
can be extracted, manually corrected by adjusting the labels to increase the training set and iteratively improve the
feature detectors. Such an active learning framework can be used to achieve a predefined level of confidence for all
images with minimal labeling cost (discussed in Mathis et al 2018). Then, due to the large capacity of the neural network that underlies the feature detectors, one can continue training the network with these additional examples. One does not
necessarily need to correct all errors as common errors could be eliminated by relabeling a few examples and then
re-training. A priori, given that there is no ground truth data for analyzed videos, it is challenging to find putative
“outlier frames”. However, one can use heuristics such as the continuity of body part trajectories, to identify images
where the decoder might make large errors.

All this can be done for a specific video by typing (see other optional inputs below):

```python
deeplabcut.extract_outlier_frames(config_path, ["videofile_path"])
```

We provide various frame-selection methods for this purpose. In particular
the user can set:

```
outlieralgorithm: "fitting", "jump", or "uncertain"
```
• `outlieralgorithm="uncertain"`: select frames if the likelihood of a particular or all body parts lies below `p_bound`
(note this could also be due to occlusions rather than errors).

• `outlieralgorithm="jump"`: select frames where a particular body part or all body parts jumped more than `epsilon`
pixels from the last frame.

• `outlieralgorithm="fitting"`: select frames if the predicted body part location deviates from a state-space model fit
to the time series of individual body parts. Specifically, this method fits an Auto Regressive Integrated Moving Average
(ARIMA) model to the time series for each body part. Thereby each body part detection with a likelihood smaller than
`p_bound` is treated as missing data.  Putative outlier frames are then identified as time points, where the average
body part estimates are at least `epsilon` pixels away from the fits. The parameters of this method are `epsilon`,
`p_bound`, the ARIMA parameters as well as the list of body parts to average over (can also be `all`).

• `outlieralgorithm="manual"`: manually select outlier frames based on visual inspection from the user.

 As an example:
```python
deeplabcut.extract_outlier_frames(config_path, ["videofile_path"], outlieralgorithm="manual")
```

In general, depending on the parameters, these methods might return much more frames than the user wants to
extract (`numframes2pick`). Thus, this list is then used to select outlier frames either by randomly sampling from
this list (`extractionalgorithm="uniform"`), by performing `extractionalgorithm="kmeans"` clustering on the
corresponding frames.

In the automatic configuration, before the frame selection happens, the user is informed about the amount of frames
satisfying the criteria and asked if the selection should proceed. This step allows the user to perhaps change the
parameters of the frame-selection heuristics first (i.e. to make sure that not too many frames are qualified). The user
can run the `extract_outlier_frames` method iteratively, and (even) extract additional frames from the same video.
Once enough outlier frames are extracted the refinement GUI can be used to adjust the labels based on user feedback
(see below).

#### API Docs
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.extract_outlier_frames.rst
```
````

 ### (N) Refine Labels: Augmentation of the Training Dataset

 Based on the performance of DeepLabCut, four scenarios are possible:

(A) Visible body part with accurate DeepLabCut prediction. These labels do not need any modifications.

(B) Visible body part but wrong DeepLabCut prediction. Move the label’s location to the actual position of the
body part.

(C) Invisible, occluded body part. Remove the predicted label by DeepLabCut with a middle click. Every predicted
label is shown, even when DeepLabCut is uncertain. This is necessary, so that the user can potentially move
the predicted label. However, to help the user to remove all invisible body parts the low-likelihood predictions
are shown as open circles (rather than disks).

(D) Invalid images: In the unlikely event that there are any invalid images, the user should remove such an image
and their corresponding predictions, if any. Here, the GUI will prompt the user to remove an image identified
as invalid.

The labels for extracted putative outlier frames can be refined by opening the GUI:
```python
deeplabcut.refine_labels(config_path)
```
This will launch a GUI where the user can refine the labels.

Please refer to the [napari-deeplabcut docs](napari-gui) for more information about the labelling workflow.

After correcting the labels for all the frames in each of the subdirectories, the users should merge the data set to
create a new dataset. In this step the iteration parameter in the config.yaml file is automatically updated.
```python
deeplabcut.merge_datasets(config_path)
```
Once the dataset is merged, the user can test if the merging process was successful by plotting all the labels (Step E).
Next, with this expanded training set the user can now create a novel training set and train the network as described
in Steps F and G. The training dataset will be stored in the same place as before but under a different `iteration-#`
subdirectory, where the ``#`` is the new value of `iteration` variable stored in the project’s configuration file
(this is automatically done).

Now you can run `create_training_dataset`, then `train_network`, etc. If your original labels were adjusted at all,
start from fresh weights (the typically recommended path anyhow), otherwise consider using your already trained network
weights (see Box 2).

If after training the network generalizes well to the data, proceed to analyze new videos. Otherwise, consider labeling
more data.

#### API Docs for deeplabcut.refine_labels
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.refine_labels.rst
```
````

#### API Docs for deeplabcut.merge_datasets
````{admonition} Click the button to see API Docs
:class: dropdown
```{eval-rst}
.. include:: ./api/deeplabcut.merge_datasets.rst
```
````

### Jupyter Notebooks for Demonstration of the DeepLabCut Workflow

We also provide two Jupyter notebooks for using DeepLabCut on both a pre-labeled dataset, and on the end user’s
own dataset. Firstly, we prepared an interactive Jupyter notebook called
[Demo_yourowndata.ipynb](https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/JUPYTER/Demo_yourowndata.ipynb)
that can serve as a template for the user to develop a project. Furthermore, we provide a notebook for an already
started project with labeled data. The example project, named as
[Reaching-Mackenzie-2018-08-30](https://github.com/DeepLabCut/DeepLabCut/tree/main/examples/Reaching-Mackenzie-2018-08-30)
consists of a project configuration file with default parameters and 20 images, which are cropped around the region of
interest as an example dataset. These images are extracted from a video, which was recorded in a study of skilled motor
control in mice. Some example labels for these images are also provided. See more details
[here](https://github.com/DeepLabCut/DeepLabCut/tree/main/examples).

## 3D Toolbox

Please see [3D overview](3D-overview) for information on using the 3D toolbox of
DeepLabCut (as of 2.0.7+).

## Other functions, some are yet-to-be-documented:

We suggest you [check out these additional helper functions](helper-functions), that
could be useful (they are all optional).


--- File: docs/HelperFunctions.md ---
(helper-functions)=
# Helper & Advanced Optional Function Documentation

There are additional functions that are not required, but can be extremely helpful.

First off, if you are new to Python, you might not know this handy trick: you can see
ALL the functions in deeplabcut by typing ``deeplabcut.`` then hitting "tab." You will see a massive list!

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1567907875609-57X4S1LVZWTRJ8GPM34T/ke17ZwdGBToddI8pDm48kKLvSvW2qdCTCjZZgzhLzasUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKc0_818bg8q0aD7_W_W22OLw0yYD6y1fnQ3mVB6beYNdnbXafewWM7FbBaWqQqcLy-/options.png?format=1000w" width="90%">
</p>

Or perhaps you sort of know the name of the function, but not fully, then you can start typing the command, i.e. as in ``deeplabcut.a `` then hit tab:

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1567907844296-STHTZ2SD6UB5WCVEN2I8/ke17ZwdGBToddI8pDm48kJEw9Z-3B5ptjcdSkknf02DlfiSMXz2YNBs8ylwAJx2qgRUppHe6ToX8uSOdETM-XipuQpH02DE1EkoTaghKW779xRs5veesYFcByqmynT9oByNVWkh1tiIAZLs8gRhPycqbSMdPDHKAvTCdk8NbVnE/optionsA.png?format=1000w" width="90%">
</p>


Now, for any of these functions, you type ``deeplabcut.analyze_videos_converth5_to_csv?`` you get:

```python
Signature: deeplabcut.analyze_videos_converth5_to_csv(videopath, videotype='.avi')
Docstring:
By default the output poses (when running analyze_videos) are stored as MultiIndex Pandas Array, which contains the name of the network, body part name, (x, y) label position  in pixels, and the likelihood for each frame per body part. These arrays are stored in an efficient Hierarchical Data Format (HDF) in the same directory, where the video is stored. If the flag save_as_csv is set to True, the data is also exported as comma-separated value file. However, if the flag was *not* set, then this function allows the conversion of all h5 files to csv files (without having to analyze the videos again)!

This functions converts hdf (h5) files to the comma-separated values format (.csv), which in turn can be imported in many programs, such as MATLAB, R, Prism, etc.

 Parameters
----------

    videopath : string
        A strings containing the full paths to videos for analysis or a path to the directory where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.
Only videos with this extension are analyzed. The default is ``.avi``

 Examples
-----------

    Converts all pose-output files belonging to mp4 videos in the folder '/media/alex/experimentaldata/cheetahvideos' to csv files.
    deeplabcut.analyze_videos_converth5_to_csv('/media/alex/experimentaldata/cheetahvideos','.mp4')  
```

While some of the names are ridiculously long, we wanted them to be "self-explanatory." Here is a list
(that is bound to be continually updated)
of currently available helper functions. To see information about any of them, including HOW
to use them, use the ``?`` at the end of the call, as described above.


```python
deeplabcut.analyze_videos_converth5_to_csv

deeplabcut.mergeandsplit

deeplabcut.analyze_time_lapse_frames

deeplabcut.convertcsv2h5

deeplabcut.ShortenVideo

deeplabcut.DownSampleVideo

deeplabcut.CropVideo

deeplabcut.adddatasetstovideolistandviceversa

deeplabcut.comparevideolistsanddatafolders

deeplabcut.dropannotationfileentriesduetodeletedimages

deeplabcut.dropduplicatesinannotatinfiles

deeplabcut.load_demo_data

deeplabcut.merge_datasets

deeplabcut.export_model

```

## Model Export function:

This function allows you to export a well-trained single animal model for real-time applications, etc. This function is part of [Kane et al, 2020 eLife](https://elifesciences.org/articles/61909). Please see the paper and related code-base on how to use this utility.

- Another example use case is with the [Bonsai-DeepLabCut](https://github.com/bonsai-rx/deeplabcut) plug in. Namely, you need to first export your trained model from DLC, then follow the instructions for Bonsai-specific use.

```python
deeplabcut.export_model(cfg_path, iteration=None, shuffle=1, trainingsetindex=0, snapshotindex=None, TFGPUinference=True, overwrite=False, make_tar=True)
```

## Advanced Labeling across Cameras:

### If you have two cameras and you want to make a 3D Project from your data, you can leverage this in the Labeling GUI:

If you have multiple cameras, you may want to use epipolar lines projected on the images you are labeling to help you label the same position on the body in each camera angle. An epipolar line is a projection from one camera to all the possible points in the second camera's image that could match the labeled point in the first camera's image. A correctly labeled point will fall somewhere along this projected line.

In order to label with epipolar lines, you must complete two additional sets of steps **prior to labeling.**

- First, you must create a 3d project and calibrate the cameras - to do so, complete
  steps 1-3 in [3D Overview](3D-overview).

- Second, you must extract imagr from `camera_1` first; here you would have run the standard `deeplabcut.extract_frames(config_path, userfeedback=True)`, but just extract files from 1 camera. Next, you need to extract matching frames from `camera_2`:
```python
deeplabcut.extract_frames(config_path, mode = 'match', config3d=config_path3d, extracted_cam=0)
```
You can set `extracted_cam=0` to match all other camera images to the frame numbers in the `camera_1` folder, or change this to match to other cameras. If you `deeplabcut.extract_frames` with `mode='automatic'` before, it shouldn't matter which camera you pick. If you already extracted from both cameras, be warned this will overwrite the images for `camera_2`.

- Three, now you can label with epipolar lines:

     - Here, label `camera_1` as you would normally, i.e.:
    ```python
    deeplabcut.label_frames(config_path)
    ```
    - Then for `camera_2` (now it will compute the epipolar lines based on camera_1 labels and project them onto the GUI):
    ```python
    deeplabcut.label_frames(config_path, config3d=config_path3d)
    ```


--- File: docs/MISSION_AND_VALUES.md ---
(mission-and-values)=
# Mission and Values of DeepLabCut

This document is meant to help guide decisions about the future of `DeepLabCut`, be it in terms of
whether to accept new functionality, changes to the styling of the code or graphical user interfaces (GUI),
or whether to take on new dependencies, when to break into other repos, among other things. It serves as a point of
reference for core developers actively working on the project, and an introduction for
newcomers who want to learn a little more about where the project is going and what the team's
values are. You can also learn more about how the project is managed by looking at our
[governance model](governance-model).

## Our founding principles

The founding DeepLabCut team came together around a shared vision for building the first open-source animal pose
estimation framework that is:

- user defined pose estimation - i.e. species or object agnostic.
- access to SOTA deep learning models that can be swiftly re-trained for customized applications
- fast (GPU-powered)
- scalable (project focused for ease of portability and sharability)


As the project has grown we've turned these original principles into the mission statement and set of values that we
described below.

## Our mission

DeepLabCut aims to be **the animal pose software package for Python** and to **provide access to deep learning-based
pose estimation for people to use in their daily work** without the need to be able to program in a deep learning
framework. We hope to accomplish this by:

- being **easy to use and install**. We are careful in taking on new dependencies, sometimes making them optional, and
aim support a fully (Python) packaged installation that works cross-platform.

- being **well-documented** with **comprehensive tutorials and examples**. All functions in our API have thorough
docstrings clarifying expected inputs and outputs, and we maintain a separate
[tutorials and information website](http://deeplabcut.org).

- providing **GUI access** to all critical functionality so DeepLabCut can be used by people without coding experience.

- being **interactive** and **highly performant** in order to support large data pipelines.

- providing a **consistent and stable API** to enable plugin developers to build on top of DeepLabCut without their
code constantly breaking and to enable advanced users to build out sophisticated Python workflows, if needed.

- **ensuring correctness**. We strive for complete test coverage of both the code and GUI, with all code reviewed by a
core developer before being included in the repository.

## Our values

- We are **inclusive**. We welcome newcomers who are making their first contribution and strive to grow our most
dedicated contributors into [core developers](https://github.com/orgs/DeepLabCut/teams/core-developers).
We have a [Code of Conduct](https://github.com/DeepLabCut/DeepLabCut/blob/main/CODE_OF_CONDUCT.md) to make DeepLabCut
a welcoming place for all.

- We are **community-engaged**. We respond to feature requests and proposals on our
- [issue tracker](https://github.com/DeepLabCut/DeepLabCut/issues).

- We serve **scientific applications** primarily, over “consumer or commercial” pose estimation tools. This often means
prioritizing core functionality support, and rejecting implementations of “flashy” features that have little
scientific value.

- We are **domain agnostic** within the sciences. Functionality that is highly specific to particular scientific
domains belongs in plugins, whereas functionality that cuts across many domains and is likely to be widely used belongs
inside DeepLabCut.

- We value **education and documentation**. All functions should have docstrings, preferably with examples, and major
functionality should be explained in our [tutorials](http://deeplabcut.org). Core developers can take an active role
in finishing documentation examples.


## Acknowledgements

We share a lot of our mission and values with [`napari`](https://napari.org/stable/community/mission_and_values.html)
and [`scikit-image`](https://scikit-image.org/docs/stable/about/values.html) and acknowledge the influence of their
mission and values statements on this document.


--- File: docs/Governance.md ---
(governance-model)=
# Governance Model of DeepLabCut
(adapted from https://napari.org/stable/community/governance.html)

## Abstract

The purpose of this document is to formalize the governance process used by the
`DeepLabCut` project, to clarify how decisions are made and how the various
elements of our community interact.

This is a consensus-based community project. Anyone with an interest in the
project can join the community, contribute to the project design, and
participate in the decision making process. This document describes how that
participation takes place, how to find consensus, and how deadlocks are
resolved.

## Roles And Responsibilities

### The Community

The `DeepLabCut` community consists of anyone using or working with the project
in any way.

### Contributors

A community member can become a contributor by interacting directly with the
project in concrete ways, such as:

- proposing a change to the code via a GitHub
  [GitHub pull request](https://github.com/DeepLabCut/DeepLabCut/pulls);
- reporting issues on our
  [GitHub issues page](https://github.com/DeepLabCut/DeepLabCut/issues);
- proposing a change to the [documentation](https://deeplabcut.github.io/DeepLabCut/README.html) via a
  GitHub pull request;
- discussing the design of the `DeepLabCut` or its tutorials on in existing
  [issues](https://github.com/DeepLabCut/DeepLabCut/issues) and
  [pull requests](https://github.com/DeepLabCut/DeepLabCut/pulls);
- discussing examples or use cases on the
  [image.sc forum](https://forum.image.sc/tags/DeepLabCut) under the #deeplabcut tag; or
- reviewing [open pull requests](https://github.com/DeepLabCut/DeepLabCut/pulls)

among other possibilities. Any community member can become a contributor, and
all are encouraged to do so. By contributing to the project, community members
can directly help to shape its future.

Contributors are encouraged to read the [contributing guide](https://github.com/DeepLabCut/DeepLabCut/blob/main/CONTRIBUTING.md).

### Core developers

Core developers are community members that have demonstrated continued
commitment to the project through ongoing contributions. They
have shown they can be trusted to maintain `DeepLabCut` with care. Becoming a
core developer allows contributors to merge approved pull requests, cast votes
for and against merging a pull-request, and be involved in deciding major
changes to the API, and thereby more easily carry on with their project related
activities. Core developers appear as organization members on the `DeepLabCut`
[GitHub organization](https://github.com/orgs/DeepLabCut/people) and are on our
[@deeplabcut/core-developers](https://github.com/orgs/DeepLabCut/teams/core-developers) GitHub team. Core
developers are expected to review code contributions while adhering to the
core developer guide. New core developers can be nominated by any existing core developer,
and for details on that process see our core developer guide.

### Steering Council

The Steering Council (SC) members are core developers who have additional
responsibilities to ensure the smooth running of the project. SC members are
expected to participate in strategic planning, approve changes to the
governance model, and make decisions about funding granted to the project
itself. (Funding to community members is theirs to pursue and manage). The
purpose of the SC is to ensure smooth progress from the big-picture
perspective. Changes that impact the full project require analysis informed by
long experience with both the project and the larger ecosystem. When the core
developer community (including the SC members) fails to reach such a consensus
in a reasonable timeframe, the SC is the entity that resolves the issue.

Members of the steering council also have the "owner" role within the [DeepLabCut GitHub organization](https://github.com/DeepLabCut/)
and are ultimately responsible for managing the DeepLabCut GitHub account, the [@DeepLabCut](https://twitter.com/DeepLabCut)
twitter account, the [DeepLabCut website](http://www.DeepLabCut.org), and other similar DeepLabCut owned resources.

The current steering council of DeepLabCut consists of the original developers:

- [Mackenzie Mathis](https://github.com/mmathislab)
- [Alexander Mathis](https://github.com/alexemg)

The SC membership is revisited every January. SC members who do
not actively engage with the SC duties are expected to resign. New members are
added by nomination by a core developer. Nominees should have demonstrated
long-term, continued commitment to the project and its [mission and values](mission-and-values). A
nomination will result in discussion that cannot take more than a month and
then admission to the SC by consensus. During that time deadlocked votes of the SC will
be postponed until the new member has joined and another vote can be held.

The DeepLabCut steering council may be contacted at `admin@deeplabcut.org`.

## Decision Making Process

Decisions about the future of the project are made through discussion with all
members of the community. All non-sensitive project management discussion takes
place on the [issue tracker](https://github.com/deeplabcut/deeplabcut/issues). Occasionally,
sensitive discussions may occur on a private core developer medium.

Decisions should be made in accordance with the [mission and values](mission-and-values)
of the DeepLabCut project.

DeepLabCut uses a “consensus seeking” process for making decisions. The group
tries to find a resolution that has no open objections among core developers.
Core developers are expected to distinguish between fundamental objections to a
proposal and minor perceived flaws that they can live with, and not hold up the
decision-making process for the latter.  If no option can be found without
objections, the decision is escalated to the SC, which will itself use
consensus seeking to come to a resolution. In the unlikely event that there is
still a deadlock, the proposal will move forward if it has the support of a
simple majority of the SC.

Decisions (in addition to adding core developers and SC membership as above)
are made according to the following rules:

- **Minor documentation changes**, such as typo fixes, or addition / correction of a
  sentence, require approval by a core developer *and* no disagreement or requested
  changes by a core developer on the issue or pull request page (lazy
  consensus). Core developers are expected to give “reasonable time” to others
  to give their opinion on the pull request if they’re not confident others
  would agree.

- **Code changes and major documentation changes** require agreement by *one*
  core developer *and* no disagreement or requested changes by a core developer
  on the issue or pull-request page (lazy consensus). For all changes of this type,
  core developers are expected to give “reasonable time” after approval and before
  merging for others to weigh in on the pull request in its final state.

- **Changes to the API principles** require a dedicated issue on our
  [issue tracker](https://github.com/DeepLabCut/DeepLabCut/issues) and follow the
  decision-making process outlined above.

- **Changes to this governance model or our mission, vision, and values**
  require a  dedicated issue on our [issue tracker](https://github.com/DeepLabCut/DeepLabCut/issues)
  and follow the decision-making process outlined above,
  *unless* there is unanimous agreement from core developers on the change in
  which case it can move forward faster.

If an objection is raised on a lazy consensus, the proposer can appeal to the
community and core developers and the change can be approved or rejected by
escalating to the SC.


--- File: docs/intro.md ---
Please see the main [READ ME!](https://deeplabcut.github.io/DeepLabCut/README.html)


--- File: docs/benchmark.md ---
# DeepLabCut benchmark

For further information and the leaderboard, see [the official homepage](https://benchmark.deeplabcut.org/).

## High Level API

When implementing your own benchmarks, the most important functions are directly accessible
under the ``deeplabcut.benchmark`` package.

```{eval-rst}
.. automodule:: deeplabcut.benchmark
   :members:
   :show-inheritance:
```

## Available benchmark definitions

See [the official benchmark page](https://benchmark.deeplabcut.org/datasets.html) for a full overview
of the available datasets. A benchmark submission should contain a result for at least one of these
benchmarks. For an example of how to implement a benchmark submission, refer to the baselines in the
[DeepLabCut benchmark repo](https://github.com/DeepLabCut/benchmark/tree/main/benchmark/baselines).

```{eval-rst}
.. automodule:: deeplabcut.benchmark.benchmarks
   :members:
   :show-inheritance:
```

## Metric calculation

```{eval-rst}
.. automodule:: deeplabcut.benchmark.metrics
   :members:
   :show-inheritance:
```


--- File: docs/README.md ---
Please see https://deeplabcut.github.io/DeepLabCut for documentation on how to use this software. 

This directory contains the source code for the docs.


--- File: docs/course.md ---
## DeepLabCut Self-paced Course

::::{warning}
This course was designed for DLC 2.
An updated version for DLC 3 is in the works.
::::

Do you have video of animal behaviors? Step 1: Get Poses ... 

 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572296495650-Y4ZTJ2XP2Z9XF1AD74VW/ke17ZwdGBToddI8pDm48kMulEJPOrz9Y8HeI7oJuXxR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UZiU3J6AN9rgO1lHw9nGbkYQrCLTag1XBHRgOrY8YAdXW07ycm2Trb21kYhaLJjddA/DLC_logo_blk-01.png?format=1000w" width="150" title="DLC-live" alt="DLC LIVE!" align="right" vspace = "50">

This document is an outline of resources for a course for those wanting to learn to use `Python` and `DeepLabCut`.
We expect it to take *roughly* 1-2 weeks to get through if you do it rigorously. To get the basics, it should take 1-2 days.

[CLICK HERE to launch the interactive graphic to get started!](https://view.genial.ly/5fb40a49f8a0ef13943d4e5e/horizontal-infographic-review-learning-to-use-deeplabcut) (mini preview below) Or, jump in below!

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1605642639913-OIUBVR8R0JLYZQPRIYIR/ke17ZwdGBToddI8pDm48kMMAxEenKbh651VJujierMxZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpw-CO5bsXt3Lwn3O5kv-PfTgGtLU9oye8D4J7Fixq38Gl-o9tfrEtbnqpPzC5bXTas/ezgif.com-gif-maker.gif?format=750w" width="95%">
</p>


## Installation:

You need Python and DeepLabCut installed!
- [See these "beginner docs" for help!](beginners-guide)

- **WATCH:** overview of conda:  [Python Tutorial: Anaconda - Installation and Using Conda](https://www.youtube.com/watch?v=YJC6ldI3hWk)


## Outline:

### **The basics of computing in Python, terminal, and overview of DeepLabCut:**

- **Learning:** Using the program terminal / cmd on your computer: [Video Tutorial!](https://www.youtube.com/watch?v=5XgBd6rjuDQ)

- **Learning:** although minimal to no Python coding is required (i.e. you could use the DLC GUI to run the full program without it), here are some resources you may want to check out. [Software Carpentry: Programming with Python](https://swcarpentry.github.io/python-novice-inflammation/)

- **Learning:** learning and teaching signal processing, and overview from Prof. Demba Ba [talk at JupyterCon](https://www.youtube.com/watch?v=ywz-LLYwkQQ)

- **DEMO:** Can I DEMO DEEPLABCUT (DLC) quickly?
    - Yes: [you can click through this DEMO notebook](https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/COLAB/COLAB_DEMO_mouse_openfield.ipynb)
    - AND follow along with me: [Video Tutorial!](https://www.youtube.com/watch?v=DRT-Cq2vdWs)


- **WATCH:** How do you know DLC is installed properly? (i.e. how to use our test script!) [Video Tutorial!](https://youtu.be/IOWtKn3l33s)


<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1587608364285-A8R2F24K4DCP0KLAYI91/ke17ZwdGBToddI8pDm48kOhrDvKq54Xu9oStUCFZX0R7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0p4XabXLlNWpcJMv7FrN_NLe3GEN018us8vX03EdtIDHsW7dEh7nvL5CemxAxOy1gg/EKlIEXyXUAE0cy3.jpeg?format=1000w" width="350" title="DLC" alt="review!" align="right" vspace = "50">

- **REVIEW PAPER:** The state of animal pose estimation w/ deep learning i.e. "Deep learning tools for the measurement of animal behavior in neuroscience" [arXiv](https://arxiv.org/abs/1909.13868) & [published version](https://www.sciencedirect.com/science/article/pii/S0959438819301151)

- **REVIEW PAPER:** [A Primer on Motion Capture with Deep Learning: Principles, Pitfalls and Perspectives](https://www.sciencedirect.com/science/article/pii/S0896627320307170)


- **WATCH:** There are a lot of docs... where to begin: [Video Tutorial!](https://www.youtube.com/watch?v=A9qZidI7tL8)

### **Module 1: getting started on data**

**What you need:** any videos where you can see the animals/objects, etc.
You can use our demo videos, grab some from the internet, or use whatever older data you have. Any camera, color/monochrome, etc will work. Find diverse videos, and label what you want to track well :)
- IF YOU ARE PART OF THE COURSE: you will be contributing to the DLC Model Zoo 😊

   - **Slides:** [Overview of starting new projects](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials/blob/main/part1-labeling.pdf)
   - **READ ME PLEASE:** [DeepLabCut, the science](https://rdcu.be/4Rep)
   - **READ ME PLEASE:** [DeepLabCut, the user guide](https://rdcu.be/bHpHN)
   - **WATCH:** Video tutorial 1: [using the Project Manager GUI](https://www.youtube.com/watch?v=KcXogR-p5Ak)
     - Please go from project creation (use >1 video!) to labeling your data, and then check the labels!
   - **WATCH:** Video tutorial 2: [using the Project Manager GUI for multi-animal pose estimation](https://www.youtube.com/watch?v=Kp-stcTm77g)
     - Please go from project creation (use >1 video!) to labeling your data, and then check the labels!
   - **WATCH:** Video tutorial 3: [using ipython/pythonw (more functions!)](https://www.youtube.com/watch?v=7xwOhUcIGio)
      - multi-animal DLC: [labeling](https://www.youtube.com/watch?v=Kp-stcTm77g)
      - Please go from project creation (use >1 video!) to labeling your data, and then check the labels!


### **Module 2: Neural Networks**

   - **Slides:** [Overview of creating training and test data, and training networks](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials/blob/main/part2-network.pdf)
   - **READ ME PLEASE:** [What are convolutional neural networks?](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)

   - **READ ME PLEASE:** Here is a new paper from us describing challenges in robust pose estimation, why PRE-TRAINING really matters - which was our major scientific contribution to low-data input pose-estimation - and it describes new networks that are available to you. [Pretraining boosts out-of-domain robustness for pose estimation](https://paperswithcode.com/paper/pretraining-boosts-out-of-domain-robustness)

       - **MORE DETAILS:** ImageNet: check out the original paper and dataset: http://www.image-net.org/

   - **REVIEW PAPER:** [A Primer on Motion Capture with Deep Learning: Principles, Pitfalls and Perspectives](https://www.sciencedirect.com/science/article/pii/S0896627320307170)


 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1603101997909-7IEYAYZYE9C8AX6SK7GR/ke17ZwdGBToddI8pDm48kND1NDuHF9nqrgeclEdLoeR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z4YTzHvnKhyp6Da-NYroOW3ZGjoBKy3azqku80C789l0qN_-Z3B7EvygvPOPmeOryWYMQ3pkjXJ5SX4aMqPMuK4PimCRlyu3R6yKl-KltrlZA/networks.jpg?format=2500w" width="350" title="DLC" alt="review!" align="right" vspace = "50">

  Before you create a training/test set, please read/watch:
   - **More information:** [Which types neural networks are available, and what should I use?](https://github.com/DeepLabCut/DeepLabCut/wiki/What-neural-network-should-I-use%3F-(Trade-offs,-speed-performance,-and-considerations))
   - **WATCH:** Video tutorial 1: [How to test different networks in a controlled way](https://www.youtube.com/watch?v=WXCVr6xAcCA)
     - Now, decide what model(s) you want to test.
        - IF you want to train on your CPU, then run the step `create_training_dataset`, in the GUI etc. on your own computer.
        - IF you want to use GPUs on google colab, [**(1)** watch this FIRST/follow along here!](https://www.youtube.com/watch?v=qJGs8nxx80A) **(2)** move your whole project folder to Google Drive, and then [**use this notebook**](https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb)

        **MODULE 2 webinar**: https://youtu.be/ILsuC4icBU0


### **Module 3: Evaluation of network performance**

   - **Slides** [Evaluate your network](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials/blob/master/part3-analysis.pdf)
   - **WATCH:** [Evaluate the network in ipython](https://www.youtube.com/watch?v=bgfnz1wtlpo)
      - why evaluation matters; how to benchmark; analyzing a video and using scoremaps, conf. readouts, etc.

### **Module 4: Scaling your analysis to many new videos**

Once you have good networks, you can deploy them. You can create "cron jobs" to run a timed analysis script, for example. We run this daily on new videos collected in the lab. Check out a simple script to get started, and read more below:

   - [Analyzing videos in batches, over many folders, setting up automated data processing](https://github.com/DeepLabCut/DLCutils/tree/master/SCALE_YOUR_ANALYSIS)

  - How to automate your analysis in the lab: [datajoint.io](https://datajoint.io), Cron Jobs: [schedule your code runs](https://www.ostechnix.com/a-beginners-guide-to-cron-jobs/)

### **Module 5: Got Poses? Now what ...**

Pose estimation took away the painful part of digitizing your data, but now what? There is a rich set of tools out there to help you create your own custom analysis, or use others (and edit them to your needs). Check out more below:

   - [Helper code and packages for use on DLC outputs](https://github.com/DeepLabCut/DLCutils)

   - Create your own machine learning classifiers: https://scikit-learn.org/stable/

   - **REVIEW PAPER:**  [Toward a Science of Computational Ethology](https://www.sciencedirect.com/science/article/pii/S0896627314007934)

   - **REVIEW PAPER:** The state of animal pose estimation w/ deep learning i.e. "Deep learning tools for the measurement of animal behavior in neuroscience" [arXiv](https://arxiv.org/abs/1909.13868) & [published version](https://www.sciencedirect.com/science/article/pii/S0959438819301151)

   - **REVIEW PAPER:** [Big behavior: challenges and opportunities in a new era of deep behavior profiling](https://www.nature.com/articles/s41386-020-0751-7)

   - **READ**: [Automated measurement of mouse social behaviors using depth sensing, video tracking, and machine learning](https://www.pnas.org/content/112/38/E5351)


*compiled and edited by Mackenzie Mathis*


--- File: docs/roadmap.md ---
(dev-roadmap)=
## A development roadmap for DeepLabCut


📢 ⏳ 🚧

**General Enhancements:**
- [ ] DeepLabCut PyTorch & Model Zoo --> DLC 3.0 🔥
- [X] DLC-CookBook v0.1
- [X] DLC BLog for releases and user-highlights
- [X] New Docker containers into main repo / linked to Docker hub and repo(s)
- [ ] 3D >2 camera support --> better 3D in PyTorch version 🔥

**General NN Improvements:**
- [X] EfficientNet backbones added (currently SOTA on ImageNet). https://openaccess.thecvf.com/content/WACV2021/html/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html https://github.com/DeepLabCut/DeepLabCut/commit/96da2cacf837a9b84ecdeafb50dfb4a93b402f33
- [X] New multi-fusion multi-scale networks; DLCRNet_ms5
- [ ] BUCTD Integration, see ICCV 2023 paper at https://arxiv.org/abs/2306.07879

**deeplabcut 2.2: multi-animal pose estimation and tracking with DeepLabCut**
- [X] alpha testing complete (early May 2020)
- [X] beta release: 2.2.b5 on 5 / 22 / 20 :smile:
- [X] beta release: 2.2b8 released 9/2020 :smile:
- [X] beta release 2.2b9 (rolled into 2.1.9 --> candidate release, slotted for Oct 2020)
- [X] 2.2rc1
- [X] 2.2rc2
- [X] 2.2rc3
- [X] Manuscript Lauer et al 2021 https://www.biorxiv.org/content/10.1101/2021.04.30.442096v1
- [X] full 2.2 stable release

**real-time module with DEMO for how to set up on your camera system, integration with our [Camera Control Software]**(https://github.com/AdaptiveMotorControlLab/Camera_Control)
- [X] Integration with Bonsai completed! See: https://github.com/bonsai-rx/deeplabcut
- [X] Integration with Auto-pi-lot. See: https://auto-pi-lot.com/
- [X] DeepLabCut-live! released Aug 5th, 2020: preprint & code: https://www.biorxiv.org/content/10.1101/2020.08.04.236422v1
- [X] DeepLabCut-live! published in eLife

**DeepLabCut Model Zoo: a collection of pretrained models for plug-in-play DLC and community crowd-sourcing.**
- [X] BETA release with 2.1.8b0: https://www.mackenziemathislab.org/deeplabcut
- [X] full release with 2.1.8.1 https://www.mackenziemathislab.org/deeplabcut
- [X] Manuscript forthcoming! --> see arXiv https://arxiv.org/abs/2203.07436
- [X] new models added; horse, cheetah
- [X] TopView_Mouse model
- [X] Quadruped model
- [ ] contribution module
- [ ] PyTorch Model zoo code

**DeepLabCut GUI and DeepLabCut-core:**
- [X] to make DLC more modular, we will move core functions to https://github.com/DeepLabCut/DeepLabCut-core
- [X] DLC-core depreciated, and core is now simply `pip install deeplabcut` GUI is with `pip install deeplabcut[gui]`
- [X] new GUI for DeepLabCut; due to extended issues with wxPython, we will be moving to release a napari plugin https://github.com/napari/napari
- [X] New project management GUI
- [X] tensorflow 2.2 support in DeepLabCut-core: https://github.com/DeepLabCut/DeepLabCut/issues/601
- [X] DeepLabCut-Core to be depreciated; TF2 will go into main repo.
- [X] TF2 support while also maintaining TF1 support until 2022.
- [ ] Web-based GUI for labeling --> Colab training pipeline for users (full no-install DLC)


--- File: docs/ModelZoo.md ---
# The DeepLabCut Model Zoo! 

![image](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/8957c690-4f27-4430-8581-4161fd58d052/68747470733a2f2f696d616765732e73717561726573706163652d63646e2e636f6d2f636f6e74656e742f76312f3537663664353163396637343536366635356563663237312f313631363439323337333730302d50474f41433732494f4236415545343756544a582f6b6531375a77644742546f646449.png?format=450w)


## 🏠 [Home page](http://modelzoo.deeplabcut.org/)


Started in 2020, expanded in 2022 with PhD student [Shaokai Ye et al.](https://arxiv.org/abs/2203.07436v1), and the
first proper [SuperAnimal Foundation Models](#about-the-superanimal-models) published in 2024 🔥, the Model Zoo is four things:

- (1) a collection of models that are trained on diverse data across (typically) large datasets, which means you do not need to train models yourself, rather you can use them in your research applications.
- (2) a contribution website for community crowd sourcing of expertly labeled keypoints to improve models! You can get involved here: [contrib.deeplabcut.org](https://contrib.deeplabcut.org/).
- (3) a no-install DeepLabCut that you can use on ♾[Google Colab](https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/COLAB/COLAB_DEMO_SuperAnimal.ipynb), 
test our models in 🕸[the browser](https://contrib.deeplabcut.org/), or on our 🤗[HuggingFace](https://huggingface.co/spaces/DeepLabCut/DeepLabCutModelZoo-SuperAnimals) app!
- (4) new methods to make SuperAnimal Foundation Models that combine data across different labs/datasets, keypoints, animals/species, and use on your data!

## Quick Start:
```
pip install deeplabcut[gui,modelzoo]
```

## About the SuperAnimal Models

Animal pose estimation is critical in applications ranging from neuroscience to veterinary medicine. However, reliable inference of animal poses currently requires domain knowledge and labeling effort. To ease access to high-performance animal pose estimation models across diverse environments and species, we present a new paradigm for pre-training and fine-tuning that provides excellent zero-shot (no training required) performance on two major classes of animal pose data: quadrupeds and lab mice. 

To provide the community with easy access to such high performance models across diverse environments and species, we present a new paradigm for building pre-trained animal pose models -- which we call SuperAnimal models -- and the ability to use them for transfer learning (e.g., fine-tune them if needed).

## SuperAnimal members:
- Models are based on what they are trained on, for example `superanimal_quadruped_x` is trained on [SuperAnimal-Quadruped-80K](https://zenodo.org/records/10619173). Each model class is described below:



### SuperAnimal-Quadruped: 


- `superanimal_quadruped_x` models aim to work across a large range of quadruped animals, from horses, dogs, sheep, rodents, to elephants. The camera perspective is orthogonal to the animal ("side view"), and most of the data includes the animals face (thus the front and side of the animal). You will note we have several variants that differ in speed vs. performance, so please do test them out on your data to see which is best suited for your application. Also note we have a "video adaptation" feature, which lets you adapt your data to the model in a self-supervised way. No labeling needed!
- [Please see the full datasheet here](https://zenodo.org/records/10619173)
- [More details on the models (detector, pose estimators)](https://huggingface.co/mwmathis/DeepLabCutModelZoo-SuperAnimal-Quadruped)
- We provide several models:
    - `superanimal_quadruped_hrnetw32` (pytorch engine)
        - `superanimal_quadruped_hrnetw32` is a top-down model that is paired with a detector. That means it takes a cropped image from an object detector and predicts the keypoints. The object detector is currently a trained [ResNet50-based Faster-RCNN](https://pytorch.org/vision/stable/models/faster_rcnn.html).
    - `superanimal_quadruped_dlcrnet` (tensorflow engine)
        - `superanimal_quadruped_dlcrnet` is a bottom-up model that predicts all keypoints, then groups them into individuals. This can be faster, but more error prone.
    - `superanimal_quadruped` -> This is the same as `superanimal_quadruped_dlcrnet`, this was the old naming and being depreciated.
    - For all models, they are automatically downloaded to modelzoo/checkpoints when used.

- Here are example images of what the model is trained on:
![SA_Q](https://user-images.githubusercontent.com/28102185/209957688-954fb616-7750-4521-bb52-20a51c3a7718.png)



### SuperAnimal-TopViewMouse:


-  `superanimal_topviewmouse_x` aims to work across lab mice in different lab settings from a top-view perspective; this is very polar in many behavioral assays in freely moving mice.
- [Please see the full datasheet here](https://zenodo.org/records/10618947)
- [More details on the models (detector, pose estimators)](https://huggingface.co/mwmathis/DeepLabCutModelZoo-SuperAnimal-TopViewMouse)
- We provide several models:
    - `superanimal_topviewmouse_hrnetw32` (pytorch engine)
        - `superanimal_topviewmouse_hrnetw32` is a top-down model that is paired with a detector. That means it takes a cropped image from an object detector and predicts the keypoints. The object detector is currently a trained [ResNet50-based Faster-RCNN](https://pytorch.org/vision/stable/models/faster_rcnn.html).
    - `superanimal_topviewmouse_dlcrnet` (tensorflow engine)
        - `superanimal_topviewmouse_dlcrnet` is a bottom-up model that predicts all keypoints then groups them into individuals. This can be faster, but more error prone.
    - `superanimal_topviewmouse` -> This is the same as `superanimal_topviewmouse_dlcrnet`, this was the old naming and being depreciated.
    - For all models, they are automatically downloaded to modelzoo/checkpoints when used.
    
-  Here are example images of what the model is trained on:
![SA-TVM](https://user-images.githubusercontent.com/28102185/209957260-c0db72e0-4fdf-434c-8579-34bc5f27f907.png)


#### Practical example: Using SuperAnimal models for inference without training.

You can simply call the model and run video inference. 

To note, a good step is typically to use our self-supervised video adaptation method to reduce jitter. In the `deeplabcut.video_inference_superanimal` simply function set the `video_adapt` option to __True__. Be aware, that enabling this option will (minimally) extend the processing time. 

```python
import deeplabcut
video_path = "demo-video.mp4"
superanimal_name = "superanimal_quadruped"

deeplabcut.video_inference_superanimal([video_path],
                                        superanimal_name,
                                        model_name="hrnet_w32",
                                        detector_name="fasterrcnn_resnet50_fpn_v2",
                                        video_adapt = False)
```


#### Practical example: Using SuperAnimal model bottom up, considering video/animal size.

In our work we introduced a spatial-pyramid for smartly rescaling images. Imagine if you frames are much larger than what we trained on, it would be hard for the model to find the animal! Here, you can simply guide the model with the `scale_list`:

```python
import deeplabcut
video_path = "demo-video.mp4"
superanimal_name = "superanimal_quadruped"

# The purpose of the scale list is to aggregate predictions from various image sizes. We anticipate the appearance size of the animal in the images to be approximately 400 pixels.
scale_list = range(200, 600, 50)

deeplabcut.video_inference_superanimal([video_path],
                                        superanimal_name,
                                        model_name="hrnet_w32",
                                        detector_name="fasterrcnn_resnet50_fpn_v2",
                                        scale_list=scale_list,
                                        video_adapt = False)
```

#### Practical example: Using transfer learning with superanimal weights.
In the `deeplabcut.train_network` function, the `superanimal_transfer_learning` option plays a pivotal role. If it's set to __True__, it uses a new decoding layer and allows you to use superanimal weights in any project, no matter the number of keypoints. However, if it's set to __False__, you are doing fine-tuning. So, make sure your dataset has the right number of keypoints.  

Specifically:
* `superanimal_quadruped_x` uses 39 keypoints and,
* `superanimal_topviewmouse_x` uses 27 keypoints

```python
import os
import deeplabcut
from deeplabcut.modelzoo import build_weight_init

superanimal_name = "superanimal_topviewmouse"

config_path = os.path.join(os.getcwd(), "openfield-Pranav-2018-10-30", "config.yaml")

weight_init = build_weight_init(
    cfg=config_path,
    super_animal=superanimal_name,
    model_name="hrnet_w32",
    detector_name="fasterrcnn_resnet50_fpn_v2",
    with_decoder=False,
)

deeplabcut.create_training_dataset(config_path, weight_init = weight_init)

deeplabcut.train_network(config_path,
                         epochs=10,
                         superanimal_name = superanimal_name,
                         superanimal_transfer_learning = True)
```

### Potential failure modes for SuperAnimal Models and how to fix it.

Spatial domain shift: typical DNN models suffer from the spatial resolution shift between training datasets and test
videos. To help find the proper resolution for our model, please try a range of `scale_list` in the API (details in the
API docs). For `superanimal_quadruped`, we empirically observe that if your video is larger than 1500 pixels, it is
better to pass `scale_list` in the range within 1000.

Pixel statistics domain shift: The brightness of your video might look very different from our training datasets.
This might either result in jittering predictions in the video or fail modes for lab mice videos (if the brightness of
the mice is unusual compared to our training dataset). You can use our "video adaptation" model to counter this.



### Our longer term perspective ...

Via DeepLabCut Model Zoo, we aim to provide plug and play models that do not need any labeling and will just work
decently on novel videos. If the predictions are not great enough due to failure modes described below, please give us
feedback! We are rapidly improving our models and adaptation methods. We will also continue to expand this project to
new model/data classes. Please do get in touch is you have data or ideas: modelzoo@deeplabcut.org

## Publication:

To see the first preprint on the work, click [here](https://arxiv.org/abs/2203.07436v1).

Our first [publication](https://www.nature.com/articles/s41467-024-48792-2) on this project is now published at Nature
Communications:

```{hint}
Here is the citation:
@article{Ye2024,
  title={SuperAnimal pretrained pose estimation models for behavioral analysis},
  author={Shaokai Ye and Anastasiia Filippova and Jessy Lauer and Steffen Schneider and Maxime Vidal and Tian Qiu and Alexander Mathis and Mackenzie Weygandt Mathis},
  journal={Nature Communications},
  year={2024},
  preprint={abs/2203.07436}
}
```


--- File: docs/citation.md ---
# How to Cite DeepLabCut

Thank you for using DeepLabCut! Here are our recommendations for citing and documenting your use of DeepLabCut in your Methods section:


If you use this code or data we kindly ask that you please [cite Mathis et al, 2018](https://www.nature.com/articles/s41593-018-0209-y)
and, if you use the Python package (DeepLabCut2.x+) please also cite [Nath, Mathis et al, 2019](https://doi.org/10.1038/s41596-019-0176-0).
If you utilize the MobileNetV2s or EfficientNets please cite [Mathis, Biasi et al. 2021](https://openaccess.thecvf.com/content/WACV2021/papers/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.pdf).
If you use multi-animal versions 2.2beta+ or 2.2rc1+, please cite [Lauer et al. 2022](https://www.nature.com/articles/s41592-022-01443-0).
If you use our SuperAnimal models, please cite [Ye et al. 2024](https://www.nature.com/articles/s41467-024-48792-2).

DOIs (#ProTip, for helping you find citations for software, check out [CiteAs.org](http://citeas.org/)!):

- Mathis et al 2018: [10.1038/s41593-018-0209-y](https://doi.org/10.1038/s41593-018-0209-y)
- Nath, Mathis et al 2019: [10.1038/s41596-019-0176-0](https://doi.org/10.1038/s41596-019-0176-0)
- Lauer et al 2022: [10.1038/s41592-022-01443-0](https://doi.org/10.1038/s41592-022-01443-0)
- Ye et al 2024: [10.1038/s41467-024-48792-2](https://www.nature.com/articles/s41467-024-48792-2)

## Formatted citations:

    @article{Mathisetal2018,
        title = {DeepLabCut: markerless pose estimation of user-defined body parts with deep learning},
        author = {Alexander Mathis and Pranav Mamidanna and Kevin M. Cury and Taiga Abe  and Venkatesh N. Murthy and Mackenzie W. Mathis and Matthias Bethge},
        journal = {Nature Neuroscience},
        year = {2018},
        url = {https://www.nature.com/articles/s41593-018-0209-y}}

     @article{NathMathisetal2019,
        title = {Using DeepLabCut for 3D markerless pose estimation across species and behaviors},
        author = {Nath*, Tanmay and Mathis*, Alexander and Chen, An Chi and Patel, Amir and Bethge, Matthias and Mathis, Mackenzie W},
        journal = {Nature Protocols},
        year = {2019},
        url = {https://doi.org/10.1038/s41596-019-0176-0}}
        
    @InProceedings{Mathis_2021_WACV,
        author    = {Mathis, Alexander and Biasi, Thomas and Schneider, Steffen and Yuksekgonul, Mert and Rogers, Byron and Bethge, Matthias and Mathis, Mackenzie W.},
        title     = {Pretraining Boosts Out-of-Domain Robustness for Pose Estimation},
        booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
        month     = {January},
        year      = {2021},
        pages     = {1859-1868}}
        
    @article{Lauer2022MultianimalPE,
        title={Multi-animal pose estimation, identification and tracking with DeepLabCut},
        author={Jessy Lauer and Mu Zhou and Shaokai Ye and William Menegas and Steffen Schneider and Tanmay Nath and Mohammed Mostafizur Rahman and     Valentina Di Santo and Daniel Soberanes and Guoping Feng and Venkatesh N. Murthy and George Lauder and Catherine Dulac and M. Mathis and Alexander Mathis},
        journal={Nature Methods},
        year={2022},
        volume={19},
        pages={496 - 504}}

    @article{Ye2024SuperAnimal,
        title={SuperAnimal pretrained pose estimation models for behavioral analysis},
        author={Shaokai Ye and Anastasiia Filippova and Jessy Lauer and Steffen Schneider and Maxime Vidal and and Tian Qiu and Alexander Mathis and Mackenzie W. Mathis},
        journal={Nature Communications},
        year={2024},
        volume={15}}


### Review & Educational articles:

    @article{Mathis2020DeepLT,
        title={Deep learning tools for the measurement of animal behavior in neuroscience},
        author={Mackenzie W. Mathis and Alexander Mathis},
        journal={Current Opinion in Neurobiology},
        year={2020},
        volume={60},
        pages={1-11}}

    @article{Mathis2020Primer,
        title={A Primer on Motion Capture with Deep Learning: Principles, Pitfalls, and Perspectives},
        author={Alexander Mathis and Steffen Schneider and Jessy Lauer and Mackenzie W. Mathis},
        journal={Neuron},
        year={2020},
        volume={108},
        pages={44-65}}

### Other open-access pre-prints related to our work on DeepLabCut:

    @article{MathisWarren2018speed,
        author = {Mathis, Alexander and Warren, Richard A.},
        title = {On the inference speed and video-compression robustness of DeepLabCut},
        year = {2018},
        doi = {10.1101/457242},
        publisher = {Cold Spring Harbor Laboratory},
        URL = {https://www.biorxiv.org/content/early/2018/10/30/457242},
        eprint = {https://www.biorxiv.org/content/early/2018/10/30/457242.full.pdf},
        journal = {bioRxiv}}



## Methods Suggestion:

For body part tracking we used DeepLabCut (version 2.X.X)* [Mathis et al, 2018, Nath et al, 2019, Lauer et al. 2022]. Specifically, we labeled X number of frames taken from X videos/animals (then X% was used for training (default is 95%). We used a X-based neural network (i.e. X = ResNet-50, ResNet-101, MobileNetV2-0.35, MobileNetV2-0.5, MobileNetV2-0.75, MobileNetV2-1***) with default parameters* for X number of training iterations. We validated with X number of shuffles, and found the test error was: X pixels, train: X pixels (image size was X by X). We then used a p-cutoff of X (i.e. 0.9) to condition the X,Y coordinates for future analysis. This network was then used to analyze videos from similar experimental settings. 

> Mathis, A. et al. Deeplabcut: markerless pose estimation
> of user-defined body parts with deep learning. Nature
> Neuroscience 21, 1281–1289 (2018).

> Nath, T. et al. Using deeplabcut for 3d markerless pose
> estimation across species and behaviors. Nature Protocols
> 14, 2152–2176 (2019).

*If any defaults were changed in *`pose_config.yaml`*, mention them here. 

i.e. common things one might change: 
* the loader (options are `default`, `imgaug`, `tensorpack`, `deterministic`). 
* the `post_dist_threshold` (default is 17 and determines training resolution).
* optimizer: do you use the default `SGD` or `ADAM`? 

*** here, you could add additional citations. 
If you use ResNets, consider citing Insafutdinov et al 2016 & He et al 2016. If you use the MobileNetV2s consider citing Mathis et al 2019, and Sandler et al, 2018.


> Mathis, A. et al. Pretraining boosts out-of-domain robustness for pose estimation
> arXiv 1909.11229 (2019)

> Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
> M. & Schiele, B. DeeperCut: A deeper, stronger, and
> faster multi-person pose estimation model. In European
> Conference on Computer Vision, 34–50 (Springer, 2016).

> Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. &
> Chen, L.-C. Mobilenetv2: Inverted residuals and linear
> bottlenecks. In Proceedings of the IEEE Conference
> on Computer Vision and Pattern Recognition, 4510–4520
> (2018).

> He, K., Zhang, X., Ren, S. & Sun, J. Deep residual
> learning for image recognition. In Proceedings of the
> IEEE conference on computer vision and pattern recognition,
> 770–778 (2016). URL https://arxiv.org/abs/
> 1512.03385.

## Graphics 

We also have the network graphic freely available on SciDraw.io if you'd like to use it! https://scidraw.io/drawing/290

You are welcome to use our logo in your works as well.


--- File: docs/pytorch_dlc.md ---
# DeepLabCut: PyTorch API

## Modules

- [data](https://github.com/nastya236/DLCdev/blob/69005057eeac3c1492712863303f8268cee776e6/deeplabcut/pose_estimation_pytorch/data/project.py#L7):
The `deeplabcut.pose_estimations_pytorch.data` package contains all code for pytorch 
dataset creation and test/train splitting.
  - `Project` class provides train and test splitting and converts dataset to required
  format. For instance, to [COCO]() format.
  - `PoseTrainDataset` class is a [torch.utils.Dataset](https://pytorch.org/docs/stable/data.html) class, which converts raw 
  images and keypoints to a tensor dataset for training and evaluation.
- [models](https://github.com/nastya236/DLCdev/blob/69005057eeac3c1492712863303f8268cee776e6/deeplabcut/pose_estimation_pytorch/data/models):
The `deeplabcut.pose_estimations_pytorch.models` package contains all related to 
building a model with `backbone`, `neck` (optional) and `head`.
- [train_module](https://github.com/nastya236/DLCdev/blob/69005057eeac3c1492712863303f8268cee776e6/deeplabcut/pose_estimation_pytorch/data/models):
The `deeplabcut.pose_estimations_pytorch.train_module` contains all classes for model 
training and validation.

## API

The PyTorch implementation of DeepLabCut is very similar to the Tensorflow multi-animal
implementation: the same steps need to be followed, just with slightly different API 
calls (and different model names).

Up until it's time to create the training dataset, there are no changes to the way a
PyTorch or Tensorflow project should be created.

### Creating a Training Dataset

To create a training dataset for a DeepLabCut PyTorch model, simply call:
```python
import deeplabcut
deeplabcut.create_training_dataset(
    path_config_file,
    net_type="dekr_32",
)
```

This will create folders for the training dataset in the same way as the Tensorflow
version, with an addition configuration file in the `train` folder: 
`pytorch_config.yaml`. This is the file that can be edited to modify the model 
architecture or training parameters.

There are currently two "families" of models implemented in PyTorch: DEKR (Geng, Zigang,
et al. "Bottom-up human pose estimation via disentangled keypoint regression." 
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 
2021.) and Tokenpose (Li, Yanjie, et al. "Tokenpose: Learning keypoint tokens for human
pose estimation." Proceedings of the IEEE/CVF International conference on computer 
vision. 2021.). The choices of `net_type` that will create PyTorch training sets are:
- `"dekr_16"`
- `"dekr_32"`
- `"dekr_48"`
- `"token_pose_w16"`
- `"token_pose_w32"`
- `"token_pose_w48"`

Note that Tokenpose models cannot currently be used with projects that contain unique 
keypoints. 

### Training the network
Training a PyTorch model is done in a very similar manner as a tensorflow model, though 
currently the PyTorch API needs to be called directly:
```python
import deeplabcut.pose_estimation_pytorch.apis as api
api.train_network(config_path, shuffle=1, trainingsetindex=0)
```

**Parameters**
```
config : path to the yaml config file of the project
shuffle : index of the shuffle we want to train on
trainingsetindex : training set index
transform: Augmentation pipeline for the images
    if None, the augmentation pipeline is built from config files
    Advice if you want to use custom transformations:
        Keep in mind that in order for transfer learning to be efficient, your
        data statistical distribution should resemble the one used to pretrain your backbone
        In most cases (e.g backbone was pretrained on ImageNet), that means it should be Normalized with
        A.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])
transform_cropped: Augmentation pipeline for the cropped images around animals
    if None, the augmentation pipeline is built from config files
    Advice if you want to use custom transformations:
        Keep in mind that in order for transfer learning to be efficient, your
        data statistical distribution should resemble the one used to pretrain your backbone
        In most cases (e.g backbone was pretrained on ImageNet), that means it should be Normalized with
        A.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])
modelprefix: directory containing the deeplabcut configuration files to use
    to train the network (and where snapshots will be saved). By default, they
     are assumed to exist in the project folder.
snapshot_path: if resuming training, used to specify the snapshot from which to resume
detector_path: if resuming training of a top down model, used to specify the detector snapshot from
    which to resume
**kwargs : could be any entry of the pytorch_config dictionary. Examples are
    to see the full list see the pytorch_cfg.yaml file in your project folder
```

### Evaluating the network
As for training, the main difference is the need to call the API directly.
```python
import deeplabcut.pose_estimation_pytorch.apis as api
api.evaluate_network(config_path, shuffle=1, trainingsetindex="all")
```

**Parameters**
```
config: path to the project's config file
shuffles: Iterable of integers specifying the shuffle indices to evaluate.
trainingsetindex: Integer specifying which training set fraction to use.
    Evaluates all fractions if set to "all"
snapshotindex: index (starting at 0) of the snapshot we want to load. To
    evaluate the last one, use -1. To evaluate all snapshots, use "all". For
    example if we have 3 models saved
        - snapshot-0.pt
        - snapshot-50.pt
        - snapshot-100.pt
    and we want to evaluate snapshot-50.pt, snapshotindex should be 1. If None,
    the snapshotindex is loaded from the project configuration.
plotting: Plots the predictions on the train and test images. If provided it must
    be either ``True``, ``False``, ``"bodypart"``, or ``"individual"``. Setting
    to ``True`` defaults as ``"bodypart"`` for multi-animal projects.
show_errors: display train and test errors.
transform: transformation pipeline for evaluation
    ** Should normalise the data the same way it was normalised during training **
modelprefix: directory containing the deeplabcut models to use when evaluating
    the network. By default, they are assumed to exist in the project folder.
batch_size: the batch size to use for evaluation
```

### Analyzing novel videos
One big difference between the PyTorch and Tensorflow implementations comes in the way
animal assembly happens (for multi-animal models). While in Tensorflow, assembly was a
separate step that needed to be done from the keypoints, in the PyTorch version it's 
integrated directly into the models. From an API standpoint, that does not change much.

Again, the PyTorch API needs to be invoked directly (it also has the `auto_track` 
option).
```python
import deeplabcut.pose_estimation_pytorch.apis as api
api.analyze_videos(config_path, ["/fullpath/project/videos/test.mp4"], videotype=".mp4")
```

The PyTorch detections need to be converted to tracklets using the PyTorch API, but then
the original tracklet stitching can be used. 
```python
import deeplabcut
import deeplabcut.pose_estimation_pytorch.apis as api
api.convert_detections2tracklets(
    config_path,
    videos=['/fullpath/project/videos/test.mp4'],
    videotype=".mp4",
)
deeplabcut.stitch_tracklets(
    config_path,
    videos=['/fullpath/project/videos/test.mp4'],
    videotype=".mp4",
)
```

Creating labeled videos can then be called in exactly the same way as before. 
```python
import deeplabcut
deeplabcut.create_labeled_video(
    config_path,
    videos=['/fullpath/project/videos/test.mp4'],
    videotype=".mp4",
)
```


--- File: docs/deeplabcutlive.md ---
(deeplabcut-live)=
# DeepLabCut-Live!

We provide two additional pip packages that allow you to record and stream camera data and run DeeplabCut models in real-time.
You can get an indepth overview of this work in [Kane et al, 2020 eLife](https://elifesciences.org/articles/61909).

Here is information on the DLC-Live! Camera GUI:

- [DLC-Live! Camera GUI](https://github.com/DeepLabCut/DeepLabCut-live-GUI)

Here is information on the DLC-Live! software SDK:

- [DLC-Live! Software](https://github.com/DeepLabCut/DeepLabCut-live)


--- File: docs/docker.md ---
(docker-containers)=
# DeepLabCut Docker containers

For DeepLabCut 2.2.0.2 and onwards, we provide container containers on [DockerHub](https://hub.docker.com/r/deeplabcut/deeplabcut). Using Docker is an alternative approach to using DeepLabCut, which only requires the user to install [Docker](https://www.docker.com/) on your machine, vs. following the step-by-step installation guide for a Anaconda setup. All dependencies needed to run DeepLabCut in terminal or GUI mode, or running Jupyter notebooks with DeepLabCut pre-installed are shipped with the provided Docker images.

Advanced users can directly head to [DockerHub](https://hub.docker.com/r/deeplabcut/deeplabcut) and use the provided images there. To get started with using the images, we however also provide a helper tool, `deeplabcut-docker`, which makes the transition to docker images particularly convenient; to install the tool, run

``` bash
$ pip install deeplabcut-docker
```

on your machine (potentially in a virtual environment, or an existing Anaconda environment).
Note that this will *not* disprupt or install Tensorflow, or any other DeepLabCut dependencies on your computer---the Docker containers are completely isolated from your existing software installation!

## Usage modes

With `deeplabcut-docker`, you can use the images in two modes.

- *Note 1: When running any of the following commands first, it can take some time to complete (a few minutes, depending on your internet connection), since it downloads the Docker image in the background. If you do not see any errors in your terminal, assume that everything is working fine! Subsequent runs of the command will be faster.*
- *Note 2: The Terminal mode image can be used on all supported platforms (Linux and MacOS). The GUI images can only be considered stable on Linux systems as of DeepLabCut 2.2.0.2 and need additional configuration on Mac.*
- *Note 3: For any mode below, you might want to set which directory is the base, namely, so you can have read/write (or read-only access). Here is how to do so:
If you want to mount the whole directory could e.g., pass*

`deeplabcut-docker bash -v /home/mackenzie/DEEPLABCUT:/home/mackenzie/DEEPLABCUT`

(which will mount the full directory into the container in read/write mode)

If read-only access is enough, `deeplabcut-docker bash -v /home/mackenzie/DEEPLABCUT:/home/mackenzie/DEEPLABCUT:ro`


### Terminal mode 

If you not need the GUI, you can run the light version of DeepLabCut and open a terminal by running

``` bash
$ deeplabcut-docker bash
```

Inside the terminal, you can confirm that DeepLabCut is correctly installed by running and noting which version installs.

``` bash
$ ipython
>>> import deeplabcut
```

### Jupyter Notebook mode

Finally, you can run DeepLabCut by starting a jupyter notebook server. The corresponding image can be pulled and started by running

``` bash
$ deeplabcut-docker notebook 
```

which will start a Jupyter notebook server. Follow the terminal instructions to open the notebook, by entering `http://127.0.0.1:8888` in your favorite browser. When prompted for a password, use `deeplabcut`, which is the pre-set option in the container.

The DeepLabCut version in this container is equivalent to the one you install with `pip install deeplabcut[gui]`. This means that you can start the DeepLabCut GUI with the appropriate commands in your notebook!

### Advanced usage

Advanced users and developers can visit the `/docker` subdirectory in the DeepLabCut codebase on Github. We provide Dockerfiles for all images, along with build instructions there.

## Prerequisites (if you don't have Docker installed already)

**(1)** Install Docker. See https://docs.docker.com/install/ & for Ubuntu: https://docs.docker.com/install/linux/docker-ce/ubuntu/
Test docker: 

    $ sudo docker run hello-world
    
 The output should be: ``Hello from Docker! This message shows that your installation appears to be working correctly.``

*if you get the error ``docker: Error response from daemon: Unknown runtime specified nvidia.`` just simply restart docker: 
  
       $ sudo systemctl daemon-reload
       $ sudo systemctl restart docker

    
**(2)** Add your user to the docker group (https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user)
Quick guide  to create the docker group and add your user: 
Create the docker group.

    $ sudo groupadd docker
Add your user to the docker group.

    $ sudo usermod -aG docker $USER

(perhaps restart your computer (best) or (at min) open a new terminal to make sure that you are added from now on)


## Notes and troubleshooting

We dropped GUI support in 2.3.5+ due to too many numerous issues supporting them. Also please note these are tested on unix systems.

When running containers on Linux, in some systems it might be necessary to run `host +local:docker` before starting the image via `deeplabcut-docker`.

If you encounter errors while using the images, please open an issue in the DeepLabCut repo---especially the `deeplabcut-docker` is still in its alpha version, and we appreciate user feedback to make the tool robust to use across many operating systems!


--- File: docs/installation.md ---
(how-to-install)=
# How To Install DeepLabCut

- DeepLabCut can be run on Windows, Linux, or MacOS as long as you have Python 3.10 installed (see also [technical considerations](tech-considerations-during-install) and if you run into issues also check out the [Installation Tips](https://deeplabcut.github.io/DeepLabCut/docs/recipes/installTips.html) page).
- Please note, there are several modes of installation, and the user should decide to either use a **system-wide** (see [note below](system-wide-considerations-during-install)), **conda environment** based installation (**recommended**), or the supplied [**Docker container**](docker-containers) (recommended for Ubuntu advanced users). One can of course also use other Python distributions than Anaconda, but **Anaconda is the easiest route.**
- We recommend for most users to use our supplied CONDA environment.
- Please note, you will get the best performance with using a GPU! Please see the section on [GPU support](https://deeplabcut.github.io/DeepLabCut/docs/installation.html#gpu-support) to install your GPU driver and CUDA.

````{admonition} Familiar with python packages and conda? Quick install here.
:class: dropdown

This assumes you have `conda`/`mamba` installed and installs DeepLabCut in a fresh
environment. If you have an NVIDIA GPU, install PyTorch according to [their instructions
](https://pytorch.org/get-started/locally/) (with your desired CUDA version) - you just
need your GPU drivers installed.

```bash
conda create -n DEEPLABCUT python=3.10
conda activate DEEPLABCUT
conda install -c conda-forge pytables==3.8.0

# install torch with your desired CUDA version (or CPU) - check their website 
# for the exact command
pip install torch torchvision 

# install the latest version of DeepLabCut
pip install --pre deeplabcut
# or if you want to use the GUI
pip install deeplabcut[gui]

# ONLY IF YOU HAVE A CUDA GPU - check that PyTorch can access your GPU; this
# should print `True`
python -c "import torch; print(torch.cuda.is_available())"
```

Why do we install [pytables](https://www.pytables.org/usersguide/installation.html) with
`conda` and not `pip`? Because it requires some libraries that not all users will have
installed, and conda will ensure that they are installed as well.

If you're familiar with the command line and want TensorFlow support, look [below](
deeplabcut-with-tf-install) for a fresh installation that has worked for us (on Linux)
and makes it possible to use the GPU with both PyTorch and TensorFlow.
````

## CONDA: The installation process is as easy as this figure! -->

 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/71e5d954-75a0-4534-9fa6-7ecc4bf1b76d/installDLC.png?format=1500w" width="250" title="DLC" alt="DLC" align="right" vspace = "50">

#### 🚨 Before you start, do you have a GPU?
````{admonition} 🚨 Click here for more information!
:class: dropdown
- We recommend having a GPU if possible!
- You **need to decide if you want to use a CPU or GPU for your models**: (Note, you can also use the CPU-only for project management and labeling the data! Then, for example, use Google Colaboratory GPUs for free (read more [here](https://github.com/DeepLabCut/DeepLabCut/tree/master/examples#demo-4-deeplabcut-training-and-analysis-on-google-colaboratory-with-googles-gpus) and there are a lot of helper videos on [our YouTube channel!](https://www.youtube.com/playlist?list=PLjpMSEOb9vRFwwgIkLLN1NmJxFprkO_zi)).

  - **CPU?** Great, jump to the next section below!

  - **NVIDIA GPU?**  If you want to use your own GPU (i.e., a GPU is in your workstation), then you need to be sure you have a CUDA compatible GPU, CUDA, and cuDNN installed. Please note, which CUDA you install depends on what version of PyTorch you want to use. So, please check "GPU Support" below carefully. **Note, DeepLabCut is up to date with the latest CUDA and PyTorch!**
  
  - **Apple M-chip GPU?** Be sure to install miniconda3, and your GPU will be used by default.
````

### Step 1: Install Python via Anaconda

#### Install [anaconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html#), or use miniconda3 for MacOS users (see below)

- Anaconda is an easy way to install Python and additional packages across various operating systems. With Anaconda you create all the dependencies in an [environment](https://conda.io/docs/user-guide/tasks/manage-environments.html) on your machine.

```{Hint}
Download anaconda for your operating system: [anaconda.com/download/
](https://www.anaconda.com/download/)
```

- IF you use a M1 or M2 chip in your MacBook with v12.5+ (typically 2020 or newer machines), we recommend **miniconda3,** which operates with the same principles as anaconda. This is straight forward and explained in detail here: https://docs.conda.io/projects/conda/en/latest/user-guide/install/macos.html. But in short, open the program "terminal" and copy/paste and run the code that is supplied below.

#### 💡 miniconda for Mac
````{admonition} Click the button to see code for miniconda for Mac
:class: dropdown
wget https://repo.anaconda.com/miniconda/Miniconda3-py310_4.12.0-MacOSX-arm64.sh -O ~/miniconda.sh
bash ~/miniconda.sh -b -p $HOME/miniconda
source ~/miniconda/bin/activate
conda init zsh
````

### Step 2: Build an Env using our Conda file!

You simply need to have this `.yaml` file anywhere locally on your computer. So, let's download it!

```{Hint}
Windows users: Be sure you have `git` installed along with anaconda: https://gitforwindows.org/
```

- TO DIRECTLY DOWNLOAD THE CONDA FILE conda:

  - click ➡️ for [CONDA FILE](https://github.com/DeepLabCut/DeepLabCut/blob/main/conda-environments/DEEPLABCUT.yaml#:~:text=Raw%20file%20content-,Download,-%E2%8C%98) and then click the "..." and select Download
    <img width="274" alt="Screen Shot 2023-09-13 at 10 33 32 PM" src="https://github.com/DeepLabCut/DeepLabCut/assets/28102185/ec4295a5-e85c-4ce7-8c16-e6517a2cfa22">

-  **Now, in Terminal (or Anaconda Command Prompt for Windows users)**, if you clicked to download, go to your downloads folder.

```{Hint}
Windows users: Be sure to open the program terminal/cmd/anaconda prompt with a RIGHT-click, "open as admin"
```

```{Hint}
:class: dropdown
If you cloned the repo onto your Desktop, the command may look like:
``cd C:\Users\YourUserName\Desktop\DeepLabCut\conda-environments``
You can (on Windows) hold SHIFT and right-click > Copy as path, or (on Mac) right-click and while in the menu press the OPTION key to reveal Copy as Pathname.
```
Be sure you are in the folder that has the `.yaml` file, then run:

``conda env create -f DEEPLABCUT.yaml``


- You can now use this environment from anywhere on your computer (i.e., no need to go back into the conda- folder). Just enter your environment by running:
     - Ubuntu/MacOS: ``source/conda activate nameoftheenv`` (i.e. on your Mac: ``conda activate DEEPLABCUT``)
     - Windows: ``activate nameoftheenv`` (i.e. ``activate DEEPLABCUT``)

Now you should see (`nameofenv`) on the left of your terminal screen, i.e. ``(DEEPLABCUT) YourName-MacBook...``
NOTE: no need to run pip install deeplabcut, as it is already installed!!! :)

(deeplabcut-with-tf-install)=
#### 💡 Notice: PyTorch and TensorFlow Support within DeepLabCut

````{admonition} DeepLabCut TensorFlow Support
:class: dropdown
As of June 2024 we have a PyTorch Engine backend and we will be depreciating the 
TensorFlow backend by the end of 2024. Currently, if you want to use TensorFlow, you 
need to run `pip install deeplabcut[tf]` in order to install the correct version of 
TensorFlow in your conda env. Please note, we will be providing bug fixes, but we will 
not be supporting new TensorFlow versions beyond 2.10 (Windows), and 2.12 for other OS.

Installing TensorFlow and getting it to have access to the GPU can be a bit tricky. 
Check TensorFlow's [compatibility matrix](https://www.tensorflow.org/install/source#gpu)
to know which version of CUDA and cuDNN you should install.

We have found that installing DeepLabCut with the following commands works well for
Linux users to install PyTorch 2.3.1, TensorFlow 2.12, CUDA 11.8 and cuDNN 8 in a Conda
environment:

```bash
conda create -n deeplabcut-with-tf "python=3.10"
conda activate deeplabcut-with-tf

# Install the desired TensorFlow version, built for CUDA 11.8 and cuDNN 8
pip install "tensorflow==2.12" "tensorpack>=0.11" "tf_slim>=1.1.0"

# Install PyTorch with a version using CUDA 11.8 and cuDNN 8
pip install "torch==2.3.1" torchvision --index-url https://download.pytorch.org/whl/cu118

# Create symbolic links to NVIDIA shared libraries for TensorFlow
#   -> as described in their installation docs:
#      https://www.tensorflow.org/install/pip#step-by-step_instructions

pushd $(dirname $(python -c 'print(__import__("tensorflow").__file__)'))
ln -svf ../nvidia/*/lib/*.so* .
popd

pip install  --pre deeplabcut
```
````

**Great, that's it! DeepLabCut is installed!** 🎉💜


### Step 3: Really, that's it! Let's run DeepLabCut

Head over to the [User Guide Overview](https://deeplabcut.github.io/DeepLabCut/docs/UseOverviewGuide.html) for information. 

🎉 Launch DeepLabCut in your new env by running `python -m deeplabcut`

## Other ways to install DeepLabCut and additional tips

### Alternatively, you can git clone this repo and install from source!
i.e., if the download did not work or you just want to have the source code handy!

- **Windows/Linux/MacBooks:** git clone this repo (in the terminal/cmd program, while **in a folder** you wish to place DeepLabCut
To git clone type: ``git clone https://github.com/DeepLabCut/DeepLabCut.git``). Note, this can be anywhere, even downloads is fine.)
- Then follow the same steps as in Step 2 above, adjusting for the file now being in the downloaded folder.

### PIP:

- Everything you need to build custom models within DeepLabCut (i.e., use our source code and our dependencies) can be installed with `pip install 'deeplabcut[gui]'` (for GUI support w/PyTorch) or without the gui: `pip install 'deeplabcut'`.
- If you want to use the SuperAnimal models, then please use `pip install 'deeplabcut[gui,modelzoo]'`. 

## DOCKER:

- We also have docker containers. Docker is the most reproducible way to use and deploy code. Please see our dedicated docker package and page [here](https://deeplabcut.github.io/DeepLabCut/docs/docker.html).

## Pro Tips:

More [installation ProTips](installation-tips) are also available.

If you ever want to update your DLC, just run `pip install --upgrade deeplabcut` once
you are inside your env. If you want to use a specific release, then you need to specify
the version you want, such as `pip install deeplabcut==3.0`. Once installed, you can
check the version by running `import deeplabcut` `deeplabcut.__version__`. Don't be 
afraid to update, DLC is backwards compatible with your 2.0+ projects and performance 
continues to get better and new features are added nearly monthly.

**All of the data you labelled in version 2.X is also compatible with version 3+ and the
PyTorch engine**! There is no change in the workflow or the way labels are handled: the
big changes happen under-the-hood! If you've been working with DeepLabCut 2.X and want
to learn more about moving to the PyTorch engine, checkout our docs on [moving from 
TensorFlow to PyTorch](dlc3-user-guide)

Here are some conda environment management tips: [kapeli.com: Conda Cheat Sheet](
https://kapeli.com/cheat_sheets/Conda.docset/Contents/Resources/Documents/index)

**Pro Tip:** If you want to modify code and then test it, you can use our provided 
testscripts. This would mean you need to be up-to-date with the latest GitHub-based code
though! Please see [here](installation-tips) on how to get the latest GitHub code, and
how to test your installation by following this video: 
https://www.youtube.com/watch?v=IOWtKn3l33s.

## Creating your own customized conda env (recommended route for Linux: Ubuntu, CentOS, Mint, etc.)

*Note in a fresh ubuntu install, you will often have to run: ``sudo apt-get install gcc python3-dev`` to install the GNU Compiler Collection and the python developing environment.

Some users might want to create their own customize env. -  Here is an example.

In the terminal type:

`conda create -n DLC python=3.10`

**Current version:** The only thing you then need to add to the env is deeplabcut (
`pip install deeplabcut`) or `pip install 'deeplabcut[gui]'` which has a napari based
GUI.


## **GPU Support:**

The ONLY thing you need to do **first** if you have an NVIDIA GPU and the matching NVIDIA CUDA+driver installed.
- CUDA: https://developer.nvidia.com/cuda-downloads (just follow the prompts here!)
- DRIVERS: https://www.nvidia.com/Download/index.aspx

### The most common "new user" hurdle is installing and using your GPU, so don't get discouraged!

**CRITICAL:** If you have a GPU, you should FIRST **install an appropriate driver for 
your specific GPU**, then you can use the supplied conda file. You'll need an NVIDIA GPU
which is compatible with CUDA. To see a list of CUDA-enabled NVIDIA GPUs, please [see 
their website](https://developer.nvidia.com/cuda-gpus).

- Here we provide notes on how to install and check your GPU use with TensorFlow (which
is used by DeepLabCut and already installed with the Anaconda files above). Thus, you do
not need to independently install tensorflow.

**FIRST**, install a driver for your GPU. Find DRIVER HERE: 
https://www.nvidia.com/download/index.aspx

- Check which driver is installed by typing this into the terminal: ``nvidia-smi``.

**SECOND**, install CUDA: https://developer.nvidia.com/ (Note that cuDNN, https://developer.nvidia.com/cudnn, is supplied inside the anaconda environment files, so you don't need to install it again).

**THIRD:** Follow the steps above to get the `DEEPLABCUT` conda file and install it!

#### Notes:

- **As of version 3.0+ we moved to PyTorch. The Last supported version of TensorFlow is 
2.10  (window users) and 2.12 for others (we have not tested beyond this).**
- Please be mindful different versions of TensorFlow require different CUDA versions.
- As the combination of TensorFlow and CUDA matters, we strongly encourage you to 
**check your driver/cuDNN/CUDA/TensorFlow versions** [on this StackOverflow post](
https://stackoverflow.com/questions/30820513/what-is-version-of-cuda-for-nvidia-304-125/30820690#30820690
).
- To check your GPU is working, in the terminal, run:

`nvcc -V` to check your installed version(s).

- The best practice is to then run the supplied `testscript_pytorch_single_animal.py` 
(or `testscript.py` for the TensorFlow engine); this is inside the examples folder you
acquired when you git cloned the repo. Here is more information/a short 
[video on running the testscript](https://www.youtube.com/watch?v=IOWtKn3l33s).
- Additionally, if you want to use the bleeding edge, with yout git clone you also get
the latest code. While inside the main DeepLabCut folder, you can run `./reinstall.sh`
to be sure it's installed (more [here](installation-tips))
- You can test that your GPU is being properly engaged with these additional [tips](
https://www.tensorflow.org/programmers_guide/using_gpu).
- Ubuntu users might find this [installation guide](
https://deeplabcut.github.io/DeepLabCut/docs/recipes/installTips.html#installation-on-ubuntu-20-04-lts
) for a fresh ubuntu install useful as well.

## Troubleshooting:

TensorFlow:
Here are some additional resources users have found helpful (posted without endorsement):

- https://stackoverflow.com/questions/30820513/what-is-the-correct-version-of-cuda-for-my-nvidia-driver/30820690

<p align="center">
<img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c3e46ca1ae6cfbb5c5d1ee0/1547585235033/cuda_driver.png?format=750w" width="50%">
</p>

- https://www.tensorflow.org/install/source#gpu

- http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html

- https://developer.nvidia.com/cuda-toolkit-archive

- http://www.python36.com/install-tensorflow-gpu-windows/


FFMPEG:

- A few Windows users report needing to install re-install ffmpeg (after windows updates) as described here: https://video.stackexchange.com/questions/20495/how-do-i-set-up-and-use-ffmpeg-in-windows (A potential error could occur when making new videos). On Ubuntu, the command is: `sudo apt install ffmpeg`

DEEPLABCUT:

- if you git clone or download this folder, and are inside of it then ``import deeplabcut`` will import the package from there rather than from the latest on PyPi!

(system-wide-considerations-during-install)=
## System-wide considerations:

If you perform the system-wide installation, and the computer has other Python packages or TensorFlow versions installed that conflict, this will overwrite them. If you have a dedicated machine for DeepLabCut, this is fine. If there are other applications that require different versions of libraries, then one would potentially break those applications. The solution to this problem is to create a virtual environment, a self-contained directory that contains a Python installation for a particular version of Python, plus additional packages. One way to manage virtual environments is to use conda environments (for which you need Anaconda installed).

(tech-considerations-during-install)=
## Technical Considerations:

- Computer:

     - For reference, we use e.g. Dell workstations (79xx series) with **Ubuntu 16.04 LTS, 18.04 LTS, 20.04 LTS, 22.04 LTS** and for versions prior to 2.2, we run a Docker container that has TensorFlow, etc. installed (https://github.com/DeepLabCut/Docker4DeepLabCut2.0). Now we use the new Docker containers supplied on this repo (linux support only), also available through [DockerHub](https://hub.docker.com/r/deeplabcut/deeplabcut) or the [`deeplabcut-docker`](https://pypi.org/project/deeplabcut-docker/) helper script.

- Computer Hardware:
     - Ideally, you will use a strong NVIDIA GPU with *at least* 8GB memory.  A GPU is not necessary, but on a CPU the (training and evaluation) code is considerably slower (10x) for ResNets, but MobileNets are faster (see WIKI). You might also consider using cloud computing services like [Google cloud/amazon web services](https://github.com/DeepLabCut/DeepLabCut/issues/47) or Google Colaboratory.

- Camera Hardware:
     - The software is very robust to track data from any camera (cell phone cameras, grayscale, color; captured under infrared light, different manufacturers, etc.). See demos on our [website](https://www.mousemotorlab.org/deeplabcut/).

- Software:
     - Operating System: Linux (Ubuntu), MacOS* (Mojave), or Windows 10. However, the authors strongly recommend Ubuntu! *MacOS does not support NVIDIA GPUs (easily), so we only suggest this option for CPU use or a case where the user wants to label data, refine data, etc and then push the project to a cloud resource for GPU computing steps, or use MobileNets.
     - Anaconda/Python3: Anaconda: a free and open source distribution of the Python programming language (download from https://www.anaconda.com/). DeepLabCut is written in Python 3 (https://www.python.org/) and not compatible with Python 2.
     - `pip install deeplabcut`
     - TensorFlow
       - If you want to use a pre3.0 version, you will need [TensorFlow](https://www.tensorflow.org/) (we used version 1.0 in the Nature Neuroscience paper, later versions also work with the provided code (we tested **TensorFlow versions 1.0 to 1.15, and 2.0 to 2.10**; we recommend TF2.10 now) for Python 3.8, 3.9, 3.10 with GPU support.
        - To note, is it possible to run DeepLabCut on your CPU, but it will be VERY slow (see: [Mathis & Warren](https://www.biorxiv.org/content/early/2018/10/30/457242)). However, this is the preferred path if you want to test DeepLabCut on your own computer/data before purchasing a GPU, with the added benefit of a straightforward installation! Otherwise, use our COLAB notebooks for GPU access for testing.
     - Docker: We highly recommend advanced users use the supplied [Docker container](docker-containers)


--- File: docs/Overviewof3D.md ---
(3D-overview)=
# 3D DeepLabCut

In this repo we directly support 2-camera based 3D pose estimation. If you want n camera support, plus nicer
optimization methods, please see our work that was published at
[ICRA 2021 on strong baseline 3D models (and a 3D dataset)](https://github.com/African-Robotics-Unit/AcinoSet). In the
link you will find how we optimize 6+ camera DLC output data for cheetahs (and see more below).

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1589578632599-HQENUYUIBI9KYTZA2WXV/ke17ZwdGBToddI8pDm48kBgERiRoVg6XJpnbAnG076FZw-zPPgdn4jUwVcJE1ZvWhcwhEtWJXoshNdA9f1qD7Y5_KuY_fkOEvGrDVB8aRb13EC_7Ld97nVeJG4MMJk1tqSdWG3KOMGCA68a4XjyT5g/3D.png?format=300w" width="350" title="DLC-3D" alt="DLC 3D" align="right" vspace = "50">


## **ATTENTION: Our code base in this repo assumes you:**

A. You have 2D videos and a DeepLabCut network to analyze them as described in the
[main documentation](overview). This can be with multiple
separate networks for each camera (less recommended), or one network trained on all views - recommended! (See
[Nath*, Mathis* et al., 2019](https://www.biorxiv.org/content/10.1101/476531v1)). We also support multi-animal 3D with this code (please see
[Lauer et al. 2022](https://doi.org/10.1038/s41592-022-01443-0)).

B. You are using 2 cameras, in a [stereo configuration](https://github.com/DeepLabCut/DeepLabCut/blob/5ac4c8cb6bcf2314a3abfcf979b8dd170608e094/deeplabcut/pose_estimation_3d/camera_calibration.py#L223), for 3D*.

C. You have calibration images taken (see details below!).


### ***If you need more than 2 camera support:**
Here are other excellent options for you to use that extend DeepLabCut:

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1628432165795-BBF6AWCK1BEKV3AJ6GF5/cheetah.gif?format=1500w" width="350" title="AcinoSet-3D" alt="DLC 3D" align="right" vspace = "50">

- **[AcinoSet](https://github.com/African-Robotics-Unit/AcinoSet)**; **n**-camera support with triangulation, extended Kalman filtering, and trajectory optimization
code (see video to the right for a min demo, courtesy of Prof. Patel), plus a GUI to visualize 3D data. It is built to
work directly with DeepLabCut (but currently tailored to cheetah's, thus some coding skills are required at this time).


- **[anipose.org](https://anipose.readthedocs.io/en/latest/)**; a wrapper for 3D deeplabcut that provides >3 camera support and is built to work directly with
DeepLabCut. You can `pip install anipose` into your DLC conda environment.

- **Argus, easywand or DLTdv** w/DeepLabCut see https://github.com/haliaetus13/DLCconverterDLT; this can be used with
the the highly popular Argus or DLTdv tools for wand calibration.

## Jump in with direct DeepLabCut 2-camera support:

- single animal DeepLabCut and multi-animal DeepLabCut (maDLC) projects are supported:

<p align="center">
<img src= "https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1560968522350-COKR986AQESF5N1N7QNK/ke17ZwdGBToddI8pDm48kNaO57GzHjWqV-xM6jVvY6ZZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpyR5k0u27ivMv3az5DOhUvLuYQefjfUWYPEDVexVC_mSas4X78tjQKn3yE00zHvnK8/3D_maousLarger.gif?format=750w" height="200">

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/cd423302-0389-4b63-8869-b787a2c52b8b/maDLC_3d.gif?format=1500w" height="200">
</p>

### (1) Create a New 3D Project:

Watch a [DEMO VIDEO](https://youtu.be/Eh6oIGE4dwI) on how to use this code, and check out the Notebook [here](https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/JUPYTER/Demo_3D_DeepLabCut.ipynb)!


You will run this function **one** time per project; a project is defined as a given set of cameras and calibration
images. You can always analyze new videos within this project.

The function **create\_new\_project\_3d** creates a new project directory specifically for converting the 2D pose to 3D
pose, required subdirectories, and a basic 3D project configuration file. Each project is identified by the name of the
project (e.g. Task1), name of the experimenter (e.g. YourName), as well as the date at creation.

Thus, this function requires the user to enter the name of the project, the name of the experimenter and number of
cameras to be used. Currently, DeepLabCut supports triangulation using 2 cameras, but will expand to more than 2 cameras
in a future version.

To start a 3D project type the following in ipython:
```python
deeplabcut.create_new_project_3d("ProjectName", "NameofLabeler", num_cameras=2)
```
TIP 1: you can also pass `working_directory="Full path of the working directory"` if you want to place this folder
somewhere beside the current directory you are working in. If the optional argument `working_directory` is unspecified,
the project directory is created in the current working directory.

TIP 2: you can also place `config_path3d` in front of `deeplabcut.create_new_project_3d` to create a variable that holds
the path to the config.yaml file, i.e. `config_path3d=deeplabcut.create_new_project_3d(...` Or, set this variable for
easy use. Please note that `config_path3d='Full path of the 3D project configuration file'`.

This function will create a project directory with the name **Name of the project+name of the experimenter+date of
creation of the project+3d** in the **Working directory**. The project directory will have subdirectories:
**calibration_images**, **camera_matrix**, **corners**, and **undistortion**.  All the outputs generated during the
course of a project will be stored in one of these subdirectories, thus allowing each project to be curated in
separation from other projects.

The purpose of the subdirectories is as follows:

**calibration_images:** This directory will contain a set of calibration images acquired from the two cameras. A
calibration image can be acquired using a printed checkerboard and its pair wise images are taken from both the cameras
to consider as a set of calibration images.

**camera_matrix:** This directory will store the parameter for both the cameras as a pickle file. Specifically, these
pickle files contain the intrinsic and extrinsic camera parameters. While the intrinsic parameters represent a
transformation from 3-D camera's coordinates into the image coordinates, the extrinsic parameters represent a rigid
transformation from world coordinate system to the 3-D camera's coordinate system.

**corners:**  As a part of camera calibration, the checkerboard pattern is detected in the calibration images and these
patterns will be stored in this directory. Each row of the checkerboard grid is marked with a unique color.

**undistortion:** In order to check for calibration, the calibration images and the corresponding corner points are
undistorted. These undistorted images are overlaid with undistorted points and will be stored in this directory.

Here is an overview of the calibration and triangulation workflow that follows:

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559751031211-IOTHQDAEEFP939AD8L8Q/ke17ZwdGBToddI8pDm48kCpBvlJgRextwO-RLKSiThBZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIMRcHFn6kFHmdB2CIEK_nkzQWBtRKl1IphR3INrGSAiA/3dworkflow.png?format=1000w" width="55%">
</p>

### (2) Take and Process Camera Calibration Images:

(**CRITICAL!**) You must take images of a checkerboard to calibrate your images. Here are example boards you could
print and use (mount it on a flat, hard surface!):
https://markhedleyjones.com/projects/calibration-checkerboard-collection.
- You must save the image pairs as .jpg files.
- They should be named with the **camera-#** as the prefix, i.e. **camera-1-01.jpg** and **camera-2-01.jpg** for the
first pair of images. Please note, this cannot be changed after the project is created.

**TIP:** If you want to take a short video (vs. snapping pairs of frames) while you move the checkerboard around, you
can use this command inside your conda environment (but outside of ipython!) to convert the video to **.jpg** frames
(this will take the first 20 frames (set with `-vframes`) and name them camera-1-001.jpg, etc; edit appropriately):

```python
ffmpeg -i videoname.mp4 -vframes 20 camera-1-%03d.jpg
```
- While taking the images:
  - Keep the orientation of the checkerboard the same and do not rotate it more than 30 degrees. Rotating the
  checkerboard circular will change the origin across the frames and may result in incorrect order of detected corners.

  - Cover several distances, and within each distance, cover all parts of the image view (all corners and center).

  - Use a checkerboard as big as possible, ideally with at least 8x6 squares.

  - Aim for taking at least 30-70 pair of images, as after corner detection, some of the images might need to be
  discarded due to either incorrect corner detection or incorrect order of detected corners.

  - You can take the images as a series of .jpg images, or a video where you post-hoc pair sync'd frames (see tip
  above).


The camera calibration is an **iterative process**, where the user needs to select a set of calibration images where the
grid pattern is correctly detected. The function `deeplabcut.calibrate_cameras(config_path)`
extracts the grid pattern from the calibration images and store them under the `corners` directory. The grid pattern
could be 8x8 or 5x5 etc. We use a pattern of the 8x6 grid to find the internal corners of the checkerboard.

In some cases, it may happen that the corners are not detected correctly or the order of corners detected in the
camera-1 image and camera-2 image is incorrect. You need to remove these pair of images from the **calibration_images**
folder as they will reduce the calibration accuracy.

To begin, please place your images into the **calibration_images** directory.

(**CRITICAL!**) Edit the **config.yaml** file to set the camera names; note that once this is set, **do not change the
names!**

Then, run:

```python
deeplabcut.calibrate_cameras(config_path3d, cbrow=8, cbcol=6, calibrate=False, alpha=0.9)
```

NOTE: you need to specify how many rows (`cbrow`) and columns (`cbcol`) your checkerboard has (beware, we count
edges between squares and not squares themselves, so for a 8 x 8 squares checkerboard set `cbrow=7` and `cbcol=7`).
Also, first set the variable `calibrate` to **False**, so you can remove any faulty images. You need to visually
inspect the output to check for the detected corners and select those pair of images where the corners are correctly
detected. Please note, If the scaling parameter `alpha=0`, it returns undistorted image with minimum unwanted pixels.
So it may even remove some pixels at image corners. If `alpha=1`, all pixels are retained with some extra black images.

Here is what they might look like:

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559776966423-RATM6ZQT8JXHYAN768F6/ke17ZwdGBToddI8pDm48kKmw982fUOZVIQXHUCR1F55Zw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpw5XnxLBmEFHJGf_0qFdDpmIncOw4kq9OpCHNTYqzGO-E1YJr-Thht9Tdog4YtCwrE/right02_corner.jpg?format=500w" height="220">
 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559776952829-KRHFX74CDO3BPIY9E9U0/ke17ZwdGBToddI8pDm48kKmw982fUOZVIQXHUCR1F55Zw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpw5XnxLBmEFHJGf_0qFdDpmIncOw4kq9OpCHNTYqzGO-E1YJr-Thht9Tdog4YtCwrE/left02_corner.jpg?format=500w" height="220">
</p>


Once all the set of images has been selected (namely, delete from the folder any bad pairs!) where the corners and their
orders are detected correctly, then the two cameras can be calibrated using:

```python
deeplabcut.calibrate_cameras(config_path3d, cbrow=8, cbcol=6, calibrate=True, alpha=0.9)
```

This computes the intrinsic and extrinsic parameters for each camera. A re-projection error is also computed using the
intrinsic and extrinsic parameters which provide an estimate of how good the parameters are. The transformation between
the two cameras is estimated and the cameras are stereo calibrated. Furthermore, the above function brings both the
camera image plane to the same plane by computing the stereo rectification. These parameters are stored as a pickle file
named as `stereo_params.pickle` under the directory `camera_matrix`.

Once you have run this for the project, you do not need to do so again (unless you want to re-calibrate your cameras);
be advised, if you do re-calibrate, you may want to clearly mark which videos are analyzed with "old" vs. "new"
calibration images.

### (3) Check for Undistortion:

In order to check how well the stereo calibration is, it is recommended to undistort the calibration images and the
corner points using camera matrices and project these undistorted points on the undistorted images to check if they
align correctly. This can be done in deeplabcut as:

```python
deeplabcut.check_undistortion(config_path3d, cbrow=8, cbcol=6)
```

Each calibration image is undistorted and saved under the directory `undistortion`. A plot with a pair of undistorted
camera images with its undistorted corner points overlaid is also stored. Please visually inspect this image. All the
undistorted corner points from all the calibration images are triangulated and plotted for the user to visualize for any
undistortion related errors. If they are not correct, go check and revise the calibration images (then repeat the
calibration and this step)!

### (4) Triangulation --> Take your 2D to 3D!

If there are no errors in the undistortion, then the pose from the 2 cameras can be triangulated to get the 3D
DeepLabCut coordinates!

(**CRITICAL!**) Name the video files in such a way that the file name **contains the name of the cameras** as specified
in the `config file`. e.g. if the cameras as named as `camera-1` and `camera-2` (or `cam-1`, `cam-2` etc.) then the
video filename must contain this naming, i.e. this could be named as `rig-1-mouse-day1-camera-1.avi` and
`rig-1-mouse-day1-camera-2.avi` or could be `rig-1-mouse-day1-camera-1-date.avi` and
`rig-1-mouse-day1-camera-2-date.avi`.

- **Note** that to correctly pair the videos, the file names otherwise need to be the same!
- If helpful, [here is the software we use to record videos](https://github.com/AdaptiveMotorControlLab/Camera_Control).  

(**CRITICAL!**) You must also edit the **3D project config.yaml** file to denote which DeepLabCut projects have the
information for the 2D views.

- Of critical importance is that you need to input the **same** body part names as in the config.yaml file of the 2D
project.
- You must set the snapshot to use inside the 2D config file (default is -1, namely the last training snapshot of the
network).
- You need to set a "scorer 3D" name; this will point to the project file and be set in future 3D output file names.
- You should define a "skeleton" here as well (note, this is not rigid, it just connects the points in the plotting
step). Not every point needs to be "skeletonized", i.e. these points can be a subset of the full body parts list. The
other points will just be plotted into the 3D space. Here is how the config.yaml looks with some example inputs:

 <p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559756808766-2G6FG91S2I4ZX2SSP6QF/ke17ZwdGBToddI8pDm48kEULogWWASOhGi36VEr2SOlZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIoI8wFyxyzDq4NO_A5fg6hgZUWi6FxVv9SjR8GkGxb-wKMshLAGzx4R3EDFOm1kBS/config3d.jpg?format=1000w" width="95%">
</p>

(**CRITICAL!**) This step will also run the equivalent of `analyze_videos` (in 2D) for you and then apply a median
filter to the 2D data (`filterpredictions=True`)! If you already ran the 2D analysis and there is a filtered output
file, it will take this by default (otherwise it will take your unfiltered 2D analysis files)!

Next, pass the `config_path3d` and now the video folder path, which is the path to the **folder** where all the videos
from two cameras are stored. The triangulation can be done in deeplabcut by typing:

```python
deeplabcut.triangulate(
  config_path3d,
  "/yourcomputer/fullpath/videofolder",
  filterpredictions=True/False
)
```
NOTE: Windows users, you must input paths as: ``r`C:\Users\computername\videofolder'`` or
``C:\\Users\\computername\\videofolder'``.

**TIP:** Here are all the parameters you can pass:

```python
Parameters
----------
config : string
    Full path of the config.yaml file as a string.

video_path : string
    Full path of the directory where videos are saved.

videotype: string, optional
    Checks for the extension of the video in case the input to the video is a directory.
Only videos with this extension are analyzed. The default is ``.avi``

filterpredictions: Bool, optional
    Filter the predictions by fitting median (by default) or arima filter. If specified it should be either ``True`` or ``False``.

filtertype: string
    Select which filter, 'arima' or 'median' filter.

gputouse: int, optional. Natural number indicating the number of your GPU (see number in nvidia-smi). If you do not have a GPU put None.
    See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

destfolder: string, optional
    Specifies the destination folder for analysis data (default is the path of the video)

save_as_csv: bool, optional
    Saves the predictions in a .csv file. The default is ``False``; if provided it must be either ``True`` or ``False``

track_method: str, optional
    Method used for tracking: "box" or "ellipse"
```
The **triangulated file** is now saved under the same directory where the video files reside (or the destination folder
you set)! This can be used for future analysis. This step can be run at anytime as you collect new videos, and easily
added to your automated analysis pipeline, i.e. such as **replacing**
`deeplabcut.triangulate(config_path3d, video_path)` with `deeplabcut.analyze_videos` (as if it's not analyzed in 2D
already, this function will take care of it ;):

<p align="center">
<img src= https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559758477126-B9PU1EFA7L7L1I24Z2EH/ke17ZwdGBToddI8pDm48kH6mtUjqMdETiS6k4kEkCoR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UQf4d-kVja3vCG3Q_2S8RPAcZTZ9JxgjXkf3-Un9aT84H3bqxw7fF48mhrq5Ulr0Hg/howtouseDLC2d_3d-01.png?format=1000w width="65%">
 </p>

### (5) Visualize your 3D DeepLabCut Videos:

In order to visualize both the 2D videos with tracked points plut the pose in 3D, the user can create a 3D video for
certain frames (these are large files, so we advise just looking at a subset of frames). The user can specify the config
file, the **path of the triangulated file folder**, and specify the start and end frame indices to create a 3D labeled
video. Note that the `triangulated_file_folder` is where the newly created file that ends with
`yourDLC_3D_scorername.h5` is located. This can be done using:

```python
deeplabcut.create_labeled_video_3d(
  config_path,
  ["triangulated_file_folder"],
  start=50,
  end=250
)
```

**TIP:** (see more parameters below) You can set how the axis of the 3D plot on the far right looks by changing the
variables `xlim`, `ylim`, `zlim` and `view`. Your checkerboard_3d.png image which was created above will show you the
axis ranges. Here is an example:

 <p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559864026106-B6XQHUDUA8VB6F0FNVBA/ke17ZwdGBToddI8pDm48kKmw982fUOZVIQXHUCR1F55Zw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpx7krGdD6VO1HGZR3BdeCbrijc_yIxzfnirMo-szZRSL5-VIQGAVcQr6HuuQP1evvE/checkerboard_3d.png?format=750w" width="45%">
</p>

`View` is used to set the elevation and azimuth of the axes (defaults are [113, 270], and you should play around to find
the view-point you like!). Also note that the video is created from a set of .png files in a "temp" directory, so as
soon as you run this command you can open the first image, and if you don't like the view, hit `CNTRL+C` to stop, edit
the values, and start again!

**Other optional parameters include:**
here
```python
videofolder: string
    Full path of the folder where the videos are stored. Use this if the vidoes are stored in a different location other than where the triangulation files are stored. By default is ``None`` and therefore looks for video files in the directory where the triangulation file is stored.

trailpoints: int
    Number of previous frames whose body parts are plotted in a frame (for displaying history). Default is set to 0.

videotype: string
    Checks for the extension of the video in case the input is a directory.
Only videos with this extension are analyzed. The default is ``.avi``

view: list
    A list that sets the elevation angle in z plane and azimuthal angle in x,y plane of 3d view. Useful for rotating the axis for 3d view

xlim: list
    A list of integers specifying the limits for xaxis of 3d view. By default it is set to [None,None], where the x limit is set by taking the minimum and maximum value of the x coordinates for all the bodyparts.

ylim: list
    A list of integers specifying the limits for yaxis of 3d view. By default it is set to [None,None], where the y limit is set by taking the minimum and maximum value of the y coordinates for all the bodyparts.

zlim: list
    A list of integers specifying the limits for zaxis of 3d view. By default it is set to [None,None], where the z limit is set by taking the minimum and maximum value of the z coordinates for all the bodyparts.

draw_skeleton: bool
    If True adds a line connecting the body parts making a skeleton on on each frame. The body parts to be connected and the color of these connecting lines are specified in the config file. By default: True

color_by : string, optional (default='bodypart')
    Coloring rule. By default, each bodypart is colored differently.
    If set to 'individual', points belonging to a single individual are colored the same.

figsize: tuple[int, int], optional, default=(80, 8)
    Size of the figure

fps: int, optional, default=30
    Frames per second

dpi: int, optional, default=300
    Dots per inch (resplution)
```

### If you use this code:

We kindly ask that you cite [Mathis et al, 2018](https://www.nature.com/articles/s41593-018-0209-y) **&** [Nath*, Mathis*, et al., 2019](https://doi.org/10.1038/s41596-019-0176-0). If you use 3D
multi-animal: [Lauer et al. 2022](https://doi.org/10.1038/s41592-022-01443-0).


--- File: docs/UseOverviewGuide.md ---
(overview)=
# 🥳 Get started with DeepLabCut: our key recommendations 

Below we will first outline what you need to get started, the different ways you can use DeepLabCut, and then the full workflow. Note, we highly recommend you also read and follow our [Nature Protocols paper](https://www.nature.com/articles/s41596-019-0176-0), which is (still) fully relevant to standard DeepLabCut.

```{Hint}
💡📚 If you are new to Python and DeepLabCut, you might consider checking our [beginner guide](https://deeplabcut.github.io/DeepLabCut/docs/beginner-guides/beginners-guide.html) once you are ready to jump into using the DeepLabCut App!
```


## [How to install DeepLabCut](how-to-install)

We don't cover installation in depth on this page, so click on the link above if that is what you are looking for. See below for details on getting started with DeepLabCut!

## What we support:

We are primarily a package that enables deep learning-based pose estimation. We have a lot of models and options, but don't get overwhelmed -- the developer team has tried our best to "set the best defaults we possibly can"!

- Decide on your needs: there are **two main modes, standard DeepLabCut or multi-animal DeepLabCut**. We highly recommend carefully considering which one is best for your needs. For example, a white mouse + black mouse would call for standard, while two black mice would use multi-animal. **[Important Information on how to use DLC in different scenarios (single vs multi animal)](important-info-regd-usage)** Then pick a user guide:

  - (1) [How to use standard DeepLabCut](single-animal-userguide)
  - (2) [How to use multi-animal DeepLabCut](multi-animal-userguide)

- To note, as of DLC3+ the single and multi-animal code bases are more integrated and we support **top-down**,  **bottom-up**, and a new "hybrid" approach that is state-of-the-art, called **BUCTD** (bottom-up conditional top down), models.
  - If these terms are new to you, check out our [Primer on Motion Capture with Deep Learning!](https://www.sciencedirect.com/science/article/pii/S0896627320307170). In brief, both work for single or multiple animals and each method can be better or worse on your data.

<p align="center">
<img src= https://ars.els-cdn.com/content/image/1-s2.0-S0896627320307170-gr5_lrg.jpg?format=1000w width="50%">
 </p>

  - Here is more information on BUCTD:
<p align="center">
<img src= https://github.com/amathislab/BUCTD/raw/main/media/BUCTD_fig1.png?format=1000w width="50%">
 </p>
 
 **Additional Learning Resources:**

 - [TUTORIALS:](https://www.youtube.com/channel/UC2HEbWpC_1v6i9RnDMy-dfA?view_as=subscriber) video tutorials that demonstrate various aspects of using the code base.
 - [HOW-TO-GUIDES:](overview) step-by-step user guidelines for using DeepLabCut on your own datasets (see below)
 - [EXPLANATIONS:](https://github.com/DeepLabCut/DeepLabCut-Workshop-Materials) resources on understanding how DeepLabCut works
 - [REFERENCES:](https://github.com/DeepLabCut/DeepLabCut#references) read the science behind DeepLabCut
 - [BEGINNER GUIDE TO THE GUI](https://deeplabcut.github.io/DeepLabCut/docs/beginners-guide.html)

Getting Started: [a video tutorial on navigating the documentation!](https://www.youtube.com/watch?v=A9qZidI7tL8)


### What you need to get started:

 - **a set of videos that span the types of behaviors you want to track.** Having 10 videos that include different backgrounds, different individuals, and different postures is MUCH better than 1 or 2 videos of 1 or 2 different individuals (i.e. 10-20 frames from each of 10 videos is **much better** than 50-100 frames from 2 videos).

 - **minimally, a computer w/a CPU.** If you want to use DeepLabCut on your own computer for many experiments, then you should get an NVIDIA GPU. See technical specs [here](https://github.com/DeepLabCut/DeepLabCut/wiki/FAQ). You can also use cloud computing resources, including COLAB ([see how](https://github.com/DeepLabCut/DeepLabCut/blob/master/examples/README.md)).


### What you DON'T need to get started:

 - no specific cameras/videos are required; color, monochrome, etc., is all fine. If you can see what you want to measure, then this will work for you (given enough labeled data).

 - no specific computer is required (but see recommendations above), our software works on Linux, Windows, and MacOS.


### Overview:
**DeepLabCut** is a software package for markerless pose estimation of animals performing various tasks. The software can manage multiple projects for various tasks. Each project is identified by the name of the project (e.g. TheBehavior), name of the experimenter (e.g. YourName), as well as the date at creation. This project folder holds a ``config.yaml`` (a text document) file containing various (project) parameters as well as links the data of the project.


<p align="center">
<img src=   https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572293604382-W6BWA63LZ9J8R7N0QEA5/ke17ZwdGBToddI8pDm48kIw6YkRUEyoge4858uAJfaMUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYwL8IeDg6_3B-BRuF4nNrNcQkVuAT7tdErd0wQFEGFSnH9wUPiI8bGoX-EQadkbLIJwhzjIpw393-uEwSKO7VZIL9gN_Sb5I_dLwvWryjeCJg/dlc_overview-01.png?format=1000w width="80%">
 </p>

 <p align="center">
 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1560124235138-A9VEZB45SQPD5Z0BDEXA/ke17ZwdGBToddI8pDm48kKsvCFNoOAts8bgs5LXY20UUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcZaDohTswVrVk6oKw3G03bTl18OXeDyNJsBjNlGiyPYGo9Ewyd5AI5wx6CleNeBtf/dlc_steps.jpg?format=1000w" width="80%">
</p>

### Overview of the workflow:
This page contains a list of the essential functions of DeepLabCut as well as demos. There are many optional parameters with each described function, which you can find [here](functionDetails.md). For additional assistance, you can use the [help](UseOverviewGuide.md#help) function to better understand what each function does.

<p align="center">
  <img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5cca272524a69435c3251c40/1556752170424/flowfig.jpg?format=1000w" width=95%>
  <br>
  <em>
   <a href="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5cca272524a69435c3251c40/1556752170424/flowfig.jpg?format=1000w">View in full screen</a>
  </em>
</p>

You can have as many projects on your computer as you wish. You can have DeepLabCut installed in an [environment](/conda-environments) and always exit and return to this environment to run the code. You just need to point to the correct ``config.yaml`` file to [jump back in](/docs/UseOverviewGuide.md#tips-for-daily-use)! The documentation below will take you through the individual steps.

<p align="center">
<img src=  https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559758477126-B9PU1EFA7L7L1I24Z2EH/ke17ZwdGBToddI8pDm48kH6mtUjqMdETiS6k4kEkCoR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UQf4d-kVja3vCG3Q_2S8RPAcZTZ9JxgjXkf3-Un9aT84H3bqxw7fF48mhrq5Ulr0Hg/howtouseDLC2d_3d-01.png?format=500w width="60%">
 </p>


(important-info-regd-usage)=

# Specific Advice for Using DeepLabCut:

## Important information on using DeepLabCut:

We recommend first using **DeepLabCut for a single animal scenario** to understand the workflow - even if it's just our demo data. Multi-animal tracking is more complex - i.e. it has several decisions the user needs to make. Then, when you are ready you can jump into multi-animals...

### Additional information for getting started with maDeepLabCut:

We highly recommend using it first in the Project Manager GUI ([Option 3](docs/functionDetails.md#deeplabcut-project-manager-gui)). This will allow you to get used to the additional steps by being walked through the process. Then, you can always use all the functions in your favorite IDE, notebooks, etc.

#### *What scenario do you have?*

- **I have single animal videos:**
   - quick start: when you `create_new_project` (and leave the default flag to False in `multianimal=False`). This is the typical work path for many of you.

- **I have single animal videos, but I want to use the updated network capabilities introduced for multi-animal projects:**
   - quick start: when you `create_new_project` just set the flag `multianimal=True`. This enables you to use maDLC features even though you have only one animal. To note, this is rarely required for single animal projects, and not the recommended path. Some tips for when you might want to use this: this is good for say, a hand or a mouse if you feel the "skeleton" during training would increase performance. DON'T do this for things that could be identified an individual objects. i.e., don't do whisker 1, whisker 2, whisker 3 as 3 individuals. Each whisker always has a specific spatial location, and by calling them individuals you will do WORSE than in single animal mode.

[VIDEO TUTORIAL AVAILABLE!](https://youtu.be/JDsa8R5J0nQ)

- **I have multiple *identical-looking animals* in my videos:**
   - quick start: when you `create_new_project` set the flag `multianimal=True`. If you can't tell them apart, you can assign the "individual" ID to any animal in each frame. See this [labeling w/2.2 demo video](https://www.youtube.com/watch?v=_qbEqNKApsI)

[VIDEO TUTORIAL AVAILABLE!](https://youtu.be/Kp-stcTm77g)

- **I have multiple animals, *but I can tell them apart,* in my videos and want to use DLC2.2:**
   - quick start: when you `create_new_project` set the flag `multianimal=True`. And always label the "individual" ID name the same; i.e. if you have mouse1 and mouse2 but mouse2 always has a miniscope, in every frame label mouse2 consistently. See this [labeling w/2.2 demo video](https://www.youtube.com/watch?v=_qbEqNKApsI). Then, you MUST put the following in the config.yaml file: `identity: true`

[VIDEO TUTORIAL AVAILABLE!](https://youtu.be/Kp-stcTm77g) - ALSO, if you can tell them apart, label animals them consistently!

- **I have a pre-2.2 single animal project, but I want to use 2.2:**

Please read [this convert 2 maDLC guide](convert-maDLC)

# The options for using DeepLabCut:

Great - now that you get the overall workflow let's jump in! Here, you have several options.

[**Option 1**](using-demo-notebooks) DEMOs: for a quick introduction to DLC on our data.

[**Option 2**](using-project-manager-gui) Standalone GUI: is the perfect place for
beginners who want to start using DeepLabCut on your own data.

[**Option 3**](using-the-terminal) In the terminal: is best for more advanced users, as
with the terminal interface you get the most versatility and options.

(using-demo-notebooks)=
## Option 1: Demo Notebooks:
[VIDEO TUTORIAL AVAILABLE!](https://www.youtube.com/watch?v=DRT-Cq2vdWs)

We provide Jupyter and COLAB notebooks for using DeepLabCut on both a pre-labeled dataset, and on the end user's
own dataset. See all the demo's [here!](/examples) Please note that GUIs are not easily supported in Jupyter in MacOS, as you need a framework build of python. While it's possible to launch them with a few tweaks, we recommend using the Project Manager GUI or terminal, so please follow the instructions below.

(using-project-manager-gui)=
## Option 2: using the Project Manager GUI:
[VIDEO TUTORIAL!](https://www.youtube.com/watch?v=KcXogR-p5Ak)

[VIDEO TUTORIAL#2!](https://youtu.be/Kp-stcTm77g)

Start Python by typing ``ipython`` or ``python`` in the terminal (note: using pythonw for Mac users was depreciated in 2022).
If you are using DeepLabCut on the cloud, you cannot use the GUIs. If you use Windows, please always open the terminal with administrator privileges. Please read more in our Nature Protocols paper [here](https://www.nature.com/articles/s41596-019-0176-0). And, see our [troubleshooting wiki](https://github.com/DeepLabCut/DeepLabCut/wiki/Troubleshooting-Tips).

Simply open the terminal and type:
```python
python -m deeplabcut
```
That's it! Follow the GUI for details

(using-the-terminal)=
## Option 3: using the program terminal, Start iPython*:

[VIDEO TUTORIAL AVAILABLE!](https://www.youtube.com/watch?v=7xwOhUcIGio)

Please decide with mode you want to use DeepLabCut, and follow one of the following:

- (1) [How to use standard DeepLabCut](single-animal-userguide)
- (2) [How to use multi-animal DeepLabCut](multi-animal-userguide)


--- File: docs/recipes/TechHardware.md ---
# Technical (Hardware) Considerations

## Quick summary:
[On our install page](tech-considerations-during-install)
we highlight that for GPU computing through standard installation you need a NVIDIA GPU, with at least 8 GB of memory. If you have an Intel or AMD GPU, and are on windows, there is an alternative method of installation available which is shown on the [installation tips page](installation-tips) under "How to install Deeplabcut for Intel and AMD GPUs".
Note, some info is repeated here, and will be updated as systems and hardware changes.

### Computer:

For reference, we use e.g. Dell workstations (79xx series) with **Ubuntu 16.04 LTS, 18.04 LTS, or 20.04 LTS** and run a Docker container that has TensorFlow, etc. installed (https://github.com/DeepLabCut/Docker4DeepLabCut2.0).

### Computer Hardware:

Ideally, you will use a strong GPU with *at least* 8GB memory such as the [NVIDIA GeForce 1080 Ti,  2080 Ti, or 3090](https://www.nvidia.com/en-us/shop/geforce/?page=1&limit=9&locale=en-us).  A GPU is not strictly necessary, but on a CPU the (training and evaluation) code is considerably slower (10x) for ResNets, but MobileNets and EfficientNets are slightly faster. Still, a GPU will give you a massive speed boost. You might also consider using cloud computing services like [Google cloud/amazon web services](https://github.com/DeepLabCut/DeepLabCut/issues/47) or Google Colaboratory.

### Camera Hardware:

The software is very robust to track data from any camera (cell phone cameras, grayscale, color; captured under infrared light, different manufacturers, etc.). See demos on our [website](https://www.mousemotorlab.org/deeplabcut/).

### Software:

**Operating System:** Linux (Ubuntu), MacOS* (Mojave), or Windows 10. However, the authors strongly recommend Ubuntu! *MacOS does not support NVIDIA GPUs (easily), so we only suggest this option for CPU use or a case where the user wants to label data, refine data, etc and then push the project to a cloud resource for GPU computing steps, or use MobileNets.

**Anaconda/Python3:** Anaconda: a free and open source distribution of the Python programming language (download from https://www.anaconda.com/). DeepLabCut is written in Python 3 (https://www.python.org/) and not compatible with Python 2.

**For the TensorFlow Engine:** You will need [TensorFlow](https://www.tensorflow.org/).
We used version 1.0 in the paper, later versions also work with the provided code (we
tested **TensorFlow versions 1.0 to 1.15, and 2.0 to 2.12 (2.10 for Windows)**; we
recommend TF2.12 for MacOS/Ubuntu and 2.10 for Windows) for Python 3.10 with GPU
support.

To note, is it possible to run DeepLabCut on your CPU, but it will be VERY slow (see: 
[Mathis & Warren](https://www.biorxiv.org/content/early/2018/10/30/457242)). However, this is the preferred path if you want to test
DeepLabCut on your own computer/data before purchasing a GPU, with the added benefit of
a straightforward installation! Otherwise, use our COLAB notebooks for GPU access for
testing.

Docker: We highly recommend advanced users use the supplied [Docker container](
docker-containers).

NOTE: [Currently GPU support in Docker Desktop is only available on Windows with the 
WSL2 backend.](https://docs.docker.com/desktop/features/gpu/)


--- File: docs/recipes/OtherData.md ---
# How to use data labeled outside of DeepLabCut
- and/or if you merge projects across scorers (see below):



## Using data labeled elsewhere:

Some users may have annotation data in different formats, yet want to use the DLC pipeline. In this case, you need to convert the data to our format. Simply, you can format your data in an excel sheet (.csv file) or pandas array (.h5 file).

Here is a guide to do this via the ".csv" route: (the pandas array route is identical, just format the pandas array in the same way). 

**Step 1**: create a project as describe in the user guide: https://github.com/DeepLabCut/DeepLabCut/blob/main/docs/UseOverviewGuide.md#create-a-new-project

**Step 2**: edit the ``config.yaml`` file to include the body part names, please take care that spelling, spacing, and capitalization are IDENTICAL to the "labeled data body part names". 

**Step 3**: Please inspect the excel formatted sheet (.csv) from our [demo project](https://github.com/DeepLabCut/DeepLabCut/tree/main/examples/Reaching-Mackenzie-2018-08-30/labeled-data/reachingvideo1)
- i.e. this file: https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/Reaching-Mackenzie-2018-08-30/labeled-data/reachingvideo1/CollectedData_Mackenzie.csv

**Step 4**: Edit the .csv file such that it contains the X, Y pixel coordinates, the body part names, the scorer name as well as the relative path to the image: e.g. /labeled-data/somefolder/img017.jpg 
Then make sure the scorer name, and body parts are the same in the config.yaml file. 

Also add for each folder a video to the `video_set` in the config.yaml file. This can also be a dummy variable, but should be e.g. 
C://somefolder.avi if the folder is called somefolder. See demo config.yaml file for proper formatting.

**Step 5**: When you are done, run ``deeplabcut.convertcsv2h5('path_to_config.yaml', scorer= 'experimenter')``

 - The scorer name must be identical to the input name for experimenter that you used when you created the project. This will automatically update "Mackenzie" to your name in the example demo notebook. 

## If you merge projects:

**Step 1**: rename the CSV files to be the target name.

**Step 2**: run and pass the target name ``deeplabcut.convertcsv2h5('path_to_config.yaml', scorer= 'experimenter')``. This will overwrite the H5 file so the data is all merged under the target name.


--- File: docs/recipes/ClusteringNapari.md ---

# Clustering in the napari-DeepLabCut GUI

To increase model performance, one can find the errors in the user-defined label (or in output H5 files after video
inference). You can correct the errors and add them back into the training dataset, a process called active learning.

User errors can be detrimental to model performance, so beyond just `check_labels`, this tool allows you to find your
mistakes. If you are curious about how errors affect performance, read the paper:
[A Primer on Motion Capture with Deep Learning: Principles, Pitfalls, and Perspectives](https://www.sciencedirect.com/science/article/pii/S0896627320307170).

**TL;DR: your data quality matters!**

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661886442646-A9JAWGH3JU3WTTTPMNCW/swaps.jpg?format=1000w" width="900" title="DLC" alt="DLC" align="center" vspace = "10">

```{Hint}
**Labeling Pitfalls: How Corruptions Affect Performance**
(A) Illustration of two types of labeling errors. Top is ground truth, middle is missing a label at the tailbase, and
bottom is if the labeler swapped the ear identity (left to right, etc.). (B) Using a small training dataset of 106
frames, how do the corruptions in (A) affect the percent of correct keypoints (PCK) on the test set as the distance
to ground truth increases from 0 pixels (perfect prediction) to 20 pixels (larger error)? The x axis denotes the
difference in the ground truth to the predicted location (RMSE in pixels), whereas the y axis is the fraction of
frames considered accurate (e.g., z80% of frames fall within 9 pixels, even on this small training dataset, for
points that are not corrupted, whereas for swapped points this falls to z65%). The fraction of the dataset that is
corrupted affects this value. Shown is when missing the tailbase label (top) or swapping the ears in 1%, 5%, 10%,
and 20% of frames (of 106 labeled training images). Swapping versus missing labels has a more notable adverse effect
on network performance.
```

The DeepLabCut toolbox supports **active learning** by extracting outlier frames be several methods and allowing the
user to correct the frames, then retrain the model. See the
[Nature Protocols paper](https://www.nature.com/articles/s41596-019-0176-0) for the detailed steps, or in the docs,
[here](active-learning).

To facilitate this process, here we propose a new way to detect 'outlier frames'.
Your contributions and suggestions are welcomed, so test the
[PR](https://github.com/DeepLabCut/napari-deeplabcut/pull/38) and give us feedback!

This #cookbook recipe aims to show a usecase of **clustering in napari** and is contributed by 2022 DLC AI Resident
[Sabrina Benas](https://twitter.com/Sabrineiitor) 💜.


## Detect Outliers to Refine Labels

### Open `napari` and the `DeepLabCut plugin`

Then open your `CollectedData_<ScorerName>.h5` file. We used the Horse-30 dataset, presented in
[Mathis, Biasi et al. WACV 2022](http://horse10.deeplabcut.org/), as our demo and development set. Here is an example of what it should look like:


<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661885256863-M67UV06P8JHAR1243K1F/1.png?format=750w" width="900" title="DLC" alt="DLC" align="center" vspace = "10">

### Clustering

Click on the button `cluster` and wait a few seconds until it displays a new layer with the cluster:

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661885257126-HRBHYJNJHE0TFH42L034/2.png?format=750w" width="900" title="DLC" alt="DLC" align="center" vspace = "10">

You can click on a point and see the image on the right with the keypoints:

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661885255947-G8PFQC41KDMV6JH75RSO/2_b.png?format=750w" width="900" title="DLC" alt="DLC" align="center" vspace = "10">

### Visualize & refine

If you decided to refine that frame (we moved the points to make outliers obvious), click `show img` and refine them
using the plugin features and instructions:

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661885255421-B9QEUDOJANXWYX4K649G/3.png?format=750w" width="900" title="DLC" alt="DLC" align="center" vspace = "10">

```{Attention}
When you're done, you need to click `ctl-s` to save it.
```

You can go back to the cluster layer by clicking on `close img` and refine another image. Reminder, when you're done
editing you need to click `ctl-s` to save your work. And now you can take the updated `CollectedData` file, create
and **new training shuffle**, and train the network! Read more about how to
[create a training dataset](create-training-dataset).

```{hint}
If you want to change the clustering method, you can modify the file
[kmeans.py](https://github.com/DeepLabCutAIResidency/napari-deeplabcut/blob/cluster1/src/napari_deeplabcut/kmeans.py)
```

::::{important}
You have to keep the way the file is opened (pandas dataframe) and the output has to be the cluster points, the points
colors in the cluster colors and the frame names (in this order).
::::

```

### What's coming

- Right now we demo the feature with user-labels, which are always worth checking and correcting for the best models!
- Next, we will support the machine-labeled.h5 files for full active learning support.

Happy Hacking!


--- File: docs/recipes/BatchProcessing.md ---

# Automate training and video analysis: Batch Processing

## Tips for working with DLC networks:

Now you have a DLC network and are happy with the performance on selected videos, you may want to run it on all your
videos without hassle. If all your videos are in one folder this is easy, simply pass the foldername to
`deeplabcut.analyze_videos(config,[folder])` and you are fine. What if the videos are scattered?

You can create a simple script that runs over all your video folders with the network of choice. Your "key" to this
network is your config.yaml file.

![](https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5ccc5abe0d9297405a428522/1556896461304/howtouseDLC-01.png?format=1000w)

Here is a script that you can use to run video analysis over all the folders.

https://github.com/DeepLabCut/DLCutils/tree/master/SCALE_YOUR_ANALYSIS (see below as well)

Note, if a video is analyzed already, it will not be analyzed again! Alternatively, you can push the outputs elsewhere with the flag `destfolder`. See your options by typing: `deeplabcut.analyze_videos?`

Here is an example script. You can copy/paste into a file and end with ".py" to make it a python script.

```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Feb 10 16:04:37 2019

@author: alex
"""

import os

import deeplabcut

def getsubfolders(folder):
    ''' returns list of subfolders '''
    return [os.path.join(folder, p) for p in os.listdir(folder) if os.path.isdir(os.path.join(folder, p))]

project = "ComplexWheelD3-12-Fumi-2019-01-28"

shuffle = 1

prefix = "/home/alex/DLC-workshopRowland"

projectpath = os.path.join(prefix, project)
config = os.path.join(projectpath, "config.yaml")

basepath = "/home/alex/BenchmarkingExperimentsJan2019"

'''

Imagine that the data (here: videos of 3 different types) are in subfolders:
    /January/January29 ..
    /February/February1
    /February/February2

    etc.

'''

subfolders = getsubfolders(basepath)
for subfolder in subfolders: #this would be January, February etc. in the upper example
    print("Starting analyze data in: ", subfolder)
    subsubfolders = getsubfolders(subfolder)
    for subsubfolder in subsubfolders: #this would be Febuary1, etc. in the upper example...
        print("Starting analyze data in: ", subsubfolder)
        for vtype in [".mp4", ".m4v", ".mpg"]:
            deeplabcut.analyze_videos(config,[subsubfolder],shuffle=shuffle,videotype=vtype,save_as_csv=True)

```

## Now, what about training over multiple Projects

Make your labmates happy by helping run everyone's projects! We use this for workshops, but can easily be adapted for your needs. Here is an example script. You can copy/paste into a file and end with ".py" to make it a python script.
```
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Nov 17 14:12:43 2018

An example script to automate analysis on 3 different GPUs for different projects. Feel free to adapt this to your needs!

@author: alex mathis

"""

import subprocess, sys
import numpy as np
import itertools
import os

import deeplabcut

epochs = 200

model=int(sys.argv[1])

Projects=[["project1-phoenix-2019-01-28"], ["ComplexWheelD3-12-Fumi-2019-01-28", "maze-ariel-2019-01-28"], ["TBI-BvA-2019-01-28", "group-eli-2019-01-28"]]

shuffle=1

prefix = "/home/alex/DLC-workshopRowland"

for project in Projects[model]:
    projectpath = os.path.join(prefix, project)
    config = os.path.join(projectpath, "config.yaml")

    cfg = deeplabcut.auxiliaryfunctions.read_config(config)
    previous_path = cfg["project_path"]

    cfg["project_path"]=projectpath
    deeplabcut.auxiliaryfunctions.write_config(config, cfg)

    print("This is the name of the script: ", sys.argv[0])
    print("Shuffle: ", shuffle)
    print("config: ", config)

    deeplabcut.create_training_dataset(config, Shuffles=[shuffle])

    deeplabcut.train_network(config, shuffle=shuffle, max_snapshots_to_keep=5, epochs=epochs)
    print("Evaluating...")
    deeplabcut.evaluate_network(config, Shuffles=[shuffle], plotting=True)

    print("Analyzing videos..., switching to last snapshot...")
    for vtype in ['.mp4','.m4v','.mpg']:
        try:
            deeplabcut.analyze_videos(config, [str(os.path.join(projectpath, "videos"))], shuffle=shuffle, videotype=vtype, save_as_csv=True)
        except:
            pass

    print("DONE WITH ", project," resetting to original path")
    cfg["project_path"] = previous_path
    deeplabcut.auxiliaryfunctions.write_config(config, cfg)
    ```


--- File: docs/recipes/pose_cfg_file_breakdown.md ---
# The `pose_cfg.yaml` Guideline Handbook

::::{warning}
The following is specific to Tensorflow-based models. To read the equivalent explanations for Pytorch-based models,
click [here](dlc3-pytorch-config)
::::

👋 Hello! Mabuhay! Hola! This recipe was written by the [2023 DLC AI Residents](https://www.deeplabcutairesidency.org/)!

When you train, evaluate, and run inference with a neural network there are hyperparatmeters you must consider. While DLC attempts to set the "globally good for everyone" parameters, you might want to change them. Therefore, in this recipe we will review the pose config parameters related to neural network models' and the related data augmentation!

# 1. What is the *pose_cfg.yml* file?
<a id="whatisposecfg"></a>
- The `pose_cfg.yaml` file offers easy access to a range of training parameters that the user may want or have to adjust depending on the used dataset and task. 
- You will find the file in the dlc-models > test and train sub-directories. There is also a button in the GUI to directly open this file.
- This recipe is aimed at giving an average user an intuition on those hyperparameters and situations in which addressing them can be useful.

# 2. Quick start: full parameter list TOC
<a id="fullparamlist"></a>
- [2. Full parameter list](#2-full-parameter-list)
  - [2.1 Training Hyperparameters](#21-training-hyperparameters)
    - [2.1.A `max_input_size` and `min_input_size`](#21a-max_input_size-and-min_input_size)
    - [2.1.B `global_scale`](#21b-global_scale)
    - [2.1.C `batch_size`](#21c-batch_size)
    - [2.1.D `pos_dist_thresh`](#21d-pos_dist_thresh)
    - [2.1.E `pafwidth`](#21e-pafwidth)
  - [2.2 Data augmentation parameters](#22-data-augmentation-parameters)
    - [Geometric transformations](#geometric-transformations)
    - [2.2.1 `scale_jitter_lo` and `scale_jitter_up`](#221-scale_jitter_lo-and-scale_jitter_up)
    - [2.1.2 `rotation`](#212-rotation)
    - [2.2.3 `rotratio` (rotation ratio)](#223-rotratio-rotation-ratio)
    - [2.2.4 `fliplr` (or a horizontal flip)](#224-fliplr-or-a-horizontal-flip)
    - [2.2.5 `crop_size`](#225-crop_size)
    - [2.2.6 `crop_ratio`](#226-crop_ratio)
    - [2.2.7 `max_shift`](#227-max_shift)
    - [2.2.8 `crop_sampling`](#228-crop_sampling)
    - [Kernel transformations](#kernel-transformations)
    - [2.2.9 `sharpening` and `sharpenratio`](#229-sharpening-and-sharpenratio)
    - [2.2.10 `edge`](#2210-edge)
- [References](#references)

<a id="hyperparam"></a>
## 2.1 Training Hyperparameters 

<a id="input_size"></a>
### 2.1.A `max_input_size` and `min_input_size`
The default values are `1500` and `64`, respectively. 

💡Pro-tip:💡
- change `max_input_size` when the resolution of the video is higher than 1500x1500 or when `scale_jitter_up` will possibly go over that value
- change `min_input_size` when the resolution of the video is smaller than 64x64 or when `scale_jitter_lo` will possibly go below that value

<a id="global_scale"></a>
### 2.1.B `global_scale`
The default value is `0.8`. It's the most basic, first scaling that happens to all images in the training queue.

💡Pro-tip:💡
- With images that are low resolution or lack detail, it may be beneficial to increase the `global_scale` to 1, to keep the original size and retain as much information as possible.

### 2.1.C `batch_size`
<a id="batch_size"></a>

The default for single animal projects is 1, and for maDLC projects it's `8`. It's the number of frames used per training iteration.

In both cases, you can increase the batchsize up to the limit of your GPU memory and train for a lower number of iterations. The relationship between the number of iterations and `batch_size` is not linear, so `batch_size: 8` doesn't mean you can train for 8x less iterations, but like with every training, plateauing loss can be treated as an indicator of reaching optimal performance.

💡Pro-tip:💡
- Having a higher `batch_size` can be beneficial in terms of models' generalization

___________________________________________________________________________________

Values mentioned above and the augmentation parameters are often intuitive, and knowing our own data, we are able to decide on what will and won't be beneficial. Unfortunately, not all hyperparameters are this simple or intuitive. Two parameters that might require some tuning on challenging datasets are `pafwidth` and `pos_dist_thresh`. 

<a id="pos"></a>
### 2.1.D `pos_dist_thresh`
The default value is `17`. It's the size of a window within which detections are considered positive training samples, meaning they tell the model that it's going in the right direction. 

<a id="paf"></a>
### 2.1.E `pafwidth`
The default value is `20`. PAF stands for part affinity fields. It is a method of learning associations between pairs of bodyparts by preserving the location and orientation of the limb (the connection between two keypoints). This learned part affinity helps in proper animal assembly, making the model less prone to associating bodyparts of one individual with those of another. [1](#ref1)
<a id="data_aug"></a>

## 2.2 Data augmentation parameters
In the simplest form, we can think of data augmentation as something similar to imagination or dreaming. Humans imagine different scenarios based on experience, ultimately allowing us to better understand our world. [2, 3, 4](#references)

Similarly, we train our models to different types of "imagined" scenarios, which we limit to the foreseeable ones, so we ultimately get a robust model that can more likely handle new data and scenes. 

Classes of data augmentations, characterized by their nature, are given by:
- [**Geometric transformations**](#geometric)
    1. [`scale_jitter_lo` and `scale_jitter_up`](#scale_jitter)
    2. [`rotation`](#rot)
    3. [`rotratio`](#rotratio)
    4. [`mirror`](#mirror)
    5. [`crop size`](#crop_size)
    6. [`crop ratio`](#crop_ratio)
    7. [`max shift`](#max_shift)
    8. [`crop sampling`](#crop_sampling)
- [**Kernel transformations**](#kernel)
    9. [`sharpening` and `sharpen_ratio`](#sharp)
    10. [`edge_enhancement`](#edge)

<a id="geometric"></a>
### Geometric transformations
**Geometric transformations** such as *flipping*, *rotating*, *translating*, *cropping*, *scaling*, and *injecting noise*, which are very good for positional biases present in the training data.

<a id="scale_jitter"></a>
### 2.2.1 `scale_jitter_lo` and `scale_jitter_up`
*Scale jittering* resizes an image within a given resize range. This allows the model to learn from different sizes of objects in the scene, therefore increasing its robustness to generalize, especially on newer scenes or object sizes.

The image below, retrieved from [3](#ref3), illustrates the difference between two scale jittering methods.

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1690471482096-VLLQJU4H6AH6ESMZGNQW/scale_jittering.png?format=1000w">

During training, each image is randomly scaled within the range `[scale_jitter_lo, scale_jitter_up]` to augment training data. The default values for these two parameters are:
- `scale_jitter_lo = 0.5`
- `scale_jitter_up = 1.25`

💡Pro-tips:💡
- ⭐⭐⭐ If the target animal/s do not have an incredibly high variance in size throughout the video (e.g., jumping or moving towards the static camera), keeping the **default** values **unchanged** will give just enough variability in the data for the model to generalize better ✅

- ⭐⭐However, you may want to adjust these parameters if you want your model to:
  - handle new data with possibly **larger (25% bigger than original)** animal subjects ➡️ in this scenario, increase the value of *scale_jitter_up*
  - handle new data with possibly **smaller (50% smaller than the original)** animal subjects ➡️ in this scenario, decrease the value of *scale_jitter_lo*
  - **generalize well in new set-ups/environments** with minimal to no pre-training
  ⚠️ But as a consequence, **training time will take longer**.😔🕒
- ⭐If you have a fully static camera set-up and the sizes of the animals do not vary much, you may also try to **shorten** this range to **reduce training time**.😃🕒(⚠️ but, as a consequence, your model might only fit your data and not generalize well)

<a id="rot"></a>
### 2.1.2 `rotation`
*Rotation augmentations* are done by rotating the image right or left on an axis between $1^{\circ}$ and $359^{\circ}$. The safety of rotation augmentations is heavily determined by the rotation degree parameter. Slight rotations such as between $+1^{\circ}$ and $+20^{\circ}$ or $-1^{\circ}$ to $-20^{\circ}$ is generally an acceptable range. Keep in mind that as the rotation degree increases, the precision of the label placement can decrease 

The image below, retrieved from [2](#ref2), illustrates the difference between the different rotation degrees.

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1690471478493-Z4JEWJG0I7MB9AYCB322/augset_rot.png?format=750w">

During training, each image is rotated $+/-$ the `rotation` degree parameter set. By default, this parameter is set to `25`, which means that the images are augmented with a $+25^{\circ}$ rotation of itself and a $-25^{\circ}$ degree rotation of itself. Should you want to opt out of this augmentation, set the rotation value to `False`.

💡Pro-tips:💡
- ⭐If you have labelled all the possible rotations of your animal/s, keeping the **default** value **unchanged** is **enough** ✅ 

- However, you may want to adjust this parameter if you want your model to:
  - handle new data with new rotations of the animal subjects 
  - handle the possibly unlabelled rotations of your minimally-labeled data 
  - But as a consequence, the more you increase the rotation degree, the more the original keypoint labels may not be preserved

<a id="rotratio"></a>
### 2.2.3 `rotratio` (rotation ratio)
This parameter in the DLC module is given by the percentage of sampled data to be augmented from your training data. The default value is set to `0.4` or $40\%$. This means that there is a $40\%$ chance that images within the current batch will be rotated.

💡Pro-tip:💡
- ⭐ Generally, keeping the **default** value **unchanged** is **enough** ✅ 

<a id="fliplr"></a>
### 2.2.4 `fliplr` (or a horizontal flip)
**Mirroring**, otherwise called **horizontal axis fipping**, is much more common than flipping the vertical axis. This augmentation is one of the easiest to implement and has proven useful on datasets such as CIFAR-10 and ImageNet. However, on datasets involving text recognition, such as MNIST or SVHN, this is not a label-preserving transformation.

The image below is an illustration of this property (shown on the right-most column).

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1690471476980-RGW6NDYR5BSMN27G9K30/augset_flip.png?format=1500w">

This parameter randomly flips an image horizontally to augment training data.
By default, this parameter is set to `False` especially on poses with mirror symmetric joints (for example, so the left hand and right hand are not swapped).

💡Pro-tip:💡
- ⭐ If you work with labels with symmetric joints, keep the **default** value **unchanged** - unless the dataset is biased (animal moves mostly in one direction, but sometimes in the opposite)✅
- Keeping the default value to `False` will work well in most cases.

<a id ="crop_size"></a>
 ### 2.2.5 `crop_size`
 Cropping consists of removing unwanted pixels from the image, thus selecting a part of the image and discarding the rest, reducing the size of the input. 

 In DeepLabCut *pose_config.yaml* file, by default, `crop_size` is set to (`400,400`), width, and height, respectively. This means it will cut out parts of an image of this size.

 💡Pro-tip:💡
  - If your images are very large, you could consider increasing the crop size. However, be aware that you'll need a strong GPU, or you will hit memory errors!
  - If your images are very small, you could consider decreasing the crop size. 

 <a id ="cropratio"></a>
 ### 2.2.6 `crop_ratio`
  Also, the number of frames to be cropped is defined by the variable `cropratio`, which is set to `0.4` by default. That means that there is a $40\%$ the images within the current batch will be cropped. By default, this value works well. 

  <a id ="max_shift"></a>
 ### 2.2.7 `max_shift`

  The crop shift between each cropped image is defined by `max_shift` variable, which explains the max relative shift to the position of the crop centre. By default is set to `0.4`, which means it will be displaced 40% max from the center to not apply identical cropping each time the same image is encountered during training - this is especially important for `density` and `hybrid` cropping methods.

 The image below is modified from 
 [2](#references). 
 
 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1690471479692-R078RFZXIQ8K552OIOFP/cropping.png?format=750w">

 <a id ="crop_sampling"></a>
 ### 2.2.8 `crop_sampling`
 Likewise, there are different cropping sampling methods (`crop_sampling`), we can use depending on how our image looks like. 

 💡Pro-tips💡
 - For highly crowded scenes, `hybrid` and `density` approaches will work best. 
 - `uniform` will take out random parts of the image, disregarding the annotations completely
 - 'keypoint' centers on a random keypoint and crops based on that location - might be best in preserving the whole animal (if reasonable `crop_size` is used)

 <a id ="kernel"></a>
 ### Kernel transformations 
 Kernel filters are very popular in image processing to sharpen and blur images. Intuitively, blurring an image might increase the motion blur resistance during testing. Otherwise, sharpening for data enhancement could result in capturing more detail on objects of interest.

 <a id ="sharp"></a>
 ### 2.2.9 `sharpening` and `sharpenratio`
 In DeepLabCut *pose_config.yaml* file, by default, `sharpening` is set to `False`, but if we want to use this type of data augmentation, we can set it `True` and specify a value for `sharpenratio`, which by default is set to `0.3`. Blurring is not defined in the *pose_config.yaml*, but if the user finds it convenient, it can be added to the data augmentation pipeline. 

 The image below is modified from 
 [2](#references). 
 
 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1690471480991-HBZAYJP1FY0K8H2KB8DB/kernelfilter.png?format=1500w">

 <a id ="edge"></a>
 ### 2.2.10 `edge`
 Concerning sharpness, we have an additional parameter, `edge` enhancement, which enhances the edge contrast of an image to improve its apparent sharpness. Likewise, by default, this parameter is set `False`, but if you want to include it, you just need to set it `True`.


# References 
 <ol id="references">
     <li id="ref1">Cao, Z., Simon, T., Wei, S. E., & Sheikh, Y. (2017). Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (pp. 7291-7299).<a href="https://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html">https://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html</a></li>
     <li id="ref2">Mathis, A., Schneider, S., Lauer, J., & Mathis, M. W. (2020). A Primer on Motion Capture with Deep Learning: Principles, Pitfalls, and Perspectives. In Neuron (Vol. 108, Issue 1, pp. 44-65). <a href="https://doi.org/10.1016/j.neuron.2020.09.017">https://doi.org/10.1016/j.neuron.2020.09.017</a></li>
     <li id="ref3">Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk, E. D., Le, Q. V., & Zoph, B. (2020). Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation (Version 2). arXiv. <a href="https://doi.org/10.48550/ARXIV.2012.07177">https://doi.org/10.48550/ARXIV.2012.07177</a></li>
     <li id="ref4">Shorten, C., & Khoshgoftaar, T. M. (2019). A survey on Image Data Augmentation for Deep Learning. In Journal of Big Data (Vol. 6, Issue 1). <a href="https://doi.org/10.1186/s40537-019-0197-0">https://doi.org/10.1186/s40537-019-0197-0</a> </li>
 </ol>


--- File: docs/recipes/MegaDetectorDLCLive.md ---
# 💚 MegaDetector+DeepLabCut 💜

[DeepLabCut-Live](https://github.com/DeepLabCut/DeepLabCut-live) is an open source and free real-time package from DeepLabCut that allows for real-time, low-latency pose estimation.  [The DeepLabCut-ModelZoo](http://modelzoo.deeplabcut.org/) is our growing collection of pretrained animal models for rapid deployment; no training is typically required to use these models. MegaDetector is a free open software trained to detect animals, people, and vehicles from camera trap images. Check [here](https://github.com/microsoft/CameraTraps/blob/main/megadetector.md) for further information.

In this #cookbook recipe, we show you how to use MegaDetector to detect animals and run DeepLabCut-Live (using ModelZoo models) to get the pose estimation. This doc is contributed by 2022 DLC AI Resident [Nirel Kadzo](https://github.com/Kadzon) 💜!

## What is MegaDetector?

 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661853650953-3L3EZYF69701J8FJZCPT/anim1.jpeg?format=500ww" width="250" title="DLC" alt="DLC" align="right" vspace = "5">

MegaDetector detects an animal and generates a bounding box around the animal. Thanks to [Sara Beery](https://beerys.github.io/) for visiting the #DLCAIResidents in the summer of 2022 to tell us more about this amazing project. An example result is shown:



## DeepLabCut-Live

DeepLabCut-Live! is a real-time package for running DeepLabCut. However, you can also use it as a lighter-weight
package for running DeeplabCut even if you don't need real-time. It's very useful to use in HPC or servers, or in Apps,
as we do here. To read more, check out the [docs](deeplabcut-live).

### MegaDetector meets DeepLabCut

The combination of MegaDetector and DeepLabCut now enables animal pose estimation on animal-bounded images. Here is an example of the `full_macaque` model, which is from  MacaquePose. Model contributed by Jumpei Matsumoto, at the Univ of Toyama. See their paper for many details [here](https://www.biorxiv.org/content/10.1101/2020.07.30.229989v2). If you use this model, please [cite their paper](https://doi.org/10.3389/fnbeh.2020.581154).

 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661853652273-KG8FYHYDVJ5IBPY0UDVS/monkmddlc.png?format=500w" width="600" title="DLC" alt="DLC" align="center" vspace = "5">

# 🤗 HuggingFace App

We use the [Hugging Face](huggingface.co) spaces based on gradio to create an App of MegaDetector+DeepLabCut for you to interact with it and to try it out for yourself. Thanks to [Merve Noyan](https://github.com/merveenoyan) who visited the #DLCAIResidents in the summer of 2022 to teach us about their ecosystem, and thanks to the other App co-authors from the DLC Residency Program (see the [App page](https://huggingface.co/spaces/DeepLabCut/MegaDetector_DeepLabCut)).

Let's get into the details of how to use the App:

1. Click on this link to be redirected to the [MegaDetector+DeepLabCut application](https://huggingface.co/spaces/DeepLabCut/MegaDetector_DeepLabCut) on **Hugging Face**.

2. Upload your image on the *Input Image* section or drag and drop.

3. Choose the features for your image

 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661853652069-DDS019L4HA245HZOOI3F/toggle.png?format=500w" width="400" title="DLC" alt="DLC" align="center" vspace = "15">

- ``Select MegaDetector model`` lets you choose between md_v5a and md_v5b, you can find out more about them [here](https://github.com/microsoft/CameraTraps/releases). They run on YOLOv5 which makes it 3x-4x faster than prior versions.

- ``Select DeepLabCut Model`` choose the relevant ModelZoo model closest to the image you uploaded. The selected model will run on the image to predict the keypoints on the animal.

```{hint}
To get close to accurate keypoints in your model, the animal you upload into the interface should have the animal model listed in "Select DeepLabCut Model" panel.
```

- ``Run DLClive`` checkbox allows you to run DeepLabCut-Live directly on the image without MegaDetector. However, MegaDetector often simplifies the pose estimation by blocking out the pixels outside the bounding box. But no harm to run it (just might be slower), test it out for yourself ;)

- ``Set confidence threshold for animal detections`` in the example above, the confidence threshold is set for 0.8, this means MegaDetector will put a bounding box if it is >0.8 sure it is an animal. The image displayed has a 0.94 confidence level.

- ``Set confidence threshold for keypoints`` suggests how confident the model is about predicting the accurate key points on the animal. This is displayed by the opacity of the coloured keypoints on the animal.

- ``Set marker size, Set font size, Select keypoiny label font`` are design specs you can choose for yourself - we all love pretty plots!

4. Once set and you are satisfied with the image and features, submit the image. The expected output will display your input image: with the animal(s) surrounded by a bounding box (if used), the tracked keypoints, and the labels. A downloadable `JSON` file with the markings as shown below:

 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661853655974-YUBP0QQ1LR144NT37TLP/outputdog.png?format=500w" width="400" title="DLC" alt="DLC" align="center" vspace = "15">

 Image from [Scientific American](https://www.scientificamerican.com/article/dogs-personalities-arent-determined-by-their-breed/).

All information seen on the output image is recorded on the **Download JSON file**. The snippet below is commented on to give you an overall understanding of what the code means 😀
```
{
 "date": "2022-08-26",
 "MD_model": "md_v5a",  //Megadetector model used
 "file": "image0.jpg",  //image filename uploaded
 "number_of_bb": 1,     //number of bounding boxes detected on the image
 "dlc_model": "full_dog",  //model used
 "bb_0": {              
  "corner_1": [          //top left corner
   76.08082580566406,    //x  
   91.02932739257812     //y
  ],
  "corner_2": [          //bottom right corner
   393.8626708984375,    //x
   399.9506530761719     //y
  ],
  "predict MD": "animal",  // MegaDetector predicts the image is in class animal
  "confidence MD": 0.9437874555587769,  // 0.94% confident it's an animal
  "dlc_pred": {     //keypoints prediction coordinates
   "Nose": [        //label
    264.89501953125,    //x
    89.19121551513672,  //y
    0.9611953496932983  //z
   ],
   ...
}
```


```{hint}
To experiment with more camera trap images, check out [Lila Science!](https://lila.science/)
```

Examples have also been added to the Hugging Face interface where you can try out a variety of animals to get a feel of things and also add your own.

## Examples

We encourage you to try out and experiment on your camera trap or other animal images. Indeed, we found it is not only limited to camera trap images you can test it out with photos taken from your camera. Have a look at a 🦊picture Mackenzie took in Geneva and used the MegaDetector+DeepLabCut [Hugging Face](huggingface.co).

 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661854010041-5RQGQTRSTKUDYU9KSTIE/foxGeneva.png?format=750ww" width="400" title="DLC" alt="DLC" align="center" vspace = "15">

Or these lil' cuties 🐶🐶🙀🐶 outside a restaurant, from the [Twitter meme](https://twitter.com/standardpuppies/status/1563188163962515457?s=21&t=f2kM2HoUygyLmmAH7Ho-HQ).

 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661853654276-WEA4UUD7I1VEGHXSMIXE/pupscat.png?format=300w" width="400" title="DLC" alt="DLC" align="center" vspace = "15">

```{note}
DLC-Live allows you to process videos and frames in bulk, however the current release of MegaDetetctor+DeepLabCut-Live allows you to process one image at a time. But stay tuned for further releases, we are just getting started ;)
```


### Developer Mode:
To run it locally you can `git clone` the repository on your terminal and explore MegaDetector+DeepLabCut for yourself :)

In your terminal run each line:
```bash
git clone https://huggingface.co/spaces/DeepLabCut/MegaDetector_DeepLabCut

conda create -n megaDLC python==3.8
conda activate megaDLC

cd MegaDetector_DeepLabCut

pip install -r requirements.txt
python3 app.py
```

It should then print out links for you to locally load. Have fun and happy hacking!


--- File: docs/recipes/installTips.md ---
(installation-tips)=
# Installation Tips

## How to use the latest updates directly from GitHub

We often update the master deeplabcut code base on GitHub, and then ~1 a month we push out a stable release on pypi. This is what most users turn to on a daily basis (i.e. pypi is where you get your `pip install deeplabcut` code from! But, sometimes we add things to the repo that are not yet integrated, or you might want to edit the code yourself. Here, we show you how to do this.

### Method 1:

If you want to *use* the latest, you can use pip and add the specific tags, such as `gui`, etc. by modifying and running: 
```
pip install --upgrade 'git+https://github.com/deeplabcut/deeplabcut.git#egg=deeplabcut[gui]'
```

which will download and update deeplabcut, and any dependencies that don't match the new version. If you want to force upgrade all of the dependencies to the latest available versions, too, then use the additional `--upgrade-strategy eager`, i.e.:

```
pip install --upgrade --upgrade-strategy eager 'git+https://github.com/deeplabcut/deeplabcut.git#egg=deeplabcut[gui]'
```

### Method 2: 

If you want to be able to *edit* the source code of DeepLabCut, i.e., maybe add a feature or fix a 🐛, then you need to "clone" the source code:

**Step 1:**

- git clone the repo into a folder on your computer:  

- click on this green button and copy the link:

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1581984907363-G8AFGX4V20Y1XD1PSZAK/ke17ZwdGBToddI8pDm48kGJBV0_F4LE4_UtCip_K_3lZw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVE0ejQCe16973Pm-pux3j5_Oqt57D2H0YbaJ3tl8vn_eR926scO3xePJoa6uVJa9B4/gitclone.png?format=500w)

- then in the terminal type: `git clone https://github.com/DeepLabCut/DeepLabCut.git`

**Step 2:**

- Now you will work from the terminal inside this cloned folder:

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1581985288123-V8XUAY0C0ZDNJ5WBHB7Y/ke17ZwdGBToddI8pDm48kIsGBOdR9tS_SxF6KQXIcDtZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpz3c8X74DzCy4P3pv-ZANOdh-3ZL9iVkcryTbbTskaGvEc42UcRKU-PHxLXKM6ZekE/terminal.png?format=750w)

- Now, when you start `ipython` and `import deeplabcut` you are importing the folder "deeplabcut" - so any changes you make, or any changes we made before adding it to the pip package, are here.

- You can also check which deeplabcut you are importing by running: `deeplabcut.__file__`

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1581985466026-94OCSZJ5TL8U52JLB5VU/ke17ZwdGBToddI8pDm48kNdOD5iqmBzHwUaWGKS6qHBZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpyQPoegsR7K4odW9xcCi1MIHmvHh95_BFXYdKinJaRhV61R4G3qaUq94yWmtQgdj1A/importlocal.png?format=750w)

If you make changes to the code/first use the code, be sure you run `./resinstall.sh`, which you find in the main DeepLabCut folder:

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1609353210708-FRNREI7HUNS4GLDSJ00G/ke17ZwdGBToddI8pDm48kAya1IcSd32bok4WHvykeicUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dq18t0tDkB2HMfL2JGcLHN27k5rSOPIU8nEAZT0p1MiSCjLISwBs8eEdxAxTptZAUg/Screen+Shot+2020-12-30+at+7.33.16+PM.png?format=2500w)

Then, you can see what version you have with `deeplabcut.__version__`

If you make changes, you can also then utilize our test scripts. Run the desired test script found here (you will need to git clone first): https://github.com/DeepLabCut/DeepLabCut/blob/master/examples/.

i.e., for example:
```
# Testing with the PyTorch engine
python testscript_pytorch_multi_animal.py

# Testing with the TensorFlow engine
python testscript_multianimal.py
```


## Installation on Ubuntu 18.04 LTS

### Here are our tips for an easy installation. This is done on a fresh computer installation (Ubuntu 18.04 LTS)

install gcc:

```
sudo apt install gcc
```

then, download CUDA 10 from here: https://developer.nvidia.com/cuda-downloads and follow the instructions... ie:

```
wget http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda_10.1.243_418.87.00_linux.run
sudo sh cuda_10.1.243_418.87.00_linux.run
```
 with the exception that I also (afterwards):

```
sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt update
sudo ubuntu-drivers autoinstall
```

**then reboot**

Check gcc -version:

```
gcc --version
```

output:
```
gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE
```
Then:
```
sudo apt install nvidia-cuda-toolkit gcc-7
```

```
nvcc --version
```

then you can check:

```
nvidia-smi
```

and you'll see driver version, CUDA version, status of graphics card(s).

## Installation on Ubuntu 20.04 LTS

Hello! Another cookbook entry on how to install your freshly installed 20.04 LTS system for DLC use. Namely, CUDA, drivers, Docker, and anaconda!

### Let's start with CUDA support for your GPU:

`sudo apt install gcc`

then:

```python

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.1-465.19.01-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2004-11-3-local_11.3.1-465.19.01-1_amd64.deb
sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub
sudo apt-get update
sudo apt-get -y install cuda

```

Then:
```
sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt update
sudo ubuntu-drivers autoinstall
```

then:

`reboot`

re-open terminal and check gcc version:

`gcc --version`

output:
```python
gcc --version
gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Copyright (C) 2019 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```
Then finish installation:

`sudo apt install nvidia-cuda-toolkit gcc-9`

Then check:

`nvcc --version`

All set! If error messages, read them carefully as they often tell you how to fix it, or what to google :D

Now you can see CUDA, DRIVER, GPU(s):

`nvidia-smi`

output:

```python
nvidia-smi
Tue Jun 22 18:46:26 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:0B:00.0  On |                  N/A |
|  0%   46C    P8    11W / 200W |    252MiB /  8116MiB |      5%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

```

### Next, Docker!

```
sudo apt-get install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg \
    lsb-release
```
add key: `curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg`

```
echo \
  "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
```
Then:
```
sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io
```

some clean up:
`sudo apt autoremove`

now you can run `sudo docker run hello-world`

and get:
```
Hello from Docker!
This message shows that your installation appears to be working correctly.

To generate this message, Docker took the following steps:
 1. The Docker client contacted the Docker daemon.
 2. The Docker daemon pulled the "hello-world" image from the Docker Hub.
    (amd64)
 3. The Docker daemon created a new container from that image which runs the
    executable that produces the output you are currently reading.
 4. The Docker daemon streamed that output to the Docker client, which sent it
    to your terminal.

To try something more ambitious, you can run an Ubuntu container with:
 $ docker run -it ubuntu bash

Share images, automate workflows, and more with a free Docker ID:
 https://hub.docker.com/

For more examples and ideas, visit:
 https://docs.docker.com/get-started/
```

### Next, Anaconda!

Click here to get the ubuntu/linux package: https://www.anaconda.com/products/individual#linux

this downloads a file, save it (I save into downloads)

then `cd Downloads`:

and run:

`bash Anaconda3-2021.05-Linux-x86_64.sh`

and you get:
```python
Welcome to Anaconda3 2021.05

In order to continue the installation process, please review the license
agreement.
Please, press ENTER to continue
>>>
```

Follow prompts!

### Next, DeepLabCut!

Given this is a totally fresh install, here are a few things that I also needed: `sudo apt install libcanberra-gtk-module libcanberra-gtk3-module`

We strongly recommend for Ubuntu users to use Docker (https://hub.docker.com/r/deeplabcut/deeplabcut) - it's a much more reproducible environment.

If you want to use our conda file, then I proceeded below:

I grab the conda file from the website at www.deeplabcut.org. Simply click to download. For me, this goes into Downloads.

So, I open a terminal, `cd Downloads`, and then run: `conda env create -f DEEPLABCUT.yaml`

Follow prompts!

## Troubleshooting: Note, if you get a failed build due to wxPython (note, this does not happen on Ubuntu 18, 16, etc), i.e.:

```{warning}
DeepLabCut no longer uses `wxpython` for its GUI - if you're getting such an error, 
you're likely installing an old version of DeepLabCut.
```

```python
ERROR: Command errored out with exit status 1: /home/mackenzie/anaconda3/envs/DLC-GPU/bin/python -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '"'"'/tmp/pip-install-0jsmkrr1/wxpython_aeff462b2060421a9cf65df55f63a126/setup.py'"'"'; __file__='"'"'/tmp/pip-install-0jsmkrr1/wxpython_aeff462b2060421a9cf65df55f63a126/setup.py'"'"';f = getattr(tokenize, '"'"'open'"'"', open)(__file__) if os.path.exists(__file__) else io.StringIO('"'"'from setuptools import setup; setup()'"'"');code = f.read().replace('"'"'\r\n'"'"', '"'"'\n'"'"');f.close();exec(compile(code, __file__, '"'"'exec'"'"'))' install --record /tmp/pip-record-pzy9q5u2/install-record.txt --single-version-externally-managed --compile --install-headers /home/mackenzie/anaconda3/envs/DLC-GPU/include/python3.7m/wxpython Check the logs for full command output.

failed

CondaEnvException: Pip failed
```
You can either: remove conda env: `conda remove --name DEEPLABCUT --all`, open the DLC-GPU.yaml file (any text editor!) and change `deeplabcut[gui]` to `deeplabcut`. Then run: `conda env create -f DEEPLABCUT.yaml` again...

then you will get:
```python

 Successfully uninstalled decorator-5.0.9
Successfully installed PyWavelets-1.1.1 absl-py-0.13.0 astor-0.8.1 bayesian-optimization-1.2.0 chardet-4.0.0 click-8.0.1 cycler-0.10.0 cython-0.29.23 decorator-4.4.2 deeplabcut-2.1.10.4 filterpy-1.4.5 gast-0.2.2 google-pasta-0.2.0 grpcio-1.38.1 h5py-2.10.0 idna-2.10 imageio-2.9.0 imageio-ffmpeg-0.4.4 imgaug-0.4.0 intel-openmp-2021.2.0 joblib-1.0.1 keras-applications-1.0.8 keras-preprocessing-1.1.2 kiwisolver-1.3.1 llvmlite-0.34.0 markdown-3.3.4 matplotlib-3.1.3 moviepy-1.0.1 msgpack-1.0.2 msgpack-numpy-0.4.7.1 networkx-2.5.1 numba-0.51.1 numexpr-2.7.3 numpy-1.17.5 opencv-python-4.5.2.54 opencv-python-headless-3.4.9.33 opt-einsum-3.3.0 pandas-1.2.5 patsy-0.5.1 pillow-8.2.0 proglog-0.1.9 protobuf-3.17.3 psutil-5.8.0 pytz-2021.1 pyyaml-5.4.1 requests-2.25.1 ruamel.yaml-0.17.9 ruamel.yaml.clib-0.2.2 scikit-image-0.18.1 scikit-learn-0.24.2 scipy-1.7.0 statsmodels-0.12.2 tables-3.6.1 tabulate-0.8.9 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.5 tensorpack-0.9.8 termcolor-1.1.0 threadpoolctl-2.1.0 tifffile-2021.6.14 tqdm-4.61.1 urllib3-1.26.5 werkzeug-2.0.1 wrapt-1.12.1

done
#
# To activate this environment, use
#
#     $ conda activate DEEPLABCUT
#
# To deactivate an active environment, use
#
#     $ conda deactivate
```

Activate! `conda activate DEEPLABCUT` and then run: `conda install -c conda-forge wxpython`.

Then run `python -m deeplabcut` which launches the DLC GUI.

## DeepLabCut MacOS M-chip installation environment instructions:

This only assumes you have anaconda installed. Use the `DEEPLABCUT_M1.yaml` conda file
if you have a newer MacBook (with an M1, M2, M3, M4 chip or more later), and follow
these steps:

(1) git clone the deeplabcut cut repo:

```bash
git clone https://github.com/DeepLabCut/DeepLabCut.git
```

(2) in the program terminal run: `cd DeepLabCut/conda-environments`

(3) Then, run:

```bash
conda env create -f DEEPLABCUT.yaml
```

(4) Finally, activate your environment and to launch DLC with the GUI

```bash
conda activate DEEPLABCUT
python -m deeplabcut
```

The GUI will open. Of course, you can also run DeepLabCut in headless mode.

If **you want to use the TensorFlow engine**, you'll need to install the `apple_mchips`
extra with DeepLabCut. You can do so by running:

```bash
pip install deeplabcut[apple_mchips]
```

## How to confirm that your GPU is being used by DeepLabCut

During training and analysis steps, DeepLabCut does not use the GPU processor heavily. To confirm that DeepLabCut is properly using your GPU:

**On Windows**:

(1) Open the task manager. If it looks like the image below, click on "More Details" 

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/a0db3157-2228-4444-8084-36801659f272/installBrandon1.png?format=500w)

(2) That will bring up the following, which still isn't helpful and has caused confusion for users. The %GPU does not reflect DeepLabCut usage.

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/117e3573-60bb-4599-b00b-c75276b24173/installBrandon2.png?format=500w)

(3) Click on the **Performance** tab. On that page, click on the small arrow under GPU (it might start as **3D**, and change it to **CUDA**.  

(4) During training, you should see the **Dedicated GPU memory usage** increase to near maximum, and you should see some activity in the **CUDA** graph. The graph below is the activity while running `testscript.py`.

![](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/b1d03ca0-f8ba-4a31-a399-6e86856c81b0/installBrandon3.png?format=500w)

(5) If you don't see activity there during training, then your GPU is likely not installed correctly for DeepLabCut. Return to the installation instructions, and be sure you installed CUDA 11+, and ran `conda install cudnn -c conda-forge` after installing DeepLabCut.

## How to install DeepLabCut for Intel and AMD GPUs on Windows for the TensorFlow engine

If you are on Windows 10/11 and have a DirectX 12 compatible GPU from any vendor (AMD, Intel, or Nvidia), you utilise GPU acceleration for inference, with an installation that is consistent between devices. This method uses [Tensorflow-directml](https://github.com/microsoft/tensorflow-directml) which uses DirectML instead of Cuda for ML training and inference.

To check the DirectX version of your installed GPU, type in dxdiag into windows search and select the run command. In system information, the bottom item of the list shows your DirectX version. In addition to this ensure your standard GPU drivers are up-to-date. Updating drivers by any official means (Nvidia Geforce experience, AMD radeon software, direct from the vendor website) is fine.

The following instructions are using conda and pip for environment management, executing within the Anaconda prompt program that was installed along with Anaconda python. The # lines are not to be typed, they are for guidance.

```shell
conda create --name dlc_dml python=3.7
conda activate dlc_dml
#specific versions noted that are validated as of DLC 2.2.0.6, but other versions may work:
pip install 'deeplabcut[gui]'==2.2.0.6
pip install tensorflow-directml==1.15.5
pip install pip install imageio==2.9.0
conda install ffmpeg==4.2.2
```

Attention: Please note the order of execution of these commands are important, as pip's dependency manager may change package versions to incorrect ones if done in the wrong version.


--- File: docs/recipes/publishing_notebooks_into_the_DLC_main_cookbook.md ---
# Publishing Notebooks into the Main DLC Cookbook
### Your Recipe Guide to Contributing to the DLC Cookbook

## Introduction
Hey there, DLC enthusiast! 🌟 Ready to sprinkle your magic into the main DLC cookbook? Whether you're introducing a zesty new dish or giving an old one a twist, this guide's got your back. We'll walk you through how to publish a new notebook or spice up an existing one in the DLC cookbook. Let's get cooking! 🍲📘

## Preliminary Checks
#### Check Existing Recipes or Tutorials
   - **Search and Review**: Before you start writing a new recipe, go through the existing DLC Jupyter book to ensure there isn't a tutorial or recipe that covers the topic you have in mind.
   - **Expand Existing Content**: If your content is related to an existing topic, like I/O manipulations, consider expanding or refining that section instead of creating an entirely new recipe. This ensures that the Jupyter book remains concise and that related information is found in one place.
      - **Locate and Review**: Navigate to the particular recipe or tutorial you wish to update in the DLC Jupyter book.
      - **Consider Minor vs. Major Changes**: If you're adding a new section or significantly altering the current content, it might be worth noting the changes at the beginning or end of the recipe for clarity.
      - **Maintain Consistency**: Ensure your updates adhere to the current style, tone, and structure of the existing content to maintain a seamless reading experience.


## Structure of a Recipe
   When crafting your recipe, adhere to the following structure:
   - **Introduction**: Begin with an introductory paragraph that highlights the importance and relevance of the recipe. This sets the stage and gives readers context.
    
   - **Examples/Workflow**: Provide step-by-step instructions or a workflow, supported by examples. This makes it easy for readers to understand and follow along.
    
   - **Conclusion**: Conclude with a summary or highlight the key takeaways of your recipe. You can also provide references or further reading.


Now, let's dive into the process of contributing your content to the DLC Jupyter book.
## Steps

1. **Set-up your local environment.** You need `deeplabcut[docs]` installed:
   You can do this by running the following command:
   ```    
   pip install deeplabcut[docs]
   ```

This command installs DeepLabCut along with the dependencies required to build the documentation.

2. **Fork the DLC Repository**:
   - Go to the DeepLabCut GitHub repository: [https://github.com/DeepLabCut/DeepLabCut](https://github.com/DeepLabCut/DeepLabCut)


   - Click on the `Fork` button on the top-right corner of the page. This will create a copy of the repository in your own GitHub account.
3. **Clone your forked repository**:
   - Navigate to your forked repo on GitHub.
   - Click the `Code` button and copy the URL.
   - Clone the repository to your local machine:
   ```
   git clone [REPO_URL]
   ```
4. **Create a new branch**:
   It's a good practice to create a new branch for each new feature or change:
   ```
    cd [YOUR_REPO_DIRECTORY]
    git checkout -b my-new-notebook
   ```
5. **Create a new notebook** or **update an existing one**.
   - **Creating a new notebook**
      - **Choose Your Topic Wisely:** Before you start, make sure your topic fits the DLC Jupyter book's theme and brings value to its readers. A novel topic or a unique twist on an existing topic can be particularly impactful.
      - **Craft with Care:** Remember, your notebook will be a reference for many. Begin with an engaging introduction, followed by well-structured content, and wrap it up with a conclusion.
      - **Interactive Elements:** One of the strengths of Jupyter notebooks is the ability to combine code, visuals, and narrative. Use interactive plots, widgets, or any other tools that enhance the content and make it engaging.
      - **Save Regularly:** Jupyter auto-saves your work, but it's a good habit to manually save your notebook frequently, especially after making significant changes.
      - **Naming Convention:** Name your notebook in a way that reflects its content and is consistent with other notebook titles in the DLC Jupyter book. This makes it easier for readers to understand the topic at a glance.      
   - **Updating an existing notebook** 
      - Navigate to the location of the existing recipe within the directory: 
          ```
          [YOUR_REPO_DIRECTORY]/docs/recipes/
          ```
      - Open the corresponding Jupyter notebook (.ipynb file) you wish to update.
      - Make the necessary changes or additions to the content.
      - Save the notebook once your updates are finalized.
      - Proceed to **Step 6** *(Proofreading)* and **9** *(Testing the documentation)* (skip Steps 7 and 8).
6. **Proofread:** Double check for spelling and grammatical errors by using Jupyter notebook's spellcheck extension called `spellchecker` (or your preferred spell-checker).
   ```
   jupyter nbextension enable spellchecker/main
   ```
   Once installed, restart your notebook, and when you load your notebook again, you will see the incorrectly spelled words highlighted in red.
7. **Add your notebook** to the recipe directory at `[YOUR_REPO_DIRECTORY]/docs/recipes/`

    - Navigate to the appropriate directory where the Jupyter notebooks are stored for the Jupyter book.
    - Add your Jupyter notebook (.ipynb file) to this directory.
    
    To copy via terminal:
    
    - Unix-based OS users
    
      ```
      cp [YOUR_NOTEBOOK_FILENAME].ipynb [YOUR_REPO_DIRECTORY]/docs/recipes
      ```
    
    - WinOS users:
      ```
      copy new_recipe.ipynb [YOUR_REPO_DIRECTORY]\docs\recipes

      ```

8. **Update `[YOUR_REPO_DIRECTORY]/_toc.yml`** by adding under the *Tutorials & Cookbook* section a **new line** containing the path to your notebook. This creates a link to your notebook on the main DLC book sidebar.

    * For example:
      ```      
      - file: docs/recipes/[YOUR_NOTEBOOK_FILENAME]
      ```

9. **Test the documentation:**

    - Build your notebook into the DLC recipe book
      ```
      jupyter book build [YOUR_REPO_DIRECTORY]
      ```
    - Once build is successful, the newly built book can be accessed at `[YOUR_REPO_DIRECTORY]/_build/html/`.
    - Open `index.html` and check whether your notebook was rendered properly and if the links are working.

10. **Commit your changes:**
   When everything is a-okay, commit your changes to your branch. If not, edit your file and go to back to step 1.
    
    ```
    git add [YOUR_NOTEBOOK_FILENAME]
    git commit -m "Added a new notebook about [YOUR_TOPIC]"
    ```

11. **Push your branch to your fork:**

    ```
    git push origin my-new-notebook
    ```


12. **Submit a Pull Request (PR):**

    - Go to your forked repository on GitHub.
    - You'll likely see a message prompting you to create a pull request from your recently pushed branch. Click `Compare & pull request`.
    - Fill out the PR form with a descriptive title and comments describing your notebook. This will help the maintainers understand the context and purpose of your notebook.
    - Click `Create pull request`.

13. **Make Necessary Changes**: The DeepLabCut maintainers will then review your PR and provide feedback. If changes are required, make the necessary changes on your local branch, commit them, and push the branch again. The PR will automatically update.

14. **🎉PR Approval:🎉** Once your PR is approved, the maintainers will merge it into the main repository. Your notebook will then be a part of the DeepLabCut Jupyter book! Yay!

Remember to always check the [DLC contributing guidelines](https://github.com/DeepLabCut/DeepLabCut/blob/main/CONTRIBUTING.md).


## Wrap-Up 🎉
Alright! 🌟 By now, you've got the playbook to jazz up the DeepLabCut Jupyter book. Remember, it's not just about cooking up new recipes but also spicing up the old ones. Dive in, have fun, and let's make this book a flavor-packed feast for all DLC enthusiasts out there. High-five for joining the party! 🙌🎈


--- File: docs/recipes/post.md ---
# Some data processing recipes!

## Flagging frames with abnormal bodypart distances

Beyond `deeplabcut.check_labels`, you may want to automatically detect
labeled frames where the distance between two body parts exceeds a
given threshold. For example, the frames where the head–tail length
is greater than 100 pixels could be found as follows:

```python
import numpy as np
import pandas as pd

max_dist = 100
df = pd.read_hdf('path_to_your_labeled_data_file')
bpt1 = df.xs('head', level='bodyparts', axis=1).to_numpy()
bpt2 = df.xs('tail', level='bodyparts', axis=1).to_numpy()
# We calculate the vectors from a point to the other
# and group them per frame and per animal.
try:
    diff = (bpt1 - bpt2).reshape((len(df), -1, 2))
except ValueError:
    diff = (bpt1 - bpt2).reshape((len(df), -1, 3))
dist = np.linalg.norm(diff, axis=2)
mask = np.any(dist >= max_dist, axis=1)
flagged_frames = df.iloc[mask].index
```


--- File: docs/recipes/UsingModelZooPupil.md ---
# Using ModelZoo models on your own datasets

<p style='text-align: justify;'>Animal behavior has to be analyzed with painstaking accuracy. Therefore, animal pose estimation has been
an important tool to study animal behavior precisely.

Beside providing an open source toolbox for researchers to develop customized deep neural networks for markerless pose
estimation, we at DeepLabCut also aim to build robust, generalizable models. Part of this effort is via the
[DeeplabCut ModelZoo](http://modelzoo.deeplabcut.org/).

The Zoo hosts user-contributed and DLC-team developed models that are trained on specific animals and scenarios. You can
analyze your videos directly with these models without training. The models have strong zero-shot performance on unseen
out-of-domain data which can be further improved via pseudo-labeling. Please check the first
[ModelZoo manuscript](https://arxiv.org/abs/2203.07436v1) for further details.

This recipe aims to show a usecase of the **mouse_pupil_vclose** and is contributed by 2022 DLC AI Resident
[Neslihan Wittek](https://github.com/neslihanedes) 💜.

## `mouse_pupil_vclose` model

This model was contributed by Jim McBurney-Lin at University of California Riverside, USA.
The model was trained on images of C57/B6J mice eyes, and also then augmented with mouse eye data from the Mathis Lab at
EPFL.


 <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661439618442-RAACYCYD4RWEND4X1UFU/pupil_one.png?format=500w" width="250" title="DLC" alt="DLC" align="left" vspace = "50">

  <img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661439618750-97KC2HW8HH6VOJHLMO46/pupil_two.png?format=300w" width="250" title="DLC" alt="DLC" align="right" vspace = "50">

| Landmark_Number  | Landmark_Name  | Description|
| --- | --- | --- |
| 1 | Lpupil  | Left aspect of pupil |
| 2 | LDpupil | Left/dorsal aspect of pupil |
| 3 | Dpupil  | Dorsal aspect of pupil |
| 4 | DRpupil  | Dorsal/Right aspect of pupil |
| 5 | Rpupil  | Right aspect of pupil |
| 6 | RVpupil  | Right/ventral aspect of pupil |
| 7 | Vpupil  | Ventral aspect of pupil |
| 8 | VLpupil  | Ventral/left aspect of pupil |


Since we would like to evaluate the models performance on out-of-domain data, we will analyze pigeon pupils. For more
discussions and work on so-called out-of-domain data, see
[Mathis, Biasi 2020](https://paperswithcode.com/dataset/horse-10).

## Pigeon Pupil

The eye pupil admits and regulates the amount of light entering the retina in order to enable image perception. Beside
this curicial role, the pupil also reflects the state of the brain. The systemic behavior of the pupil has not been
vastly studied in birds, although researchers from
<a href="https://www.sciencedirect.com/science/article/pii/S0960982221013166?via%3Dihub" target="_blank">Max Planck Institute for Ornithology in Seewiesen</a>
have shed light on pupil behaviors in pigeons.

The pupils of male pigeons get smaller during courtship behavior. This is in contrast to mammals, for which the pupil
size dilates in response to an increase in arousal. In addition, the pupil size of pigeons dilates during non-REM sleep,
while they rapidly constrict during REM sleep. Examining these differences and the reason behind them, might be helpful
to understand the pupillary behavior in general.

In light of these findings, we wanted to show whether the **mouse_pupil_vclose** model give us an accurate tracking
performance for the pigeon pupil as well.

### Jupyter & Google Colab Notebook

DeepLabCut provides a Google Colab Notebook to analyze your video with a pretrained networks from the ModelZoo. No need
for local installation of DeepLabCut!

Since we are interested in the accuracy of the **mouse_pupil_vclose** on pigeon pupil data, we will use a video which
consists of 7 recordings of pigeon pupils.

Check the
[ModelZoo Colab page](https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/COLAB/COLAB_DLC_ModelZoo.ipynb)
and a video tutorial on how to use the ModelZoo on Google Colab.

<div align="center">
  <a href="https://www.youtube.com/watch?v=twHBa1ZvXM8" target= "_blank"><img src="http://img.youtube.com/vi/twHBa1ZvXM8/0.jpg" alt="IMAGE ALT TEXT"></a>
</div>

```{hint}
You are happy with the model and want to go on analyzing further videos on your local machine or you want to refine the model for your specific usecase?
```html
!zip -r /content/file.zip /content/pigeon_modelZoo-nessi-2022-08-22
from google.colab import files
files.download("/content/file.zip")

```

### Analyze Videos at Your Local Machine

DeepLabCut host models from the [DeepLabCut ModelZoo Project](http://modelzoo.deeplabcut.org/).

The `create_pretrained_project` function will create a new project directory with the necessary sub-directories and a basic configuration file.
It will also initialize your project with a pre-trained model from the DeepLabCut ModelZoo.

The rest of the code should be run within your DeepLabCut environment.
Check [here](how-to-install) for the instructions for the DeepLabCut installation.

To initialize a new project directory with a pre-trained model from the DeepLabCut ModelZoo, run the code below.

::::{warning}
This method is currently implemented for Tensorflow only, Pytorch compatibility is coming soon.
::::

```python
import deeplabcut

deeplabcut.create_pretrained_project(
    "projectname",
    "experimenter",
    [r"path_for_the_videos"],
    model="mouse_pupil_vclose",
    working_directory=r"project_directory",
    copy_videos=True,
    videotype=".mp4 or .avi?",
    analyzevideo=True,
    filtered=True,
    createlabeledvideo=True,
    trainFraction=None,
    engine=deeplabcut.Engine.TF,
)
```

::::{important}
Your videos should be cropped around the eye for better model accuracy! 👁🐭
::::

Excitingly, 6 out of the 7 pigeon pupils were tracked nicely:

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/44858d34-dca7-4bb5-a6e5-cd8078b50bec/Screen+Shot+2022-08-25+at+5.39.33+PM.png?format=1500w" width="500" title="DLC" alt="DLC" align="center" vspace = "50">

When we further evaluate the model accuracy by checking the likelihood of tracked points, we see that the tracking is
low confidience when the pigeons close their eyelid (which is of course expected, and can be leveraged to measure
blinking 👁).

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661439615047-OVOOMU1Z5NJIWJ1HHNFD/likelihood.png?format=500w" width="600" title="DLC" alt="6 pigeion eyes tracked with deeplabcut" align="center" vspace = "50">

But you also might encounter larger problems than small tracking glitches:

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661439618037-4GNTZD476MJMQX19N0Z4/pigeon_7.png?format=500w" width="250" title="DLC" alt="1eye" align="center" vspace = "5">

To deal with this, you can extract poorly tracked outlier frames, refine them and feed the training data set with them for re-training.
Be sure that you set the number of frames to label in the `config.yaml` file of your project folder.
The more problems you encounter, the higher the number of frames you might want to label.
You should also add the path of the video(s) into the `config.yaml` file, or run the following command to add the videos to your project:

```python
deeplabcut.add_new_videos(
    "/pathofproject/config.yaml",
    ["/pathofvideos/pigeon.mp4"],
    copy_videos=False,
    coords=None,
    extract_frames=False
)
```
The `deeplabcut.extract_outlier_frames` function will check for outliers and ask your feedback on whether to extract these outliers frames.

```python
deeplabcut.analyze_videos(
    "/pathofproject/config.yaml",
    ["/pathofvideos/pigeon.mp4"]
)
deeplabcut.extract_outlier_frames(
    "/pathofproject/config.yaml",
    ["/pathofvideos/pigeon.mp4"],
    automatic=True
)
```
The `deeplabcut.refine_labels` function starts the GUI which allows you to refine the outlier frames manually.
You should load the outlier frames directory and corresponding `.h5` file from the previous model.
It will ask you to define the `likelihood` threshold: labels under the threshold should be refined at this stage.

After refining, you should combine these data with your previous model's data set and create a new training data set.
```python
deeplabcut.refine_labels("/pathofproject/config.yaml")
deeplabcut.merge_datasets("/pathofproject/config.yaml")
deeplabcut.create_training_dataset("/pathofproject/config.yaml")
```
Before starting the training of your model, there is one last step left: editing the `init_weights` parameter in your `pose_cfg.yaml` file.
Go to your project and check the latest snapshot (e.g., `snapshot-610000`) of your model in `dlc-models/train` directory.
Edit the value of the `init_weights` key in the `pose_cfg.yaml` file and start to re-train your model!


`init_weights: pathofyourproject\dlc-models\iteration-0\DLCFeb31-trainset95shuffle1\train\snapshot-610000`

```python
deeplabcut.train_network("/pathofproject/config.yaml", shuffle=1, saveiters=25000)
```
```{hint}
Check this video for model refining!
<div align="center">
  <a href="https://www.youtube.com/watch?v=bgfnz1wtlpo" target="_blank"><img src="http://img.youtube.com/vi/bgfnz1wtlpo/0.jpg" alt="IMAGE ALT TEXT"></a>
</div>
```


--- File: docs/recipes/nn.md ---
(tf-training-tips-and-tricks)=
# Model training tips & tricks

## TensorFlow Engine: Limiting a GPU's memory consumption

With TensorFlow, all GPU memory is allocated to training by default, preventing
other Tensorflow processes from being run on the same machine.

A flexible solution to limiting memory usage is to call 
`deeplabcut.train(..., allow_growth=True)`, which dynamically grows the GPU memory
region as it is needed. Another, stricter option is to explicitly cap GPU usage to only
a fraction of the available memory. For example, allocating a maximum of 1/4 of the
total memory could be done as follows:

```python
import tensorflow as tf

gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.25)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
```

(tf-custom-image-augmentation)=
## Using custom image augmentation

Image augmentation is the process of artificially expanding the training set
by applying various transformations to images (e.g., rotation or rescaling)
in order to make models more robust and more accurate (read our
[primer](https://www.sciencedirect.com/science/article/pii/S0896627320307170) for
more information). Although data augmentation is automatically accomplished
by DeepLabCut, default values can be readily overwritten prior to training. See the
augmentation variables defined in the:

- PyTorch Engine: [docs for the `pytorch_config.yaml` file](dlc3-pytorch-config)
- TensorFlow Engine: [default pose_cfg.yaml file](
https://github.com/DeepLabCut/DeepLabCut/blob/main/deeplabcut/pose_cfg.yaml#L23-L74)

For the single-animal TensorFlow models, [you have several options](
https://deeplabcut.github.io/DeepLabCut/docs/standardDeepLabCut_UserGuide.html#f-create-training-dataset-s-and-selection-of-your-neural-network)
for image augmentation when calling `create_training_dataset`

An in-depth tutorial on image augmentation and training hyperparameters can be found [
here](
https://deeplabcut.github.io/DeepLabCut/docs/recipes/pose_cfg_file_breakdown.html).

## Evaluating intermediate (and all) snapshots

The latest snapshot stored during training may not necessarily be the one that yields
the highest performance. Therefore, you should analyze ALL snapshots, and select the
best. Put 'all' in the snapshots section of the `config.yaml` to do this.

(what-neural-network-should-i-use)=
## What neural network should I use? (Trade offs, speed performance, and considerations)

You always select the network type when you create a training data set: i.e., standard
dlc: `deeplabcut.create_training_dataset(config, net_type=resnet_50)` , or maDLC: 
`deeplabcut.create_multianimaltraining_dataset(config, net_type=dlcrnet_ms5)`. There is
nothing else you should change.

### PyTorch Engine

The different architectures available are described in the [PyTorch model architectures
](dlc3-architectures) page.

### TensorFlow Engine

With the release of even more network options, you now have to decide what to use! This
additionally flexibility is hopefully helpful, but we want to give you some guidance on
where to start.

**TL;DR - your best performance for most everything is ResNet-50; MobileNetV2-1 is much
faster, needs less memory on your GPU to train and nearly as accurate.**

***
#### ResNets:

In Mathis et al. 2018 we benchmarked three networks: **ResNet-50, ResNet-101, and
ResNet-101ws**. For ALL lab applications, ResNet-50 was enough. For all the demo videos
on [www.deeplabcut.org](http://www.mousemotorlab.org/deeplabcut) the backbones are
ResNet-50's. Thus, we recommend making this your go-to workhorse for data analysis. Here
is a figure from the paper, see panel "B" (they are all within a few pixels of each
other on the open-field dataset):

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1548558406678-S32H6T3M3U7BWVS4IGYD/ke17ZwdGBToddI8pDm48kD4CqqHoJgLzZVYacqX5G8QUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYy7Mythp_T-mtop-vrsUOmeInPi9iDjx9w8K4ZfjXt2dqTB9h4P9po3-YSCqzKkit0PccqviqYX7RTAdOBUgXwbCjLISwBs8eEdxAxTptZAUg/SupplFig2-01.png?format=1000w" width="80%">
</p>

This is also one of the main result figures, generated with ResNet-50. BLUE is 
training - RED is testing - BLACK is our best human-level performance, and 10 pixels is
the width - of the mouse nose -so anything under that is good performance for us on this
task!

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1547585317499-0QWTWL5KVPK8ZWINQ30U/ke17ZwdGBToddI8pDm48kH23KVWagbNOYpajbj_MQLNZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PI-4DGGLi3WdhIPQDa6khDzRWGU5SknjCO3Yd6rloU2Zw/ErrorvsTrainingsetSize.png?format=1000w" width="60%">
</p>

Here are also some speed stats for analyzing videos with ResNet-50, see 
https://www.biorxiv.org/content/early/2018/10/30/457242 for more details:

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1547585393723-8BQ6RGSPUUEQ1NNGUQDZ/ke17ZwdGBToddI8pDm48kCebzxgICDi_Bmgq_409OyxZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PICdDqlshOygx3FUsifuoze123Z0BWMsGmyODBJYiFvQc/inferencespeed.png?format=1000w" width="60%">
</p>

**So, why use a ResNet-101 or even 152?** if you have a much more challenging problem,
like multiple humans dancing, this is a good option. You should then also set
`intermediate_supervision=True` for best performance in the `pose_config.yaml` of that
shuffle folder (before you train). Note, for ResNet-50 this does NOT help, and can
hurt.

#### When should I use a MobileNet?

MobileNets are fast to run, fast to train, more memory efficient, and faster for
analysis (inference) - e.g. on CPUs they are 4 times faster, on GPUs up to 2x! So, if
you don't have a GPU (or a GPU with little memory), and don't want to use Google COLAB,
etc, then these are a great starting point.

They are smaller/shallower networks though, so you don't want to be pushing in very
large images. So, be sure to use `deeplabcut.DownSampleVideo` on your data (which is
frankly never a bad idea).

Additionally, these are good options for running on "live" videos, i.e. if you want to
give real-time feedback in an experiment, you can run a video around a smaller cropped
area, and run this rather fast!

**So, how fast are they?**

Here are comparisons of 4 MobileNetV2 variants to ResNet-50 and ResNet-101 (darkest
red - read more here: https://arxiv.org/abs/1909.11229)

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1570054128042-51HCY1Y9GV7GAQTZ5BMB/ke17ZwdGBToddI8pDm48kKr5oWkDv6XTQOpQfQOqjiAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKchrM-h1v5jGhVgANO1xgMJaHKhYxZ0-Cf0LQLHXkOaBUlIOyXFtu3PNQa47ngsqiu/mbnetv2speed.png?format=1000w" width="100%">
</p>

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1570054117297-YA8WOYG50EK55WM6Y8ZI/ke17ZwdGBToddI8pDm48kAWg0301pwdoqO-Bo48aILYUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcZbh5EzlyubXk7Q3qHw5ayJHISnXwMOq8Pp90__8eMJefaZFcnumpU7B4DHTHEFkQ/speedtables.png?format=1000w" width="100%">
</p>

#### When should I use an EfficientNet?

Built with inverse residual blocks like MobileNets, but more powerful than ResNets, due
to optimal depth/width/resolution scaling, [EfficientNet](
https://arxiv.org/abs/1905.11946) are an excellent choice if you want speed and
performance. They do require more careful handling though! Especially for small
datasets, you will need to tune the batch size and learning rates. So, we suggest these
for more advanced users, or those willing to run experiments to find the best settings.
Here is the speed comparison, and for performance see our latest work at: 
http://horse10.deeplabcut.org

<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1615891029784-87JAZJN1C5S4HS62F752/ke17ZwdGBToddI8pDm48kLId9V2zDiOqQ5EIZz4b_S0UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKctCpCjeabgTq1Hv_G9BIks_zjAnmEpAaVGioFPvsrieXDegXGHA0z-h8QeHOQDokM/speedTest.png?format=1000w" width="100%">
</p>

#### How can I compare them?

Great question! So, the best way to do this is to use the **same** test/train split (
that is generated in create_training_dataset) with different models. Here, as of 2.1+,
we have a **new** function that lets you do this easily. Instead of using
`create_training_dataset` you will run `create_training_model_comparison` (see the
docstrings by `deeplabcut.create_training_model_comparison?` or run the Project Manager
GUI - `deeplabcut.launch_dlc()`-  for assistance.


--- File: docs/recipes/DLCMethods.md ---
# How to write a DLC Methods Section

**Pose estimation using DeepLabCut**

For body part tracking we used DeepLabCut (version 3.X.X) [Mathis et al, 2018, Nath et al, 2019]. Specifically, we
labeled X number of frames taken from X videos/animals (then X% was used for training (default is 95%). We used a
X-based neural network (i.e., X = ResNet-50, ResNet-101, MobileNetV2-0.35, MobileNetV2-0.5, MobileNetV2-0.75,
MobileNetV2-1, EfficientNet ..X, dlcrnet_ms5, cspnext_s, dekr_w32, rtmpose_s, etc.)*** with default parameters* for X
number of training iterations. We validated with X number of shuffles, and found the test error was: X pixels, train:
X pixels (image size was X by X). We then used a p-cutoff of X (i.e. 0.9) to condition the X,Y coordinates for future
analysis. This network was then used to analyze videos from similar experimental settings.

*If any defaults were changed in *`pose_config.yaml`*, mention them.

i.e. common things one might change:
* the loader (options are `default`, `imgaug`, `tensorpack`, `deterministic`).
* the `post_dist_threshold` (default is 17 and determines training resolution).
* optimizer: do you use the default `SGD` or `ADAM`?

*** here, you could add additional citations.
If you use ResNets, consider citing Insafutdinov et al 2016 & He et al 2016. If you use the MobileNetV2s consider citing Mathis et al 2021, and Sandler et al, 2018. If you use DLCRNet, please cite Lauer et al, 2021.

> Mathis, A. et al. Deeplabcut: markerless pose estimation
> of user-defined body parts with deep learning. Nature
> Neuroscience 21, 1281–1289 (2018).

> Nath, T. et al. Using deeplabcut for 3d markerless pose
> estimation across species and behaviors. Nature Protocols
> 14, 2152–2176 (2019).

> Mathis, A. Biasi, T. et al. Pretraining boosts out-of-domain robustness for pose estimation. WACV (2021).

> Lauer et al. Multi-animal pose estimation and tracking with DeepLabCut. BioRxiv (2021).

> Insafutdinov, E., Pishchulin, L., Andres, B., Andriluka,
> M. & Schiele, B. DeeperCut: A deeper, stronger, and
> faster multi-person pose estimation model. In European
> Conference on Computer Vision, 34–50 (Springer, 2016).

> Sandler, M., Howard, A., Zhu, M., Zhmoginov, A. &
> Chen, L.-C. Mobilenetv2: Inverted residuals and linear
> bottlenecks. In Proceedings of the IEEE Conference
> on Computer Vision and Pattern Recognition, 4510–4520
> (2018).

> He, K., Zhang, X., Ren, S. & Sun, J. Deep residual
> learning for image recognition. In Proceedings of the
> IEEE conference on computer vision and pattern recognition,
> 770–778 (2016). URL https://arxiv.org/abs/
> 1512.03385.

We also have the network graphic freely available on SciDraw.io if you'd like to use it! https://scidraw.io/drawing/290.
If you use our DLC logo, please include the TM symbol, thank you!


--- File: docs/recipes/OpenVINO.md ---
# Intel OpenVINO backend

::::{warning}
This feature is currently implemented for TensorFlow-based models only.
::::

DeepLabCut provides an option to run deep learning model with [OpenVINO](https://github.com/openvinotoolkit/openvino) backend.
To enable OpenVINO in your pipeline, use `use_openvino` flag of `analyze_videos` method with one of string values
indicating device:
* ```"CPU"``` - Use CPU. This is a default value.
* ```"GPU"``` - Use GPU (requires OpenCL to be installed). First launch might take some time for kernels initialization.
* ```"MULTI:CPU,GPU"``` - Use CPU and GPU simultaneously. In most cases this option provides the best efficiency.

```python
def analyze_videos(
    ...
    use_openvino="MULTI:CPU,GPU",
)
```

OpenVINO is an optional dependency. You can install it with DeepLabCut by the following command:

```bash
pip install deeplabcut[openvino]
```


--- File: docs/recipes/io.md ---
# Input/output manipulations with DeepLabCut

## Analyzing very large videos in chunks
Analyzing hour-long videos may take a while, but the task can be
conveniently broken down into the analysis of smaller video clips:

```python
import deeplabcut
import os
from deeplabcut.utils.auxfun_videos import VideoWriter

_, ext = os.path.splitext(video_path)
vid = VideoWriter(video_path)
clips = vid.split(n_splits=10)
deeplabcut.analyze_videos(config_path, clips, ext)
```

## Tips on video re-encoding and preprocessing 

While moving videos between computers or from your computer to cloud storage you can encounter issues with `analyze_videos` or `create_labeled_video` due to video corruption. 
The issue can present itself during those steps and you have to carefully review the traceback. Sometimes it might look like the videos were analyzed but in fact analysis stopped right before the end of the video (corruption of the metadata when more indices are assigned than there are actual frames in a video). 
To tackle this issue, the easiest solution might be to re-encode the video, this will not only help with corruption but can also – if you choose so – compress the video without perceivable loss of quality. Common package used for video processing is FFmpeg which you can use from the terminal inside your DEEPLABCUT environment (without going into iPython).
There are number of video codecs that can be used to re-encode your video and if you want to keep the video in the same container (`.avi`, `.mp4`, `.ts` etc.) you should check which codec allows encoding to a certain container. For instance, for `.avi` it will be MJPEG and for `.mp4` H264 and H265. 
To re-encode your video, simply use:
```
ffmpeg -i "path_to_video" -c:v codec_name "output_path"
```
For instance, to re-encode to `.mp4` format with compression use:
```
ffmpeg -i "path_to_video" -c:v h264 -crf 18 -preset fast "output_path"
```
`-crf` is a quality-size tradeoff from 0 to 63 with 0 being highest quality but lowest compression. Ideally you’d want to use values between 18-23 for similar visual quality to the original.
`-preset` is a quality-speed tradeoff. Higher values give you faster encoding but will result in bigger filesizes and/or worse quality.
For `.avi` files you want to change the codec and the quality metric, since `crf` is used by H264/265 and not MJPEG. For instance, the encoding with some compression would be:
```
ffmpeg -i "path_to_video" -c:v mjpeg -q:v 10 "output_path"
```
`-q:v` is a quality metric with values ranging from 1 to 31 with reasonable values being around 10. 
If you want to compress all your recordings for easier storage or moving to cloud storage, you can use a for loop that will go through all videos in a directory that are in a certain container. Let’s say we want to transcode our `.avi` videos to `.mp4` and make them smaller without quality loss. Note, that the loop has be run from inside the folder the videos are in:
```
for %i in (*.avi) do ffmpeg -i "%i" -c:v libx265 -preset fast -crf 18 "%~ni.mp4" 
```
This command will re-encode all of your videos into an `.mp4` container and save them with the same name as the original (without overwriting them).
Additionally, ffmpeg allows you to also crop or rescale the videos for possible improvement in inference speed further down the line in DLC workflow. To either crop or rescale you need to use 
`-filter:v` parameter after which you’d add either `"crop=Xsize:Ysize:Xstart:Ystart"` for cropping or 
`"scale=Xsize:Ysize"` for rescale. Note that when using “scale” the values how be a result of integer division of the original video size. If you want to keep the aspect ratio, you can simply set either X or Y to `-1` and only give one of the  or you can use `“scale=iw/2:ih/2”` which will simply make the video 2 times smaller in both dimensions. For instance, if you have a videos at 1920x1080 resolution and want to rescale it to 960x540 for faster inference while also reencoding from `.avi` and doing some compression in a loop, the command would be something like this:
```
for %i in (*.avi) do ffmpeg -i "%i" -c:v libx265 -preset fast -crf 18 -filter:v "scale= iw/2:ih/2" "%~ni.mp4"
```
If audio is not a necessary in the videos you can also save some space by requesting specifically for the encoder to not encode any audio stream by adding `-an` just before specifying output filename, like so:
```
for %i in (*.avi) do ffmpeg -i "%i" -c:v libx265 -preset fast -crf 18 -filter:v "scale= iw/2:ih/2" -an "%~ni.mp4"
```


--- File: docs/gui/PROJECT_GUI.md ---
(project-manager-gui)=
# Interactive Project Manager GUI

As some users may be more comfortable working with an interactive interface, we wanted to provide an easy-entry point to the software. All the main functionality is available in an  easy-to-deploy GUI interface. Thus, while the many advanced features are not fully available in this Project GUI, we hope this gets more users up-and-running quickly.

**Release notes:** As of DeepLabCut 2.1+ now provide a full front-end user experience for DeepLabCut, and as of 2.3+ we changed the GUI from wxPython to PySide6 with napari support.

## Get Started:

(1) Install DeepLabCut using the simple-install with Anaconda found [here!](how-to-install)*.
Now you have DeepLabCut installed, but if you want to update it, either follow the prompt in the GUI which will ask you to upgrade when a new version is available, or just go into your env (activate DEEPLABCUT) then run:

` pip install 'deeplabcut[gui,modelzoo]'` *but please see [full install guide](how-to-install)!


(2) Open the terminal and run: `python -m deeplabcut`


<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/07ae2633-dc3e-4b6d-beec-27c08d9f8531/ezgif.com-gif-maker+%284%29.gif?format=2500w" width="80%">
</p>

Start at the Project Management Tab and work your way through the tabs to built your customized model and deploy it on new data.
We recommend to keep the terminal visible (as well as the GUI) so you can see the ongoing processes as you step through your project, or any errors that might arise.

- For specific napari-based labeling features, see the ["napari gui" docs](napari-gui-usage).
- To change from dark to light mode, set appearance at the top:
<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/5e41b01d-3101-40b2-9c53-129d8988370f/Screen+Shot+2022-10-09+at+3.45.46+PM.png?format=2500w
" width="30%">
</p>

## Video Demos: How to launch and run the Project Manager GUI:

**Click on the images!**

Note that currently the video demo is the wxPython version, but the logic is the same!

[![Watch the video](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572824438905-QY9XQKZ8LAJZG6BLPWOQ/ke17ZwdGBToddI8pDm48kIIa76w436aRzIF_cdFnEbEUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcLthF_aOEGVRewCT7qiippiAuU5PSJ9SSYal26FEts0MmqyMIhpMOn8vJAUvOV4MI/guilaunch.jpg?format=1000w)](https://youtu.be/KcXogR-p5Ak)

### Using the Project Manager GUI with the latest DLC code (single animals, plus objects): ⬇️

[![Watch the video](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1589046800303-OV1CCNZINWDMF1PZWCWE/ke17ZwdGBToddI8pDm48kB4PVlRPKDmSlQNbUD3wvXgUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcaja1QZ1SznGf7WzFOi-J6zLusnaF2VdeZcKivwxvFiDfGDqVYuwbAlftad9hfoui/dlc_gui_22.png?format=1000w)](https://www.youtube.com/watch?v=JDsa8R5J0nQ)

[Read more here](important-info-regd-usage)

### Using the Project Manager GUI with the latest DLC code (multiple identical-looking animals, plus objects):

[![Watch the video](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1589047147498-G1KTFA5BXR4PVHOOR7OG/ke17ZwdGBToddI8pDm48kJDij24pM2COisBTLIGjR1pZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZamWLI2zvYWH8K3-s_4yszcp2ryTI0HqTOaaUohrI8PIel60EThn7SDFlTiSprUhmjQQHn9bhdY9dnQSKs8bCCo/Untitled.png?format=1000w)](https://www.youtube.com/watch?v=Kp-stcTm77g)

[Read more here](important-info-regd-usage)

## VIDEO DEMO: How to benchmark your data with the new networks and data augmentation pipelines:

[Watch the video](https://youtu.be/WXCVr6xAcCA)


--- File: docs/gui/napari_GUI.md ---
(napari-gui)=
# napari labeling GUI

We replaced wxPython with PySide6 + as of version 2.3. Here is how to use the napari-aspects of the new GUI. It is available in napari-hub as a stand alone GUI as well as integrated into our main GUI, [please see docs here](https://deeplabcut.github.io/DeepLabCut/docs/PROJECT_GUI.html).

[![License: BSD-3](https://img.shields.io/badge/License-BSD3-blue.svg)](https://www.gnu.org/licenses/bsd3)
[![PyPI](https://img.shields.io/pypi/v/napari-deeplabcut.svg?color=green)](https://pypi.org/project/napari-deeplabcut)
[![Python Version](https://img.shields.io/pypi/pyversions/napari-deeplabcut.svg?color=green)](https://python.org)
[![tests](https://github.com/DeepLabCut/napari-deeplabcut/workflows/tests/badge.svg)](https://github.com/DeepLabCut/napari-deeplabcut/actions)
[![codecov](https://codecov.io/gh/DeepLabCut/napari-deeplabcut/branch/main/graph/badge.svg)](https://codecov.io/gh/DeepLabCut/napari-deeplabcut)
[![napari hub](https://img.shields.io/endpoint?url=https://api.napari-hub.org/shields/napari-deeplabcut)](https://napari-hub.org/plugins/napari-deeplabcut)

A napari plugin for keypoint annotation with DeepLabCut.


## Installation

You can install the full DeepLabCut napari-based GUI via [pip] by running this in your conda env:

`pip install 'deeplabcut[tf,gui]'` or mac M1/M2 chip users: `pip install 'deeplabcut[apple_mchips,gui]'`

*please note this is available since v2.3

This is not needed if you ran the above installation, but you can install the stand-alone `napari-deeplabcut` via [pip]:

`     pip install napari-deeplabcut `


To install latest development version:

  `  pip install git+https://github.com/DeepLabCut/napari-deeplabcut.git `


(napari-gui-usage)=
## Usage

To use the full GUI, please run:

`python -m deeplabcut`

To use the stand-alone napari plugin, please launch napari:

`napari `

Then, activate the plugin in Plugins > napari-deeplabcut: Keypoint controls.

All accepted files (`config.yaml`, images, `.h5` data files) can be loaded either by dropping them directly onto the canvas or via the File menu.

The easiest way to get started is to drop a folder (typically a folder from within a DeepLabCut's `labeled-data` directory), and, if labeling from scratch, drop the corresponding `config.yaml` to automatically add a `Points layer` and populate the dropdown menus.

[🎥 DEMO](https://youtu.be/hsA9IB5r73E)

**Tools & shortcuts are:**

- `2` and `3`, to easily switch between labeling and selection mode
- `4`, to enable pan & zoom (which is achieved using the mouse wheel or finger scrolling on the Trackpad)
- `M`, to cycle through regular (sequential), quick, and cycle annotation mode (see the description [here](https://github.com/DeepLabCut/DeepLabCut-label/blob/ee71b0e15018228c98db3b88769e8a8f4e2c0454/dlclabel/layers.py#L9-L19))
- `E`, to enable edge coloring (by default, if using this in refinement GUI mode, points with a confidence lower than 0.6 are marked
in red)
- `F`, to toggle between animal and body part color scheme.
- `V`, to toggle visibility of the selected layer.
- `backspace` to delete a point.
- Check the box "display text" to show the label names on the canvas.
- To move to another folder, be sure to save (Ctrl+S), then delete the layers, and re-drag/drop the next folder.

![napari_shortcuts](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/192345a5-e411-4d56-b718-ef52f91e195e/Qwerty.png?format=1500w)



### Save Layers

Annotations and segmentations are saved with `File > Save Selected Layer(s)...` (or its shortcut `Ctrl+S`).
Only when saving segmentation masks does a save file dialog pop up to name the destination folder;
keypoint annotations are otherwise automatically saved in the corresponding folder as `CollectedData_<ScorerName>.h5`.
- As a reminder, DLC will only use the H5 file; so be sure if you open already labeled images you save/overwrite the H5.
- Note, before saving a layer, make sure the points layer is selected. If the user clicked on the image(s) layer first, does `Save As`, then closes the window, any labeling work during that session will be lost!
- Modifying and then saving points in a `machinelabels...` layer will add to or overwrite the existing `CollectedData` layer and will **not** save to the `machinelabels` file.

### Video frame extraction and prediction refinement

Since v0.0.4, videos can be viewed in the GUI.

Since v0.0.5, trailing points can be visualized; e.g., helping in the identification
of swaps or outlier, jittery predictions.

Loading a video (and its corresponding output h5 file) will enable the video actions
at the top of the dock widget: they offer the option to manually extract video
frames from the GUI, or to define cropping coordinates.
Note that keypoints can be displaced and saved, as when annotating individual frames.


## Workflow

Suggested workflows, depending on the image folder contents:

1. **Labeling from scratch** – the image folder does not contain `CollectedData_<ScorerName>.h5` file.

    Open *napari* as described in [Usage](#usage) and open an image folder together with the DeepLabCut project's `config.yaml`.
    The image folder creates an *image layer* with the images to label.
    Supported image formats are: `jpg`, `jpeg`, `png`.
    The `config.yaml` file creates a *Points layer*, which holds metadata (such as keypoints read from the config file) necessary for labeling.
    Select the *Points layer* in the layer list (lower left pane on the GUI) and click on the *+*-symbol in the layer controls menu (upper left pane) to start labeling.
    The current keypoint can be viewed/selected in the keypoints dropdown menu (right pane).
    The slider below the displayed image (or the left/right arrow keys) allows selecting the image to label.

    To save the labeling progress refer to [Save Layers](#save-layers).
    `Data successfully saved` should be shown in the status bar, and the image folder should now contain a `CollectedData_<ScorerName>.h5` file.
    (Note: For convenience, a CSV file with the same name is also saved.)

2. **Resuming labeling** – the image folder contains a `CollectedData_<ScorerName>.h5` file.

    Open *napari* and open an image folder (which needs to contain a `CollectedData_<ScorerName>.h5` file).
    In this case, it is not necessary to open the DLC project's `config.yaml` file, as all necessary metadata is read from the `h5` data file.

    Saving works as described in *1*.

    ***Note that if a new body part has been added to the `config.yaml` file after having started to label, loading the config in the GUI is necessary to update the dropdown menus and other metadata.***

    ***As `viridis` is `napari-deeplabcut` default colormap, selecting the colormap in the GUI or loading the config in the GUI can be used to update the color scheme.***

4. **Refining labels** – the image folder contains a `machinelabels-iter<#>.h5` file.

    The process is analog to *2*.
   Open *napari* and open an image folder.
    If the video was originally labeled, *and* had outliers extracted it will contain a `CollectedData_<ScorerName>.h5` file and a `machinelabels-iter<#>.h5` file. In this case, select the `machinelabels` layer in the GUI, and type `e` to show edges. Red indicates likelihood < 0.6. As you navigate through frames, images with labels with edges will need to be refined (moved, deleted, etc). Images with labels without edges will be on the `CollectedData` (previous manual annotations) layer and shouldn't need refining. However, you can switch to that layer and fix errors. You can also right-click on the `CollectedData` layer and select `toggle visibility` to hide that layer. Select the `machinelabels` layer before saving which will append your refined annotations to `CollectedData`.

    If the folder only had outliers extracted and wasn't originally labeled, it will not have a `CollectedData` layer. Work with the `machinelabels` layer selected to refine annotation positions, then save.

    In this case, it is not necessary to open the DLC project's `config.yaml` file, as all necessary metadata is read from the `h5` data file.

    Saving works as described in *1*.

6. **Drawing segmentation masks**

    Drop an image folder as in *1*, manually add a *shapes layer*. Then select the *rectangle* in the layer controls (top left pane),
    and start drawing rectangles over the images. Masks and rectangle vertices are saved as described in [Save Layers](#save-layers).
    Note that masks can be reloaded and edited at a later stage by dropping the `vertices.csv` file onto the canvas.

### Workflow flowchart

```{mermaid}
graph TD
  id1[What stage of labeling?]
  id2[deeplabcut.label_frames]
  id3[deeplabcut.refine_labels]
  id4[Add labels to, or modify in, \n `CollectedData...` layer and save that layer]
  id5[Modify labels in `machinelabels` layer and save \n which will create a `CollectedData...` file]
  id6[Have you refined some labels from the most recent iteration and saved already?]
  id7["All extracted frames are already saved in `CollectedData...`.
1. Hide or trash all `machinelabels` layers.
2. Then modify in and save `CollectedData`"]
  id8["
1. hide or trash all `machinelabels` layers except for the most recent.
2. Select most recent `machinelabels` and hit `e` to show edges.
3. Modify only in `machinelabels` and skip frames with labels without edges shown.
4. Save `machinelabels` layer, which will add data to `CollectedData`.
	- If you need to revisit this video later, ignore `machinelabels` and work only in `CollectedData`"]

  id1 -->|I need to manually label new frames \n or fix my labels|id2
  id1 ---->|I need to refine outlier frames \nfrom analyzed videos|id3
  id2 -->id4
  id3 -->|I only have a `machinelabels...` file|id5
  id3 ---->|I have both `machinelabels` and `CollectedData` files|id6
  id6 -->|yes|id7
  id6 ---->|no, I just extracted outliers|id8
```

### Labeling multiple image folders

Labeling multiple image folders has to be done in sequence; i.e., only one image folder can be opened at a time.
After labeling the images of a particular folder is done and the associated *Points layer* has been saved, *all* layers should be removed from the layers list (lower left pane on the GUI) by selecting them and clicking on the trashcan icon.
Now, another image folder can be labeled, following the process described in *1*, *2*, or *3*, depending on the particular image folder.


### Defining cropping coordinates

Prior to defining cropping coordinates, two elements should be loaded in the GUI:
a video and the DLC project's `config.yaml` file (into which the crop dimensions will be stored).
Then it suffices to add a `Shapes layer`, draw a `rectangle` in it with the desired area,
and hit the button `Store crop coordinates`; coordinates are automatically written to the configuration file.


## Contributing

Contributions are very welcome. Tests can be run with [tox], please ensure
the coverage at least stays the same before you submit a pull request.

To locally install the code, please git clone the repo and then run `pip install -e .`


## Issues

If you encounter any problems, please [file an issue] along with a detailed description.

[file an issue]: https://github.com/DeepLabCut/napari-deeplabcut/issues


## Acknowledgements


This [napari] plugin was generated with [Cookiecutter] using [@napari]'s [cookiecutter-napari-plugin] template. We thank the Chan Zuckerberg Initiative (CZI) for funding this work!

<!--
Don't miss the full getting started guide to set up your new package:
https://github.com/napari/cookiecutter-napari-plugin#getting-started

and review the napari docs for plugin developers:
https://napari.org/plugins/stable/index.html
-->


[napari]: https://github.com/napari/napari
[Cookiecutter]: https://github.com/audreyr/cookiecutter
[@napari]: https://github.com/napari
[cookiecutter-napari-plugin]: https://github.com/napari/cookiecutter-napari-plugin
[BSD-3]: http://opensource.org/licenses/BSD-3-Clause
[tox]: https://tox.readthedocs.io/en/latest/
[pip]: https://pypi.org/project/pip/
[PyPI]: https://pypi.org/


--- File: docs/quick-start/tutorial_maDLC.md ---
# Multi-animal pose estimation with DeepLabCut: A 5-minute tutorial

## GUI:

Full graphical user interface: just follow the tabs in the GUI! `python -m deeplabcut` launches the GUI.

## Terminal:

**Import deeplabcut**
```python
import deeplabcut
```

**(1) Create a project**
```python
project_name = "cutemice"
experimenter = "teamdlc"
video_path = "path_to_a_video_file"
config_path = deeplabcut.create_new_project(
    project_name,
    experimenter,
    [video_paths],
    multianimal=True,
    copy_videos=True,
)
```
> **_NOTE:_**  Make sure to specify the absolute path to the video file(s).
> It is quickly obtained on Windows with <kbd>⇧ Shift</kbd>+<kbd>Right click</kbd> and `Copy as path`,
> and on Mac with <kbd>⌥ Option</kbd>+<kbd>Right click</kbd> and `Copy as Pathname`.
> Ubuntu users only need to copy the file and its path gets added to the clipboard.

> Next, you can set a variable for the config_path: 'Full path of the project configuration file*'

**(2) Edit the config.ymal file to set up your project**
> **_NOTE:_** Here is were you will define your key point names and animal IDs. Also you can change the default # of frames to extract for the next step.

**(3) Extract video frames to annotate**
```python
deeplabcut.extract_frames(
    config_path,
    mode="automatic",
    algo="kmeans",
    userfeedback=False,
)
```
> **_NOTE:_** try to extract a few frames from many videos vs. a lot of frames from one video!

**(4) Annotate Frames**
```python
deeplabcut.label_frames(config_path)
```


**(5) Visually check annotated frames**
```python
deeplabcut.check_labels(
    config_path,
    draw_skeleton=False,
)
```

**(6) Create the training dataset**
```python
deeplabcut.create_multianimaltraining_dataset(
    config_path,
    num_shuffles=1,
    net_type="dlcrnet_ms5",
)
```

**(7) Train the network**

```python
# PyTorch Engine
deeplabcut.train_network(
    config_path,
    device="cuda",
    save_epochs=5,
    epochs=200,
)

# TensorFlow Engine
deeplabcut.train_network(
    config_path,
    saveiters=10000,
    maxiters=50000,
    allow_growth=True,
)
```

**(8) Evaluate the network**
```python
deeplabcut.evaluate_network(
    config_path,
    plotting=True,
)
```

**(9) Analyze a video (extracts detections and association costs)**
```python
deeplabcut.analyze_videos(
    config_path,
    [video],
    auto_track=True,
)
```
> **_NOTE:_** `auto_track=True` will complete steps 10-11 for you automatically so you get the "final" H5 file. Use the below steps if you need to change the parameters of tracking based on your dataset.


**(10) Spatial and (locally) temporal grouping: Track body part assemblies frame-by-frame**
```python
deeplabcut.convert_detections2tracklets(
    config_path,
    [video],
    track_method="ellipse",
)
```


**(11) Reconstruct full animal trajectories (tracks from tracklets)**
```python
deeplabcut.stitch_tracklets(
    config_path,
    [video],
    track_method="ellipse",
    min_length=5,
)
```


**(12) Create a pretty video output**
```python
deeplabcut.create_labeled_video(
    config_path,
    [video],
    color_by="individual",
    keypoints_only=False,
    trailpoints=10,
    draw_skeleton=False,
    track_method="ellipse",
)
```


--- File: docs/quick-start/single_animal_quick_guide.md ---
### QUICK GUIDE to single Animal Training:
**The main steps to take you from project creation to analyzed videos:**

Open ipython in the terminal:
```
ipython
```

Import DeepLabCut:
```
import deeplabcut
```

Create a new project:
```
deeplabcut.create_new_project("project_name", "experimenter", ["path of video 1", "path of video2", ..])
```
    
Set a config_path variable for ease of use + go edit this file!:
```
config_path = "yourdirectory/project_name/config.yaml"
```
        
Extract frames:
```
deeplabcut.extract_frames(config_path)
```

Label frames:
``` 
deeplabcut.label_frames(config_path)
```
  
Check labels [OPTIONAL]:
```
deeplabcut.check_labels(config_path)
```
   
Create training dataset:
```
deeplabcut.create_training_dataset(config_path)
```
 
Train the network:
```
deeplabcut.train_network(config_path)
```

Evaluate the trained network:
```
deeplabcut.evaluate_network(config_path)
```

 Video analysis:
```
deeplabcut.analyze_videos(config_path, ["path of video 1", "path of video2", ..])
```

Filter predictions [OPTIONAL]:
```
deeplabcut.filterpredictions(config_path, ["path of video 1", "path of video2", ..])
```

Plot results (trajectories):
```
deeplabcut.plot_trajectories(config_path, ["path of video 1", "path of video2", ..], filtered=True)
```

Create a video:
```
deeplabcut.create_labeled_video(config_path, ["path of video 1", "path of video2", ..], filtered=True)
```


--- File: docs/api/deeplabcut.merge_datasets.rst ---
.. autofunction:: deeplabcut.refine_training_dataset.outlier_frames.merge_datasets


--- File: docs/api/deeplabcut.label_frames.rst ---
.. autofunction:: deeplabcut.gui.tabs.label_frames.label_frames


--- File: docs/api/deeplabcut.analyzeskeleton.rst ---
.. autofunction:: deeplabcut.post_processing.analyze_skeleton.analyzeskeleton


--- File: docs/api/deeplabcut.check_labels.rst ---
.. autofunction:: deeplabcut.generate_training_dataset.trainingsetmanipulation.check_labels


--- File: docs/api/deeplabcut.extract_frames.rst ---
.. autofunction:: deeplabcut.generate_training_dataset.frame_extraction.extract_frames


--- File: docs/api/deeplabcut.create_labeled_video.rst ---
.. autofunction:: deeplabcut.utils.make_labeled_video.create_labeled_video


--- File: docs/api/deeplabcut.plot_trajectories.rst ---
.. autofunction:: deeplabcut.utils.plotting.plot_trajectories


--- File: docs/api/deeplabcut.create_training_model_comparison.rst ---
.. autofunction:: deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_model_comparison


--- File: docs/api/deeplabcut.extract_outlier_frames.rst ---
.. autofunction:: deeplabcut.refine_training_dataset.outlier_frames.extract_outlier_frames


--- File: docs/api/deeplabcut.train_network.rst ---
.. autofunction:: deeplabcut.compat.train_network


--- File: docs/api/deeplabcut.convert_detections2tracklets.rst ---
.. autofunction:: deeplabcut.compat.convert_detections2tracklets


--- File: docs/api/deeplabcut.refine_labels.rst ---
.. autofunction:: deeplabcut.gui.tabs.label_frames.refine_labels


--- File: docs/api/deeplabcut.stitch_tracklets.rst ---
.. autofunction:: deeplabcut.refine_training_dataset.stitch.stitch_tracklets


--- File: docs/api/deeplabcut.video_inference_superanimal.rst ---
.. autofunction:: deeplabcut.video_inference_superanimal


--- File: docs/api/deeplabcut.create_new_project.rst ---
.. autofunction:: deeplabcut.create_project.new.create_new_project


--- File: docs/api/deeplabcut.filterpredictions.rst ---
.. autofunction:: deeplabcut.post_processing.filtering.filterpredictions


--- File: docs/api/deeplabcut.evaluate_network.rst ---
.. autofunction:: deeplabcut.compat.evaluate_network


--- File: docs/api/deeplabcut.analyze_videos.rst ---
.. autofunction:: deeplabcut.compat.analyze_videos


--- File: docs/api/deeplabcut.create_training_dataset.rst ---
.. autofunction:: deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset


--- File: docs/api/deeplabcut.create_training_dataset_from_existing_split.rst ---
.. autofunction:: deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset_from_existing_split


--- File: docs/pytorch/architectures.md ---
(dlc3-architectures)=
# DeepLabCut 3.0 - PyTorch Model Architectures

## Introduction

You can see a list of supported architectures/variants by using:

```python
from deeplabcut.pose_estimation_pytorch import available_models
print(available_models())
```

You can see a list of supported object detection architectures/variants by using:

```python
from deeplabcut.pose_estimation_pytorch import available_detectors
print(available_detectors())
```

## Neural Networks Architectures

Several architectures are currently implemented in DeepLabCut PyTorch (more will come,
and you can add more easily in our new model registry).

**ResNets**
- Adapted from [He, Kaiming, et al. "Deep residual learning for image recognition." Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2016.](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) and [Insafutdinov, Eldar et al. "DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model". European Conference on Computer Vision (ECCV) 2016.]
- Current bottom-up variants are `resnet_50`, `resnet_101`
- Current top-down variants are `top_down_resnet_101`, `top_down_resnet_50`

**HRNet**
- Adapted from [Wang, Jingdong, et al. "Deep high-resolution representation learning for visual recognition." IEEE transactions on pattern analysis and machine intelligence 43.10 (2020): 3349-3364.](https://arxiv.org/abs/1908.07919)
- Current variants are `hrnet_w18`, `hrnet_w32`, `hrnet_w48`, 
- Current top-down variants are `top_down_hrnet_w18`, `top_down_hrnet_w32`, `top_down_hrnet_w48`
- Slower but typically more powerful than ResNets

**DEKR**
- Adapted from [Geng, Zigang et al. "Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression." Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2021.](https://openaccess.thecvf.com/content/CVPR2021/papers/Geng_Bottom-Up_Human_Pose_Estimation_via_Disentangled_Keypoint_Regression_CVPR_2021_paper.pdf)
- This model is a bottom-up model using HRNet as a backbone. It learns to predict the center of each animal, and predicts the offset between each animal center and their keypoints
- Current variants that are implemented (from smallest to largest): `dekr_w18`, `dekr_w32`, `dekr_w48`
- Note, this is a powerful multi-animal model but very heavy (slow)

**BUTCTD**
- Adapted from [Zhou, Stoffl, Mathis, Mathis. "Rethinking Pose Estimation in Crowds: Overcoming the Detection Information Bottleneck and Ambiguity." Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Rethinking_Pose_Estimation_in_Crowds_Overcoming_the_Detection_Information_Bottleneck_ICCV_2023_paper.pdf)
- [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/rethinking-pose-estimation-in-crowds/pose-estimation-on-crowdpose)](https://paperswithcode.com/sota/pose-estimation-on-crowdpose?p=rethinking-pose-estimation-in-crowds)
- Current variants are `BUCTD-hrnet_w32` and `BUCTD-hrnet_w48`
- This is a top-performing mutli-animal (and for humans, which are also animals) method that can be used with other architectures

**DLCRNet**
- From [Lauer, Zhou, et al. "Multi-animal pose estimation, identification and tracking with DeepLabCut." Nature Methods 19.4 (2022): 496-504.](https://www.nature.com/articles/s41592-022-01443-0)
- This model uses a multi-scale variant of a ResNet as a backbone, and part-affinity fields to assemble individuals
- Variants: `dlcrnet_stride16_ms5`, `dlcrnet_stride32_ms5`

**RTMPose**
- From [Jiang, Tao et al. "RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose"](https://arxiv.org/abs/2303.07399)
- Top-down pose estimation model using a fast CSPNeXt backbone with a SimCC-style head
- Variants: `rtmpose_s`, `rtmpose_m`, `rtmpose_x`

**AnimalTokenPose**
-  Adapted from [Li, Yanjie, et al. "Tokenpose: Learning keypoint tokens for human pose estimation." Proceedings of the IEEE/CVF International conference on computer vision. 2021.](https://arxiv.org/abs/2104.03516) as in Ye et al. "SuperAnimal pretrained pose estimation models for behavioral analysis." Nature Communications. 2024](https://arxiv.org/abs/2203.07436)
  - One variant is implemented as: `animal_tokenpose_base` for video inference only (we don't support directly training this within deeplabcut)


## Information on Single Animal Models

Single-animal models are composed of a backbone (encoder) and a head (decoder) 
predicting the position of keypoints. The default head contains a single deconvolutional
layer. To create the single animal model composed of a backbone and head, you can call
`deeplabcut.create_training_dataset` with `net_type` set to the backbone name (e.g. 
`resnet_50` or `hrnet_w32`).

If you want to add a second deconvolutional layer (which will make your model slower, 
but it might improve performance), you can simply edit your `pytorch_config.yaml` file.

Of course, any multi-animal model can also be used for single-animal projects!

## Information on Multi-Animal Models

### Backbones with Part-Affinity Fields

As in DeepLabCut 2.X, the base multi-animal model is composed of a backbone (encoder) 
and a head predicting keypoints and part-affinity fields (PAFs). These PAFs are used to 
assemble keypoints for individuals.

Passing a backbone as a net type (e.g., `resnet_50`, `hrnet_w32`) for a multi-animal 
project will create a model consisting of a backbone and a heatmap + PAF head.

### Top-Down Models

Top-down pose estimation models split the task into two distinct parts: individual 
localization (through an object detector), followed by pose estimation (for each 
individual). As localization of individuals is handled by the detector, this simplifies
the pose task to single-animal pose estimation!

Hence any single-animal model can be transformed into a top-down, multi-animal model. To
do so, simply prefix `top_down` to your single-animal model name. Currently, the 
following detectors are available: `ssdlite`, `fasterrcnn_mobilenet_v3_large_fpn`,
`fasterrcnn_resnet50_fpn_v2`. Other variants will be added soon!  

The pose model for top-down nets is simply the backbone followed by a single convolution
for pose estimation. It's also possible to add deconvolutional layers to top-down model
heads.

Example top-down models would be `top_down_resnet_50` and `top_down_hrnet_w32`.


--- File: docs/pytorch/pytorch_config.md ---
(dlc3-pytorch-config)=
# The PyTorch Configuration file

The `pytorch_config.yaml` file specifies the configuration for your PyTorch pose models,
from the model architecture to which optimizer will be used for training, how training 
runs will be logged, the data augmentation that will be applied and which metric should
be used to save the "best" model snapshot. 

You can create default configurations for a shuffle using 
`deeplabcut.create_training_set` or `deeplabcut.create_training_model_comparison`. This 
will create a `pytorch_config.yaml` file for your selected net type. The basic structure
of the file is as follows:

```yaml
data:  # which data augmentations will be used
  ...
device: auto # the default device to use for training and evaluation
metadata:  # metadata regarding the project (bodyparts, individuals, paths, ...) - filled automatically
  ...
method: bu # indicates how pose predictions are made (bottom-up (`bu`) or top-down (`td`))
model:  # configures the model architecture (which backbone, heads, ...)
  ...
net_type: resnet_50 # the type of neural net configured in the file
runner:  # configuring the runner used for training
  ...
train_settings:  # generic training settings, such as batch size and maximum number of epochs
  ...
logger:  # optional: the configuration for a logger if you want one
resume_training_from:  # optional: restart the training at the specific checkpoint
```

## Sections

### Singleton Parameters

There are a few singleton parameters defined in the PyTorch configuration file:

- `device`: The device to use for training/inference. The default is `auto`, which sets 
the device to `cuda` if an NVIDIA GPU is available, and `cpu` otherwise. For users 
running models on macOS with an M1/M2/M3 chip, this is set to `mps` for certain models
(not all operations are currently supported on Apple GPUs - so some models like HRNets 
need to be trained on CPU, while others like ResNets can take advantage of the GPU).
- `method`: Either `bu` for bottom-up models, or `td` for top-down models.
- `net_type`: The type of pose model configured by the file (e.g. `resnet_50`).

### Data

The data section configures:

- `bbox_margin`: The margin (in pixels) to add around ground truth pose when generating
bounding boxes. For more information, see [generating bounding boxes from pose](
#bbox-from-pose).
- `colormode`: in which format images are given to the model (e.g., `RGB`, `BGR`)
- `inference`: which transformations should be applied to images when running evaluation
or inference
- `train`: which transformations should be applied to images when training

The default configuration for a pose model is:

```yaml
data:
  bbox_margin: 20
  colormode: RGB  # should never be changed
  inference:  # the augmentations to apply to images during inference 
    normalize_images: true  # this should always be set to true
  train:
    affine:
      p: 0.5
      rotation: 30
      scaling: [0.5, 1.25]
      translation: 0
    covering: true
    crop_sampling:
      width: 448   # if your images are very small or very large, you may need to edit!
      height: 448  # see below for more information about crop_sampling! 
      max_shift: 0.1
      method: hybrid
    gaussian_noise: 12.75
    motion_blur: true
    normalize_images: true  # this should always be set to true
```

The following transformations are available for the `train` and `inference` keys.

**Affine**: Applies an affine (rotation, translation, scaling) transformation to the
images. 

```yaml
affine:
  p: 0.9  # float: the probability that an affine transform is applied
  rotation: 30  # int: the maximum angle of rotation applied to the image (in degrees)
  scaling: [ 0.5, 1.25 ]  # [float, float]: the (min, max) scale to use to resize images
  translation: 40  # int: the maximum translation to apply to images (in pixels)
```

**Auto-Padding**: Pads the image to some desired shape (e.g., a minimum height/width or 
such that the height/width are divisible by a given number). Some backbones (such as
HRNets) require the height and width of images to be multiples of 32. Setting up
auto-padding with `pad_height_divisor: 32` and `pad_width_divisor: 32` ensures that is
the case. Note that **not all keys need to be set**! The values shown are the default
values. Only one of 'min_height' and 'pad_height_divisor' parameters must be set, and 
only one of 'min_width' and 'pad_width_divisor' parameters must be set.

```yaml
auto_padding:
  min_height: null  # int: if not None, the minimum height of the image
  min_width: null  # int: if not None, the minimum width of the image
  pad_height_divisor: null  # int: if not None, ensures image height is dividable by value of this argument.
  pad_width_divisor: null  # int: if not None, ensures image width is dividable by value of this argument.
  position: random  # str: position of the image, one of 'A.PadIfNeeded.Position'
  border_mode: reflect_101  # str: 'constant' or 'reflect_101' (see cv2.BORDER modes)
  border_value: null  # str: padding value if border_mode is 'constant'
  border_mask_value: null  # str: padding value for mask if border_mode is 'constant'
```

**Covering**: Based on Albumentations's [CoarseDropout](
https://albumentations.ai/docs/api_reference/augmentations/dropout/coarse_dropout/#albumentations.augmentations.dropout.coarse_dropout)
augmentation, this "cuts" holes out of the image. As defined in 
[Improved Regularization of Convolutional Neural Networks with Cutout](
https://arxiv.org/abs/1708.04552).

```yaml
covering: true  # bool: if true, applies a coarse dropout with probability 50%
```

**Gaussian Noise**: Applies gaussian noise to the input image. Can either be a float 
(the standard deviation of the noise) or simply a boolean (the standard deviation of 
the noise will be set as 12.75).

```yaml
gaussian_noise: 12.75  # bool, float: add gaussian noise
```

**Horizontal Flips**: This flips the image horizontally around the y-axis. As the 
resulting image is mirrored, it does not preserve labels (the left hand would become the
right hand, and vice versa). This augmentation should not be used for pose models if you
have symmetric keypoints! However, it is safe to use it to train detectors. If you want
to use horizontal flips with symmetric keypoints, you need to specify them through the 
`symmetries` parameter!

```yaml
# augmentation for object detectors or when no symmetric (left/right) keypoints exist: 
hflip: true

# augmentation if your bodyparts are [snout, eye_L, eye_R, ear_L, ear_R]
hflip:
  p: 0.5  # apply a horizontal flip with 50% probability
  symmetries: [[1, 2], [3, 4]]  # the indices of symmetric keypoints
```

**Histogram Equalization**: Applies histogram equalization with probability 50%.

```yaml
hist_eq: true  # bool: whether to apply histogram equalization
```

**Motion Blur**: Applies motion blur to the image with probability 50%.

```yaml
motion_blur: true  # bool: whether to apply motion blur
```

**Normalization**: This should always be set to `true`.

```yaml
normalize_images: true  # normalizes images
```

#### Dealing with Variable Image Sizes

```{NOTE}
When training with batch size 1 (or if all images in your dataset have the same size), 
you don't need to worry about any of this! However, you can still use `crop_sampling`
which may help your model generalize.
```

When training with a batch size greater than 1, all images in a batch **must** have the
same size. PyTorch **collates** all images into one tensor of shape `[b, c, h, w]`, 
where `b` is the batch size, `c` the number of channels in the image, `h` and `w` the 
height and width of images in the batches. There are a few different ways to ensure that
all images in a batch have the same size:

1. **Crop sampling**. This is the default behavior for the PyTorch engine in DeepLabCut.
A part of each image (of a fixed size) is cropped and given to the model to train. See 
below for more information.
2. **A custom collate function**. Collate functions define a way that images of different
sizes can be combined into one tensor. This involves resizing and padding images to the
same size and aspect ratio. Available collate functions are defined in
`deeplabcut/pose_estimation_pytorch/data/collate.py`. 
3. **Resizing all images**. All images can simply be resized to the same size. This
usually doesn't lead to the best performance.

**Resizing - Crop Sampling**: An alternative way to ensure all images have the same size
is through cropping. The `crop_sampling` crops images down to a maximum width and 
height, with options to sample the center of the crop according to the positions of the
keypoints. The methods to sample the center of the crop are as follows:

- `uniform`: randomly over the image
- `keypoints`: randomly over the annotated keypoints
- `density`: weighing preferentially dense regions of keypoints
- `hybrid`: alternating randomly between `uniform` and `density`

```yaml
crop_sampling:
  height: 400  # int: the height of the crop 
  width: 400  # int: the height of the crop 
  max_shift: 0.4  # float: maximum allowed shift of the cropping center position as a fraction of the crop size.
  method: hybrid # str: the center sampling method (one of 'uniform', 'keypoints', 'density', 'hybrid') 
```

**Collate**: Defines how images are collated into batches. The default way collate
function to use is `ResizeFromDataSizeCollate` (other collate functions are defined in
`deeplabcut/pose_estimation_pytorch/data/collate.py`). For each batch to collate, this
implementation:
1. Selects the target width & height all images will be resized to by getting the size 
of the first image in the batch, and multiplying it by a scale sampled uniformly at 
random from `(min_scale, max_scale)`.
2. Resizes all images in the batch (while preserving their aspect ratio) such that they 
are the smallest size such that the target size fits entirely in the image.
3. Crops each resulting image into the target size with a random crop.

```yaml
collate:  # rescales the images when putting them in a batch
  type: ResizeFromDataSizeCollate  # You can also use `ResizeFromListCollate`
  max_shift: 10  # the maximum shift, in pixels, to add to the random crop (this means
    # there can be a slight border around the image)
  max_size: 1024  #  the maximum size of the long edge of the image when resized. If the
    # longest side will be greater than this value, resizes such that the longest side 
    # is this size, and the shortest side is smaller than the desired size. This is 
    # useful to keep some information from images with extreme aspect ratios.
  min_scale: 0.4  # the minimum scale to resize the image with
  max_scale: 1.0  # the maximum scale to resize the image with
  min_short_side: 128  # the minimum size of the target short side
  max_short_side: 1152  # the maximum size of the target short side
  multiple_of: 32  # pads the target height, width such that they are multiples of 32
  to_square: false  # instead of using the aspect ratio of the first image, only the 
    # short side of the first image will be used to sample a "side", and the images will
    # be cropped in squares
```

**Resizing**: Resizes the images while preserving the aspect ratio (first resizes to the
maximum possible size, then adds padding for the missing pixels).

```yaml
resize:
  height: 640 # int: the height to which all images will be resized
  width: 480 # int: the width to which all images will be resized
  keep_ratio: true  # bool: whether the aspect ratio should be preserved when resizing
```

### Model

The model configuration is further split into a `backbone`, optionally a `neck` and a 
number of heads.

Changing the `model` configuration should only be done by expert users, and in rare 
occasions. When updating a model configuration (e.g. adding more deconvolution layers 
to a `HeatmapHead`) must be done in a way where the model configuration still makes 
sense for the project (e.g. the number of heatmaps output needs to match the number of 
bodyparts in the project).

An example model configuration for a single-animal HRNet would look something like:

```yaml
model:
  backbone:  # the BaseBackbone used by the pose model
    type: HRNet
    model_name: hrnet_w18  # creates an HRNet W18 backbone
  backbone_output_channels: 18
  heads:  # configures how the different heads will make predictions
    bodypart:  # configures how pose will be predicted for bodyparts
      type: HeatmapHead
      predictor:  # the BasePredictor used to make predictions from the head's outputs
        type: HeatmapPredictor
          ...
      target_generator:  # the BaseTargetGenerator used to create targets for the head
        type: HeatmapPlateauGenerator
          ...
      criterion:  # the loss criterion used for the head
        ...
      ...  # head-specific options, such as `heatmap_config` or `locref_config` for a "HeatmapHead"
```

The `backbone`, `neck` and `head` configurations are loaded using the
`deeplabcut.pose_estimation_pytorch.models.backbones.base.BACKBONES`,
`deeplabcut.pose_estimation_pytorch.models.necks.base.NECKS` and 
`deeplabcut.pose_estimation_pytorch.models.heads.base.HEADS` registries. You specify 
which type to load with the `type` parameter. Any argument for the head can then be used
in the configuration.

So to use an `HRNet` backbone for your model (as defined in 
`deeplabcut.pose_estimation_pytorch.models.backbones.hrnet.HRNet`), you could set:

```yaml
model:
  backbone:
    type: HRNet
    model_name: hrnet_w32  # creates an HRNet W32
    pretrained: true  # the backbone weights for training will be loaded from TIMM (pre-trained on ImageNet)
    interpolate_branches: false  # don't interpolate & concatenate channels from all branches 
    increased_channel_count: true  # use the incre_modules defined in the TIMM HRNet
  backbone_output_channels: 128  # number of channels output by the backbone
```

### Runner

The runner contains elements relating to the runner to use (including the optimizer and 
learning rate schedulers). Unless you're experienced with machine learning and training 
models **it is not recommended to change the optimizer or scheduler**.

```yaml
runner:
  type: PoseTrainingRunner  # should not need to modify this
  key_metric: "test.mAP"  # the metric to use to select the "best snapshot"
  key_metric_asc: true  # whether "larger=better" for the key_metric
  eval_interval: 1  # the interval between each passes through the evaluation dataset
  optimizer:  # the optimizer to use to train the model
    ...
  scheduler:  # optional: a learning rate scheduler
    ...
  load_scheduler_state_dict: true/false # whether to load scheduler state when resuming training from a snapshot,
  snapshots:  # parameters for the TorchSnapshotManager
    max_snapshots: 5  # the maximum number of snapshots to save (the "best" model does not count as one of them)
    save_epochs: 25  # the interval between each snapshot save  
    save_optimizer_state: false  # whether the optimizer state should be saved with the model snapshots (very little reason to set to true)
  gpus: # GPUs to use to train the network
  - 0
  - 1
```

**Key metric**: Every time the model is evaluated on the test set, metrics are computed 
to see how the model is performing. The key metric is used to determine whether the 
current model is the "best" so far. If it is, the snapshot is saved as `...-best.pt`. 
For pose models, metrics to choose from would be `test.mAP` (with `key_metric_asc: true`
) or `test.rmse` (with `key_metric_asc: false`). 

**Evaluation interval**: Evaluation slows down training (it takes time to go through all
the evaluation images, make predictions and log results!). So instead of evaluating 
after every epoch, you could decide to evaluate every 5 epochs (by setting
`eval_interval: 5`). While this means you get coarser information about how your model 
is training, it can speed up training on large datasets.

**Optimizer**: Any optimizer inheriting `torch.optim.Optimizer`. More information about 
optimizers can be found in [PyTorch's documentation](
https://pytorch.org/docs/stable/optim.html). Examples:

```yaml
  # SGD with initial learning rate 1e-3 and momentum 0.9
  #  see https://pytorch.org/docs/stable/generated/torch.optim.SGD.html
  optimizer:
    type: SGD
    params:
      lr: 1e-3
      momentum: 0.9

  # AdamW optimizer with initial learning rate 1e-4
  #  see https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html
  optimizer:
    type: AdamW
    params:
      lr: 1e-4
```

**Scheduler**: You can use [any scheduler](
https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) defined in
`torch.optim.lr_scheduler`, where the arguments given are arguments of the scheduler. 
The default scheduler is an LRListScheduler, which changes the learning rates at each 
milestone to the corresponding values in `lr_list`. Examples:

```yaml
  # reduce to 1e-5 at epoch 160 and 1e-6 at epoch 190
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-5 ], [ 1e-6 ] ]
      milestones: [ 160, 190 ]

  # Decays the learning rate of each parameter group by gamma every step_size epochs
  #   see https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html
  scheduler:
    type: StepLR
    params:
      step_size: 100
      gamma: 0.1
```

You can also use schedulers that use other schedulers as parameters, such as a 
[`ChainedScheduler`](
https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ChainedScheduler.html)
or a [`SequentialLR`](
https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.SequentialLR.html).

The `SequentialLR` can be particularly useful, such as to use a first scheduler for some
warmup epochs, and a second scheduler later. An example usage would be:

```yaml
  # Multiply the learning rate by `factor` for the first `total_iters` epochs
  # After 5 epochs, start decaying the learning rate by `gamma` every `step_size` epochs
  # If the initial learning rate is set to 1, the learning rates will be:
  #   epoch 0: 0.01  - using ConstantLR
  #   epoch 1: 0.01  - using ConstantLR
  #   epoch 2: 1.0   - using ConstantLR
  #   epoch 3: 1.0   - using ConstantLR
  #   epoch 4: 1.0   - using ConstantLR
  #   epoch 5: 1.0   - using StepLR
  #   epoch 6: 1.0   - using StepLR
  #   epoch 7: 0.1   - using StepLR
  #   epoch 8: 0.1   - using StepLR
  scheduler:
    type: SequentialLR
    params:
      schedulers:
      - type: ConstantLR
        params:
          factor: 0.01
          total_iters: 2
      - type: StepLR
        params:
          step_size: 2
          gamma: 0.1
      milestones:
      - 5
```

### Train Settings

The `train_settings` key contains parameters that are specific to training. For more 
information about the `dataloader_workers` and `dataloader_pin_memory` settings, see
[Single- and Multi-process Data Loading](
https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading)
and [memory pinning](https://pytorch.org/docs/stable/data.html#memory-pinning). Setting
`dataloader_workers: 0` uses single-process data loading, while setting it to 1 or more
will use multi-process data loading. You should always keep 
`dataloader_pin_memory: true` when training on an NVIDIA GPU. 

```yaml
train_settings:
  batch_size: 1  # the batch size used for training
  dataloader_workers: 0  # the number of workers for the PyTorch Dataloader 
  dataloader_pin_memory: true  # pin DataLoader memory
  display_iters: 500  # the number of iterations (steps) between each log print
  epochs: 200  # the maximum number of epochs for which to train the model
  seed: 42  # the random seed to set for reproducibility
```

### Logger

Training runs are logged to the model folder (where the snapshots are stored) by 
default.

Additionally, you can log results to [Weights and Biases](https://wandb.ai/site), by adding a
`WandbLogger`. Just make sure you're logged in to your `wandb` account before starting 
your training run (with `wandb login` from your shell). For more information, see their
[tutorials](https://docs.wandb.ai/tutorials) and their documentation for [`wandb.init`](https://docs.wandb.ai/ref/python/init).

Logging to `wandb` is a good way to keep track of what you've run, including performance
and metrics.

```yaml
logger:
 type: WandbLogger
 project_name: my-dlc3-project  # the name of the project where the run should be logged
 run_name: dekr-w32-shuffle0  # the name of the run to log
 ...  # any other argument you can pass to `wandb.init`, such as `tags: ["dekr", "split=0"]`
```

You can also log images as they are seen by the model to `wandb` 
with the `image_log_interval`. This logs a random train and test image, as well as the 
targets and heatmaps for that image.

### Restarting Training at a Specific Checkpoint

If you wish to restart the training at a specific checkpoint, you can specify the
full path of the checkpoint to the `resume_training_from` variable, as shown below. In this 
example, `snapshot-010.pt` will be loaded before training starts, and the model will 
continue to train from the 10th epoch on.

```yaml
# model configuration
...
# weights from which to resume training
resume_training_from: /Users/john/dlc-project-2021-06-22/dlc-models-pytorch/iteration-0/dlcJun22-trainset95shuffle0/train/snapshot-010.pt
```

When continuing to train a model, you may want to modify the learning rate scheduling 
that was being used (by editing the configuration under the `scheduler` key). When doing
so, you *must set `load_scheduler_state_dict: false`* in your `runner` config! 
Otherwise, the parameters for the scheduler your started training with will be loaded 
from the state dictionary, and your edits might not be kept!

## Training Top-Down Models

Top-down models are split into two main elements: a detector (localizing individuals in
the images) and a pose model predicting each individual's pose (once localization is 
done, obtaining pose is just like getting pose in a single-animal model!).

The "pose" part of the model configuration is exactly the same as for single-animal or
bottom-up models (configured through the `data`, `model`, `runner` and `train_settings`
). The detector is configured through a detector key, at the top-level of the
configuration.

### Detector Configuration

When training top-down models, you also need to configure how the detector will be 
trained. All information relating to the detector is placed under the `detector` key.

```yaml
detector:
  data:  # which data augmentations will be used, same options as for the pose model
    colormode: RGB
    inference:  # default inference configuration for detectors
      normalize_images: true
    train:  # default train configuration for detectors
      affine:
        p: 0.9
        rotation: 30
        scaling: [ 0.5, 1.25 ]
        translation: 40
      hflip: true
      normalize_images: true
  model:  # the detector to train
    type: FasterRCNN
    variant: fasterrcnn_mobilenet_v3_large_fpn
    pretrained: true
  runner:  #  detector train runner configuration (same keys as for the pose model)
    type: DetectorTrainingRunner
    ...
  train_settings: # detector train settings (same keys as for the pose model)
    ...
  resume_training_from: # optional: restart the training at the specific checkpoint
```

Currently, the only detectors available are `FasterRCNN` and `SSDLite`. However, multiple variants of
`FasterRCNN` are available (you can view the different variants on 
[torchvision's object detection page](https://pytorch.org/vision/stable/models.html#object-detection)). It's recommended to use the fastest 
detector that brings enough performance. The recommended variants are the following 
(from fastest to most powerful, taken from torchvision's documentation):

| name                              | Box MAP (larger = more powerful) | Params (larger = more powerful) | GFLOPS (larger = slower) |
|-----------------------------------|---------------------------------:|--------------------------------:|-------------------------:|
| SSDLite                           |                             21.3 |                            3.4M |                     0.58 |
| fasterrcnn_mobilenet_v3_large_fpn |                             32.8 |                           19.4M |                     4.49 |
| fasterrcnn_resnet50_fpn           |                               37 |                           41.8M |                   134.38 |
| fasterrcnn_resnet50_fpn_v2        |                             46.7 |                           43.7M |                   280.37 |


### Restarting Training of an Object Detector at a Specific Checkpoint

If you wish to restart the training of a detector at a specific checkpoint, you can
specify the full path of the checkpoint to the detector's `resume_training_from` variable, as
shown below. In this example, `snapshot-detector-020.pt` will be loaded before training
starts, and the model will continue to train from the 20th epoch on.

```yaml
detector:
  # detector configuration
  ...
  # weights from which to resume training
  resume_training_from: /Users/john/dlc-project-2021-06-22/dlc-models-pytorch/iteration-0/dlcJun22-trainset95shuffle0/train/snapshot-detector-020.pt
```

When continuing to train a detector, you may want to modify the learning rate scheduling 
that was being used (by editing the configuration under the `scheduler` key). When doing
so, you *must set `load_scheduler_state_dict: false`* in your `detector`: `runner`
config! Otherwise, the parameters for the scheduler your started training with will be
loaded from the state dictionary, and your edits might not be kept!

(bbox-from-pose)=
### Generating Bounding Boxes from Pose

To train object detection models (for top-down pose estimation), ground truth bounding
boxes are needed. As they are not annotated in DeepLabCut, they are generated from the
ground truth pose: simply take the minimum and maximum for the x and y axes, add a small
margin and you have your bounding box! The default setting adds a margin of 20 pixels
around the pose. This works well in most cases, but in some cases you should update this 
value (e.g. when you have very small or large images).

You can edit that value in the `pytorch_config.yaml` for your model through the 
`data: bbox_margin` parameter for the detector:

```yaml
detector:
  data:
    bbox_margin: 20
    ...
```

![Bounding boxes generated from pose with different margins](assets/bboxes_from_kpts.png)


--- File: docs/pytorch/user_guide.md ---
(dlc3-user-guide)=
# DeepLabCut 3.0 - PyTorch User Guide

## Using DeepLabCut 3.0

**DeepLabCut 3.0 keeps the same high-level API that you know, but has a full new PyTorch backend. 
Moreover, it is a rewrite that is more developer friendly, more powerful, and built for modern deep
learning-based computer vision applications.**

**NOTE**🔥: We suggest that if you're just starting with DeepLabCut you start with the
PyTorch backend. You will easily know which "engine" you are using by looking at the
main `config.yaml` file, or top right corner in the GUI. If you have DeepLabCut projects
in TensorFlow, we've got you covered too: you can seamlessly switch to train your
already labeled data by simply switching the engine (and thereby also compare
performance). In short, expect a boost 🔥.

In short, PyTorch models can be trained in any DeepLabCut project. If you have a project
already made,  simply add a new key to your project `config.yaml` file specifying
`engine: pytorch`. Then any new training dataset that will be created will be a PyTorch
model (see [Creating Shuffles and Model Configuration](
#Creating-Shuffles-and-Model-Configuration)) to learn more about training PyTorch
models. To train Tensorflow models again, you can set `engine: tensorflow`.

### Installation

To see the DeepLabCut 3.0 installation guide, check the [installation docs](how-to-install).

### Using the GUI

You can use the GUI to train DeepLabCut projects. You can switch between the PyTorch
and TensorFlow engine through the drop-down menu in the top right corner.

## Major changes

### From iterations to epochs

Pytorch models in DeepLabCut 3.0 are trained for a set number of `epochs`, instead of a 
maximum number of `iterations`. An epoch is a single pass through the training dataset, 
which means your model has seen each training image exactly once.

- So if you have 64 training images for your network, an epoch is 64 iterations with batch
size 1 (or 32 iterations with batch size 2, 16 with batch size 4, etc.).

## API

### Creating Shuffles and Model Configuration

You can configure models using the `pytorch_config.yaml` file, as described
[here](dlc3-pytorch-config). You can use the same methods to create new shuffles in 
DeepLabCut 3.0 as you did for Tensorflow models (`deeplabcut.create_training_dataset`
and `deeplabcut.create_training_model_comparison`).

More information about the different PyTorch model architectures available in DeepLabCut
is available [here](dlc3-pytorch-config). You can see a list of supported 
architectures/variants by using:

```python
from deeplabcut.pose_estimation_pytorch import available_models
print(available_models())
```

### Development State and Road Map 🚧

The table below describes the DeepLabCut API methods that have been implemented for the
PyTorch engine, as well as indications which options are not yet implemented, and which
parameters are not valid for the DLC 3.0 PyTorch API.


| API Method                     | Implemented | Parameters not yet implemented                                                                      | Parameters invalid for pytorch                      |
|--------------------------------|:-----------:|-----------------------------------------------------------------------------------------------------|-----------------------------------------------------|
| `train_network`                |     🟢      |                                                                                                     | `maxiters`, `saveiters`, `allow_growth`, `autotune` |
| `return_train_network_path`    |     🟢      |                                                                                                     |                                                     |
| `evaluate_network`             |     🟢      |                                                                                                     |                                                     |
| `return_evaluate_network_data` |     🔴      |                                                                                                     | `TFGPUinference`, `allow_growth`                    |
| `analyze_videos`               |     🟠      | `greedy`, `calibrate`, `window_size`                                                                |                                                     |
| `create_tracking_dataset`      |     🟢      |                                                                                                     |                                                     |
| `analyze_time_lapse_frames`    |     🟢      | the name has changed to  `analyze_images` to better reflect what it actually does (no video needed) |                                                     |
| `convert_detections2tracklets` |     🟠      | `greedy`, `calibrate`, `window_size`                                                                |                                                     |
| `extract_maps`                 |     🟢      |                                                                                                     |                                                     |
| `visualize_scoremaps`          |     🟢      |                                                                                                     |                                                     |
| `visualize_locrefs`            |     🟢      |                                                                                                     |                                                     |
| `visualize_paf`                |     🟢      |                                                                                                     |                                                     |
| `extract_save_all_maps`        |     🟢      |                                                                                                     |                                                     |
| `export_model`                 |     🟢      |                                                                                                     |                                                     |


--- File: docs/pytorch/Benchmarking_shuffle_guide.md ---
# DeepLabCut Benchmarking - User Guide

## Reasoning for benchmarking models in DLC (across DLC versions and architectures)

DeepLabCut 3.0+ introduced using PyTorch 🔥 as a deep learning engine (and TensorFlow will be depreciated). 
It is of importance for replicability of data analysis to benchmark existing models created using DeepLabCut versions
prior to 3.0 against new models created in DeepLabCut 3.0+ and later versions.

When comparing different models, it's important to use the same train-test data 
split to ensure fair comparisons. If the models are trained on different datasets, 
their performance metrics can't be accurately compared. This is crucial when 
comparing the performance of models with different architectures or different 
sets of hyperparameters. For example, if we compare the RMSE of a model on an 
"easy" test image with the RMSE of another model on a "hard" test image, it 
doesn't determine whether a model is better than the other because the 
architecture performs better or because the training images were "better" to 
learn from. Thus, we not only need to compare the models based on metrics 
computed on the same test images, but also train them on an identical fixed 
training set in order to "decouple" the dataset from the model architecture.

Creating a model using the same data split can be carried out using a GUI or 
using code, and this guide outlines the steps for both.

## Important files & folders

```
dlc-project
|
|___dlc-models-pytorch
|   |__ iterationX
|       |__ shuffleX
|           |__ pytorch_config.yaml
|  
|___training-datasets
|   |__ metadata.yaml
|
|___config.yaml
```

## Benchmarking a TensorFlow model against a PyTorch model

### Creating a shuffle

Creating a new shuffle with the same train/test split as an existing one:
#### In the DeepLabCut GUI
1. Front page > Load project > Open project folder > choose *config.yaml*
2. Select *'Create training dataset'* tab
3. Tick *Use an existing data split* option    

    ![create_from_existing](<assets/img1.png>)
4. Click 'View existing shuffles':
    - This is used to view the indices of shuffles created for a project to determine which index is available to assign to a new shuffle.
    - The elements described in this window are:
        - train_fraction: The fraction of the dataset used for training.
        - index: The index of the shuffle.
        - split: The data split for the shuffle. The integer value on its own does not
hold any meaning, but this "split" value indicates which shuffles have the same split 
(as their results can then be compared)
        - engine: Whether it is a PyTorch or TensorFlow shuffle

            ![view_existing_sh](<assets/img2.png>)
5. Choose the index of the training shuffle to replicate. Let us assume we want
to replicate the train-test split from OpenfieldOct30-trainset95shuffle3, in which
`split: 3`. In this case, we insert in the *'From shuffle'* menu
    
    ![choose_existing_index](<assets/img3.png>)
6. To create this new dataset, set the shuffle option to an un-used shuffle
(here 4)
    
    ![choose_new_index](<assets/img4.png>)
7. Click *'Create training dataset'* and move on to *'train network'*. Shuffle should be 
set to the new shuffle entered at the previous step (in this case, 4)
    
    ![create_from_existing](<assets/img5.png>)
8. To view/edit the specifications of the model you created, you can go to `pytoch_config.yaml` file at:
    ```
    dlc-project
    |
    |___ dlc-models-pytorch
        |__ iterationX
            |__ shuffleX
                |__ pytorch_config.yaml
    ```

#### In Code 

With the `deeplabcut` module in Python, use the
`create_training_dataset_from_existing_split()` method to create new shuffles from
existing ones (e.g. TensorFlow shuffles).

Similarly, here, we create a new shuffle '4' from the existing shuffle '3'.

```python
import deeplabcut
from deeplabcut.core.engine import Engine

config = "path/to/project/config.yaml"

training_dataset = deeplabcut.create_training_dataset_from_existing_split(
   config=config,
   from_shuffle=3,
   from_trainsetindex=0,
   shuffles=[4],
   net_type="resnet_50",
)
```

We can then train our new PyTorch model with the same data split as the
TensorFlow model.

```python
deeplabcut.train_network(config, shuffle=4, engine=Engine.PYTORCH, batch_size=8)
```

Once trained we can evaluate our model using

```python
deeplabcut.evaluate_network(config, Shuffles=[4], snapshotindex="all")
```
Now, we can compare performances with peace of mind!

#### Good practices: naming shuffles created from existing ones

In a setting where one has multiple TensorFlow models and intends to benchmark 
their performances against new PyTorch models, it is good practice to follow 
a naming pattern for the shuffles we create.

Say we have TensorFlow shuffles 0, 1, and 2. We can create new PyTorch shuffles 
from them by naming them 1000, 1001, and 1002. This allows us to quickly 
recognize that the shuffles belonging to the 100x range are PyTorch shuffles 
and that shuffle 1001, for example, has the same data split as TensorFlow 
shuffle 1. This way, the comparison can be more straightforward and guaranteed 
to be correct!

This was contributed by the [2024 DLC AI Residents](https://www.deeplabcutairesidency.org/our-team)!


--- File: docs/beginner-guides/Training-Evaluation.md ---
# Neural Network training and evaluation in the GUI
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572296495650-Y4ZTJ2XP2Z9XF1AD74VW/ke17ZwdGBToddI8pDm48kMulEJPOrz9Y8HeI7oJuXxR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UZiU3J6AN9rgO1lHw9nGbkYQrCLTag1XBHRgOrY8YAdXW07ycm2Trb21kYhaLJjddA/DLC_logo_blk-01.png?format=1000w" width="150" title="DLC-live" alt="DLC LIVE!" align="right" vspace = "50">


Before training your model, the first step is to assemble your training dataset.

**Create Training Dataset:** Move to the corresponding tab and click **`Create Training Dataset`**. For starters, the default settings will do just fine. While there are more powerful models and data augmentations you might want to consider, you can trust that for most projects the defaults are an ideal place to start.

> 💡 **Note:** This guide assumes you have a GPU on your local machine. If you're CPU-bound and finding training challenging, consider using Google Colab. Our [Colab Guide](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb) can help you get started!

## Kickstarting the Training Process

With your training dataset ready, it's time to train your model.

- **Navigate to Train Network:** Head over to the **`Train Network`** tab.
- **Set Training Parameters:** Here, you'll specify:
  - **`Display iterations/epochs`:** To specify how often the training progress will be visually updated. Note that our TensorFlow models are "iterations" while PyTorch is epochs.
  - **`Maximum Iterations/epochs`:** Decide how many iterations to run. For TensorFlow models for a quick demo, 10K is great. For PyTorch models, 200 epochs is fine!
  - **`Number of Snapshots to keep`:** Choose how many snapshots of the model you want to keep, **`Save iterations`:** and at what iteration intervals they should be saved.
- **Launch Training:** Click on **`Train Network`** to begin.

You can keep an eye on the training progress via your terminal window. This will give you a real-time update on how your model is learning (added bonus of the PyTorch model is it also shows you evaluation metrics after each epoch!).

![DeepLabCut Training in Terminal with TF](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779598041-DC8UJA2NXJXG65ZWJH1O/training-terminal.png?format=500w)

## Evaluate the Network

After training, it's time to see how well your model performs.

### Steps to Evaluate the Network

1. Find and click on the **`Evaluate Network`** tab.
2. **Choose Evaluation Options:**
   - **Plot Predictions:** Select this to visualize the model's predictions, similar to standard DeepLabCut (DLC) evaluations.
   - **Compare Bodyparts:** Opt to compare all the bodyparts for a comprehensive evaluation.
3. Click the **`Evaluate Network`** button, located on the right side of the main window.

>💡 Tip: If you wish to evaluate all saved snapshots, go to the configuration file and change the `snapshotindex` parameter to `all`. 


### Understanding the Evaluation Results

- **Performance Metrics:** DLC will assess the latest snapshot of your model, generating a `.CSV` file with performance 
metrics. This file is stored in the **`evaluation-results`** (for TensorFlow models) or the
**`evaluation-results-pytorch`** (for PyTorch models) folder within your project.


![Combined Evaluation Results in DeepLabCut](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779617667-0RLTM9DVRALN9YIKSHJZ/combined-evaluation-results.png?format=750w))
- **Visual Feedback:** Additionally, DLC creates subfolders containing your frames overlaid with both the labeled bodyparts and the model's predictions, allowing you to visually gauge the network's performance.

![Evaluation Example in DeepLabCut](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779623162-BFDAW37B9TO94EGME2O5/check-labels.png?format=500w))

## Next, head over the beginner guide for [using your new neural network for video analysis](video-analysis)


--- File: docs/beginner-guides/labeling.md ---
(labeling)=
# Labeling GUI

## Selecting Frames to Label

In DeepLabCut, choosing the right frames for labeling is a key step. The trick is always to select the MOST DIVERSE data you can that your model will see. That means good lighting, bad lighting, anything you want to throw at it. So, first, pick a range of diverse videos! Then, we will help you pick frames. You've got two easy ways to do this:

1. **Let DeepLabCut Choose:** DeepLabCut can extract frames automatically for you. It's got two neat ways to do that:
   - **Uniform:** This is like taking a snapshot at regular time intervals.
   - **K-means clustering:** This one applies k-means and picks images from different clusters. This is typically better, as it gives you a variety of actions and poses. Note, as it is a clustering tool, it will miss rare events, so ideally run this step, then perhaps consider running the manual GUI to get some rare frames! You can do both within DLC.

2. **Pick Frames Yourself:** Just like flipping through a photo album, you can go through your video and pick the frames that catch your eye - this is great for finding rare frames. Choose the **`manual`** extraction method.

#### Here's how to get started:

- **Step 1:** Click on **`automatic`** in the frame selection area.
- **Step 2:** Choose **`k-means`** for some variety.
- **Step 3:** Hit the **`Extract Frames`** button, usually found at the bottom right corner.

By default, DeepLabCut will grab 20 frames from each of your videos and put them into sub-folders, per video, under **labeled-data** in your project. Now, you're all set to start labeling!

## Labeling Your First Set of Frames in DeepLabCut

Alright, you've got your extracted frames ready. Now comes the labeling!

### Entering the Label Frames Area

- **Click on `Label Frames`:** This takes you straight to where your frames are, sorted in the **labeled-data** folder, each video in its own sub-folder.
- **Open a Folder:** Click on the first one to start, and then click **`open`**.

### The napari DeepLabCut Labeler

- **Plugin Window Opens:** As soon as you click **`open`**, the napari DeepLabCut plugin window appears, your main stage for labeling.
- **Tutorial Popup:** A quick tutorial window shows up. It's a brief guide, so give it a look to understand the basics.

![Labeling Frames in DeepLabCut using Napari Interface](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779602092-LVR2TI6OADSHEYRCGS6F/labeling-napari.png?format=500w))

### Labeling Setup

- **Frames on Display:** Your frames are lined up in the middle, with a slider below to shuffle through them.
- **Tools and Keypoints:** To the bottom right, you find a list of bodyparts from your configuration. On the top left, all your labeling tools are ready.

### The Labeling Process

- **Start with `Add points`:** Click this to begin placing keypoints on your first frame. If you can't see a bodypart, just move to the next one.
- **Navigate Through Frames:** Use the slider to go from one frame to the next after you're done labeling.
- **Save Progress:** Remember to save your work as you go with **`Command and S`** (or **`Ctrl and S`** on Windows).

> 💡 **Note:** For a detailed walkthrough on using the Napari labeling GUI, have a look at the
[DeepLabCut Napari Guide](napari-gui). Additionally, you can watch our instructional
[YouTube video](https://www.youtube.com/watch?v=hsA9IB5r73E) for more insights and tips.


### Completing the Set

Work through all the frames in the first folder. Then, proceed to the next, continuing this way until each folder in your **labeled-data** directory is done. 

## Checking Your Labels

After you've labeled all your frames, it's important to ensure they're accurate. 

### How to Check Your Labels

- **Return to the Main Window:** Once you're done with labeling, head back to DeepLabCut's main window, and click on **`Check Labels`**. 
- **Review the Labeled Folders:** The system will have created new folders for each labeled set inside your labeled-data folder. These folders contain your original frames overlaid with the keypoints you've added.

![Checking Labels in DeepLabCut](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779615252-6BNW661XB2ULH85RTAD3/evaluation-example.png?format=500w)

Take the time to go through each folder. Accurate labels are key. If there are mistakes, the model might learn incorrectly and mislabel your videos later on. It's all about setting the right foundation for accurate analysis.



--- File: docs/beginner-guides/manage-project.md ---
# Setting up what keypoints to track
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572296495650-Y4ZTJ2XP2Z9XF1AD74VW/ke17ZwdGBToddI8pDm48kMulEJPOrz9Y8HeI7oJuXxR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UZiU3J6AN9rgO1lHw9nGbkYQrCLTag1XBHRgOrY8YAdXW07ycm2Trb21kYhaLJjddA/DLC_logo_blk-01.png?format=1000w" width="150" title="DLC-live" alt="DLC LIVE!" align="right" vspace = "50">

**Edit the Configuration File**

After creating your DeepLabCut project, you'll go to the main GUI window, where you'll start managing your project from the Project Management Tab.

**Accessing the Configuration File**

- **Locate the Configuration File:** At the top of the main window, you'll find the file path to the configuration file.
- **Edit the File:** Click on **`Edit config.yaml`**. This action allows you to:
  - Define the bodyparts you wish to track.
  - Outline the skeleton structure (optional!).

A **`Configuration Editor`** window will open, displaying all the configuration details. You'll need to modify some of these settings to align with your research requirements.

### Steps to Edit the Configuration

#### 1. Defining Bodyparts

- **Locate the Bodyparts Section:** In the Configuration Editor, find the **`bodyparts`** category.
- **Modify the List:** Click on the arrow next to **`bodyparts`** to expand the list. Here, you can:
  - Update the list with the names of the bodyparts relevant to your study.
  - Add more entries by right-clicking on a row number and selecting **`Insert`**.


![Editing Bodyparts in DeepLabCut's Config File](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779624617-CIVZCM23U69NYK9BO3GY/bodyparts.png?format=500w)


#### 2. Defining the Skeleton

- **Navigate to the Skeleton Section:** Scroll down to the **`skeleton`** category.
- **Adjust the Skeleton List:** Click on the arrow to expand this section. You can then:
  - Update the pairs of bodyparts to define the skeleton structure of your model.

![Defining the Skeleton Structure in Config File](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779598505-HQNECHIKSQ6XL033JX8M/skeleton.png?format=500w)

> 💡 **Tip:** If you're new to DeepLabCut, spend some time visualizing how the chosen bodyparts can be connected effectively to form a coherent skeleton.

### Saving Your Changes

- **Save the Configuration:** Once you're satisfied with the modifications, click **`Save`**. This will store your changes and return you to the main GUI window.

## Next, head over the beginner guide for [Labeling your data](labeling)


--- File: docs/beginner-guides/beginners-guide.md ---
(beginners-guide)=
# Using DeepLabCut 
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572296495650-Y4ZTJ2XP2Z9XF1AD74VW/ke17ZwdGBToddI8pDm48kMulEJPOrz9Y8HeI7oJuXxR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UZiU3J6AN9rgO1lHw9nGbkYQrCLTag1XBHRgOrY8YAdXW07ycm2Trb21kYhaLJjddA/DLC_logo_blk-01.png?format=1000w" width="150" title="DLC-live" alt="DLC LIVE!" align="right" vspace = "50">

This guide, and related pages, are meant as a very-new-to-python beginner guide to DeepLabCut. After you are comfortable with this material we recommend then jumping into the more detailed User Guides!

- **ProTip:** For even more 'in-depth' understanding, head over to check out the [DeepLabCut Course](https://deeplabcut.github.io/DeepLabCut/docs/course.html), which provides a deeper dive into the science behind DeepLabCut.

## Installation

Before you begin, make sure that DeepLabCut is installed on your system.

- **ProTip:** For detailed installation instructions, geared towards a bit more advanced users, refer to the [Full Installation Guide](https://deeplabcut.github.io/DeepLabCut/docs/installation.html).

## Beginner User Guide
If you are new to Python, the best way to get Python installed onto your computer is with Anaconda. [Head over here and download the version that is best for your computer](https://www.anaconda.com/download).

- "Conda", as it's often called, it a very nice way to create "environments (env)" on your computer. While there can be some cross-talk, in general, it allows you to separate the different tools you need to use to get your science done 💪. 

## Let's learn a bit and create a DeeplabCut env:

After you have installed Anaconda, open the new program (Anaconda Terminal). You will be in your "root" directory by default. 

**(0) Create a fresh `conda environment`** 

In the terminal, type:

```
conda create -n deeplabcut python=3.10
```
You will be prompted (y/n) to install, and then wait for the magic to happen. At the end, check the terminal, it should prompt you to then type: 

```
conda activate deeplabcut
```
Now, we are going to install the core dependencies. The way this works is that there are "package managers" such as `conda` itself and python's `pip`. We are going to deploy a mix based on what we know works across ooperating systems.

**(1) Install PyTorch**

`PyTorch` is the backend deep-learning language we wrote DLC3 in. To select the right version, head to the [“Install PyTorch”](https://pytorch.org/get-started/locally/) instructions in the official PyTorch Docs. Select your desired PyTorch build, operating system, select conda as your package manager and Python as the language. Select your compute platform (either a CUDA version or CPU only). Then, use the command to install the PyTorch package. Below are a few possible examples:

- **GPU version of pytorch for CUDA 12.4**
```
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124
```
- **CPU only version of pytorch, using the latest version**
```
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
```

**(2) Install DeepLabCut** 

Alright! Next, we will install `Tables` (also called pytables), which is a package to read the HDF5 files that make up the backbone of data management in DeepLabCut, then we will install all the `deeplabcut` source code 🔥. Please decide which version you want (stable or alpha), then type: 

```
conda install -c conda-forge pytables==3.8.0
```

- **Alpha release:**
```
pip install "git+https://github.com/DeepLabCut/DeepLabCut.git@pytorch_dlc#egg=deeplabcut[gui,modelzoo,wandb]"
```
- OR run for the **Stable release:**
```
pip install "deeplabcut[gui,modelzoo,wandb]"
```
- This gives you DeepLabCut, the DLC GUI (gui), our latest neural networks (modelzoo) and a cool data logger (wandb) if you choose to use it later on!

## Starting DeepLabCut

In the terminal, enter:
```bash
python -m deeplabcut
```
This will open the DeepLabCut App (note, the default is dark mode, but you can click "appearance" to change:

![DeepLabCut GUI Screenshot](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779625875-5UHPC367I293CBSP8CT6/GUI-screenshot.png?format=500w)

> 💡 **Note:** For a visual guide on navigating through the DeepLabCut GUI, check out our [YouTube tutorial](https://www.youtube.com/watch?v=tr3npnXWoD4).

## Starting a New Project

### Navigating the GUI on Initial Launch

When you first launch the GUI, you'll find three primary main options:

1. **Create New Project:** Geared towards new initiatives. A good choice if you're here to start something new.
2. **Load Project:** Use this to resume your on-hold or past work.
3. **Model Zoo:** Best suited for those who want to explore Model Zoo.

#### Commencing Your Work:

- For a first-time or new user, please click on **`Start New Project`**.

## 🐾 Steps to Start a New Project

1. **Launch New Project:**
   - When you start a new project, you'll be presented with an empty project window. In DLC3+ you will see a new option "Engine".
   - We recommend using the PyTorch Engine:
  
 ![DeepLabCut Engine](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717780414978-17LOVBUJ8JR102QVSFDY/Screen+Shot+2024-06-07+at+7.13.14+PM.png?format=1500w))

2. **Filling in Project Details:**
   - **Naming Your Project:**
     - Give a specific, well-defined name to your project.
      
      > **💡 Tip:** Avoid empty spaces in your project name.

   - **Naming the Experimenter:**
     - Fill in the name of the experimenter. This part of the data remains immutable.

3. **Determine Project Location:** 
   - By default, your project will be located on the **Desktop**.
   - To pick a different home, modify the path as needed.

4. **Multi-Animal or Single-Animal Project:**
   - Tick the 'Multi-Animal' option in the menu, but only if that's the mode of the project.
   - Choose the 'Number of Cameras' as per your experiment.

5. **Adding Videos:**
   - First, click on **`Browse Videos`** button on the right side of the window, to search for the video contents.
   - Once the media selection tool opens, navigate and select the folder with your videos.
     
     > **💡 Tip:** DeepLabCut supports **`.mp4`**, **`.avi`**, **`.mkv`** and **`.mov`** files.
   - A list will be created with all the videos inside this folder.
   - Unselect the videos you wish to remove from the project.
     
6. **Create your project:**
   - Click on **`Create`** button on the bottom, right side of the main window.
   - A new folder named after your project's name will be created in the location you chose above.
     

### 📽 Video Tutorial: Setting Up Your Project in DeepLabCut

![DeepLabCut Create Project GIF](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779616437-30U5RFYV0OY6ACGDG7F4/create-project.gif?format=500w)

## Next, head over the beginner guide for [Setting up what keypoints to track](https://deeplabcut.github.io/DeepLabCut/docs/manage-project)


--- File: docs/beginner-guides/video-analysis.md ---
# Video Analysis with DeepLabCut
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572296495650-Y4ZTJ2XP2Z9XF1AD74VW/ke17ZwdGBToddI8pDm48kMulEJPOrz9Y8HeI7oJuXxR7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UZiU3J6AN9rgO1lHw9nGbkYQrCLTag1XBHRgOrY8YAdXW07ycm2Trb21kYhaLJjddA/DLC_logo_blk-01.png?format=1000w" width="150" title="DLC-live" alt="DLC LIVE!" align="right" vspace = "50">


After training and evaluating your model, the next step is to apply it to your videos.

**How to Analyze Videos**

1. **Navigate to the 'Analyze Videos' Tab:** Begin applying your trained model to video data here.
2. **Select Your Video Format and Files:**
  - **Choose Video Format:** Pick the format of your video (`.mp4`, `.avi`, `.mkv`, or `.mov`).
  - **Select Videos:** Click **`Select Videos`** to find and open your video file.
3. **Start Analysis:** Click **`Analyze`**. The analysis time depends on video length and resolution. Track progress in the terminal or Anaconda prompt.

### Reviewing Analysis Results

- **Find Results in Your Project Folder:** After analysis, go to your project's video folder.
- **Analysis Files:** Look also for a `.metapickle`, an `.h5`, and possibly a `.csv` file for detailed analysis data.
- **Review the "plot-poses" subfolder:** This contains visual outputs of the video analysis.

![Plot poses](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1717779600836-YOWM5T2MBY0JN1LB537B/plot-poses.png?format=500w)

## Creating a Labeled Video

1. **Go to 'Create Labeled Video' Tab:** The previously analyzed video should be selected.
2. If not already selected, choose your video.
3. Click **`Create Videos`**.

### Viewing the Labeled Video

- Your labeled video will be in your video folder, named after the original video plus model details and 'labeled'.
- Watch the video to assess the model's labeling accuracy.

## Happy DeepLabCutting! 
- Check out the more advanced user guides for even more options!


--- File: deeplabcut/inference_cfg.yaml ---
# Hyperparameters for combinatorics code to assemble individuals.
# Mathis et al. xyz

variant: 0

##############################################################################
#### Assembly of PAFs to individuals
##############################################################################

# Filtering connections:
# Least strength of a paf cost (to be included in assembly)
pafthreshold: 0.1

method: 'm1'

# Assembly:
#least number of connections (to actually form an animal)
minimalnumberofconnections: willbeautomaticallyupdatedbycreate_training_datasetcode
#reasonable default: len(cfg['multianimalbodyparts'])/2
pcutoff: 0.1

# max. number of objects to keep:
topktoretain: willbeautomaticallyupdatedbycreate_training_datasetcode
#reasonable default: len(cfg['individuals'])+1*(len(cfg['uniquebodyparts'])>0)

# Also extract ID:
withid: False

##############################################################################
#### Tracker variables (should be crossvalidated)
##############################################################################
#p/m pixels in width and height for increasing bounding boxes.
boundingboxslack : 0

# Intersection over Union (IoU) threshold for linking two bounding boxes
iou_threshold: .6
# maximum duration of a lost tracklet before it's considered a "new animal" (in frames)
max_age: 1
# minimum number of consecutive frames before a detection is tracked
min_hits: 1


--- File: deeplabcut/version.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

__version__ = "3.0.0rc7"
VERSION = __version__


--- File: deeplabcut/compat.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Compatibility file for methods available with either PyTorch or Tensorflow"""
from __future__ import annotations

from pathlib import Path
from typing import Iterable

import numpy as np
from ruamel.yaml import YAML

import deeplabcut.core.visualization as visualization
from deeplabcut.core.engine import Engine
from deeplabcut.generate_training_dataset.metadata import get_shuffle_engine

DEFAULT_ENGINE = Engine.PYTORCH


def get_project_engine(cfg: dict) -> Engine:
    """
    Args:
        cfg: the project configuration file

    Returns:
        the engine specified for the project, or the default engine if none is specified
    """
    if cfg.get("engine") is not None:
        return Engine(cfg["engine"])

    return DEFAULT_ENGINE


def get_available_aug_methods(engine: Engine) -> tuple[str, ...]:
    """
    Args:
        engine: the engine for which augmentation methods should be returned

    Returns:
        the augmentations available for the given engine, where the first one is the
        default method to use

    Raises:
        RuntimeError: if no augmentations methods are defined for the given engine
    """
    if engine == Engine.TF:
        return "imgaug", "default", "deterministic", "scalecrop", "tensorpack"
    elif engine == Engine.PYTORCH:
        return ("albumentations",)

    raise RuntimeError(f"Unknown augmentation for engine: {engine}")


def train_network(
    config: str | Path,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    max_snapshots_to_keep: int | None = None,
    displayiters: int | None = None,
    saveiters: int | None = None,
    maxiters: int | None = None,
    epochs: int | None = None,
    save_epochs: int | None = None,
    allow_growth: bool = True,
    gputouse: str | None = None,
    autotune: bool = False,
    keepdeconvweights: bool = True,
    modelprefix: str = "",
    superanimal_name: str = "",
    superanimal_transfer_learning: bool = False,
    engine: Engine | None = None,
    device: str | None = None,
    snapshot_path: str | Path | None = None,
    detector_path: str | Path | None = None,
    batch_size: int | None = None,
    detector_batch_size: int | None = None,
    detector_epochs: int | None = None,
    detector_save_epochs: int | None = None,
    pose_threshold: float | None = 0.1,
    pytorch_cfg_updates: dict | None = None,
):
    """
    Trains the network with the labels in the training dataset.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: int, optional, default=1
        Integer value specifying the shuffle index to select for training.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    max_snapshots_to_keep: int or None
        Sets how many snapshots are kept, i.e. states of the trained network. Every
        saving iteration many times a snapshot is stored, however only the last
        ``max_snapshots_to_keep`` many are kept! If you change this to None, then all
        are kept.
        See: https://github.com/DeepLabCut/DeepLabCut/issues/8#issuecomment-387404835

    displayiters: optional, default=None
        This variable is actually set in ``pose_config.yaml``. However, you can
        overwrite it with this hack. Don't use this regularly, just if you are too lazy
        to dig out the ``pose_config.yaml`` file for the corresponding project. If
        ``None``, the value from there is used, otherwise it is overwritten!

    saveiters: optional, default=None
        Only for the TensorFlow engine (for the PyTorch engine see the ``torch_kwargs``:
        you can use ``save_epochs``).
        This variable is actually set in ``pose_config.yaml``. However, you can
        overwrite it with this hack. Don't use this regularly, just if you are too lazy
        to dig out the ``pose_config.yaml`` file for the corresponding project.
        If ``None``, the value from there is used, otherwise it is overwritten!

    maxiters: optional, default=None
        Only for the TensorFlow engine (for the PyTorch engine see the ``torch_kwargs``:
        you can use ``epochs``).
        This variable is actually set in ``pose_config.yaml``. However, you can
        overwrite it with this hack. Don't use this regularly, just if you are too lazy
        to dig out the ``pose_config.yaml`` file for the corresponding project.
        If ``None``, the value from there is used, otherwise it is overwritten!

    epochs: optional, default=None
        Only for the PyTorch engine (equivalent to the `maxiters` parameter for the
        TensorFlow engine). The maximum number of epochs to train the model for. If
        None, the value will be read from the `pytorch_config.yaml` file. An epoch is a
        single pass through the training dataset, which means your model has seen each
        training image exactly once. So if you have 64 training images for your network,
        an epoch is 64 iterations with batch size 1 (or 32 iterations with batch size 2,
        16 with batch size 4, etc.).

    save_epochs: optional, default=None
        Only for the PyTorch engine (equivalent to the `saveiters` parameter for the
        TensorFlow engine). The number of epochs between each snapshot save. If
        None, the value will be read from the `pytorch_config.yaml` file.

    allow_growth: bool, optional, default=True.
        Only for the TensorFlow engine.
        For some smaller GPUs the memory issues happen. If ``True``, the memory
        allocator does not pre-allocate the entire specified GPU memory region, instead
        starting small and growing as needed.
        See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2

    gputouse: optional, default=None
        Only for the TensorFlow engine (for the PyTorch engine see the ``torch_kwargs``:
        you can use ``device``).
        Natural number indicating the number of your GPU (see number in nvidia-smi).
        If you do not have a GPU put None.
        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    autotune: bool, optional, default=False
        Only for the TensorFlow engine.
        Property of TensorFlow, somehow faster if ``False``
        (as Eldar found out, see https://github.com/tensorflow/tensorflow/issues/13317).

    keepdeconvweights: bool, optional, default=True
        Also restores the weights of the deconvolution layers (and the backbone) when
        training from a snapshot. Note that if you change the number of bodyparts, you
        need to set this to false for re-training.

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    superanimal_name: str, optional, default =""
        Only for the TensorFlow engine. For the PyTorch engine, you need to specify
        this through the ``weight_init`` when creating the training dataset.
        Specified if transfer learning with superanimal is desired

    superanimal_transfer_learning: bool, optional, default = False.
        Only for the TensorFlow engine. For the PyTorch engine, you need to specify
        this through the ``weight_init`` when creating the training dataset.
        If set true, the training is transfer learning (new decoding layer). If set
        false, and superanimal_name is True, then the training is fine-tuning (reusing
        the decoding layer)

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    device: str, optional, default = None.
        Only for the PyTorch engine. The device to run the training on (e.g. "cuda:0")

    snapshot_path: str or Path, optional, default = None.
        Only for the PyTorch engine. The path to the pose model snapshot to resume training from.

    detector_path: str or Path, optional, default = None.
        Only for the PyTorch engine. The path to the detector model snapshot to resume training from.

    batch_size: int, optional, default = None.
        Only for the PyTorch engine. The batch size to use while training.

    detector_batch_size: int, optional, default = None.
        Only for the PyTorch engine. The batch size to use while training the detector.

    detector_epochs: int, optional, default = None.
        Only for the PyTorch engine. The number of epochs to train the detector for.

    detector_save_epochs: int, optional, default = None.
        Only for the PyTorch engine. The number of epochs between each detector snapshot save.

    pose_threshold: float, optional, default = 0.1.
        Only for the PyTorch engine. Used for memory-replay. Pseudo-predictions with confidence lower
            than this threshold are discarded for memory-replay

    pytorch_cfg_updates: dict, optional, default = None.
        A dictionary of updates to the pytorch config. The keys are the dot-separated
        paths to the values to update in the config.
        For example, to update the gpus to run the training on, you can use:
        ```
        pytorch_cfg_updates={"runner.gpus": [0,1,2,3]}
        ```

    Returns
    -------
    None

    Examples
    --------
    To train the network for first shuffle of the training dataset

    >>> deeplabcut.train_network('/analysis/project/reaching-task/config.yaml')

    To train the network for second shuffle of the training dataset

    >>> deeplabcut.train_network(
            '/analysis/project/reaching-task/config.yaml',
            shuffle=2,
            keepdeconvweights=True,
        )

    To train the network for shuffle created with a PyTorch engine, while overriding the
    number of epochs, batch size and other parameters.

    >>> deeplabcut.train_network(
            '/analysis/project/reaching-task/config.yaml',
            shuffle=1,
            batch_size=8,
            epochs=100,
            save_epochs=10,
            display_iters=50,
        )
    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(config),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import train_network

        if max_snapshots_to_keep is None:
            max_snapshots_to_keep = 5

        return train_network(
            str(config),
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            max_snapshots_to_keep=max_snapshots_to_keep,
            displayiters=displayiters,
            saveiters=saveiters,
            maxiters=maxiters,
            allow_growth=allow_growth,
            gputouse=gputouse,
            autotune=autotune,
            keepdeconvweights=keepdeconvweights,
            superanimal_name=superanimal_name,
            superanimal_transfer_learning=superanimal_transfer_learning,
            modelprefix=modelprefix,
        )
    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch.apis import train_network

        return train_network(
            config,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            modelprefix=modelprefix,
            device=device,
            snapshot_path=snapshot_path,
            detector_path=detector_path,
            load_head_weights=keepdeconvweights,
            batch_size=batch_size,
            epochs=epochs,
            save_epochs=save_epochs,
            detector_batch_size=detector_batch_size,
            detector_epochs=detector_epochs,
            detector_save_epochs=detector_save_epochs,
            display_iters=displayiters,
            max_snapshots_to_keep=max_snapshots_to_keep,
            pose_threshold=pose_threshold,
            pytorch_cfg_updates=pytorch_cfg_updates,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def return_train_network_path(
    config,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    modelprefix: str = "",
    engine: Engine | None = None,
) -> tuple[Path, Path, Path]:
    """
    Returns the training and test pose config file names as well as the folder where the
    snapshot is

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: int
        Integer value specifying the shuffle index to select for training.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note
        that TrainingFraction is a list in config.yaml).

    modelprefix: str, optional
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    Returns the triple: trainposeconfigfile, testposeconfigfile, snapshotfolder
    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(config),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import return_train_network_path

        return return_train_network_path(
            config,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            modelprefix=modelprefix,
        )
    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch.apis.utils import (
            return_train_network_path,
        )

        return return_train_network_path(
            config,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            modelprefix=modelprefix,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def evaluate_network(
    config: str | Path,
    Shuffles: Iterable[int] = (1,),
    trainingsetindex: int | str = 0,
    plotting: bool | str = False,
    show_errors: bool = True,
    comparisonbodyparts: str | list[str] = "all",
    gputouse: str | None = None,
    rescale: bool = False,
    modelprefix: str = "",
    per_keypoint_evaluation: bool = False,
    snapshots_to_evaluate: list[str] | None = None,
    pcutoff: float | list[float] | dict[str, float] | None = None,
    engine: Engine | None = None,
    **torch_kwargs,
):
    """Evaluates the network.

    Evaluates the network based on the saved models at different stages of the training
    network. The evaluation results are stored in the .h5 and .csv file under the
    subdirectory 'evaluation_results'. Change the snapshotindex parameter in the config
    file to 'all' in order to evaluate all the saved models.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file.

    Shuffles: list, optional, default=[1]
        List of integers specifying the shuffle indices of the training dataset.

    trainingsetindex: int or str, optional, default=0
        Integer specifying which "TrainingsetFraction" to use.
        Note that "TrainingFraction" is a list in config.yaml. This variable can also
        be set to "all".

    plotting: bool or str, optional, default=False
        Plots the predictions on the train and test images.
        If provided it must be either ``True``, ``False``, ``"bodypart"``, or
        ``"individual"``. Setting to ``True`` defaults as ``"bodypart"`` for
        multi-animal projects.
        If a detector is used, the predicted bounding boxes will also be plotted.

    show_errors: bool, optional, default=True
        Display train and test errors.

    comparisonbodyparts: str or list, optional, default="all"
        The average error will be computed for those body parts only.
        The provided list has to be a subset of the defined body parts.

    gputouse: int or None, optional, default=None
        Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a
        GPU put `None``.
        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    rescale: bool, optional, default=False
        Evaluate the model at the ``'global_scale'`` variable (as set in the
        ``pose_config.yaml`` file for a particular project). I.e. every image will be
        resized according to that scale and prediction will be compared to the resized
        ground truth. The error will be reported in pixels at rescaled to the
        *original* size. I.e. For a [200,200] pixel image evaluated at
        ``global_scale=.5``, the predictions are calculated on [100,100] pixel images,
        compared to 1/2*ground truth and this error is then multiplied by 2!.
        The evaluation images are also shown for the original size!

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    per_keypoint_evaluation: bool, default=False
        Compute the train and test RMSE for each keypoint, and save the results to
        a {model_name}-keypoint-results.csv in the evalution-results folder

    snapshots_to_evaluate: List[str], optional, default=None
        List of snapshot names to evaluate (e.g. ["snapshot-5000", "snapshot-7500"]).

    pcutoff: float | list[float] | dict[str, float] | None, default=None
        Only for the PyTorch engine. For the TensorFlow engine, please set the pcutoff
        in the `config.yaml` file.
        The cutoff to use for computing evaluation metrics. When `None` (default), the
        cutoff will be loaded from the project config. If a list is provided, there
        should be one value for each bodypart and one value for each unique bodypart
        (if there are any). If a dict is provided, the keys should be bodyparts
        mapping to pcutoff values for each bodypart. Bodyparts that are not defined
        in the dict will have pcutoff set to 0.6.

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    torch_kwargs:
        You can add any keyword arguments for the deeplabcut.pose_estimation_pytorch
        evaluate_network function here. These arguments are passed to the downstream
        function. Available parameters are `snapshotindex`, which overrides the
        `snapshotindex` parameter in the project configuration file. For top-down models
        the `detector_snapshot_index` parameter can override the index of the detector
        to use for evaluation in the project configuration file.

    Returns
    -------
    None

    Examples
    --------
    If you do not want to plot and evaluate with shuffle set to 1.

    >>> deeplabcut.evaluate_network(
            '/analysis/project/reaching-task/config.yaml', Shuffles=[1],
        )

    If you want to plot and evaluate with shuffle set to 0 and 1.

    >>> deeplabcut.evaluate_network(
            '/analysis/project/reaching-task/config.yaml',
            Shuffles=[0, 1],
            plotting=True,
        )

    If you want to plot assemblies for a maDLC project

    >>> deeplabcut.evaluate_network(
            '/analysis/project/reaching-task/config.yaml',
            Shuffles=[1],
            plotting="individual",
        )

    If you have a PyTorch model for which you want to set a different p-cutoff for
    "left_ear" and "right_ear" bodyparts, and keep the one set in the project config
    for other bodyparts:

    >>> deeplabcut.evaluate_network(
    >>>     "/analysis/project/reaching-task/config.yaml",
    >>>     Shuffles=[0, 1],
    >>>     pcutoff={"left_ear": 0.8, "right_ear": 0.8},
    >>> )

    Note: This defaults to standard plotting for single-animal projects.
    """
    if engine is None:
        cfg = _load_config(config)
        engines = set()
        for shuffle in Shuffles:
            engines.add(
                get_shuffle_engine(
                    cfg,
                    trainingsetindex=trainingsetindex,
                    shuffle=shuffle,
                    modelprefix=modelprefix,
                )
            )
        if len(engines) == 0:
            raise ValueError(
                f"You must pass at least one shuffle to evaluate (had {list(Shuffles)})"
            )
        elif len(engines) > 1:
            raise ValueError(
                f"All shuffles must have the same engine (found {list(engines)})"
            )
        engine = engines.pop()

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import evaluate_network

        return evaluate_network(
            str(config),
            Shuffles=Shuffles,
            trainingsetindex=trainingsetindex,
            plotting=plotting,
            show_errors=show_errors,
            comparisonbodyparts=comparisonbodyparts,
            gputouse=gputouse,
            rescale=rescale,
            modelprefix=modelprefix,
            per_keypoint_evaluation=per_keypoint_evaluation,
            snapshots_to_evaluate=snapshots_to_evaluate,
        )
    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch.apis import evaluate_network

        _update_device(gputouse, torch_kwargs)
        return evaluate_network(
            config,
            shuffles=Shuffles,
            trainingsetindex=trainingsetindex,
            plotting=plotting,
            show_errors=show_errors,
            comparison_bodyparts=comparisonbodyparts,
            snapshots_to_evaluate=snapshots_to_evaluate,
            per_keypoint_evaluation=per_keypoint_evaluation,
            modelprefix=modelprefix,
            pcutoff=pcutoff,
            **torch_kwargs,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def return_evaluate_network_data(
    config: str,
    shuffle: int = 0,
    trainingsetindex: int = 0,
    comparisonbodyparts: str | list[str] = "all",
    Snapindex: str | int | None = None,
    rescale: bool = False,
    fulldata: bool = False,
    show_errors: bool = True,
    modelprefix: str = "",
    returnjustfns: bool = True,
    engine: Engine | None = None,
):
    """
    Returns the results for (previously evaluated) network. deeplabcut.evaluate_network(..)
    Returns list of (per model): [trainingsiterations,trainfraction,shuffle,trainerror,testerror,pcutoff,trainerrorpcutoff,testerrorpcutoff,Snapshots[snapindex],scale,net_type]

    This function is only implemented for tensorflow models/shuffles, and will throw
    an error if called with a PyTorch shuffle.

    If fulldata=True, also returns (the complete annotation and prediction array)
    Returns list of: (DataMachine, Data, data, trainIndices, testIndices, trainFraction, DLCscorer,comparisonbodyparts, cfg, Snapshots[snapindex])
    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 0.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to "all".

    comparisonbodyparts: list of bodyparts, Default is "all".
        The average error will be computed for those body parts only (Has to be a subset of the body parts).

    rescale: bool, default False
        Evaluate the model at the 'global_scale' variable (as set in the test/pose_config.yaml file for a particular project). I.e. every
        image will be resized according to that scale and prediction will be compared to the resized ground truth. The error will be reported
        in pixels at rescaled to the *original* size. I.e. For a [200,200] pixel image evaluated at global_scale=.5, the predictions are calculated
        on [100,100] pixel images, compared to 1/2*ground truth and this error is then multiplied by 2!. The evaluation images are also shown for the
        original size!

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    Examples
    --------
    If you do not want to plot
    >>> deeplabcut._evaluate_network_data('/analysis/project/reaching-task/config.yaml', shuffle=[1])
    --------
    If you want to plot
    >>> deeplabcut.evaluate_network('/analysis/project/reaching-task/config.yaml',shuffle=[1],plotting=True)
    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(config),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import return_evaluate_network_data

        return return_evaluate_network_data(
            config,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            comparisonbodyparts=comparisonbodyparts,
            Snapindex=Snapindex,
            rescale=rescale,
            fulldata=fulldata,
            show_errors=show_errors,
            modelprefix=modelprefix,
            returnjustfns=returnjustfns,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def analyze_videos(
    config: str,
    videos: list[str],
    videotype: str = "",
    shuffle: int = 1,
    trainingsetindex: int = 0,
    gputouse: str | None = None,
    save_as_csv: bool = False,
    in_random_order: bool = True,
    destfolder: str | None = None,
    batchsize: int = None,
    cropping: list[int] | None = None,
    TFGPUinference: bool = True,
    dynamic: tuple[bool, float, int] = (False, 0.5, 10),
    modelprefix: str = "",
    robust_nframes: bool = False,
    allow_growth: bool = False,
    use_shelve: bool = False,
    auto_track: bool = True,
    n_tracks: int | None = None,
    animal_names: list[str] | None = None,
    calibrate: bool = False,
    identity_only: bool = False,
    use_openvino: str | None = None,
    engine: Engine | None = None,
    **torch_kwargs,
):
    """Makes prediction based on a trained network.

    The index of the trained network is specified by parameters in the config file
    (in particular the variable 'snapshotindex').

    The labels are stored as MultiIndex Pandas Array, which contains the name of
    the network, body part name, (x, y) label position in pixels, and the
    likelihood for each frame per body part. These arrays are stored in an
    efficient Hierarchical Data Format (HDF) in the same directory where the video
    is stored. However, if the flag save_as_csv is set to True, the data can also
    be exported in comma-separated values format (.csv), which in turn can be
    imported in many programs, such as MATLAB, R, Prism, etc.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos: list[str]
        A list of strings containing the full paths to videos for analysis or a path to
        the directory, where all the videos with same extension are stored.

    videotype: str, optional, default=""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed. If left unspecified,
        videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional, default=1
        An integer specifying the shuffle index of the training dataset used for
        training the network.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        By default the first (note that TrainingFraction is a list in config.yaml).

    gputouse: int or None, optional, default=None
        Only for the TensorFlow engine (for the PyTorch engine see the ``torch_kwargs``:
        you can use ``device``).
        Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a
        GPU put ``None``.
        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    save_as_csv: bool, optional, default=False
        Saves the predictions in a .csv file.

    in_random_order: bool, optional (default=True)
        Whether or not to analyze videos in a random order.
        This is only relevant when specifying a video directory in `videos`.

    destfolder: string or None, optional, default=None
        Specifies the destination folder for analysis data. If ``None``, the path of
        the video is used. Note that for subsequent analysis this folder also needs to
        be passed.

    batchsize: int or None, optional, default=None
        Currently not supported by the PyTorch engine.
        Change batch size for inference; if given overwrites value in ``pose_cfg.yaml``.

    cropping: list or None, optional, default=None
        Currently not supported by the PyTorch engine.
        List of cropping coordinates as [x1, x2, y1, y2].
        Note that the same cropping parameters will then be used for all videos.
        If different video crops are desired, run ``analyze_videos`` on individual
        videos with the corresponding cropping coordinates.

    TFGPUinference: bool, optional, default=True
        Only for the TensorFlow engine.
        Perform inference on GPU with TensorFlow code. Introduced in "Pretraining
        boosts out-of-domain robustness for pose estimation" by Alexander Mathis,
        Mert Yüksekgönül, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis.
        Source: https://arxiv.org/abs/1909.11229

    dynamic: tuple(bool, float, int) triple containing (state, det_threshold, margin)
        If the state is true, then dynamic cropping will be performed. That means that
        if an object is detected (i.e. any body part > detectiontreshold), then object
        boundaries are computed according to the smallest/largest x position and
        smallest/largest y position of all body parts. This  window is expanded by the
        margin and from then on only the posture within this crop is analyzed (until the
        object is lost, i.e. <detectiontreshold). The current position is utilized for
        updating the crop window for the next frame (this is why the margin is important
        and should be set large enough given the movement of the animal).

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    robust_nframes: bool, optional, default=False
        Evaluate a video's number of frames in a robust manner.
        This option is slower (as the whole video is read frame-by-frame),
        but does not rely on metadata, hence its robustness against file corruption.

    allow_growth: bool, optional, default=False.
        Only for the TensorFlow engine.
        For some smaller GPUs the memory issues happen. If ``True``, the memory
        allocator does not pre-allocate the entire specified GPU memory region, instead
        starting small and growing as needed.
        See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2

    use_shelve: bool, optional, default=False
        By default, data are dumped in a pickle file at the end of the video analysis.
        Otherwise, data are written to disk on the fly using a "shelf"; i.e., a
        pickle-based, persistent, database-like object by default, resulting in
        constant memory footprint.

    The following parameters are only relevant for multi-animal projects:

    auto_track: bool, optional, default=True
        By default, tracking and stitching are automatically performed, producing the
        final h5 data file. This is equivalent to the behavior for single-animal
        projects.

        If ``False``, one must run ``convert_detections2tracklets`` and
        ``stitch_tracklets`` afterwards, in order to obtain the h5 file.

    This function has 3 related sub-calls:

    identity_only: bool, optional, default=False
        If ``True`` and animal identity was learned by the model, assembly and tracking
        rely exclusively on identity prediction.

    calibrate: bool, optional, default=False
        If ``True``, use training data to calibrate the animal assembly procedure. This
        improves its robustness to wrong body part links, but requires very little
        missing data.

    n_tracks: int or None, optional, default=None
        Number of tracks to reconstruct. By default, taken as the number of individuals
        defined in the config.yaml. Another number can be passed if the number of
        animals in the video is different from the number of animals the model was
        trained on.

    animal_names: list[str], optional
        If you want the names given to individuals in the labeled data file, you can
        specify those names as a list here. If given and `n_tracks` is None, `n_tracks`
        will be set to `len(animal_names)`. If `n_tracks` is not None, then it must be
        equal to `len(animal_names)`. If it is not given, then `animal_names` will
        be loaded from the `individuals` in the project config.yaml file.

    use_openvino: str, optional
        Only for the TensorFlow engine.
        Use "CPU" for inference if OpenVINO is available in the Python environment.

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    torch_kwargs:
        Any extra parameters to pass to the PyTorch API, such as ``device`` which can
        be used to specify the CUDA device to use for training.

    Returns
    -------
    DLCScorer: str
        the scorer used to analyze the videos

    Examples
    --------

    Analyzing a single video on Windows

    >>> deeplabcut.analyze_videos(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi'],
        )

    Analyzing a single video on Linux/MacOS

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/reachingvideo1.avi'],
        )

    Analyze all videos of type ``avi`` in a folder

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos'],
            videotype='.avi',
        )

    Analyze multiple videos

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
        )

    Analyze multiple videos with ``shuffle=2``

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
            shuffle=2,
        )

    Analyze multiple videos with ``shuffle=2``, save results as an additional csv file

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
            shuffle=2,
            save_as_csv=True,
        )
    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(config),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import analyze_videos

        kwargs = {}
        if use_openvino is not None:  # otherwise default comes from tensorflow API
            kwargs["use_openvino"] = use_openvino

        return analyze_videos(
            config,
            videos,
            videotype=videotype,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            gputouse=gputouse,
            save_as_csv=save_as_csv,
            in_random_order=in_random_order,
            destfolder=destfolder,
            batchsize=batchsize,
            cropping=cropping,
            TFGPUinference=TFGPUinference,
            dynamic=dynamic,
            modelprefix=modelprefix,
            robust_nframes=robust_nframes,
            allow_growth=allow_growth,
            use_shelve=use_shelve,
            auto_track=auto_track,
            n_tracks=n_tracks,
            animal_names=animal_names,
            calibrate=calibrate,
            identity_only=identity_only,
            **kwargs,
        )
    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch.apis import analyze_videos

        _update_device(gputouse, torch_kwargs)

        if batchsize is not None:
            if "batch_size" in torch_kwargs:
                print(
                    f"You called analyze_videos with parameters ``batchsize={batchsize}"
                    f"`` and batch_size={torch_kwargs['batch_size']}. Only one is "
                    f"needed/used. Using batch size {torch_kwargs['batch_size']}"
                )
            else:
                torch_kwargs["batch_size"] = batchsize

        return analyze_videos(
            config,
            videos=videos,
            videotype=videotype,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            save_as_csv=save_as_csv,
            in_random_order=in_random_order,
            destfolder=destfolder,
            dynamic=dynamic,
            modelprefix=modelprefix,
            use_shelve=use_shelve,
            robust_nframes=robust_nframes,
            auto_track=auto_track,
            n_tracks=n_tracks,
            animal_names=animal_names,
            calibrate=calibrate,
            identity_only=identity_only,
            overwrite=False,
            cropping=cropping,
            **torch_kwargs,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def create_tracking_dataset(
    config: str,
    videos: list[str],
    track_method: str,
    videotype: str = "",
    shuffle: int = 1,
    trainingsetindex: int = 0,
    gputouse: int | None = None,
    destfolder: str | None = None,
    batchsize: int | None = None,
    cropping: list[int] | None = None,
    TFGPUinference: bool = True,
    modelprefix: str = "",
    robust_nframes: bool = False,
    n_triplets: int = 1000,
    engine: Engine | None = None,
) -> str:
    """Creates a tracking dataset to train a ReID tracklet stitcher.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos: list[str]
        A list of strings containing the full paths to videos from which to create a
        tracking dataset, or a path to the directory where all the videos with same
        extension are stored.

    track_method: str
        Specifies the tracker used to generate the pose estimation data. Must be either
        'box', 'skeleton', or 'ellipse'.

    videotype: str, optional, default=""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed. If left unspecified,
        videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional, default=1
        An integer specifying the shuffle index of the training dataset used for
        training the network.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        By default the first (note that TrainingFraction is a list in config.yaml).

    gputouse: int or None, optional, default=None
        Only for the TensorFlow engine (for the PyTorch engine use ``device``).
        Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a
        GPU put ``None``. See:
            https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    TFGPUinference: bool, optional, default=True
        Only for the TensorFlow engine.
        Perform inference on GPU with TensorFlow code. Introduced in "Pretraining
        boosts out-of-domain robustness for pose estimation" by Alexander Mathis,
        Mert Yüksekgönül, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis.
        Source: https://arxiv.org/abs/1909.11229

    destfolder:
        Specifies the destination folder for analysis data. If ``None``, the path of
        the video is used. Note that for subsequent analysis this folder also needs to
        be passed.

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    robust_nframes: bool, optional, default=False
        Evaluate a video's number of frames in a robust manner.
        This option is slower (as the whole video is read frame-by-frame),
        but does not rely on metadata, hence its robustness against file corruption.

    n_triplets: int, default=1000
        The number of triplets to extract for the dataset.

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    Returns
    -------
    DLCScorer: str
        the scorer used to analyze the videos
    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(config),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import create_tracking_dataset

        return create_tracking_dataset(
            config,
            videos,
            track_method,
            videotype=videotype,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            gputouse=gputouse,
            destfolder=destfolder,
            batchsize=batchsize,
            cropping=cropping,
            TFGPUinference=TFGPUinference,
            modelprefix=modelprefix,
            robust_nframes=robust_nframes,
            n_triplets=n_triplets,
        )
    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch.apis import create_tracking_dataset
        return create_tracking_dataset(
            config,
            videos,
            track_method,
            videotype=videotype,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            destfolder=destfolder,
            batch_size=batchsize,
            cropping=cropping,
            modelprefix=modelprefix,
            robust_nframes=robust_nframes,
            n_triplets=n_triplets,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def analyze_images(
    config: str | Path,
    images: str | Path | list[str] | list[Path],
    frame_type: str | None = None,
    destfolder: str | Path | None = None,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    max_individuals: int | None = None,
    device: str | None = None,
    snapshot_index: int | None = None,
    detector_snapshot_index: int | None = None,
    save_as_csv: bool = False,
    modelprefix: str = "",
    plotting: bool | str = False,
    pcutoff: float | None = None,
    bbox_pcutoff: float | None = None,
    plot_skeleton: bool = False,
) -> dict[str, dict[str, np.ndarray | np.ndarray]]:
    """Analyzes images with a DeepLabCut model and stores the output in an H5 file.

    This method is only implemented for PyTorch models.

    The labels are stored as Pandas DataFrame, which contains the name of the network,
    body part name, (x, y) label position in pixels, and the likelihood for each frame
    per body part.

    Parameters
    ----------
    config : str, Path
        Full path of the project's config.yaml file.

    images: str, Path, list[str], list[Path]
        The image(s) to run inference on. Can be the path to an image, the path
        to a directory containing images, or a list of image paths or directories
        containing images.

    frame_type: string, optional
        Filters the images to analyze to only the ones with the given suffix (e.g.
        setting `frame_type`=".png" will only analyze ".png" images). The default
        behavior analyzes all ".jpg", ".jpeg" and ".png" images.

    destfolder: str, Path, optional
        The directory where the predictions will be stored. If None, the predictions
        will be stored in the same directory as the first image given in the `images`
        argument (if it's a directory, that directory will be used; if it's an image,
        the directory containing the image will be used).

    shuffle: int, optional
        An integer specifying the shuffle with which to run image analysis.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default, the first one
        is used (note that TrainingFraction is a list in config.yaml).

    max_individuals: int, optional
        The maximum number of individuals to detect in each image. Set to the number of
        individuals in the project if None.

    device: str, optional
        The CUDA device to use for training. If None, the device will be taken from the
        ``pytorch_config.yaml`` file. Examples: {"cpu", "cuda", "cuda:0", "cuda:1"}. For
        more information, see https://pytorch.org/docs/stable/notes/cuda.html

    snapshot_index: int, optional
        Index (starting at 0) of the snapshot to use for image analysis. To evaluate the
        last one, use -1. Default uses the value set in the project config.

    detector_snapshot_index: int, optional
        Only for Top-Down PyTorch models. If defined, uses the detector with the given
        index for pose estimation. To evaluate the last one, use -1. Default uses the
        value set in the project config.

    save_as_csv: bool, optional
        Saves the predictions in a .csv file. The default is ``False``; if provided it
        must be either ``True`` or ``False``.

    modelprefix: str, optional
        Directory containing the deeplabcut models to use when running image analysis.
        By default, the models are assumed to exist in the project folder.

    plotting: bool, str, default=False
        Plots the predictions made by the model on the analyzed images. Results will be
        stored in a folder named `LabeledImages_{scorer}`, where scorer is the name
        of the model used to analyze the images. This folder will be in the same
        directory as the file containing the predictions (either the given `destfolder`,
        or the folder containing the first image to analyze).

        If provided it must be either ``True``, ``False``, ``"bodypart"``, or
        ``"individual"``. Setting to ``True`` defaults as ``"bodypart"`` for
        multi-animal projects. If a detector is used, the predicted bounding boxes
        will also be plotted.

    pcutoff: float, optional, default=None
        The cutoff score when plotting pose predictions. Must be None or in
        (0, 1). If None, the pcutoff is read from the project configuration file.

    bbox_pcutoff: float, optional, default=None
        The cutoff score when plotting bounding box predictions. Must be
        None or in (0, 1). If None, it is read from the project configuration file.

    plot_skeleton: bool, default=False
        If a skeleton is defined in the project's config.yaml, whether
        to plot the skeleton connecting the predicted bodyparts on the images.

    Returns
    -------
        A dictionary mapping image paths (as strings) to model predictions.

    Examples
    --------
    If you want to analyze all frames in /analysis/project/my_images
        >>> import deeplabcut
        >>> deeplabcut.analyze_images(
        >>>     "/analysis/project/reaching-task/config.yaml",
        >>>     "/analysis/project/my_images",
        >>> )
        >>>

    If you want to analyze two specific images with your shuffle 3 model:
        >>> import deeplabcut
        >>> deeplabcut.analyze_images(
        >>>     "/analysis/project/reaching-task/config.yaml",
        >>>     images=["image_001.png", "img_002.jpg"],
        >>>     shuffle=3,
        >>> )
        >>>

    If you want to analyze frames in a folder, save them and plot predictions:
        >>> import deeplabcut
        >>> deeplabcut.analyze_images(
        >>>     "/analysis/project/reaching-task/config.yaml",
        >>>     "/analysis/project/my_images",
        >>>     shuffle=3,
        >>>     destfolder="/analysis/project/my_images_analyzed",
        >>>     plotting=True,
        >>> )
        >>>
    --------
    """
    engine = get_shuffle_engine(
        _load_config(config),
        trainingsetindex=trainingsetindex,
        shuffle=shuffle,
        modelprefix=modelprefix,
    )

    if engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch import analyze_images

        return analyze_images(
            config=config,
            images=images,
            frame_type=frame_type,
            output_dir=destfolder,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            snapshot_index=snapshot_index,
            detector_snapshot_index=detector_snapshot_index,
            modelprefix=modelprefix,
            device=device,
            save_as_csv=save_as_csv,
            max_individuals=max_individuals,
            plotting=plotting,
            pcutoff=pcutoff,
            bbox_pcutoff=bbox_pcutoff,
            plot_skeleton=plot_skeleton,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def analyze_time_lapse_frames(
    config: str,
    directory: str,
    frametype: str = ".png",
    shuffle: int = 1,
    trainingsetindex: int = 0,
    gputouse: int | None = None,
    device: str | None = None,
    save_as_csv: bool = False,
    modelprefix: str = "",
    engine: Engine | None = None,
):
    """
    Analyzed all images (of type = frametype) in a folder and stores the output in one file.

    You can crop the frames (before analysis), by changing 'cropping'=True and setting
    'x1','x2','y1','y2' in the config file.

    Output: The labels are stored as MultiIndex Pandas Array, which contains the name
    of the network, body part name, (x, y) label position in pixels, and the likelihood
    for each frame per body part. These arrays are stored in an efficient Hierarchical
    Data Format (HDF) in the same directory, where the video is stored. However, if the
    flag save_as_csv is set to True, the data can also be exported in comma-separated
    values format (.csv), which in turn can be imported in many programs, such as
    MATLAB, R, Prism, etc.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    directory: string
        Full path to directory containing the frames that shall be analyzed

    frametype: string, optional
        Checks for the file extension of the frames. Only images with this extension are
        analyzed. The default is ``.png``

    shuffle: int, optional
        An integer specifying the shuffle index of the training dataset used for
        training the network. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note
        that TrainingFraction is a list in config.yaml).

    gputouse: int, optional.
        Only for TensorFlow models. For PyTorch models, please use `device`. Natural
        number indicating the number of your GPU (see number in nvidia-smi). If you do
        not have a GPU put None. See:
            https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    device: str, optional
        The CUDA device to use for training. If None, the device will be taken from the
        ``pytorch_config.yaml`` file. Examples: {"cpu", "cuda", "cuda:0", "cuda:1"}. For
        more information, see https://pytorch.org/docs/stable/notes/cuda.html

    save_as_csv: bool, optional
        Saves the predictions in a .csv file. The default is ``False``; if provided if
        must be either ``True`` or ``False``

    Examples
    --------
    If you want to analyze all frames in /analysis/project/timelapseexperiment1
    >>> import deeplabcut
    >>> deeplabcut.analyze_time_lapse_frames(
    >>>     '/analysis/project/reaching-task/config.yaml',
    >>>     '/analysis/project/timelapseexperiment1'
    >>> )

    --------

    Note: for test purposes one can extract all frames from a video with ffmeg, e.g.
    >>> ffmpeg -i testvideo.avi "thumb%04d.png"

    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(config),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import analyze_time_lapse_frames

        return analyze_time_lapse_frames(
            config,
            directory,
            frametype=frametype,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            gputouse=gputouse,
            save_as_csv=save_as_csv,
            modelprefix=modelprefix,
        )
    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch import analyze_images

        return analyze_images(
            config=config,
            images=directory,
            output_dir=directory,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            device=_gpu_to_use_to_device(gputouse, device),
            save_as_csv=save_as_csv,
            modelprefix=modelprefix,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def convert_detections2tracklets(
    config: str,
    videos: list[str],
    videotype: str = "",
    shuffle: int = 1,
    trainingsetindex: int = 0,
    overwrite: bool = False,
    destfolder: str | None = None,
    ignore_bodyparts: list[str] | None = None,
    inferencecfg: dict | None = None,
    modelprefix: str = "",
    greedy: bool = False,
    calibrate: bool = False,
    window_size: int = 0,
    identity_only: int = False,
    track_method: str = "",
    engine: Engine | None = None,
):
    """
    This should be called at the end of deeplabcut.analyze_videos for multianimal projects!

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    videos : list
        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.\n Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional
        An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    overwrite: bool, optional.
        Overwrite tracks file i.e. recompute tracks from full detections and overwrite.

    destfolder: string, optional
        Specifies the destination folder for analysis data (default is the path of the video). Note that for subsequent analysis this
        folder also needs to be passed.

    ignore_bodyparts: optional
        List of body part names that should be ignored during tracking (advanced).
        By default, all the body parts are used.

    inferencecfg: Default is None.
        Configuration file for inference (assembly of individuals). Ideally
        should be obtained from cross validation (during evaluation). By default
        the parameters are loaded from inference_cfg.yaml, but these get_level_values
        can be overwritten.

    calibrate: bool, optional (default=False)
        If True, use training data to calibrate the animal assembly procedure.
        This improves its robustness to wrong body part links,
        but requires very little missing data.

    window_size: int, optional (default=0)
        Recurrent connections in the past `window_size` frames are
        prioritized during assembly. By default, no temporal coherence cost
        is added, and assembly is driven mainly by part affinity costs.

    identity_only: bool, optional (default=False)
        If True and animal identity was learned by the model,
        assembly and tracking rely exclusively on identity prediction.

    track_method: string, optional
         Specifies the tracker used to generate the pose estimation data.
         For multiple animals, must be either 'box', 'skeleton', or 'ellipse'
         and will be taken from the config.yaml file if none is given.

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    Examples
    --------
    If you want to convert detections to tracklets:
    >>> import deeplabcut
    >>> deeplabcut.convert_detections2tracklets(
    >>>    "/analysis/project/reaching-task/config.yaml",
    >>>    ["/analysis/project/video1.mp4"],
    >>>    videotype='.mp4',
    >>> )

    If you want to convert detections to tracklets based on box_tracker:
    >>> import deeplabcut
    >>> deeplabcut.convert_detections2tracklets(
    >>>    "/analysis/project/reaching-task/config.yaml",
    >>>    ["/analysis/project/video1.mp4"],
    >>>    videotype=".mp4",
    >>>    track_method="box",
    >>> )

    --------

    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(config),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import convert_detections2tracklets

        return convert_detections2tracklets(
            config,
            videos,
            videotype=videotype,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            overwrite=overwrite,
            destfolder=destfolder,
            ignore_bodyparts=ignore_bodyparts,
            inferencecfg=inferencecfg,
            modelprefix=modelprefix,
            greedy=greedy,
            calibrate=calibrate,
            window_size=window_size,
            identity_only=identity_only,
            track_method=track_method,
        )

    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch.apis import convert_detections2tracklets

        if greedy or calibrate or window_size:
            raise NotImplementedError(
                f"The 'greedy', 'calibrate' and 'window_size' option are not yet "
                f"implemented with {engine}"
            )

        return convert_detections2tracklets(
            config,
            videos,
            videotype=videotype,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            overwrite=overwrite,
            destfolder=destfolder,
            ignore_bodyparts=ignore_bodyparts,
            inferencecfg=inferencecfg,
            modelprefix=modelprefix,
            identity_only=identity_only,
            track_method=track_method,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def extract_maps(
    config,
    shuffle: int = 0,
    trainingsetindex: int = 0,
    gputouse: int | None = None,
    device: str | None = None,
    rescale: bool = False,
    Indices: list[int] | None = None,
    modelprefix: str = "",
    engine: Engine | None = None,
):
    """
    Extracts the scoremap, locref, partaffinityfields (if available).

    Returns a dictionary indexed by: trainingsetfraction, snapshotindex, and imageindex
    for those keys, each item contains: (image, scmap, locref, paf, bpt_names,
    partaffinity_graph, imagename, True/False if this image was in trainingset).

    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 0.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note
        that TrainingFraction is a list in config.yaml). This variable can also be set
        to "all".

    gputouse: int or None, optional, default=None
        For the TensorFlow engine (for the PyTorch engine see ``device``). Specifies
        the GPU to use (see number in ``nvidia-smi``). If you do not have a GPU put
        ``None``. See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    device: str or None, optional, default=None
        The CUDA device to use for training. If None, the device will be taken from the
        ``pytorch_config.yaml`` file. Examples: {"cpu", "cuda", "cuda:0", "cuda:1"}. See
        https://pytorch.org/docs/stable/notes/cuda.html for more information.

    rescale: bool, default False
        Evaluate the model at the 'global_scale' variable (as set in the test/pose_config.yaml file for a particular project). I.e. every
        image will be resized according to that scale and prediction will be compared to the resized ground truth. The error will be reported
        in pixels at rescaled to the *original* size. I.e. For a [200,200] pixel image evaluated at global_scale=.5, the predictions are calculated
        on [100,100] pixel images, compared to 1/2*ground truth and this error is then multiplied by 2!. The evaluation images are also shown for the
        original size!

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    Examples
    --------
    If you want to extract the data for image 0 and 103 (of the training set) for model trained with shuffle 0.
    >>> deeplabcut.extract_maps(configfile,0,Indices=[0,103])

    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(config),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import extract_maps

        return extract_maps(
            config,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            gputouse=gputouse,
            rescale=rescale,
            Indices=Indices,
            modelprefix=modelprefix,
        )
    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch import extract_maps

        return extract_maps(
            config,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            device=_gpu_to_use_to_device(gputouse, device),
            rescale=rescale,
            indices=Indices,
            modelprefix=modelprefix,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def visualize_scoremaps(image: np.ndarray, scmap: np.ndarray):
    """Plots scoremaps as an image overlay.

    Args:
        image: An image as a numpy array of shape (h, w, channels)
        scmap: A scoremap of shape (h, w)

    Returns:
        The figure and axis on which the image scoremap was plot.
    """
    return visualization.visualize_scoremaps(image, scmap)


def visualize_locrefs(
    image: np.ndarray,
    scmap: np.ndarray,
    locref_x: np.ndarray,
    locref_y: np.ndarray,
    step: int = 5,
    zoom_width: int = 0,
):
    """Plots a scoremap and the corresponding location refinement field on an image.

    Args:
        image: An image as a numpy array of shape (h, w, channels)
        scmap: A scoremap of shape (h, w)
        locref_x: The x-coordinate of the location refinement field, of shape (h, w)
        locref_y: The y-coordinate of the location refinement field, of shape (h, w)
        step: The step with which to plot the location refinement field.
        zoom_width: The zoom width with which to plot the scoremaps.

    Returns:
        The figure and axis on which the image scoremap and locref field were plot.
    """
    return visualization.visualize_locrefs(
        image, scmap, locref_x, locref_y, step=step, zoom_width=zoom_width
    )


def visualize_paf(
    image: np.ndarray,
    paf: np.ndarray,
    step: int = 5,
    colors: list | None = None,
):
    """Plots the PAF on top of the image.

    Args:
        image: Shape (height, width, channels). The image on which the model was run.
        paf: Shape (height, width, 2 * len(paf_graph)). The PAF output by the model.
        step: The step with which to plot the scoremaps.
        colors: The colormap to use.

    Returns:
        The figure and axis on which the image PAF was plot.
    """
    return visualization.visualize_paf(image, paf, step=step, colors=colors)


def extract_save_all_maps(
    config,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    comparisonbodyparts: str | list[str] = "all",
    extract_paf: bool = True,
    all_paf_in_one: bool = True,
    gputouse: int | None = None,
    device: str | None = None,
    rescale: bool = False,
    Indices: list[int] | None = None,
    modelprefix: str = "",
    dest_folder: str = None,
    snapshot_index: int | str | None = None,
    detector_snapshot_index: int | str | None = None,
    engine: Engine | None = None,
):
    """
    Extracts the scoremap, location refinement field and part affinity field prediction of the model. The maps
    will be rescaled to the size of the input image and stored in the corresponding model folder in /evaluation-results.

    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to "all".

    comparisonbodyparts: list of bodyparts, Default is "all".
        The average error will be computed for those body parts only (Has to be a subset of the body parts).

    extract_paf : bool
        Extract part affinity fields by default.
        Note that turning it off will make the function much faster.

    all_paf_in_one : bool
        By default, all part affinity fields are displayed on a single frame.
        If false, individual fields are shown on separate frames.

    gputouse: int or None, optional, default=None
        For the TensorFlow engine (for the PyTorch engine see ``device``). Specifies
        the GPU to use (see number in ``nvidia-smi``). If you do not have a GPU put
        ``None``. See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    device: str or None, optional, default=None
        The CUDA device to use for training. If None, the device will be taken from the
        ``pytorch_config.yaml`` file. Examples: {"cpu", "cuda", "cuda:0", "cuda:1"}. See
        https://pytorch.org/docs/stable/notes/cuda.html for more information.

    Indices: default None
        For which images shall the scmap/locref and paf be computed? Give a list of images

    nplots_per_row: int, optional (default=None)
        Number of plots per row in grid plots. By default, calculated to approximate a squared grid of plots

    snapshot_index: Only for PyTorch models. Index (starting at 0) of the snapshot we
        want to extract maps with. To evaluate the last one, use -1. To extract maps
        for all snapshots, use "all". Default uses the value set in the project config.

    detector_snapshot_index: Only for TD PyTorch models. If defined, uses the detector
        with the given index for pose estimation. To extract maps for all detector
        snapshots, use "all". Default uses the value set in the project config.

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    Examples
    --------
    Calculated maps for images 0, 1 and 33.
    >>> deeplabcut.extract_save_all_maps('/analysis/project/reaching-task/config.yaml', shuffle=1,Indices=[0,1,33])

    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(config),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import extract_save_all_maps

        return extract_save_all_maps(
            config,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            comparisonbodyparts=comparisonbodyparts,
            extract_paf=extract_paf,
            all_paf_in_one=all_paf_in_one,
            gputouse=gputouse,
            rescale=rescale,
            Indices=Indices,
            modelprefix=modelprefix,
            dest_folder=dest_folder,
        )
    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch import extract_save_all_maps

        return extract_save_all_maps(
            config,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            comparison_bodyparts=comparisonbodyparts,
            extract_paf=extract_paf,
            all_paf_in_one=all_paf_in_one,
            device=_gpu_to_use_to_device(gputouse, device),
            rescale=rescale,
            indices=Indices,
            modelprefix=modelprefix,
            snapshot_index=snapshot_index,
            detector_snapshot_index=detector_snapshot_index,
            dest_folder=dest_folder,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def export_model(
    cfg_path: str,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    snapshotindex: int | None = None,
    iteration: int = None,
    TFGPUinference: bool = True,
    overwrite: bool = False,
    make_tar: bool = True,
    wipepaths: bool = False,
    without_detector: bool = False,
    modelprefix: str = "",
    engine: Engine | None = None,
) -> None:
    """Export DeepLabCut models for the model zoo or for live inference.

    Saves the pose configuration, snapshot files, and frozen TF graph of the model to
    directory named exported-models within the project directory (and an
    `exported-models-pytorch` directory for PyTorch models).

    Parameters
    -----------

    cfg_path : string
        path to the DLC Project config.yaml file

    shuffle : int, optional
        the shuffle of the model to export. default = 1

    trainingsetindex : int, optional
        the index of the training fraction for the model you wish to export. default = 1

    snapshotindex : int, optional
        the snapshot index for the weights you wish to export. If None,
        uses the snapshotindex as defined in 'config.yaml'. Default = None

    iteration : int, optional
        The model iteration (active learning loop) you wish to export. If None,
        the iteration listed in the config file is used.

    TFGPUinference : bool, optional
        use the tensorflow inference model? Default = True
        For inference using DeepLabCut-live, it is recommended to set TFGPIinference=False

    overwrite : bool, optional
        if the model you wish to export has already been exported, whether to overwrite. default = False

    make_tar : bool, optional
        Do you want to compress the exported directory to a tar file? Default = True
        This is necessary to export to the model zoo, but not for live inference.

    wipepaths : bool, optional
        Removes the actual path of your project and the init_weights from pose_cfg.

    without_detector: bool, optional
        PyTorch engine only. Exports top-down models without the detector.

    engine: Engine, optional, default = None.
        The default behavior loads the engine for the shuffle from the metadata. You can
        overwrite this by passing the engine as an argument, but this should generally
        not be done.

    Example:
    --------
    Export the first stored snapshot for model trained with shuffle 3:
    >>> deeplabcut.export_model('/analysis/project/reaching-task/config.yaml',shuffle=3, snapshotindex=-1)
    --------
    """
    if engine is None:
        engine = get_shuffle_engine(
            _load_config(cfg_path),
            trainingsetindex=trainingsetindex,
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.TF:
        from deeplabcut.pose_estimation_tensorflow import export_model

        return export_model(
            cfg_path=cfg_path,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            snapshotindex=snapshotindex,
            iteration=iteration,
            TFGPUinference=TFGPUinference,
            overwrite=overwrite,
            make_tar=make_tar,
            wipepaths=wipepaths,
            modelprefix=modelprefix,
        )
    elif engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch.apis.export import export_model

        return export_model(
            config=cfg_path,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            snapshotindex=snapshotindex,
            iteration=iteration,
            overwrite=overwrite,
            wipe_paths=wipepaths,
            without_detector=without_detector,
            modelprefix=modelprefix,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def _update_device(gpu_to_use: int | None, torch_kwargs: dict) -> None:
    if "device" not in torch_kwargs and gpu_to_use is not None:
        device = _gpu_to_use_to_device(gpu_to_use, device=None)
        if device is not None:
            torch_kwargs["device"] = device


def _gpu_to_use_to_device(gpu_to_use: int | None, device: str | None) -> str | None:
    if device is None and gpu_to_use is not None:
        if isinstance(gpu_to_use, int):
            device = f"cuda:{gpu_to_use}"
        else:
            device = gpu_to_use

    return device


def _load_config(config: str) -> dict:
    config_path = Path(config)
    if not config_path.exists():
        raise FileNotFoundError(
            f"Config {config} is not found. Please make sure that the file exists."
        )

    with open(config, "r") as f:
        project_config = YAML(typ="safe", pure=True).load(f)

    return project_config


--- File: deeplabcut/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import os

DEBUG = True and "DEBUG" in os.environ and os.environ["DEBUG"]
from deeplabcut.version import __version__, VERSION

print(f"Loading DLC {VERSION}...")

try:
    from deeplabcut.gui.tracklet_toolbox import refine_tracklets
    from deeplabcut.gui.launch_script import launch_dlc
    from deeplabcut.gui.tabs.label_frames import (
        label_frames,
        refine_labels,
    )
    from deeplabcut.gui.widgets import SkeletonBuilder
except (ModuleNotFoundError, ImportError):
    print(
        "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)"
    )

from deeplabcut.core.engine import Engine
from deeplabcut.create_project import (
    create_new_project,
    create_new_project_3d,
    add_new_videos,
    load_demo_data,
    create_pretrained_project,
    create_pretrained_human_project,
)
from deeplabcut.generate_training_dataset import (
    check_labels,
    create_training_dataset,
    extract_frames,
    mergeandsplit,
)
from deeplabcut.generate_training_dataset import (
    create_training_dataset_from_existing_split,
    create_training_model_comparison,
    create_multianimaltraining_dataset,
)
from deeplabcut.generate_training_dataset import (
    dropannotationfileentriesduetodeletedimages,
    comparevideolistsanddatafolders,
    dropimagesduetolackofannotation,
    adddatasetstovideolistandviceversa,
    dropduplicatesinannotatinfiles,
    dropunlabeledframes,
)

from deeplabcut.modelzoo.video_inference import video_inference_superanimal

from deeplabcut.utils import (
    create_labeled_video,
    create_video_with_all_detections,
    plot_trajectories,
    auxiliaryfunctions,
    convert2_maDLC,
    convertcsv2h5,
    analyze_videos_converth5_to_csv,
    analyze_videos_converth5_to_nwb,
    auxfun_videos,
)

try:
    from deeplabcut.pose_tracking_pytorch import transformer_reID
except ModuleNotFoundError as e:
    import warnings

    warnings.warn(
        """
        As PyTorch is not installed, unsupervised identity learning will not be available.
        Please run `pip install torch`, or ignore this warning.
        """
    )

from deeplabcut.utils.auxfun_videos import (
    ShortenVideo,
    DownSampleVideo,
    CropVideo,
    check_video_integrity,
)

# Train, evaluate & predict functions / all require TF
from deeplabcut.compat import (
    train_network,
    return_train_network_path,
    evaluate_network,
    return_evaluate_network_data,
    analyze_videos,
    create_tracking_dataset,
    analyze_images,
    analyze_time_lapse_frames,
    convert_detections2tracklets,
    extract_maps,
    visualize_scoremaps,
    visualize_locrefs,
    visualize_paf,
    extract_save_all_maps,
    export_model,
)


from deeplabcut.pose_estimation_3d import (
    calibrate_cameras,
    check_undistortion,
    triangulate,
    create_labeled_video_3d,
)

from deeplabcut.refine_training_dataset.stitch import stitch_tracklets
from deeplabcut.refine_training_dataset import (
    extract_outlier_frames,
    merge_datasets,
    find_outliers_in_raw_data,
)
from deeplabcut.post_processing import filterpredictions, analyzeskeleton


--- File: deeplabcut/cli.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from pathlib import Path

import click

CONTEXT_SETTINGS = dict(help_option_names=["-h", "--help"])


@click.group(invoke_without_command=True)
# @click.version_option()
@click.option("-v", "--verbose", is_flag=True, help="Verbose printing")
@click.pass_context
def main(ctx, verbose):
    if ctx.invoked_subcommand is None:
        click.echo("deeplabcut v0.0.")
        click.echo(main.get_help(ctx))


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("project")
@click.argument("experimenter")
@click.argument("videos", nargs=-1, type=click.Path(exists=True, dir_okay=False))
@click.option(
    "-d",
    "--wd",
    "working_directory",
    type=click.Path(exists=True, file_okay=False, resolve_path=True),
    default=Path.cwd(),
    help="Directory to create project in. Default is cwd().",
)
@click.option(
    "--copy_videos/--dont_copy_videos",
    is_flag=True,
    default=True,
    help="Specify if you need to create the symlinks of the video and store in the videos directory. Default is True.",
)
#              type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),
#              help='Directory to create project in. Default is cwd().')
@click.pass_context
def create_new_project(_, *args, **kwargs):
    """Create a new project directory, sub-directories and a basic configuration file. The configuration file is loaded with default values. Change its parameters to your projects need.\n

    Options \n
    ---------- \n
    project : string \n
    \tString containing the name of the project.\n
    experimenter : string \n
    \tString containing the name of the experimenter. \n
    videos : list \n
    \tA list of string containing the full paths of the videos to include in the project.\n
    working_directory : string, optional \n
    \tThe directory where the project will be created. The default is the ``current working directory``; if provided, it must be a string\n
    copy_videos : bool, optional \n
    If this is set to True, the symlink of the videos are copied to the project/videos directory. The default is ``True``; if provided it must be either ``True`` or ``False`` \n

    Example \n
    -------- \n
    To create the project in the current working directory \n
    python3 dlc.py create_new_project reaching-task Tanmay /data/videos/mouse1.avi /data/videos/mouse2.avi /data/videos/mouse3.avi /analysis/project/

    To create the project in the current working directory but do not want to create the symlinks \n
    python3 dlc.py create_new_project reaching-task Tanmay /data/videos/mouse1.avi /data/videos/mouse2.avi /data/videos/mouse3.avi /analysis/project/ -c False

    To create the project in another directory \n
    python3 dlc.py create_new_project reaching-task Tanmay /data/vies/mouse1.avi /data/videos/mouse2.avi /data/videos/mouse3.avi analysis/project -d home/project

    """
    from deeplabcut.create_project import new

    new.create_new_project(*args, **kwargs)


###########################################################################################################################


@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.argument("videos", nargs=-1, type=click.Path(exists=True, dir_okay=False))
@click.option(
    "--copy_videos/--dont_copy_videos",
    is_flag=True,
    default=True,
    help="Specify if you need to create the symlinks of the video and store in the videos directory. Default is True.",
)
@click.pass_context
def add_new_videos(_, *args, **kwargs):
    """
    Add new videos to the config file at any stage of the project.\n

    Options\n
    ----------\n
    config : string\n
        String containing the full path of the config file in the project.

    videos : list \n
        A list of string containing the full paths of the videos to include in the project.

    copy_videos : bool, optional\n
        If this is set to True, the symlink of the videos are copied to the project/videos directory. The default is
        ``True``; if provided it must be either ``True`` or ``False``

    Examples\n
    --------\n
    >>> python3 dlc.py add_new_videos /home/project/reaching-task-Tanmay-2018-08-23/config.yaml /data/videos/mouse5.avi

    """
    from deeplabcut.create_project import add

    add.add_new_videos(*args, **kwargs)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.argument("mode")
@click.option(
    "-a",
    "--algo",
    "algo",
    default="uniform",
    help='For automatic extraction, specify the algorithm- "kmeans" or "uniform". Default is uniform.',
)
@click.option(
    "--crop",
    is_flag=True,
    default=False,
    help="Specify if you need to crop the image. Default is True.",
)
@click.pass_context
def extract_frames(_, *args, **kwargs):
    """
    Extracts frames from the videos in the config.yaml file. Only the videos in the config.yaml will be used to select the frames.\n
    Use the function ``add_new_videos`` at any stage of the project to add new videos to the config file and extract their frames.\n

    CONFIG : string \n
        Full path of the config.yaml file as a string.  \n \n \n
    MODE : string \n \n
        String containing the mode of extraction. It must be either ``automatic`` or ``manual``.  \n

    Examples \n
    -------- \n
    for selecting frames automatically with 'kmeans' and do not want to crop the frames \n
    >>> python3 dlc.py extract_frames /analysis/project/reaching-task/config.yaml automatic --algo kmeans \n
    -------- \n
    for selecting frames automatically with 'uniform' and want to crop the frames based on the ``crop`` parameters in config.yaml \n
    >>> python3 dlc.py extract_frames /analysis/project/reaching-task/config.yaml automatic --crop
    -------- \n
    for selecting frames manually, \n
    >>> deeplabcut.extract_frames /analysis/project/reaching-task/config.yaml manual \n
    While selecting the frames manually, you do not need to specify the cropping parameters. Rather, you will get a prompt in the graphic user interface to choose if you need to crop or not. \n
    -------- \n

    """
    from deeplabcut.generate_training_dataset import frameExtraction

    frameExtraction.extract_frames(*args, **kwargs)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.pass_context
def label_frames(_, config):
    """Manually label/annotate the extracted frames. Update the list of body parts you want to localize in the config.yaml file first.\n
    Example\n
    --------\n
    python3 dlc.py label_frames /analysis/project/reaching-task/config.yaml
    """
    from deeplabcut.generate_training_dataset import labelFrames

    labelFrames.label_frames(config)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.pass_context
def check_labels(_, config):
    """Check if labels were stored correctly by plotting annotations and inspect them visually. If some are wrong, then use the refine_labels to correct the labels.\n"""
    from deeplabcut.generate_training_dataset import labelFrames

    labelFrames.check_labels(config)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.option(
    "-num",
    "--num_shuffles",
    "num_shuffles",
    default=1,
    help="Number of shuffles of training dataset to create. Default is set to 1.",
)
@click.pass_context
def create_training_dataset(_, *args, **kwargs):
    """Combine frame and label information into a an array. Create training and test sets. Update parameters TrainFraction, iteration in config.yaml
        Also update parameters for pose_config.yaml as wanted.\n
    CONFIG: Full path of the config.yaml file in the train directory of a project.\n
    Example \n
    --------\n
    To create a training dataset with only 1 shuffle
    python3 dlc.py create_training_dataset /analysis/project/reaching-task/config.yaml

    To create a training dataset with only 2 shuffles
    python3 dlc.py create_training_dataset /analysis/project/reaching-task/config.yaml num_shuffles 2
    """
    from deeplabcut.generate_training_dataset import labelFrames

    labelFrames.create_training_dataset(*args, **kwargs)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.option(
    "-num",
    "--num_shuffles",
    "shuffle",
    default=1,
    help="Shuffle index of the training dataset. Default is set to 1.",
)
@click.pass_context
def train_network(_, *args, **kwargs):
    """Train a trained Feature detector with a specific training data set.\n
        Provide path to the pose_config file.
        CONFIG: Full path of the config.yaml file in the train directory of a project.\n

    e.g. run the script like this:
    python3 dlc.py step7_train  /home/project/reaching/config.yaml

    """
    from deeplabcut.pose_estimation_tensorflow import training

    training.train_network(*args, **kwargs)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.option(
    "-num",
    "--num_shuffles",
    "shuffle",
    default=[1],
    help="Shuffle index of the training dataset. Default is set to 1.",
)
@click.option(
    "-p", "--plot", "plotting", is_flag=True, help="Make plots. Default is False."
)
@click.pass_context
def evaluate_network(_, config, **kwargs):
    """Evaluates a trained Feature detector model.\n
        CONFIG: Full path of the "pose_config.yaml" file in the train directory of a project.\n

    Example\n
    ----------
    Evalaute the network
    python3 dlc.py evaluate_network  /home/project/reaching/config.yaml

    """
    from deeplabcut.pose_estimation_tensorflow import evaluate

    evaluate.evaluate_network(config, **kwargs)


###########################################################################################################################


@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.argument("videos", nargs=-1)
@click.option(
    "-num",
    "--num_shuffles",
    "shuffle",
    default=1,
    help="Shuffle index of the training dataset. Default is set to 1.",
)
@click.option(
    "-vtype",
    "--video_type",
    "videotype",
    default=".avi",
    help="The extension of video in case the input is a directory",
)
@click.option(
    "-c",
    "--save",
    "save_as_csv",
    is_flag=True,
    help="Saves as a .csv file. Default is False.",
)
@click.pass_context
def analyze_videos(_, *args, **kwargs):
    """Makes prediction.\n
        CONFIG: Full path of the "config.yaml" file in the train directory of a project.\n
        VIDEOS: Full path to video.\n

    Example\n
    ----------

    python3 dlc.py analyze_videos /home/project/reaching/config.yaml /home/project/reaching/newVideo/1.avi

    """
    from deeplabcut.pose_estimation_tensorflow import predict_videos

    predict_videos.analyze_videos(*args, **kwargs)

    # for video in videos:
    #     predict.predict_video(config, video,**kwargs)


###########################################################################################################################


@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.argument("videos")
@click.option(
    "-num",
    "--num_shuffles",
    "shuffle",
    default=1,
    help="The shuffle index of training dataset. The extracted frames will be stored in the labeled-dataset for the corresponding shuffle of training dataset. Default is set to 1",
)
@click.option(
    "-outlier",
    "--outlier_algo",
    "outlieralgorithm",
    default="fitting",
    help="String specifying the algorithm used to detect the outliers. Currently, deeplabcut supports only sarimax (this will be updated). \
              This method fits a Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model to data and computes confidence interval. \
              Based on the fraction of data points outside the confidence interval and the average distance (compared to delta) \
              the user can identify potential outlier frames. The default is set to ``fitting``. Other choices: `fitting`, `jump`, `uncertain`",
)
@click.option(
    "-compare",
    "--comparisonbodyparts",
    "comparisonbodyparts",
    default="all",
    help="This select the body parts for which the comparisons with the outliers are carried out. Either ``all``, \
              then all body parts from config.yaml are used orr a list of strings that are a subset of the full list.\
               E.g. [`hand`,`Joystick`] for the demo Reaching-Mackenzie-2018-08-30/config.yaml to select only these two body parts.",
)
@click.option(
    "-e",
    "--epsilon",
    "epsilon",
    default=20,
    help="Meaning depends on outlieralgoritm. The default is set to 20 pixels.For outlieralgorithm `fitting`: \
              Float bound according to which frames are picked when the (average) body part estimate deviates from model fit. \
              For outlieralgorithm `jump`: Float bound specifying the distance by which body points jump from one frame to next (Euclidean distance)",
)
@click.option(
    "-p",
    "--p_bound",
    "p_bound",
    default=0.01,
    help="For outlieralgorithm `uncertain` this parameter defines the likelihood below, below which a body part will be flagged as a putative outlier.",
)
@click.option(
    "-ard",
    "--ar_degree",
    "ARdegree",
    default=7,
    help="For outlieralgorithm `fitting`: Autoregressive degree of Sarimax model degree. \
              See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html",
)
@click.option(
    "-mad",
    "--ma_degree",
    "MAdegree",
    default=1,
    help="Int value. For outlieralgorithm `fitting`: MovingAvarage degree of Sarimax model degree.\
               See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html",
)
@click.option(
    "-a",
    "--alpha",
    "alpha",
    default=0.01,
    help="Significance level for detecting outliers based on confidence interval of fitted SARIMAX model.",
)
@click.option(
    "-extract",
    "--extraction_algo",
    "extractionalgorithm",
    default="uniform",
    help="String specifying the algorithm to use for selecting the frames from the identified outliers. \
              Currently, deeplabcut supports either ``kmeans`` or ``uniform`` based selection (same logic as for extract_frames).\
              The default is set to``uniform``, if provided it must be either ``uniform`` or ``kmeans``.",
)
@click.pass_context
def extract_outlier_frames(_, *args, **kwargs):
    """
    Extracts the outlier frames in case, the predictions are not correct for a certain video from the cropped video running from
    start to stop as defined in config.yaml.

    Another crucial parameter in config.yaml is how many frames to extract 'numframes2extract'.

    CONFIG : string \n
    Full path of the config.yaml file as a string.  \n
    VIDEO : Full path of the video to extract the frame from. Make sure that this video is already analyzed.


    Example \n
    --------\n
    for extracting the frames with default settings\n
    >>> python3 dlc.py extract_outlier_frames /analysis/project/reaching-task/config.yaml /analysis/project/video/reachinvideo1.avi \n
    --------\n
    for extracting the frames with kmeans\n
    >>> python3 dlc.py extract_outlier_frames /analysis/project/reaching-task/config.yaml /analysis/project/video/reachinvideo1.avi --extractionalgorithm 'kmeans' \n
    --------\n
    for extracting the frames with kmeans and epsilon = 5 pixels.\n
    >>> python3 dlc.py extract_outlier_frames /analysis/project/reaching-task/config.yaml /analysis/project/video/reachinvideo1.avi --epsilon 5 --extractionalgorithm kmeans \n
    --------\n

    """
    from deeplabcut.refine_training_dataset import outlier_frames

    outlier_frames.extract_outlier_frames(*args, **kwargs)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.pass_context
def refine_labels(_, config):
    """Refines the labels of the outlier frames extracted from the analyzed videos.\n Helps in augmenting the training dataset.
    Use the function ``analyze_video`` to analyze a video and extracts the outlier frames using the function
    ``extract_outlier_frames`` before refining the labels.\n

    Examples \n
    --------\n
    >>> python3 dlc.py refine_labels /analysis/project/reaching-task/config.yaml \n
    --------\n
    """
    from deeplabcut.refine_training_dataset import outlier_frames

    outlier_frames.refine_labels(config)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.argument("videos", nargs=-1)
@click.option(
    "-num",
    "--num_shuffles",
    "shuffle",
    default=1,
    help="Number of shuffles of training dataset. Default is set to 1.",
)
@click.option(
    "-v",
    "--video_type",
    "videotype",
    default=".avi",
    help="Checks for the extension of the video in case the input is a directory.\
              Only videos with this extension are analyzed. The default is ``.avi``",
)
@click.option(
    "-s",
    "--save_frames",
    "save_frames",
    is_flag=True,
    default=False,
    help="If true creates each frame individual and then combines into a video. \
              This variant is relatively slow as it stores all individual frames. However, it \
              uses matplotlib to create the frames and is therefore much more flexible \
              (one can set transparency of markers, crop, and easily customize.",
)
@click.option(
    "-d",
    "--delete",
    "delete",
    is_flag=True,
    default=False,
    help="If true then the individual frames created during the video generation will be deleted.\
              Only the video will be left.",
)
@click.pass_context
def create_labeled_video(_, *args, **kwargs):
    """
    Labels the bodyparts in a video. Make sure the video is already analyzed by the function 'analyze_video'
    """
    from deeplabcut.utils import make_labeled_video

    make_labeled_video.create_labeled_video(*args, **kwargs)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("config")
@click.argument("videos", nargs=-1)
@click.option(
    "-num",
    "--num_shuffles",
    "shuffle",
    default=1,
    help="Number of shuffles of training dataset. Default is set to 1.",
)
@click.option(
    "-v",
    "--video_type",
    "videotype",
    default=".avi",
    help="Checks for the extension of the video in case the input is a directory.\
              Only videos with this extension are analyzed. The default is ``.avi``",
)
@click.option(
    "-s",
    "--show",
    "showfigures",
    is_flag=True,
    default=False,
    help="If true then plots are also displayed simultaneously.",
)
@click.pass_context
def plot_trajectories(_, *args, **kwargs):
    """
    Plots the trajectories of various bodyparts across the video.\n

    Example\n
    --------\n
    for labeling the frames\n
    >>> python3 dlc.py plot_trajectories /analysis/project/reaching-task/config.yaml /analysis/project/videos/reachingvideo1.avi  \n
    --------\n
    """
    from deeplabcut.utils import plotting

    plotting.plot_trajectories(*args, **kwargs)


###########################################################################################################################
@main.command(context_settings=CONTEXT_SETTINGS)
@click.argument("cfg-path", nargs=1, type=click.STRING)
@click.option(
    "-i",
    "--iteration",
    "iteration",
    default=None,
    required=False,
    type=int,
    help="the model iteration you wish to export. If None, uses the iteration listed in the config file",
)
@click.option(
    "-s",
    "--shuffle",
    "shuffle",
    default=1,
    required=False,
    type=int,
    help="the shuffle of the model to export. Default is set to 1.",
)
@click.option(
    "-t",
    "--trainingsetindex",
    "trainingsetindex",
    default=0,
    required=False,
    type=int,
    help="the index of the training fraction for the model you wish to export. default = 0",
)
@click.option(
    "-n",
    "--snapshotindex",
    "snapshotindex",
    default=None,
    required=False,
    type=int,
    help="the snapshot index for the weights you wish to export",
)
@click.option(
    "--TFGPUinference/--NPinference",
    "TFGPUinference",
    default=True,
    required=False,
    help="use the tensorflow inference model? Default = True",
)
@click.option(
    "--overwrite",
    "-o",
    is_flag=True,
    required=False,
    help="if the model you wish to export has already been exported, whether to overwrite. default = False",
)
@click.option(
    "--make-tar/--no-tar",
    "make_tar",
    default=True,
    required=False,
    help="Do you want to compress the exported directory to a tar file? Default = True",
)
@click.pass_context
def export_model(_, *args, **kwargs):
    """
    Export DLC models for the model zoo or for live inference.\n
    Saves the pose configuration, snapshot files, and frozen graph of the model to a directory named exported-models within the project directory

    Parameters
    -----------

    cfg_path : string\n
    \tpath to the DLC Project config.yaml file

    iteration : int, optional\n
    \tthe model iteration you wish to export.\n
    \tIf None, uses the iteration listed in the config file

    shuffle : int, optional\n
    \tthe shuffle of the model to export. default = 1

    trainingsetindex : int, optional\n
    \tthe index of the training fraction for the model you wish to export. default = 1

    snapshotindex : int, optional\n
    \tthe snapshot index for the weights you wish to export.\n
    \tIf None, uses the snapshotindex as defined in 'config.yaml'. Default = None

    TFGPUinference : bool, optional\n
    \tuse the tensorflow inference model? Default = True\n
    \tFor inference using DeepLabCut-live, it is recommended to set TFGPIinference=False

    overwrite : bool, optional\n
    \tif the model you wish to export has already been exported, whether to overwrite. default = False

    make_tar : bool, optional\n
    \tDo you want to compress the exported directory to a tar file? Default = True\n
    \tThis is necessary to export to the model zoo, but not for live inference.
    """

    from deeplabcut import export_model

    export_model(*args, **kwargs)


###########################################################################################################################


--- File: deeplabcut/pose_cfg.yaml ---
dataset: willbeautomaticallyupdatedbycreate_training_datasetcode
metadataset: willbeautomaticallyupdatedbycreate_training_datasetcode
num_joints: willbeautomaticallyupdatedbycreate_training_datasetcode
all_joints: willbeautomaticallyupdatedbycreate_training_datasetcode
all_joints_names: willbeautomaticallyupdatedbycreate_training_datasetcode
init_weights: willbeautomaticallyupdatedbycreate_training_datasetcode
project_path: willbeautomaticallyupdatedbycreate_training_datasetcode

# Hyperparameters below worked well for our tasks in
# Mathis et al. Nature Neuroscience
# https://www.nature.com/articles/s41593-018-0209-y

# all locations within this distance threshold are considered
# positive training samples for detector
pos_dist_thresh: 17

# all images in the dataset will be rescaled by the following
# scaling factor to be processed by the CNN. You can select the
# optimal scale by cross-validation
global_scale: 0.8

##############################################################################
#### Augmentation variables
##############################################################################

dataset_type: imgaug
batch_size: 1

# Probability with which the augmenters will be applied to input images
# Note some augmentations have their own probability (e.g. claheratio/rotratio/...)
apply_prob: 0.5

# Resize images prior to augmentation
pre_resize: []  # Specify [width, height] if pre-resizing is desired

# Smart, on-the-fly image cropping, replacing deeplabcut.cropimagesandlabels
crop_size: [400, 400]  # width, height
max_shift: 0.4  # Maximum relative shift of the position of the crop center
crop_sampling: hybrid  # Sample crop centers either uniformly over the image or based on keypoint neighbor density, at random

# Other crop_sampling variants:
#- uniform -- spatially uniform sampling of crops (over the image)
#- keypoints -- keypoint based sampling of crops (over the image)
#- density -- keypoint based sampling of crops biasing towards regions with more keypoints (over the image)
#- hybrid -- 50% density and 50% uniform


#Data loaders, i.e. with additional data augmentation options (as of 2.0.9+):
#default with be with no extra dataloaders. Other options: 'tensorpack, deterministic'
#types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py
#For deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324
#For tensorpack, see https://github.com/DeepLabCut/DeepLabCut/pull/409

#what is the fraction of training samples with cropping? (used for scalecrop)
cropratio: 0.4
# see below for cropping variables for tensorpack and scalecrop (these need to be set in pose_cfg.yaml per model)
# for imagaug strength is modulated by crop_by

#to not apply rotation put False
rotation: 25  # plus/minus 25 degree rotation; only for imgaug + tensorpack!
rotratio: 0.4 # fraction of applying rotations

# During training an image will be randomly scaled within the
# range [scale_jitter_lo; scale_jitter_up] to augment training data,
scale_jitter_lo: 0.5
scale_jitter_up: 1.25

# Randomly flips an image horizontally to augment training data
mirror: False #imgaug & tensorpack do not consider mirror symmetric joints now,
#i.e. left hand and right hand are not swapped

# Functions from augmenters.contrast
# set True to use with default parameters, otherwise give a dictionary for keyword arguments
# If no ratio value is given, set to 0.4 by default

# Functions from augmenters.convolve,
# set True to use with default parameters, otherwise give a dictionary for keyword arguments
# If no ratio value is given, set to 0.4 by default

# dictionary with contrast parameters
contrast:
   clahe: True
   claheratio: 0.1
   histeq: True
   histeqratio: 0.1
   
# dictionary with convolution parameters
convolution:
   sharpen: False
   sharpenratio: 0.3
   edge: False
   emboss:
      alpha: [0.0, 1.0]
      strength: [0.5, 1.5]
   embossratio: 0.1

##############################################################################
#### Augmentation type: scalecrop + tensorpack variables
##############################################################################

# Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)
#and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.
# Parameters for augmentation with regard to cropping.

# As of 2.2.: (scalecrop is no longer the default!)
# These parameters are all set in: deeplabcut/pose_estimation_tensorflow/dataset/pose_dataset_scalecrop.py

#minsize: 100 #what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint
#leftwidth: 400
#rightwidth: 400
#topheight: 400
#bottomheight: 400
#limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval

##############################################################################
#### Augmentation type: imgaug & tensorpack
##############################################################################


##############################################################################
#### Networks
##############################################################################

# NOTE: as of DLC 2.1 these are defined when creating the training set!
# Type of the CNN to use, currently resnets + mobilenets + efficientnets + our dlcrnet are supported (see docs)
net_type: resnet_50
multi_stage: false
#init_weights: ./snapshot-5000


# Location refinement parameters (check https://arxiv.org/abs/1511.06645)
location_refinement: true
locref_huber_loss: true
locref_loss_weight: 0.05
locref_stdev: 7.2801

# Enabling this adds additional loss layer in the middle of the ConvNet,
# which helps accuracy (you should set to true for ResNet-101, or 152!).
intermediate_supervision: false
intermediate_supervision_layer: 12

# For multi-animal version starting with DLC 2.2
pairwise_huber_loss: false
partaffinityfield_predict: false
pairwise_predict: false

# all images larger with size
# width * height > max_input_size*max_input_size are not used in training.
# Prevents training from crashing with out of memory exception for very
# large images.
max_input_size: 1500
# all images smaller than 64*64 will be excluded.
min_input_size: 64

# Learning rate schedule for the SGD/adam optimizer.
multi_step:
- [0.005, 10000]
- [0.02, 430000]
- [0.002, 730000]
- [0.001, 1030000]

# Learning rate parameters for cosine decay scheduler [used for EfficientNet backbones in DLC]
# See: https://openaccess.thecvf.com/content/WACV2021/papers/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.pdf

lr_init: 0.0005
decay_steps: 30000
alpha_r: 0.02


# How often display loss
display_iters: 1000
# How often to save training snapshot
save_iters: 50000


--- File: deeplabcut/reid_cfg.yaml ---
########## Model ##########

# Using cuda or cpu for training
device: cuda

# If train with arcface loss, options: 'True', 'False'
cos_layer: false

# If train with multi-gpu ddp mode, options: 'True', 'False'
dist_train: false

# Transformer settings
drop_path: 0.1
drop_out: 0.0
att_drop_rate: 0.0

# SIE parameters
sie_coe: 3.0


########## Solver ##########

optimizer_name: Adam

max_epochs: 100

# Base learning rate
base_lr: 3e-4

# Whether using larger learning rate for fc layer
large_fc_lr: false

# Factor of learning bias
bias_lr_factor: 1

momentum: 0.9

# Settings of weight decay
weight_decay: 0.0005
weight_decay_bias: 0.0005

# Warm up epochs
warmup_epochs: 5

# Epoch number of saving checkpoints
checkpoint_period: 10

# Iteration of display training log
log_period: 100

########## Test ##########

# Whether feature is normalized before test, if yes, it is equivalent to cosine distance
feat_norm: yes

--- File: deeplabcut/__main__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

def main():
    try:
        import PySide6

        lite = False
    except ModuleNotFoundError:
        lite = True

    # if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI
    if not lite:
        print("Starting GUI...")
        from deeplabcut.gui.launch_script import launch_dlc

        launch_dlc()
    else:
        print(
            "You installed DLC lite, thus GUI's cannot be used. If you need GUI support please: pip install 'deeplabcut[gui]''"
        )


if __name__ == "__main__":
    main()

--- File: deeplabcut/benchmark/metrics.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

"""Evaluation metrics for the DeepLabCut benchmark."""

import sys
import unittest.mock

# TODO(stes) mocking a few modules to rely in fewer dependencies, without
# causing import errors when using deeplabcut.
MOCK_MODULES = ["statsmodels", "statsmodels.api", "pytables"]
for mod_name in MOCK_MODULES:
    sys.modules[mod_name] = unittest.mock.MagicMock()

import os
import pickle
from collections import defaultdict
from typing import List, Optional

import numpy as np
import pandas as pd

import deeplabcut.benchmark.utils
from deeplabcut.core import inferenceutils, crossvalutils
from deeplabcut.utils.conversioncode import guarantee_multiindex_rows


def _format_gt_data(h5file: str, test_indices: Optional[List[int]] = None):
    df = pd.read_hdf(h5file)

    animals = _get_unique_level_values(df.columns, "individuals")
    kpts = _get_unique_level_values(df.columns, "bodyparts")
    try:
        n_unique = len(
            _get_unique_level_values(
                df.xs("single", level="individuals", axis=1).columns, "bodyparts"
            )
        )
    except KeyError:
        n_unique = 0
    guarantee_multiindex_rows(df)
    file_paths = [os.path.join(*row) for row in df.index.to_list()]
    temp = (
        df.stack("individuals", dropna=False)
        .reindex(animals, level="individuals")
        .reindex(kpts, level="bodyparts", axis=1)
    )
    data = temp.to_numpy().reshape((len(file_paths), len(animals), -1, 2))
    if test_indices is not None:
        file_paths = [file_paths[i] for i in test_indices]
        data = [data[i] for i in test_indices]

    meta = {"animals": animals, "keypoints": kpts, "n_unique": n_unique}
    return {
        "annotations": dict(zip(file_paths, data)),
        "metadata": meta,
    }


def _get_unique_level_values(header, level):
    return header.get_level_values(level).unique().to_list()


def calc_prediction_errors(preds, gt):
    kpts_gt = gt["metadata"]["keypoints"]
    kpts_pred = preds["metadata"]["keypoints"]
    map_ = {kpts_gt.index(kpt): i for i, kpt in enumerate(kpts_pred)}
    annot = gt["annotations"]

    map_images = _map(list(preds["predictions"]), list(annot))

    errors = np.full(
        (
            len(preds["predictions"]),
            len(gt["metadata"]["animals"]),
            len(kpts_gt),
            2,  # Hold distance to GT and confidence
        ),
        np.nan,
    )
    for n, (path, preds_) in enumerate(preds["predictions"].items()):
        if not preds_:
            continue
        xy_gt = annot[map_images[path]].swapaxes(0, 1)
        xy_pred = preds_["coordinates"][0]
        conf_pred = preds_["confidence"]
        for i, xy_gt_ in enumerate(xy_gt):
            visible = np.flatnonzero(np.all(~np.isnan(xy_gt_), axis=1))
            xy_pred_ = xy_pred[map_[i]]
            if visible.size and xy_pred_.size:
                # Pick the predictions closest to ground truth,
                # rather than the ones the model has most confident in.
                neighbors = crossvalutils.find_closest_neighbors(
                    xy_gt_[visible], xy_pred_, k=3
                )
                found = neighbors != -1
                if ~np.any(found):
                    continue
                min_dists = np.linalg.norm(
                    xy_gt_[visible][found] - xy_pred_[neighbors[found]],
                    axis=1,
                )
                conf_pred_ = conf_pred[map_[i]]
                errors[n, visible[found], i, 0] = min_dists
                errors[n, visible[found], i, 1] = conf_pred_[neighbors[found], 0]
    return errors


def _map(strings, substrings):
    """
    Map image paths from predicted data to GT as the first are typically
    absolute whereas the latter are relative to the project path.
    """

    lookup = dict()
    strings_ = strings.copy()
    substrings_ = substrings.copy()
    while strings_:
        string = strings_.pop()
        for s in substrings_:
            if string.endswith(s):
                lookup[string] = s
                substrings_.remove(s)
                break
    return lookup


def conv_obj_to_assemblies(eval_results_obj, keypoint_names):
    """Convert predictions to deeplabcut assemblies."""
    assemblies = {}
    for image_path, results in eval_results_obj.items():
        lst = []
        for dict_ in results:
            ass = inferenceutils.Assembly(len(keypoint_names))
            for i, kpt in enumerate(keypoint_names):
                xy = dict_["pose"][kpt]
                if ~np.isnan(xy).all():
                    joint = inferenceutils.Joint(pos=(xy), label=i)
                    ass.add_joint(joint)
            # TODO(jeylau) add affinity.setter to Assembly
            ass._affinity = dict_["score"]
            ass._links = [None]
            if len(ass):
                lst.append(ass)
        assemblies[image_path] = lst
    return assemblies


def calc_map_from_obj(
    eval_results_obj,
    h5_file,
    metadata_file,
    oks_sigma=0.1,
    margin=0,
    symmetric_kpts=None,
    drop_kpts=None,
):
    """Calculate mean average precision (mAP) based on predictions."""
    df = pd.read_hdf(h5_file)
    try:
        df.drop("single", level="individuals", axis=1, inplace=True)
    except KeyError:
        pass
    n_animals = len(df.columns.get_level_values("individuals").unique())
    kpts = list(df.columns.get_level_values("bodyparts").unique())

    test_indices = _load_test_indices(metadata_file)
    df_test = df.iloc[test_indices]
    test_images = load_test_images(h5_file, metadata_file)
    missing_images = set(test_images) - set(eval_results_obj.keys())
    if len(missing_images) > 0:
        raise ValueError(
            "Failed to compute the test mAP: there are test images missing from the"
            f"prediction object: {missing_images}"
        )

    ground_truth = df_test.to_numpy().reshape((len(test_images), n_animals, -1, 2))
    temp = np.ones((*ground_truth.shape[:3], 3))
    temp[..., :2] = ground_truth
    assemblies_gt_test = {
        test_images[i]: assembly
        for i, assembly in inferenceutils._parse_ground_truth_data(temp).items()
    }

    # TODO(stes): remove/rewrite
    if drop_kpts is not None:
        temp = {}
        for k, v in assemblies_gt_test.items():
            lst = []
            for a in v:
                arr = np.delete(a.data[:, :3], drop_kpts, axis=0)
                a = inferenceutils.Assembly.from_array(arr)
                lst.append(a)
            temp[k] = lst
        assemblies_gt_test = temp
        for ind in sorted(drop_kpts, reverse=True):
            kpts.pop(ind)

    assemblies_pred = conv_obj_to_assemblies(eval_results_obj, kpts)
    with deeplabcut.benchmark.utils.DisableOutput():
        oks = inferenceutils.evaluate_assembly(
            assemblies_pred,
            assemblies_gt_test,
            oks_sigma,
            margin=margin,
            symmetric_kpts=symmetric_kpts,
            greedy_matching=True,
        )
    return oks["mAP"]


def calc_rmse_from_obj(
    eval_results_obj,
    h5_file,
    metadata_file,
    drop_kpts=None,
):
    """Calc prediction errors for submissions."""
    test_indices = _load_test_indices(metadata_file)
    gt = _format_gt_data(h5_file, test_indices=test_indices)
    kpts = gt["metadata"]["keypoints"]
    if drop_kpts:
        for k, v in gt["annotations"].items():
            gt["annotations"][k] = np.delete(v, drop_kpts, axis=1)
        for ind in sorted(drop_kpts, reverse=True):
            kpts.pop(ind)

    test_objects = {
        k: v for k, v in eval_results_obj.items() if k in gt["annotations"].keys()
    }
    if len(gt["annotations"]) != len(test_objects):
        gt_images = list(gt["annotations"].keys())
        missing_images = [img for img in gt_images if img not in test_objects]
        raise ValueError(
            "Failed to compute the test RMSE: there are test images missing from the"
            f"prediction object: {missing_images}"
        )

    assemblies_pred = conv_obj_to_assemblies(test_objects, kpts)
    preds = defaultdict(dict)
    preds["metadata"]["keypoints"] = kpts
    for image, assemblies in assemblies_pred.items():
        if assemblies:
            arr = np.stack([a.data for a in assemblies]).swapaxes(0, 1)
            data = [xy[~np.isnan(xy).any(axis=1)] for xy in arr[..., :2]]
            temp = {
                "coordinates": tuple([data]),
                "confidence": list(np.expand_dims(arr[..., 2], axis=2)),
            }
            preds["predictions"][image] = temp
    with deeplabcut.benchmark.utils.DisableOutput():
        errors = calc_prediction_errors(preds, gt)
    return np.nanmean(errors[..., 0])


def load_test_images(h5file: str, metadata: str) -> List[str]:
    """
    Returns the names of the test images for the benchmark, in the order corresponding
    to the test indices.
    """
    df = pd.read_hdf(h5file)
    test_indices = _load_test_indices(metadata)
    df_test = df.iloc[test_indices]
    test_images = []
    for img_path in df_test.index:
        if not isinstance(img_path, str):
            img_path = os.path.join(*img_path)
        test_images.append(img_path)
    return test_images


def _load_test_indices(shuffle_metadata_path: str) -> list[int]:
    """Returns the indices of test images in the training dataset dataframe"""
    with open(shuffle_metadata_path, "rb") as f:
        test_indices = set([int(i) for i in pickle.load(f)[2]])
    return list(sorted(test_indices))


--- File: deeplabcut/benchmark/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import json
import os
from typing import Container
from typing import Literal

from deeplabcut.benchmark.base import Benchmark, Result, ResultCollection

DATA_ROOT = os.path.join(os.getcwd(), "data")
CACHE = os.path.join(os.getcwd(), ".results")

__registry = []


def register(cls):
    """Add a benchmark to the list of evaluations to run.

    Apply this function as a decorator to a class. Note that the
    class needs to be a subclass of the ``benchmark.base.Benchmark``
    base class.

    In most situations, it will be a subclass of one of the pre-defined
    benchmarks in ``benchmark.benchmarks``.

    Throws:
        ``ValueError`` if the decorator is applied to a class that is
        not a subclass of ``benchmark.base.Benchmark``.
    """
    if not issubclass(cls, Benchmark):
        raise ValueError(
            f"Can only register subclasses of {type(Benchmark)}, " f"but got {cls}."
        )
    __registry.append(cls)


def evaluate(
    include_benchmarks: Container[str] = None,
    results: ResultCollection = None,
    on_error="return",
) -> ResultCollection:
    """Run evaluation for all benchmarks and methods.

    Note that in order for your custom benchmark to be included during
    evaluation, the following conditions need to be met:

        - The benchmark subclassed one of the benchmark definitions in
          in ``benchmark.benchmarks``
        - The benchmark is registered by applying the ``@benchmark.register``
          decorator to the class
        - The benchmark was imported. This is done automatically for all
          benchmarks that are defined in submodules or subpackages of the
          ``benchmark.submissions`` module. For all other locations, make
          sure to manually import the packages **before** calling the
          ``evaluate()`` function.

    Args:
        include_benchmarks:
            If ``None``, run all benchmarks that were discovered. If a container
            is passed, only include methods that were defined on benchmarks with
            the specified names. E.g., ``include_benchmarks = ["trimouse"]`` would
            only evaluate methods of the trimouse benchmark dataset.
        on_error:
            see documentation in ``benchmark.base.Benchmark.evaluate()``

    Returns:
        A collection of all results, which can be printed or exported to
        ``pd.DataFrame`` or ``json`` file formats.
    """
    if results is None:
        results = ResultCollection()
    for benchmark_cls in __registry:
        if include_benchmarks is not None:
            if benchmark_cls.name not in include_benchmarks:
                continue
        benchmark = benchmark_cls()
        for name in benchmark.names():
            if Result(
                code=benchmark.code,
                method_name=name,
                benchmark_name=benchmark_cls.name,
            ) in results:
                continue
            else:
                result = benchmark.evaluate(name, on_error=on_error)
                results.add(result)
    return results


def get_filepath(basename: str):
    return os.path.join(DATA_ROOT, basename)


def savecache(results: ResultCollection):
    with open(CACHE, "w") as fh:
        json.dump(results.todicts(), fh, indent=2)


def loadcache(
    cache=CACHE, on_missing: Literal["raise", "ignore"] = "ignore"
) -> ResultCollection:
    if not os.path.exists(cache):
        if on_missing == "raise":
            raise FileNotFoundError(cache)
        return ResultCollection()
    with open(cache, "r") as fh:
        try:
            data = json.load(fh)
        except json.decoder.JSONDecodeError as e:
            if on_missing == "raise":
                raise e
            return ResultCollection()
    return ResultCollection.fromdicts(data)


--- File: deeplabcut/benchmark/mot.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from __future__ import annotations

import warnings

import motmetrics as mm
import numpy as np
import pandas as pd
from numpy.typing import NDArray

from deeplabcut.core import trackingutils


def convert_bboxes_to_xywh(bboxes: NDArray, inplace: bool = False) -> NDArray:
    """
    Converts bounding box coordinates from [x_min, y_min, x_max, y_max] format
    to [x, y, width, height] format.

    Parameters
    ----------
    bbox : numpy.ndarray
        A 2D array of shape (N, M), where N is the number of bounding boxes
        and M >= 4. The first four columns represent the bounding box in the format
        [x_min, y_min, x_max, y_max].
    inplace : bool, optional
        If True, modifies the input array in place. If False, returns a copy of
        the array with the converted bounding box format. Defaults to False.

    Returns
    -------
    numpy.ndarray or None
        If `inplace` is False, returns a new array of the same shape as `bbox`
        with the format [x, y, width, height]. If `inplace` is True, the input
        array is modified directly, and nothing is returned.
    """
    w = bboxes[:, 2] - bboxes[:, 0]
    h = bboxes[:, 3] - bboxes[:, 1]
    if not inplace:
        new_bboxes = bboxes.copy()
        new_bboxes[:, 2] = w
        new_bboxes[:, 3] = h
        return new_bboxes
    bboxes[:, 2] = w
    bboxes[:, 3] = h

_convert_bboxes_to_xywh = convert_bboxes_to_xywh


def reconstruct_bboxes_from_bodyparts(
    data: pd.DataFrame, margin: float, to_xywh: bool = False
) -> NDArray:
    """
    Reconstructs bounding boxes from body part coordinates and likelihoods.

    Parameters
    ----------
    data : pandas.DataFrame
        A DataFrame containing body part data with a multi-level column index.
        The expected levels include 'x', 'y', and 'likelihood', where:
        - 'x' and 'y' contain the coordinates of the body parts.
        - 'likelihood' contains the confidence scores for each body part.
    margin : float
        The margin to add/subtract from the minimum/maximum coordinates when defining the bounding box.
    to_xywh : bool, optional
        If True, converts the bounding box format from [x_min, y_min, x_max, y_max]
        to [x, y, width, height]. Defaults to False.

    Returns
    -------
    numpy.ndarray
        An array of shape (N, 5), where N is the number of rows in `data`.
        Each row represents a bounding box with the following values:
        - [x_min, y_min, x_max, y_max, likelihood]
        If `to_xywh` is True, the format will be [x, y, width, height, likelihood].

    Notes
    -----
    - NaN values in the input data are ignored when computing the bounding box dimensions.
    - Warnings related to NaN values are suppressed during calculations.
    """
    x = data.xs("x", axis=1, level="coords")
    y = data.xs("y", axis=1, level="coords")
    p = data.xs("likelihood", axis=1, level="coords")
    xy = np.stack([x, y], axis=2)
    bboxes = np.full((data.shape[0], 5), np.nan)
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", category=RuntimeWarning)
        bboxes[:, :2] = np.nanmin(xy, axis=1) - margin
        bboxes[:, 2:4] = np.nanmax(xy, axis=1) + margin
        bboxes[:, 4] = np.nanmean(p, axis=1)
    if to_xywh:
        convert_bboxes_to_xywh(bboxes, inplace=True)
    return bboxes


def reconstruct_all_bboxes(
    data: pd.DataFrame, margin: float, to_xywh: bool = False
) -> NDArray:
    """
    Reconstructs bounding boxes for multiple individuals from body part data.

    Parameters
    ----------
    data : pandas.DataFrame
        A DataFrame containing body part data with a multi-level column index.
        The expected levels include:
        - 'individuals': Names of the individuals (e.g., animals).
        - 'x', 'y', and 'likelihood': Coordinate and confidence data for body parts.
    margin : float
        The margin to add/subtract from the minimum/maximum coordinates when defining the bounding box.
    to_xywh : bool
        If True, converts the bounding box format from [x_min, y_min, x_max, y_max]
        to [x, y, width, height].

    Returns
    -------
    numpy.ndarray
        A 3D array of shape (A, F, 5), where:
        - A is the number of individuals (excluding 'single', if present).
        - F is the number of frames (rows) in the input `data`.
        - Each bounding box is represented as [x_min, y_min, x_max, y_max, likelihood].
          If `to_xywh` is True, the format will be [x, y, width, height, likelihood].

    Notes
    -----
    - Individuals are extracted from the 'individuals' level of the DataFrame columns.
    - If an individual named 'single' exists, it is excluded from the bounding box computation.
    - NaN values in the input data are ignored during calculations.
    """
    animals = data.columns.get_level_values("individuals").unique().tolist()
    try:
        animals.remove("single")
    except ValueError:
        pass
    bboxes = np.full((len(animals), data.shape[0], 5), np.nan)
    for n, animal in enumerate(animals):
        bboxes[n] = reconstruct_bboxes_from_bodyparts(
            data.xs(animal, axis=1, level="individuals"), margin, to_xywh
        )
    return bboxes


def compute_mot_metrics(
    h5_file_gt: str,
    h5_file_pred: str,
    tracker_type: str = "bbox",
    **kwargs,
) -> mm.MOTAccumulator:
    df_gt = pd.read_hdf(h5_file_gt)
    df = pd.read_hdf(h5_file_pred)
    if tracker_type == "bbox":
        func = reconstruct_all_bboxes
    elif tracker_type == "ellipse":
        func = trackingutils.reconstruct_all_ellipses
    else:
        raise ValueError(f"Unrecognized tracker type {tracker_type}.")

    trackers_gt = func(df_gt, **kwargs)
    trackers = func(df, **kwargs)
    return _compute_mot_metrics(
        trackers_gt, trackers, tracker_type,
    )


def _compute_mot_metrics(
    trackers_ground_truth: NDArray,
    trackers: NDArray,
    tracker_type: str = "bbox",
) -> mm.MOTAccumulator:
    if trackers_ground_truth.shape != trackers.shape:
        raise ValueError(
            "Dimensions mismatch. There must be as many `trackers_ground_truth` as there are `trackers`."
        )

    if tracker_type == "bbox":
        sl = slice(0, 4)
        cost_func = mm.distances.iou_matrix
    elif tracker_type == "ellipse":
        sl = slice(0, 5)

        def cost_func(ellipses_gt, ellipses_hyp):
            cost_matrix = np.zeros((len(ellipses_gt), len(ellipses_hyp)))
            gt_el = [trackingutils.Ellipse(*e[:5]) for e in ellipses_gt]
            hyp_el = [trackingutils.Ellipse(*e[:5]) for e in ellipses_hyp]
            for i, el in enumerate(gt_el):
                for j, tracker in enumerate(hyp_el):
                    cost_matrix[i, j] = 1 - el.calc_similarity_with(tracker)
            return cost_matrix

    else:
        raise ValueError(f"Unrecognized tracker type {tracker_type}.")

    ids = np.arange(trackers_ground_truth.shape[0])
    acc = mm.MOTAccumulator(auto_id=True)
    for i in range(trackers_ground_truth.shape[1]):
        trackers_gt = trackers_ground_truth[:, i, sl]
        trackers_hyp = trackers[:, i, sl]
        empty_gt = np.isnan(trackers_gt).any(axis=1)
        empty_hyp = np.isnan(trackers_hyp).any(axis=1)
        trackers_gt = trackers_gt[~empty_gt]
        trackers_hyp = trackers_hyp[~empty_hyp]
        cost = cost_func(trackers_gt, trackers_hyp)
        acc.update(ids[~empty_gt], ids[~empty_hyp], cost)
    return acc


def print_all_metrics(
    accumulators: list[mm.MOTAccumulator], all_params: list[str] | None = None
):
    if not all_params:
        names = [f"iter{i + 1}" for i in range(len(accumulators))]
    else:
        s = "_".join("{}" for _ in range(len(all_params[0])))
        names = [s.format(*params.values()) for params in all_params]
    mh = mm.metrics.create()
    summary = mh.compute_many(
        accumulators, metrics=mm.metrics.motchallenge_metrics, names=names
    )
    strsummary = mm.io.render_summary(
        summary, formatters=mh.formatters, namemap=mm.io.motchallenge_metric_names
    )
    print(strsummary)
    return summary


--- File: deeplabcut/benchmark/cli.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

"""Command line interface for DeepLabCut deeplabcut.benchmark."""

import argparse

import deeplabcut.benchmark


def _parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--include", nargs="+", default=None, required=False)
    parser.add_argument(
        "--onerror",
        default="return",
        required=False,
        choices=("ignore", "return", "raise"),
    )
    parser.add_argument("--nocache", action="store_true")
    return parser.parse_args()


def main():
    """Main CLI entry point for generating deeplabcut.benchmark results."""
    args = _parse_args()
    if not args.nocache:
        results = deeplabcut.benchmark.loadcache()
    else:
        results = None
    results = deeplabcut.benchmark.evaluate(
        include_benchmarks=args.include,
        results=results,
        on_error=args.onerror,
    )
    if not args.nocache:
        deeplabcut.benchmark.savecache(results)
    try:
        print(results.toframe())
    except StopIteration:
        pass


--- File: deeplabcut/benchmark/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

"""Helper functions in this file are not affected by the main repositories
license. They are independent from the remainder of the benchmarking code. 
"""
import importlib
import os
import pkgutil
import sys


class RedirectStdStreams(object):
    """Context manager for redirecting stdout and stderr
    Reference:
        https://stackoverflow.com/a/6796752
        CC BY-SA 3.0, https://stackoverflow.com/users/46690/rob-cowie
    """

    def __init__(self, stdout=None, stderr=None):
        self._stdout = stdout or sys.stdout
        self._stderr = stderr or sys.stderr

    def __enter__(self):
        self.old_stdout, self.old_stderr = sys.stdout, sys.stderr
        self.old_stdout.flush()
        self.old_stderr.flush()
        sys.stdout, sys.stderr = self._stdout, self._stderr

    def __exit__(self, exc_type, exc_value, traceback):
        self._stdout.flush()
        self._stderr.flush()
        sys.stdout = self.old_stdout
        sys.stderr = self.old_stderr


class DisableOutput(RedirectStdStreams):
    def __init__(self):
        devnull = open(os.devnull, "w")
        super().__init__(stdout=devnull, stderr=devnull)


def import_submodules(package, recursive=True):
    """Import all submodules of a module, recursively, including subpackages

    :param package: package (name or actual module)
    :type package: str | module
    :rtype: dict[str, types.ModuleType]

    Reference:
        https://stackoverflow.com/a/25562415
        CC BY-SA 3.0, https://stackoverflow.com/users/712522/mr-b
    """
    if isinstance(package, str):
        package = importlib.import_module(package)
    results = {}
    for loader, name, is_pkg in pkgutil.walk_packages(package.__path__):
        full_name = package.__name__ + "." + name
        results[full_name] = importlib.import_module(full_name)
        if recursive and is_pkg:
            results.update(import_submodules(full_name))
    return results


--- File: deeplabcut/benchmark/benchmarks.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

"""Definition for official DeepLabCut benchmark tasks.

See benchmark.deeplabcut.org for a current leaderboard with models and metrics
for each of these benchmarks. Submissions can be done by opening a PR in the
benchmark reporistory:

https://github.com/DeepLabCut/benchmark
"""

import deeplabcut.benchmark.base


class TriMouseBenchmark(deeplabcut.benchmark.base.Benchmark):
    """Datasets with three mice with a top-view camera.

    Three wild-type (C57BL/6J) male mice ran on a paper spool following odor trails (Mathis et al 2018). These experiments were carried out in the laboratory of Venkatesh N. Murthy at Harvard University. Data were recorded at 30 Hz with 640 x 480 pixels resolution acquired with a Point Grey Firefly FMVU-03MTM-CS. One human annotator was instructed to localize the 12 keypoints (snout, left ear, right ear, shoulder, four spine points, tail base and three tail points). All surgical and experimental procedures for mice were in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and approved by the Harvard Institutional Animal Care and Use Committee. 161 frames were labeled, making this a real-world sized laboratory dataset.

    Introduced in Lauer et al. "Multi-animal pose estimation, identification and tracking with DeepLabCut." Nature Methods 19, no. 4 (2022): 496-504.
    """

    name = "trimouse"
    keypoints = (
        "snout",
        "leftear",
        "rightear",
        "shoulder",
        "spine1",
        "spine2",
        "spine3",
        "spine4",
        "tailbase",
        "tail1",
        "tail2",
        "tailend",
    )
    ground_truth = deeplabcut.benchmark.get_filepath("CollectedData_Daniel.h5")
    metadata = deeplabcut.benchmark.get_filepath(
        "Documentation_data-MultiMouse_70shuffle1.pickle"
    )
    num_animals = 3


class ParentingMouseBenchmark(deeplabcut.benchmark.base.Benchmark):
    """Datasets with three mice, one parenting, two pups.

    Parenting behavior is a pup directed behavior observed in adult mice involving complex motor actions directed towards the benefit of the offspring. These experiments were carried out in the laboratory of Catherine Dulac at Harvard University. The behavioral assay was performed in the homecage of singly housed adult female mice in dark/red light conditions. For these videos, the adult mice was monitored for several minutes in the cage followed by the introduction of pup (4 days old) in one corner of the cage. The behavior of the adult and pup was monitored for a duration of 15 minutes. Video was recorded at 30Hz using a Microsoft LifeCam camera (Part#: 6CH-00001) with a resolution of 1280 x 720 pixels or a Geovision camera (model no.: GV-BX4700-3V) also acquired at 30 frames per second at a resolution of 704 x 480 pixels. A human annotator labeled on the adult animal the same 12 body points as in the tri-mouse dataset, and five body points on the pup along its spine. Initially only the two ends were labeled, and intermediate points were added by interpolation and their positions was manually adjusted if necessary. All surgical and experimental procedures for mice were in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and approved by the Harvard Institutional Animal Care and Use Committee. 542 frames were labeled, making this a real-world sized laboratory dataset.

    Introduced in Lauer et al. "Multi-animal pose estimation, identification and tracking with DeepLabCut." Nature Methods 19, no. 4 (2022): 496-504.
    """

    name = "parenting"
    keypoints = (
        "end1",
        "interm1",
        "interm2",
        "interm3",
        "end2",
        "snout",
        "leftear",
        "rightear",
        "shoulder",
        "spine1",
        "spine2",
        "spine3",
        "spine4",
        "tailbase",
        "tail1",
        "tail2",
        "tailend",
    )

    ground_truth = deeplabcut.benchmark.get_filepath("CollectedData_Mostafizur.h5")
    metadata = deeplabcut.benchmark.get_filepath(
        "Documentation_data-CrackingParenting_70shuffle1.pickle"
    )
    num_animals = 2

    def compute_pose_map(self, results_objects):
        return deeplabcut.benchmark.metrics.calc_map_from_obj(
            results_objects,
            h5_file=self.ground_truth,
            metadata_file=self.metadata,
            oks_sigma=0.15,
            margin=10,
            symmetric_kpts=[(0, 4), (1, 3)],
        )

    def _validate_predictions(self, name: str, predictions: dict) -> dict:
        """Fixes filenames for predictions made on old versions of the dataset"""
        return super()._validate_predictions(
            name,
            {
                k.replace("Dummy", "D").replace("Dead pup", "DP"): v
                for k, v in predictions.items()
            },
        )


class MarmosetBenchmark(deeplabcut.benchmark.base.Benchmark):
    """Dataset with two marmosets.

    All animal procedures are overseen by veterinary staff of the MIT and Broad Institute Department of Comparative Medicine, in compliance with the NIH guide for the care and use of laboratory animals and approved by the MIT and Broad Institute animal care and use committees. Video of common marmosets (Callithrix jacchus) was collected in the laboratory of Guoping Feng at MIT. Marmosets were recorded using Kinect V2 cameras (Microsoft) with a resolution of 1080p and frame rate of 30 Hz. After acquisition, images to be used for training the network were manually cropped to 1000 x 1000 pixels or smaller. The dataset is 7,600 labeled frames from 40 different marmosets collected from 3 different colonies (in different facilities). Each cage contains a pair of marmosets, where one marmoset had light blue dye applied to its tufts. One human annotator labeled the 15 marker points on each animal present in the frame (frames contained either 1 or 2 animals).

    Introduced in Lauer et al. "Multi-animal pose estimation, identification and tracking with DeepLabCut." Nature Methods 19, no. 4 (2022): 496-504.
    """

    name = "marmosets"
    keypoints = (
        "Front",
        "Right",
        "Middle",
        "Left",
        "FL1",
        "BL1",
        "FR1",
        "BR1",
        "BL2",
        "BR2",
        "FL2",
        "FR2",
        "Body1",
        "Body2",
        "Body3",
    )
    ground_truth = deeplabcut.benchmark.get_filepath("CollectedData_Mackenzie.h5")
    metadata = deeplabcut.benchmark.get_filepath(
        "Documentation_data-Marmoset_70shuffle1.pickle"
    )
    num_animals = 2


class FishBenchmark(deeplabcut.benchmark.base.Benchmark):
    """Dataset with multiple fish, filmed from top-view

    Schools of inland silversides (Menidia beryllina, n=14 individuals per school) were recorded in the Lauder Lab at Harvard University while swimming at 15 speeds (0.5 to 8 BL/s, body length, at 0.5 BL/s intervals) in a flow tank with a total working section of 28 x 28 x 40 cm as described in previous work, at a constant temperature (18±1°C) and salinity (33 ppt), at a Reynolds number of approximately 10,000 (based on BL). Dorsal views of steady swimming across these speeds were recorded by high-speed video cameras (FASTCAM Mini AX50, Photron USA, San Diego, CA, USA) at 60-125 frames per second (feeding videos at 60 fps, swimming alone 125 fps). The dorsal view was recorded above the swim tunnel and a floating Plexiglas panel at the water surface prevented surface ripples from interfering with dorsal view videos. Five keypoints were labeled (tip, gill, peduncle, dorsal fin tip, caudal tip). 100 frames were labeled, making this a real-world sized laboratory dataset.

    Introduced in Lauer et al. "Multi-animal pose estimation, identification and tracking with DeepLabCut." Nature Methods 19, no. 4 (2022): 496-504.
    """

    name = "fish"
    keypoints = ("tip", "gill", "peduncle", "caudaltip", "dfintip")
    ground_truth = deeplabcut.benchmark.get_filepath("CollectedData_Valentina.h5")
    metadata = deeplabcut.benchmark.get_filepath(
        "Documentation_data-Schooling_70shuffle1.pickle"
    )
    num_animals = 14

    def compute_pose_rmse(self, results_objects):
        return deeplabcut.benchmark.metrics.calc_rmse_from_obj(
            results_objects,
            h5_file=self.ground_truth,
            metadata_file=self.metadata,
            drop_kpts=[4, 5],
        )

    def compute_pose_map(self, results_objects):
        return deeplabcut.benchmark.metrics.calc_map_from_obj(
            results_objects,
            h5_file=self.ground_truth,
            metadata_file=self.metadata,
            drop_kpts=[4, 5],
        )


--- File: deeplabcut/benchmark/__main__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from deeplabcut.benchmark.cli import main

if __name__ == "__main__":
    main()


--- File: deeplabcut/benchmark/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

"""Base classes for benchmark and result definition

Benchmarks subclass the abstract ``Benchmark`` class and are defined by ``name``, their
``keypoints`` names, as well as groundtruth and metadata necessary to run evaluation.
Right now, the metrics to compute and report for each of the multi-animal benchmarks is the
root mean-squared-error (RMSE) and the mean average precision (mAP).

Note for contributors: If you decide to contribute a benchmark which does not fit
into this evaluation framework, please feel free to extend the base classes
(e.g. to support additional metrics).
"""

import abc
import dataclasses
import warnings
from typing import Iterable
from typing import Tuple

import pandas as pd

import deeplabcut.benchmark.metrics
from deeplabcut import __version__


class BenchmarkEvaluationError(RuntimeError):
    pass


class Benchmark(abc.ABC):
    """Abstract benchmark baseclass.

    All benchmarks should subclass this class.
    """

    @abc.abstractmethod
    def names(self):
        """A unique key to describe this submission, e.g. the model name.

        This is also the name that will later appear in the benchmark table.
        The name needs to be unique across the whole benchmark. Non-unique names
        will raise an error during submission of a PR.
        """
        raise NotImplementedError()

    @abc.abstractmethod
    def get_predictions(self):
        """Return predictions for all images in the benchmark."""
        raise NotImplementedError()

    def __init__(self):
        keys = ["code", "name", "keypoints", "ground_truth", "metadata"]
        for key in keys:
            if not hasattr(self, key):
                raise NotImplementedError(
                    f"Subclass of abstract Benchmark class need "
                    f"to define the {key} property."
                )

    def compute_pose_rmse(self, results_objects):
        return deeplabcut.benchmark.metrics.calc_rmse_from_obj(
            results_objects, h5_file=self.ground_truth, metadata_file=self.metadata
        )

    def compute_pose_map(self, results_objects):
        return deeplabcut.benchmark.metrics.calc_map_from_obj(
            results_objects, h5_file=self.ground_truth, metadata_file=self.metadata
        )

    def evaluate(self, name: str, on_error="raise"):
        """Evaluate this benchmark with all registered methods."""

        if name not in self.names():
            raise ValueError(
                f"{name} is not registered. Valid names are {self.names()}"
            )
        if on_error not in ("ignore", "return", "raise"):
            raise ValueError(f"on_error got an undefined value: {on_error}")
        mean_avg_precision = float("nan")
        root_mean_squared_error = float("nan")
        try:
            predictions = self.get_predictions(name)
            predictions = self._validate_predictions(name, predictions)
            mean_avg_precision = self.compute_pose_map(predictions)
            root_mean_squared_error = self.compute_pose_rmse(predictions)
        except Exception as exception:
            if on_error == "ignore":
                # ignore the exception and continue with the next evaluation, without
                # yielding a result value.
                return
            elif on_error == "return":
                # return the result value, with NaN as the result for all metrics that
                # could not be computed due to the error.
                pass
            elif on_error == "raise":
                # raise the error and stop evaluation
                raise BenchmarkEvaluationError(
                    f"Error during benchmark evaluation for model {name}"
                ) from exception
            else:
                raise NotImplementedError() from exception
        return Result(
            code=self.code,
            method_name=name,
            benchmark_name=self.name,
            mean_avg_precision=mean_avg_precision,
            root_mean_squared_error=root_mean_squared_error,
        )

    def _validate_predictions(self, name: str, predictions: dict) -> dict:
        """Validates the submitted predictions object
        Checks that there is a prediction for each test image, and raises a warning if
        that is not the case. Returns only predictions made for test images.
        """
        test_images = deeplabcut.benchmark.metrics.load_test_images(
            self.ground_truth, self.metadata
        )
        missing_images = set(test_images) - set(predictions.keys())
        if len(missing_images) > 0:
            warnings.warn(
                f"Missing {len(missing_images)} test images in the predictions for "
                f"{name}: {list(missing_images)} Metrics will be computed as if no "
                "individuals were detected in those images."
            )

        return {img: predictions.get(img, tuple()) for img in test_images}


@dataclasses.dataclass
class Result:
    """Benchmark result."""

    code: str
    method_name: str
    benchmark_name: str
    root_mean_squared_error: float = float("nan")
    mean_avg_precision: float = float("nan")
    benchmark_version: str = __version__

    _export_mapping = dict(
        code="code",
        benchmark_name="benchmark",
        method_name="method",
        benchmark_version="version",
        root_mean_squared_error="RMSE",
        mean_avg_precision="mAP",
    )

    _primary_key = ("benchmark_name", "method_name", "benchmark_version")

    @property
    def primary_key(self) -> Tuple[str]:
        """The primary key to uniquely identify this result."""
        return tuple(getattr(self, k) for k in self._primary_key)

    @property
    def primary_key_names(self) -> Tuple[str]:
        """Names of the primary keys"""
        return tuple(self._export_mapping.get(k) for k in self._primary_key)

    def __str__(self):
        return (
            f"{self.method_name}, {self.benchmark_name}: "
            f"{self.mean_avg_precision} mAP, "
            f"{self.root_mean_squared_error} RMSE"
        )

    @classmethod
    def fromdict(cls, data: dict):
        """Construct result object from dictionary."""
        kwargs = {attr: data[key] for attr, key in cls._export_mapping.items()}
        return cls(**kwargs)

    def todict(self) -> dict:
        """Export result object to dictionary, with less verbose key names."""
        return {key: getattr(self, attr) for attr, key in self._export_mapping.items()}


class ResultCollection:
    def __init__(self, *results):
        self.results = {result.primary_key: result for result in results}

    @property
    def primary_key_names(self):
        return next(iter(self.results.values())).primary_key_names

    def toframe(self) -> pd.DataFrame:
        """Convert results to pandas dataframe"""
        return pd.DataFrame(
            [result.todict() for result in self.results.values()]
        ).set_index(list(self.primary_key_names))

    def add(self, result: Result):
        """Add a result to the collection."""
        if result.primary_key in self.results:
            raise ValueError(
                "An entry for {result.primary_key} does already "
                "exist in this collection. Did you try to add the "
                "same result twice?"
            )
        if len(self) > 0:
            if result.primary_key_names != self.primary_key_names:
                raise ValueError("Incompatible result format.")
        self.results[result.primary_key] = result

    @classmethod
    def fromdicts(cls, data: Iterable[dict]):
        return cls(*[Result.fromdict(entry) for entry in data])

    def todicts(self):
        return [result.todict() for result in self.results.values()]

    def __len__(self):
        return len(self.results)

    def __contains__(self, other: Result):
        if not isinstance(other, Result):
            raise ValueError(
                f"{type(self)} can only store objects of type Result, "
                f"but got {type(other)}."
            )
        return other.primary_key in self.results

    def __eq__(self, other):
        if not isinstance(other, ResultCollection):
            return False
        return other.results == self.results


--- File: deeplabcut/post_processing/filtering.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import argparse
from pathlib import Path

import numpy as np
import pandas as pd
from scipy import signal
from scipy.interpolate import CubicSpline

from deeplabcut.refine_training_dataset.outlier_frames import FitSARIMAXModel
from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal


def columnwise_spline_interp(data, max_gap=0):
    """
    Perform cubic spline interpolation over the columns of *data*.
    All gaps of size lower than or equal to *max_gap* are filled,
    and data slightly smoothed.

    Parameters
    ----------
    data : array_like
        2D matrix of data.
    max_gap : int, optional
        Maximum gap size to fill. By default, all gaps are interpolated.

    Returns
    -------
    interpolated data with same shape as *data*
    """
    if np.ndim(data) < 2:
        data = np.expand_dims(data, axis=1)
    nrows, ncols = data.shape
    temp = data.copy()
    valid = ~np.isnan(temp)
    x = np.arange(nrows)
    for i in range(ncols):
        mask = valid[:, i]
        if (
            np.sum(mask) > 3
        ):  # Make sure there are enough points to fit the cubic spline
            spl = CubicSpline(x[mask], temp[mask, i])
            y = spl(x)
            if max_gap > 0:
                inds = np.flatnonzero(np.r_[True, np.diff(mask), True])
                count = np.diff(inds)
                inds = inds[:-1]
                to_fill = np.ones_like(mask)
                for ind, n, is_nan in zip(inds, count, ~mask[inds]):
                    if is_nan and n > max_gap:
                        to_fill[ind : ind + n] = False
                y[~to_fill] = np.nan
            # Get rid of the interpolation beyond the spline knots
            y[y == 0] = np.nan
            temp[:, i] = y
    return temp


def filterpredictions(
    config,
    video,
    videotype="",
    shuffle=1,
    trainingsetindex=0,
    filtertype="median",
    windowlength=5,
    p_bound=0.001,
    ARdegree=3,
    MAdegree=1,
    alpha=0.01,
    save_as_csv=True,
    destfolder=None,
    modelprefix="",
    track_method="",
    return_data=False,
):
    """Fits frame-by-frame pose predictions.

    The pose predictions are fitted with ARIMA model (filtertype='arima') or median
    filter (default).

    Parameters
    ----------
    config : string
        Full path of the config.yaml file.

    video : string
        Full path of the video to extract the frame from. Make sure that this video is
        already analyzed.

    shuffle : int, optional, default=1
        The shuffle index of training dataset. The extracted frames will be stored in
        the labeled-dataset for the corresponding shuffle of training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    filtertype: string, optional, default="median".
        The filter type - 'arima', 'median' or 'spline'.

    windowlength: int, optional, default=5
        For filtertype='median' filters the input array using a local window-size given
        by windowlength. The array will automatically be zero-padded.
        https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.medfilt.html.
        The windowlenght should be an odd number.
        If filtertype='spline', windowlength is the maximal gap size to fill.

    p_bound: float between 0 and 1, optional, default=0.001
        For filtertype 'arima' this parameter defines the likelihood below,
        below which a body part will be consided as missing data for filtering purposes.

    ARdegree: int, optional, default=3
        For filtertype 'arima' Autoregressive degree of Sarimax model degree.
        see https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html

    MAdegree: int, optional, default=1
        For filtertype 'arima' Moving Average degree of Sarimax model degree.
        See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html

    alpha: float, optional, default=0.01
        Significance level for detecting outliers based on confidence interval of fitted SARIMAX model.

    save_as_csv: bool, optional, default=True
        Saves the predictions in a .csv file.

    destfolder: string, optional, default=None
        Specifies the destination folder for analysis data. If ``None``, the path of
        the video is used by default. Note that for subsequent analysis this folder
        also needs to be passed.

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    track_method: string, optional, default=""
        Specifies the tracker used to generate the data.
        Empty by default (corresponding to a single animal project).
        For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
        be taken from the config.yaml file if none is given.

    return_data: bool, optional, default=False
        If True, returns a dictionary of the filtered data keyed by video names.

    Returns
    -------
    video_to_filtered_df
        Dictionary mapping video filepaths to filtered dataframes.

        * If no videos exist, the dictionary will be empty.
        * If a video is not analyzed, the corresponding value in the dictionary will be
          None.

    Examples
    --------

    Arima model:

    >>> deeplabcut.filterpredictions(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\myproject\\trailtracking-task\\test.mp4'],
            shuffle=3,
            filterype='arima',
            ARdegree=5,
            MAdegree=2,
        )

    Use median filter over 10 bins:

    >>> deeplabcut.filterpredictions(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\myproject\\trailtracking-task\\test.mp4'],
            shuffle=3,
            windowlength=10,
        )

    One can then use the filtered rather than the frame-by-frame predictions by calling:

    >>> deeplabcut.plot_trajectories(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\myproject\\trailtracking-task\\test.mp4'],
            shuffle=3,
            filtered=True,
        )

    >>> deeplabcut.create_labeled_video(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\myproject\\trailtracking-task\\test.mp4'],
            shuffle=3,
            filtered=True,
        )
    """
    cfg = auxiliaryfunctions.read_config(config)
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)

    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
        cfg,
        shuffle,
        trainFraction=cfg["TrainingFraction"][trainingsetindex],
        modelprefix=modelprefix,
    )
    Videos = auxiliaryfunctions.get_list_of_videos(video, videotype)

    video_to_filtered_df = {}

    if not len(Videos):
        print("No video(s) were found. Please check your paths and/or 'videotype'.")
        if return_data:
            return video_to_filtered_df

    for video in Videos:
        if destfolder is None:
            destfolder = str(Path(video).parents[0])

        print("Filtering with %s model %s" % (filtertype, video))
        vname = Path(video).stem

        try:
            df, filepath, _, _ = auxiliaryfunctions.load_analyzed_data(
                destfolder, vname, DLCscorer, True, track_method
            )
            print(f"Data from {vname} were already filtered. Skipping...")
            video_to_filtered_df[video] = df
            # Data has been filtered so continue to the next video
            continue
        except FileNotFoundError:
            pass

        # Data haven't been filtered yet
        try:
            df, filepath, _, _ = auxiliaryfunctions.load_analyzed_data(
                destfolder, vname, DLCscorer, track_method=track_method
            )
        except FileNotFoundError as e:
            video_to_filtered_df[video] = None
            print(e)
            continue

        nrows = df.shape[0]
        if filtertype == "arima":
            temp = df.values.reshape((nrows, -1, 3))
            placeholder = np.empty_like(temp)
            for i in range(temp.shape[1]):
                x, y, p = temp[:, i].T
                meanx, _ = FitSARIMAXModel(
                    x, p, p_bound, alpha, ARdegree, MAdegree, False
                )
                meany, _ = FitSARIMAXModel(
                    y, p, p_bound, alpha, ARdegree, MAdegree, False
                )
                meanx[0] = x[0]
                meany[0] = y[0]
                placeholder[:, i] = np.c_[meanx, meany, p]
            data = pd.DataFrame(
                placeholder.reshape((nrows, -1)),
                columns=df.columns,
                index=df.index,
            )
        elif filtertype == "median":
            data = df.copy()
            mask = data.columns.get_level_values("coords") != "likelihood"
            data.loc[:, mask] = df.loc[:, mask].apply(
                signal.medfilt, args=(windowlength,), axis=0
            )
        elif filtertype == "spline":
            data = df.copy()
            mask_data = data.columns.get_level_values("coords").isin(("x", "y"))
            xy = data.loc[:, mask_data].values
            prob = data.loc[:, ~mask_data].values
            missing = np.isnan(xy)
            xy_filled = columnwise_spline_interp(xy, windowlength)
            filled = ~np.isnan(xy_filled)
            xy[filled] = xy_filled[filled]
            inds = np.argwhere(missing & filled)
            if inds.size:
                # Retrieve original individual label indices
                inds[:, 1] //= 2
                inds = np.unique(inds, axis=0)
                prob[inds[:, 0], inds[:, 1]] = 0.01
                data.loc[:, ~mask_data] = prob
            data.loc[:, mask_data] = xy
        else:
            raise ValueError(f"Unknown filter type {filtertype}")

        video_to_filtered_df[video] = data

        outdataname = filepath.replace(".h5", "_filtered.h5")
        data.to_hdf(outdataname, "df_with_missing", format="table", mode="w")
        if save_as_csv:
            print("Saving filtered csv poses!")
            data.to_csv(outdataname.split(".h5")[0] + ".csv")

    if return_data:
        return video_to_filtered_df


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config")
    parser.add_argument("videos")
    cli_args = parser.parse_args()


--- File: deeplabcut/post_processing/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""

from deeplabcut.post_processing.analyze_skeleton import analyzeskeleton
from deeplabcut.post_processing.filtering import *


--- File: deeplabcut/post_processing/analyze_skeleton.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

"""
Contributed by Federico Claudi - https://github.com/FedeClaudi
"""

import argparse
from math import atan2, degrees
from pathlib import Path
import os
import numpy as np
import pandas as pd
from scipy.spatial import distance

from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal


# utility functions
def calc_distance_between_points_two_vectors_2d(v1, v2):
    """calc_distance_between_points_two_vectors_2d [pairwise distance between vectors points]

    Arguments:
        v1 {[np.array]} -- [description]
        v2 {[type]} -- [description]

    Raises:
        ValueError -- [description]
        ValueError -- [description]
        ValueError -- [description]

    Returns:
        [type] -- [description]

    testing:
    >>> v1 = np.zeros((2, 5))
    >>> v2 = np.zeros((2, 5))
    >>> v2[1, :]  = [0, 10, 25, 50, 100]
    >>> d = calc_distance_between_points_two_vectors_2d(v1.T, v2.T)
    """

    # Check dataformats
    if not isinstance(v1, np.ndarray) or not isinstance(v2, np.ndarray):
        raise ValueError("Invalid argument data format")
    if not v1.shape[1] == 2 or not v2.shape[1] == 2:
        raise ValueError("Invalid shape for input arrays")
    if not v1.shape[0] == v2.shape[0]:
        raise ValueError("Error: input arrays should have the same length")

    # Calculate distance
    dist = [distance.euclidean(p1, p2) for p1, p2 in zip(v1, v2)]
    return dist


def angle_between_points_2d_anticlockwise(p1, p2):
    """angle_between_points_2d_clockwise [Determines the angle of a straight line drawn between point one and two.
        The number returned, which is a double in degrees, tells us how much we have to rotate
        a horizontal line anti-clockwise for it to match the line between the two points.]

    Arguments:
        p1 {[np.ndarray, list]} -- np.array or list [ with the X and Y coordinates of the point]
        p2 {[np.ndarray, list]} -- np.array or list [ with the X and Y coordinates of the point]

    Returns:
        [int] -- [clockwise angle between p1, p2 using the inner product and the deterinant of the two vectors]

    Testing:  - to check:     print(zero, ninety, oneeighty, twoseventy)
        >>> zero = angle_between_points_2d_clockwise([0, 1], [0, 1])
        >>> ninety = angle_between_points_2d_clockwise([1, 0], [0, 1])
        >>> oneeighty = angle_between_points_2d_clockwise([0, -1], [0, 1])
        >>> twoseventy = angle_between_points_2d_clockwise([-1, 0], [0, 1])
        >>> ninety2 = angle_between_points_2d_clockwise([10, 0], [10, 1])
        >>> print(ninety2)
    """

    """
        Determines the angle of a straight line drawn between point one and two.
        The number returned, which is a double in degrees, tells us how much we have to rotate
        a horizontal line anit-clockwise for it to match the line between the two points.
    """

    xDiff = p2[0] - p1[0]
    yDiff = p2[1] - p1[1]
    ang = degrees(atan2(yDiff, xDiff))
    if ang < 0:
        ang += 360
    # if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly')
    return ang


def calc_angle_between_vectors_of_points_2d(v1, v2):
    """calc_angle_between_vectors_of_points_2d [calculates the clockwise angle between each set of point for two 2d arrays of points]

    Arguments:
        v1 {[np.ndarray]} -- [2d array with X,Y position at each timepoint]
        v2 {[np.ndarray]} -- [2d array with X,Y position at each timepoint]

    Returns:
        [np.ndarray] -- [1d array with clockwise angle between pairwise points in v1,v2]

    Testing:
    >>> v1 = np.zeros((2, 4))
    >>> v1[1, :] = [1, 1, 1, 1, ]
    >>> v2 = np.zeros((2, 4))
    >>> v2[0, :] = [0, 1, 0, -1]
    >>> v2[1, :] = [1, 0, -1, 0]
    >>> a = calc_angle_between_vectors_of_points_2d(v2, v1)
    """

    # Check data format
    if (
        v1 is None
        or v2 is None
        or not isinstance(v1, np.ndarray)
        or not isinstance(v2, np.ndarray)
    ):
        raise ValueError("Invalid format for input arguments")
    if len(v1) != len(v2):
        raise ValueError(
            "Input arrays should have the same length, instead: ", len(v1), len(v2)
        )
    if not v1.shape[0] == 2 or not v2.shape[0] == 2:
        raise ValueError("Invalid shape for input arrays: ", v1.shape, v2.shape)

    # Calculate
    n_points = v1.shape[1]
    angs = np.zeros(n_points)
    for i in range(v1.shape[1]):
        p1, p2 = v1[:, i], v2[:, i]
        angs[i] = angle_between_points_2d_anticlockwise(p1, p2)

    return angs


# Process single bone
def analyzebone(bp1, bp2):
    """[Computes length and orientation of the bone at each frame]

    Arguments:
        bp1 {[type]} -- [description]
        bp2 {[type]} -- [description]
    """
    bp1_pos = np.vstack([bp1.x.values, bp1.y.values]).T
    bp2_pos = np.vstack([bp2.x.values, bp2.y.values]).T

    # get bone length and orientation
    bone_length = calc_distance_between_points_two_vectors_2d(bp1_pos, bp2_pos)
    bone_orientation = calc_angle_between_vectors_of_points_2d(bp1_pos.T, bp2_pos.T)

    # keep the smallest of the two likelihoods
    likelihoods = np.vstack([bp2.likelihood.values, bp2.likelihood.values]).T
    likelihood = np.min(likelihoods, 1)

    # Create dataframe and return
    df = pd.DataFrame.from_dict(
        dict(length=bone_length, orientation=bone_orientation, likelihood=likelihood)
    )
    # df.index.name=name

    return df


# MAIN FUNC
def analyzeskeleton(
    config,
    videos,
    videotype="",
    shuffle=1,
    trainingsetindex=0,
    filtered=False,
    save_as_csv=False,
    destfolder=None,
    modelprefix="",
    track_method="",
    return_data=False,
):
    """Extracts length and orientation of each "bone" of the skeleton.

    The bone and skeleton information is defined in the config file.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos: list[str]
        The full paths to videos for analysis or a path to the directory, where all the
        videos with same extension are stored.

    videotype: str, optional, default=""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions
        ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle : int, optional, default=1
        The shuffle index of training dataset. The extracted frames will be stored in
        the labeled-dataset for the corresponding shuffle of training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    filtered: bool, optional, default=False
        Boolean variable indicating if filtered output should be plotted rather than
        frame-by-frame predictions. Filtered version can be calculated with
        ``deeplabcut.filterpredictions``.

    save_as_csv: bool, optional, default=False
        Saves the predictions in a .csv file.

    destfolder: string or None, optional, default=None
        Specifies the destination folder for analysis data. If ``None``, the path of
        the video is used. Note that for subsequent analysis this folder also needs to
        be passed.

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    track_method: string, optional, default=""
        Specifies the tracker used to generate the data.
        Empty by default (corresponding to a single animal project).
        For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
        be taken from the config.yaml file if none is given.

    return_data: bool, optional, default=False
        If True, returns a dictionary of the filtered data keyed by video names.

    Returns
    -------
    video_to_skeleton_df
        Dictionary mapping video filepaths to skeleton dataframes.

        * If no videos exist, the dictionary will be empty.
        * If a video is not analyzed, the corresponding value in the dictionary will be
          None.
    """
    # Load config file, scorer and videos
    cfg = auxiliaryfunctions.read_config(config)
    if not cfg["skeleton"]:
        raise ValueError("No skeleton defined in the config.yaml.")

    video_to_skeleton_df = {}

    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)
    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
        cfg,
        shuffle,
        trainFraction=cfg["TrainingFraction"][trainingsetindex],
        modelprefix=modelprefix,
    )

    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    for video in Videos:
        print("Processing %s" % (video))
        if destfolder is None:
            destfolder = str(Path(video).parents[0])

        vname = Path(video).stem
        try:
            df, filepath, scorer, _ = auxiliaryfunctions.load_analyzed_data(
                destfolder, vname, DLCscorer, filtered, track_method
            )
        except FileNotFoundError as e:
            print(e)
            video_to_skeleton_df[video] = None
            continue

        output_name = filepath.replace(".h5", f"_skeleton.h5")
        if os.path.isfile(output_name):
            print(f"Skeleton in video {vname} already processed. Skipping...")
            video_to_skeleton_df[video] = pd.read_hdf(output_name, "df_with_missing")
            continue

        bones = {}
        if "individuals" in df.columns.names:
            for animal_name, df_ in df.groupby(level="individuals", axis=1):
                temp = df_.droplevel(["scorer", "individuals"], axis=1)
                if animal_name != "single":
                    for bp1, bp2 in cfg["skeleton"]:
                        name = "{}_{}_{}".format(animal_name, bp1, bp2)
                        bones[name] = analyzebone(temp[bp1], temp[bp2])
        else:
            for bp1, bp2 in cfg["skeleton"]:
                name = "{}_{}".format(bp1, bp2)
                bones[name] = analyzebone(df[scorer][bp1], df[scorer][bp2])

        skeleton = pd.concat(bones, axis=1)
        video_to_skeleton_df[video] = skeleton
        skeleton.to_hdf(output_name, "df_with_missing", format="table", mode="w")
        if save_as_csv:
            skeleton.to_csv(output_name.replace(".h5", ".csv"))

    if return_data:
        return video_to_skeleton_df


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config")
    parser.add_argument("videos")
    cli_args = parser.parse_args()


--- File: deeplabcut/generate_training_dataset/metadata.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""File containing methods to load and parse shuffle metadata"""
from __future__ import annotations

import logging
import pickle
import re
from dataclasses import dataclass
from pathlib import Path

import numpy as np
from ruamel.yaml import YAML

from deeplabcut.core.engine import Engine
from deeplabcut.utils import auxiliaryfunctions


@dataclass(frozen=True)
class DataSplit:
    """Class representing the metadata for a shuffle"""
    train_indices: tuple[int, ...]
    test_indices: tuple[int, ...]

    def __post_init__(self) -> None:
        """
        Raises:
            ValueError if the indices are not sorted in increasing
        """
        for indices in [self.train_indices, self.test_indices]:
            idx = np.array(indices)
            if not np.all(idx[:-1] < idx[1:]):
                raise RuntimeError(
                    f"The training and test indices in a data split must be sorted in "
                    f"strictly ascending order."
                )


@dataclass(frozen=True)
class ShuffleMetadata:
    """Class representing the metadata for a shuffle"""
    name: str
    train_fraction: float
    index: int
    engine: Engine
    split: DataSplit | None

    def load_split(self, cfg: dict, trainset_path: Path) -> "ShuffleMetadata":
        """Loads the data split for this shuffle

        Args:
            cfg: the config for the DeepLabCut project
            trainset_path: the path to the training dataset folder

        Returns:
            a new instance with the data split defined
        """
        _, doc_path = auxiliaryfunctions.get_data_and_metadata_filenames(
            trainset_path, self.train_fraction, self.index, cfg
        )
        if not Path(doc_path).exists():
            raise ValueError(
                f"Could not load the metadata file for {self} as {doc_path} does not "
                f"exist. If you deleted the shuffle, you also need to delete the "
                f"shuffle from metadata.yaml or recreate the metadata.yaml file."
            )

        with open(doc_path, "rb") as f:
            _, train_idx, test_idx, _ = pickle.load(f)
        return ShuffleMetadata(
            name=self.name,
            train_fraction=self.train_fraction,
            index=self.index,
            engine=self.engine,
            split=DataSplit(
                train_indices=tuple(sorted([int(idx) for idx in train_idx])),
                test_indices=tuple(sorted([int(idx) for idx in test_idx])),
            )
        )


@dataclass(frozen=True)
class TrainingDatasetMetadata:
    """An immutable class containing the metadata for a dataset

    When creating a new "training-datasets" folder (e.g., when creating the first
    training set for a project, or when creating the first training for a given
    iteration of a project), TrainingDatasetMetadata.create(cfg) should be called when
    the "training-datasets" folder is still empty.

    For existing projects (created with DeepLabCut < 3.0), calling
    TrainingDatasetMetadata.create(cfg) will go over documentation data for all existing
    shuffles in the training-datasets folder and add them to a new metadata instance.
    All shuffles will be given Engine.TF as an engine.

    Examples:
        # Creating the metadata file for an existing project
        config = "/data/my-dlc-project/config.yaml"
        trainset_metadata = TrainingDatasetMetadata.create(config)
        trainset_metadata.save()

        # Adding a new shuffle to the metadata file
        config = "/data/my-dlc-project-2008-06-17/config.yaml"
        trainset_metadata = TrainingDatasetMetadata.load(config)
        new_shuffle = ShuffleMetadata(
            name="my-dlc-projectJun17-trainset60shuffle5",
            train_fraction=0.6,
            index=5,
            engine=compat.Engine.PYTORCH,
            split=DataSplit(train_indices=(1, 3, 4), test_indices=(0, 2)),
        )
        trainset_metadata = trainset_metadata.add(new_shuffle)
        trainset_metadata.save()  # saves to disk
    """
    project_config: dict
    shuffles: tuple[ShuffleMetadata, ...]
    file_header: tuple[str] = (
        "# This file is automatically generated - DO NOT EDIT",
        "# It contains the information about the shuffles created for the dataset",
        "---",
    )

    def __post_init__(self) -> None:
        """
        Raises:
            ValueError if the indices are not sorted in increasing order
        """
        indices = [[s.train_fraction, s.index] for s in self.shuffles]
        for (frac1, idx1), (frac2, idx2) in zip(indices[:-1], indices[1:]):
            if not (frac1 < frac2 or (frac1 == frac2 and idx1 < idx2)):
                raise RuntimeError(
                    "The shuffles given must be sorted in order of ascending training "
                    f"fraction and index. Found {self.shuffles}"
                )

    def add(
        self,
        shuffle: ShuffleMetadata,
        overwrite: bool = False,
    ) -> TrainingDatasetMetadata:
        """
        Adds a new shuffle to the metadata file

        Args:
            shuffle: the shuffle to add
            overwrite: if a shuffle with the same index is already stored in the
                metadata file, whether to overwrite it

        Returns:
            A new instance of TrainingDatasetMetadata with updated shuffles

        Raises:
            ValueError: if overwrite=False and there is already a shuffle with the given
                index in the metadata file.
        """
        existing_indices = [
            s.index for s in self.shuffles if s.train_fraction == shuffle.train_fraction
        ]
        if shuffle.index in existing_indices:
            if not overwrite:
                raise RuntimeError(
                    f"Cannot add {shuffle} to the meta: a shuffle with index "
                    f"{shuffle.index} and train_fraction {shuffle.train_fraction} "
                    f"already exists: {self.shuffles}."
                )

        existing_shuffles = [
            s
            for s in self.shuffles
            if (s.index != shuffle.index or s.train_fraction != shuffle.train_fraction)
        ]
        shuffles = existing_shuffles + [shuffle]
        return TrainingDatasetMetadata(
            project_config=self.project_config,
            shuffles=tuple(sorted(shuffles, key=lambda s: (s.train_fraction, s.index))),
        )

    def get(self, trainset_index: int = 0, index: int = 0) -> ShuffleMetadata:
        """
        Args:
            trainset_index: the index of the trainset fraction as defined in config.yaml
            index: the index of the shuffle

        Returns:
            the shuffle with the given trainset index and shuffle index

        Raises:
            ValueError if the shuffle is not present in the metadata
        """
        train_fraction = self.project_config["TrainingFraction"][trainset_index]
        for shuffle in self.shuffles:
            if (
                shuffle.train_fraction == train_fraction
                and shuffle.index == index
            ):
                return shuffle

        raise ValueError(
            f"Could not find a shuffle with trainingset fraction {train_fraction} and "
            f"index {index}"
        )

    def save(self) -> None:
        """Saves the training dataset metadata to disk"""
        metadata = {"shuffles": {}}
        data_splits: dict[DataSplit, int] = {}
        trainset_path = self.path(self.project_config).parent
        for s in self.shuffles:
            if s.split is None:
                s = s.load_split(cfg=self.project_config, trainset_path=trainset_path)

            split_index = data_splits.get(s.split)
            if split_index is None:
                split_index = len(data_splits) + 1
                data_splits[s.split] = split_index

            metadata["shuffles"][s.name] = {
                "train_fraction": s.train_fraction,
                "index": s.index,
                "split": split_index,
                "engine": s.engine.aliases[0],
            }

        with open(self.path(self.project_config), "w") as file:
            file.write("\n".join(self.file_header) + "\n")
            YAML().dump(metadata, file)

    @staticmethod
    def load(
        config: str | Path | dict,
        load_splits: bool = False,
    ) -> TrainingDatasetMetadata:
        """Loads the metadata from disk

        Args:
            config: the config for the DeepLabCut project (or its path)
            load_splits: whether to load the data split for each shuffle
        """
        if isinstance(config, (str, Path)):
            cfg = auxiliaryfunctions.read_config(config)
        else:
            cfg = config

        metadata_path = TrainingDatasetMetadata.path(cfg)
        with open(metadata_path, "r") as file:
            metadata = YAML(typ="safe", pure=True).load(file)

        shuffles = []
        for shuffle_name, shuffle_metadata in metadata["shuffles"].items():
            shuffle = ShuffleMetadata(
                name=shuffle_name,
                train_fraction=shuffle_metadata["train_fraction"],
                index=shuffle_metadata["index"],
                engine=Engine(shuffle_metadata["engine"]),
                split=None,
            )
            if load_splits:
                shuffle = shuffle.load_split(cfg, metadata_path.parent)

            shuffles.append(shuffle)

        shuffles.sort(key=lambda s: (s.train_fraction, s.index))
        return TrainingDatasetMetadata(project_config=cfg, shuffles=tuple(shuffles))

    @staticmethod
    def create(config: str | Path | dict) -> TrainingDatasetMetadata:
        """Function to create the metadata file

        Assumes that all existing shuffles use the TensorFlow engine, as this file
        should have already been created for PyTorch shuffles.

        Args;
            config: the config for the DeepLabCut project (or its path)
            default_engine: the default engine to set for shuffles in the project

        Returns:
            the metadata for the existing shuffles in the project
        """
        if isinstance(config, (str, Path)):
            cfg = auxiliaryfunctions.read_config(config)
        else:
            cfg = config

        trainset_path = TrainingDatasetMetadata.path(cfg).parent
        if trainset_path.exists():
            shuffle_docs = [
                f
                for f in trainset_path.iterdir()
                if re.match(r"Documentation_data-.+shuffle[0-9]+\.pickle", f.name)
            ]
        else:
            trainset_path.mkdir(parents=True)
            shuffle_docs = []

        prefix = cfg["Task"] + cfg["date"]
        shuffles = []
        existing_splits: dict[tuple[tuple[int, ...], tuple[int, ...]], int] = {}
        for doc_path in shuffle_docs:
            index = int(doc_path.stem.split("shuffle")[-1])
            with open(doc_path, "rb") as f:
                _, train_idx, test_idx, train_frac = pickle.load(f)

            engine = Engine.TF
            train_idx = tuple(sorted([int(idx) for idx in train_idx]))
            test_idx = tuple(sorted([int(idx) for idx in test_idx]))
            split_idx = existing_splits.get((train_idx, test_idx))
            if split_idx is None:
                split_idx = len(existing_splits) + 1
                existing_splits[(train_idx, test_idx)] = split_idx

            shuffles.append(
                ShuffleMetadata(
                    name=f"{prefix}-trainset{int(100 * train_frac)}shuffle{index}",
                    train_fraction=train_frac,
                    index=index,
                    engine=engine,
                    split=DataSplit(train_indices=train_idx, test_indices=test_idx),
                )
            )

        shuffles = tuple(sorted(shuffles, key=lambda s: (s.train_fraction, s.index)))
        return TrainingDatasetMetadata(
            project_config=cfg,
            shuffles=shuffles,
        )

    @staticmethod
    def path(cfg: dict) -> Path:
        """
        Args:
            cfg: the config for the DeepLabCut project

        Returns:
            the path to the training dataset metadata file
        """
        meta_path = auxiliaryfunctions.get_training_set_folder(cfg) / "metadata.yaml"
        return Path(cfg["project_path"]) / meta_path


def update_metadata(
    cfg: dict,
    train_fraction: float,
    shuffle: int,
    engine: Engine,
    train_indices: list[int],
    test_indices: list[int],
    overwrite: bool = False,
) -> None:
    """Updates the metadata for a training-dataset

    Args:
        cfg: the config for the DeepLabCut project
        train_fraction: the train_fraction of the new shuffle
        shuffle: the index of the shuffle to add
        engine: the engine for the shuffle
        train_indices: the indices of images in the training set
        test_indices: the indices of images in the test set
        overwrite: whether to overwrite a shuffle with the same index and train fraction
            if one exists

    Raises:
        ValueError: if overwrite=False and there is already a shuffle with the given
            index in the metadata file.
    """
    prefix = cfg["Task"] + cfg["date"]
    metadata = TrainingDatasetMetadata.load(cfg, load_splits=True)
    new_shuffle = ShuffleMetadata(
        name=f"{prefix}-trainset{int(100 * train_fraction)}shuffle{shuffle}",
        train_fraction=train_fraction,
        index=shuffle,
        engine=engine,
        split=DataSplit(
            train_indices=tuple(sorted([int(i) for i in train_indices])),
            test_indices=tuple(sorted([int(i) for i in test_indices])),
        )
    )
    metadata = metadata.add(shuffle=new_shuffle, overwrite=overwrite)
    metadata.save()


def get_shuffle_engine(
    cfg: dict,
    trainingsetindex: int,
    shuffle: int,
    modelprefix: str = "",
) -> Engine:
    """
    Args:
        cfg: the config for the DeepLabCut project
        trainingsetindex: the training set index used
        shuffle: the shuffle for which to get the engine
        modelprefix: the model prefix, if there is one

    Returns:
        the engine that the shuffle was created with

    Raises:
        ValueError if the engine for the shuffle cannot be determined or the shuffle
        doesn't exist
    """
    if not TrainingDatasetMetadata.path(cfg).exists():
        metadata = TrainingDatasetMetadata.create(cfg)
        metadata.save()

    metadata = TrainingDatasetMetadata.load(cfg)
    shuffle_metadata = metadata.get(trainingsetindex, shuffle)
    if modelprefix:
        # try to get the engine by checking which models folder exists
        engines = find_engines_from_model_folders(
            cfg, trainingsetindex, shuffle, modelprefix
        )
        if len(engines) == 0:
            raise ValueError(
                f"Couldn't find any shuffles with trainingsetindex={trainingsetindex}, "
                f"shuffle={shuffle} and modelprefix={modelprefix}. Please check that "
                f"such a shuffle is defined."
            )

        if len(engines) == 1:
            return engines.pop()

        if shuffle_metadata.engine in engines:
            engine = shuffle_metadata.engine
        else:
            engine = engines.pop()  # take a random engine

        logging.warning(
            f"Found multiple engines for trainingsetindex={trainingsetindex}, "
            f"shuffle={shuffle} and modelprefix={modelprefix}. Using engine={engine}. "
            f"To select another engine, please specify it in your API call."
        )
        return engine

    return shuffle_metadata.engine


def find_engines_from_model_folders(
    cfg: dict,
    trainingsetindex: int,
    shuffle: int,
    modelprefix: str = "",
) -> set[Engine]:
    """Determines which engines are used with a given shuffle.

    This method can be useful when using modelprefix, as the engine for a shuffle stored
    under a "modelprefix" might not be the same as the base shuffle (for which the
    engine is stored in the training-datasets folder).

    Args:
        cfg: the config for the DeepLabCut project
        trainingsetindex: the training set index used
        shuffle: the shuffle for which to get the engine
        modelprefix: the model prefix, if there is one

    Returns:
        the engines for which a model folder exists for the given shuffle
    """
    project_path = Path(cfg["project_path"])
    train_fraction = cfg["TrainingFraction"][trainingsetindex]

    existing_engines = set()
    for engine in Engine:
        expected_model_folder = project_path / auxiliaryfunctions.get_model_folder(
            trainFraction=train_fraction,
            shuffle=shuffle,
            cfg=cfg,
            engine=engine,
            modelprefix=modelprefix,
        )
        if expected_model_folder.exists():
            existing_engines.add(engine)

    return existing_engines


--- File: deeplabcut/generate_training_dataset/frame_extraction.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


def select_cropping_area(config, videos=None):
    """
    Interactively select the cropping area of all videos in the config.
    A user interface pops up with a frame to select the cropping parameters.
    Use the left click to draw a box and hit the button 'set cropping parameters'
    to store the cropping parameters for a video in the config.yaml file.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    videos : optional (default=None)
        List of videos whose cropping areas are to be defined. Note that full paths are required.
        By default, all videos in the config are successively loaded.

    Returns
    -------
    cfg : dict
        Updated project configuration
    """
    from deeplabcut.utils import auxiliaryfunctions, auxfun_videos

    cfg = auxiliaryfunctions.read_config(config)
    if videos is None:
        videos = list(cfg.get("video_sets_original") or cfg["video_sets"])

    for video in videos:
        coords = auxfun_videos.draw_bbox(video)
        if coords:
            temp = {
                "crop": ", ".join(
                    map(
                        str,
                        [
                            int(coords[0]),
                            int(coords[2]),
                            int(coords[1]),
                            int(coords[3]),
                        ],
                    )
                )
            }
            try:
                cfg["video_sets"][video] = temp
            except KeyError:
                cfg["video_sets_original"][video] = temp

    auxiliaryfunctions.write_config(config, cfg)
    return cfg


def extract_frames(
    config,
    mode="automatic",
    algo="kmeans",
    crop=False,
    userfeedback=True,
    cluster_step=1,
    cluster_resizewidth=30,
    cluster_color=False,
    opencv=True,
    slider_width=25,
    config3d=None,
    extracted_cam=0,
    videos_list=None,
):
    """Extracts frames from the project videos.

    Frames will be extracted from videos listed in the config.yaml file.

    The frames are selected from the videos in a randomly and temporally uniformly
    distributed way (``uniform``), by clustering based on visual appearance
    (``k-means``), or by manual selection.

    After frames have been extracted from all videos from one camera, matched frames
    from other cameras can be extracted using ``mode = "match"``. This is necessary if
    you plan to use epipolar lines to improve labeling across multiple camera angles.
    It will overwrite previously extracted images from the second camera angle if
    necessary.

    Please refer to the user guide for more details on methods and parameters
    https://www.nature.com/articles/s41596-019-0176-0 or the preprint:
    https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    mode : string. Either ``"automatic"``, ``"manual"`` or ``"match"``.
        String containing the mode of extraction. It must be either ``"automatic"`` or
        ``"manual"`` to extract the initial set of frames. It can also be ``"match"``
        to match frames between the cameras in preparation for the use of epipolar line
        during labeling; namely, extract from camera_1 first, then run this to extract
        the matched frames in camera_2.

        WARNING: if you use ``"match"``, and you previously extracted and labeled
        frames from the second camera, this will overwrite your data. This will require
        you to delete the ``collectdata(.h5/.csv)`` files before labeling. Use with
        caution!

    algo : string, Either ``"kmeans"`` or ``"uniform"``, Default: `"kmeans"`.
        String specifying the algorithm to use for selecting the frames. Currently,
        deeplabcut supports either ``kmeans`` or ``uniform`` based selection. This flag
        is only required for ``automatic`` mode and the default is ``kmeans``. For
        ``"uniform"``, frames are picked in temporally uniform way, ``"kmeans"``
        performs clustering on downsampled frames (see user guide for details).

        NOTE: Color information is discarded for ``"kmeans"``, thus e.g. for
        camouflaged octopus clustering one might want to change this.

    crop : bool or str, optional
        If ``True``, video frames are cropped according to the corresponding
        coordinates stored in the project configuration file. Alternatively, if
        cropping coordinates are not known yet, crop=``"GUI"`` triggers a user
        interface where the cropping area can be manually drawn and saved.

    userfeedback: bool, optional
        If this is set to ``False`` during ``"automatic"`` mode then frames for all
        videos are extracted. The user can set this to ``"True"``, which will result in
        a dialog, where the user is asked for each video if (additional/any) frames
        from this video should be extracted. Use this, e.g. if you have already labeled
        some folders and want to extract data for new videos.

    cluster_resizewidth: int, default: 30
        For ``"k-means"`` one can change the width to which the images are downsampled
        (aspect ratio is fixed).

    cluster_step: int, default: 1
        By default each frame is used for clustering, but for long videos one could
        only use every nth frame (set using this parameter). This saves memory before
        clustering can start, however, reading the individual frames takes longer due
        to the skipping.

    cluster_color: bool, default: False
        If ``"False"`` then each downsampled image is treated as a grayscale vector
        (discarding color information). If ``"True"``, then the color channels are
        considered. This increases the computational complexity.

    opencv: bool, default: True
        Uses openCV for loading & extractiong (otherwise moviepy (legacy)).

    slider_width: int, default: 25
        Width of the video frames slider, in percent of window.

    config3d: string, optional
        Path to the project configuration file in the 3D project. This will be used to
        match frames extracted from all cameras present in the field 'camera_names' to
        the frames extracted from the camera given by the parameter 'extracted_cam'.

    extracted_cam: int, default: 0
        The index of the camera that already has extracted frames. This will match
        frame numbers to extract for all other cameras. This parameter is necessary if
        you wish to use epipolar lines in the labeling toolbox. Only use if
        ``mode='match'`` and ``config3d`` is provided.

    videos_list: list[str], Default: None
        A list of the string containing full paths to videos to extract frames for. If
        this is left as ``None`` all videos specified in the config file will have
        frames extracted. Otherwise one can select a subset by passing those paths.

    Returns
    -------
    None

    Notes
    -----
    Use the function ``add_new_videos`` at any stage of the project to add new videos
    to the config file and extract their frames.

    The following parameters for automatic extraction are used from the config file

    * ``numframes2pick``
    * ``start`` and ``stop``

    While selecting the frames manually, you do not need to specify the ``crop``
    parameter in the command. Rather, you will get a prompt in the graphic user
    interface to choose if you need to crop or not.

    Examples
    --------
    To extract frames automatically with 'kmeans' and then crop the frames

    >>> deeplabcut.extract_frames(
            config='/analysis/project/reaching-task/config.yaml',
            mode='automatic',
            algo='kmeans',
            crop=True,
        )

    To extract frames automatically with 'kmeans' and then defining the cropping area
    using a GUI

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml',
            'automatic',
            'kmeans',
            'GUI',
        )

    To consider the color information when extracting frames automatically with
    'kmeans'

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml',
            'automatic',
            'kmeans',
            cluster_color=True,
        )

    To extract frames automatically with 'uniform' and then crop the frames

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml',
            'automatic',
            'uniform',
            crop=True,
        )

    To extract frames manually

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml', 'manual'
        )

    To extract frames manually, with a 60% wide frames slider

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml', 'manual', slider_width=60,
        )

    To extract frames from a second camera that match the frames extracted from the
    first

    >>> deeplabcut.extract_frames(
            '/analysis/project/reaching-task/config.yaml',
            mode='match',
            extracted_cam=0,
        )
    """
    import os
    import sys
    import re
    import glob
    import numpy as np
    from pathlib import Path
    from skimage import io
    from skimage.util import img_as_ubyte
    from deeplabcut.utils import frameselectiontools
    from deeplabcut.utils import auxiliaryfunctions

    config_file = Path(config).resolve()
    cfg = auxiliaryfunctions.read_config(config_file)
    print("Config file read successfully.")

    if videos_list is None:
        videos = list(cfg.get("video_sets_original") or cfg["video_sets"])
    else:  # filter video_list by the ones in the config file
        videos = [v for v in cfg["video_sets"] if v in videos_list]

    if mode == "manual":
        from deeplabcut.gui.widgets import launch_napari

        _ = launch_napari(videos[0])
        return

    elif mode == "automatic":
        numframes2pick = cfg["numframes2pick"]
        start = cfg["start"]
        stop = cfg["stop"]

        # Check for variable correctness
        if start > 1 or stop > 1 or start < 0 or stop < 0 or start >= stop:
            raise Exception(
                "Erroneous start or stop values. Please correct it in the config file."
            )
        if numframes2pick < 1 and not int(numframes2pick):
            raise Exception(
                "Perhaps consider extracting more, or a natural number of frames."
            )

        if opencv:
            from deeplabcut.utils.auxfun_videos import VideoWriter
        else:
            from moviepy.editor import VideoFileClip

        has_failed = []
        for video in videos:
            if userfeedback:
                print(
                    "Do you want to extract (perhaps additional) frames for video:",
                    video,
                    "?",
                )
                askuser = input("yes/no")
            else:
                askuser = "yes"

            if (
                askuser == "y"
                or askuser == "yes"
                or askuser == "Ja"
                or askuser == "ha"
                or askuser == "oui"
                or askuser == "ouais"
            ):  # multilanguage support :)
                if opencv:
                    cap = VideoWriter(video)
                    nframes = len(cap)
                else:
                    # Moviepy:
                    clip = VideoFileClip(video)
                    fps = clip.fps
                    nframes = int(np.ceil(clip.duration * 1.0 / fps))
                if not nframes:
                    print("Video could not be opened. Skipping...")
                    continue

                indexlength = int(np.ceil(np.log10(nframes)))

                fname = Path(video)
                output_path = Path(config).parents[0] / "labeled-data" / fname.stem

                if output_path.exists():
                    if len(os.listdir(output_path)):
                        if userfeedback:
                            askuser = input(
                                "The directory already contains some frames. Do you want to add to it?(yes/no): "
                            )
                        if not (
                            askuser == "y"
                            or askuser == "yes"
                            or askuser == "Y"
                            or askuser == "Yes"
                        ):
                            sys.exit("Delete the frames and try again later!")

                if crop == "GUI":
                    cfg = select_cropping_area(config, [video])
                try:
                    coords = cfg["video_sets"][video]["crop"].split(",")
                except KeyError:
                    coords = cfg["video_sets_original"][video]["crop"].split(",")

                if crop:
                    if opencv:
                        cap.set_bbox(*map(int, coords))
                    else:
                        clip = clip.crop(
                            y1=int(coords[2]),
                            y2=int(coords[3]),
                            x1=int(coords[0]),
                            x2=int(coords[1]),
                        )
                else:
                    coords = None

                print("Extracting frames based on %s ..." % algo)
                if algo == "uniform":
                    if opencv:
                        frames2pick = frameselectiontools.UniformFramescv2(
                            cap, numframes2pick, start, stop
                        )
                    else:
                        frames2pick = frameselectiontools.UniformFrames(
                            clip, numframes2pick, start, stop
                        )
                elif algo == "kmeans":
                    if opencv:
                        frames2pick = frameselectiontools.KmeansbasedFrameselectioncv2(
                            cap,
                            numframes2pick,
                            start,
                            stop,
                            step=cluster_step,
                            resizewidth=cluster_resizewidth,
                            color=cluster_color,
                        )
                    else:
                        frames2pick = frameselectiontools.KmeansbasedFrameselection(
                            clip,
                            numframes2pick,
                            start,
                            stop,
                            step=cluster_step,
                            resizewidth=cluster_resizewidth,
                            color=cluster_color,
                        )
                else:
                    print(
                         "Please implement this method yourself and send us a pull "
                         "request! Otherwise, choose 'uniform' or 'kmeans'."
                     )
                    frames2pick = []

                if not len(frames2pick):
                    print("Frame selection failed...")
                    return []

                output_path = (
                    Path(config).parents[0] / "labeled-data" / Path(video).stem
                )
                output_path.mkdir(parents=True, exist_ok=True)
                is_valid = []
                if opencv:
                    for index in frames2pick:
                        cap.set_to_frame(index)  # extract a particular frame
                        frame = cap.read_frame(crop=True)
                        if frame is not None:
                            image = img_as_ubyte(frame)
                            img_name = (
                                str(output_path)
                                + "/img"
                                + str(index).zfill(indexlength)
                                + ".png"
                            )
                            io.imsave(img_name, image)
                            is_valid.append(True)
                        else:
                            print("Frame", index, " not found!")
                            is_valid.append(False)
                    cap.close()
                else:
                    for index in frames2pick:
                        try:
                            image = img_as_ubyte(clip.get_frame(index * 1.0 / clip.fps))
                            img_name = (
                                str(output_path)
                                + "/img"
                                + str(index).zfill(indexlength)
                                + ".png"
                            )
                            io.imsave(img_name, image)
                            if np.var(image) == 0:  # constant image
                                print(
                                    "Seems like black/constant images are extracted from your video. Perhaps consider using opencv under the hood, by setting: opencv=True"
                                )
                            is_valid.append(True)
                        except FileNotFoundError:
                            print("Frame # ", index, " does not exist.")
                            is_valid.append(False)
                    clip.close()
                    del clip

                if not any(is_valid):
                    has_failed.append(True)
                else:
                    has_failed.append(False)

            else:  # NO!
                has_failed.append(False)

        if all(has_failed):
            print("Frame extraction failed. Video files must be corrupted.")
            return has_failed
        elif any(has_failed):
            print("Although most frames were extracted, some were invalid.")
        else:
            print(
                "Frames were successfully extracted, for the videos listed in the config.yaml file."
            )
        print(
            "\nYou can now label the frames using the function 'label_frames' "
            "(Note, you should label frames extracted from diverse videos (and many videos; we do not recommend training on single videos!))."
        )
        return has_failed

    elif mode == "match":
        import cv2

        config_file = Path(config).resolve()
        cfg = auxiliaryfunctions.read_config(config_file)
        print("Config file read successfully.")
        videos = sorted(cfg["video_sets"].keys())
        if videos_list is not None:  # filter video_list by the ones in the config file
            videos = [v for v in videos if v in videos_list]
        project_path = Path(config).parents[0]
        labels_path = os.path.join(project_path, "labeled-data/")
        video_dir = os.path.join(project_path, "videos/")
        try:
            cfg_3d = auxiliaryfunctions.read_config(config3d)
        except:
            raise Exception(
                "You must create a 3D project and edit the 3D config file before extracting matched frames. \n"
            )
        cams = cfg_3d["camera_names"]
        extCam_name = cams[extracted_cam]
        del cams[extracted_cam]
        label_dirs = sorted(
            glob.glob(os.path.join(labels_path, "*" + extCam_name + "*"))
        )

        # select crop method
        crop_list = []
        for video in videos:
            if extCam_name in video:
                if crop == "GUI":
                    cfg = select_cropping_area(config, [video])
                    print("in gui code")
                coords = cfg["video_sets"][video]["crop"].split(",")

                if crop and not opencv:
                    clip = clip.crop(
                        y1=int(coords[2]),
                        y2=int(coords[3]),
                        x1=int(coords[0]),
                        x2=int(coords[1]),
                    )
                elif not crop:
                    coords = None
                crop_list.append(coords)

        for coords, dirPath in zip(crop_list, label_dirs):
            extracted_images = glob.glob(os.path.join(dirPath, "*png"))

            imgPattern = re.compile("[0-9]{1,10}")
            for cam in cams:
                output_path = re.sub(extCam_name, cam, dirPath)

                for fname in os.listdir(output_path):
                    if fname.endswith(".png"):
                        os.remove(os.path.join(output_path, fname))

                # Find the matching video from the config `video_sets`,
                # as it may be stored elsewhere than in the `videos` directory.
                video_name = os.path.basename(output_path)
                vid = ""
                for video in cfg["video_sets"]:
                    if video_name in video:
                        vid = video
                        break
                if not vid:
                    raise ValueError(f"Video {video_name} not found...")

                cap = cv2.VideoCapture(vid)
                print("\n extracting matched frames from " + video_name)
                for img in extracted_images:
                    imgNum = re.findall(imgPattern, os.path.basename(img))[0]
                    cap.set(1, int(imgNum))
                    ret, frame = cap.read()
                    if ret:
                        image = img_as_ubyte(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                        img_name = os.path.join(output_path, "img" + imgNum + ".png")
                        if crop:
                            io.imsave(
                                img_name,
                                image[
                                    int(coords[2]) : int(coords[3]),
                                    int(coords[0]) : int(coords[1]),
                                    :,
                                ],
                            )
                        else:
                            io.imsave(img_name, image)
        print(
            "\n Done extracting matched frames. You can now begin labeling frames using the function label_frames\n"
        )

    else:
        print(
            "Invalid MODE. Choose either 'manual', 'automatic' or 'match'. Check ``help(deeplabcut.extract_frames)`` on python and ``deeplabcut.extract_frames?`` \
              for ipython/jupyter notebook for more details."
        )


--- File: deeplabcut/generate_training_dataset/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


from deeplabcut.generate_training_dataset.frame_extraction import *
from deeplabcut.generate_training_dataset.trainingsetmanipulation import *
from deeplabcut.generate_training_dataset.multiple_individuals_trainingsetmanipulation import *
from deeplabcut.generate_training_dataset.metadata import (
    DataSplit,
    ShuffleMetadata,
    TrainingDatasetMetadata,
)


--- File: deeplabcut/generate_training_dataset/multiple_individuals_trainingsetmanipulation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import os
import os.path
import re
import warnings
from itertools import combinations
from pathlib import Path

import numpy as np
from tqdm import tqdm

import deeplabcut.compat as compat
import deeplabcut.generate_training_dataset.metadata as metadata
from deeplabcut.core.engine import Engine
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.generate_training_dataset import (
    merge_annotateddatasets,
    read_image_shape_fast,
    SplitTrials,
    MakeTrain_pose_yaml,
    MakeTest_pose_yaml,
    MakeInference_yaml,
    pad_train_test_indices,
    validate_shuffles,
)
from deeplabcut.utils import (
    auxiliaryfunctions,
    auxfun_models,
    auxfun_multianimal,
)


def format_multianimal_training_data(
    df,
    train_inds,
    project_path,
    n_decimals=2,
):
    train_data = []
    nrows = df.shape[0]
    filenames = df.index.to_list()
    n_bodyparts = df.columns.get_level_values("bodyparts").unique().size
    individuals = df.columns.get_level_values("individuals")
    n_individuals = individuals.unique().size
    mask_single = individuals.str.contains("single")
    n_animals = n_individuals - 1 if np.any(mask_single) else n_individuals
    array = np.full(
        (nrows, n_individuals, n_bodyparts, 3), fill_value=np.nan, dtype=np.float32
    )
    array[..., 0] = np.arange(n_bodyparts)
    temp = df.to_numpy()
    temp_multi = temp[:, ~mask_single].reshape((nrows, n_animals, -1, 2))
    n_multibodyparts = temp_multi.shape[2]
    array[:, :n_animals, :n_multibodyparts, 1:] = temp_multi
    if n_animals != n_individuals:  # There is a unique individual
        n_uniquebodyparts = n_bodyparts - n_multibodyparts
        temp_single = np.reshape(temp[:, mask_single], (nrows, 1, n_uniquebodyparts, 2))
        array[:, -1:, -n_uniquebodyparts:, 1:] = temp_single
    array = np.round(array, decimals=n_decimals)
    for i in tqdm(train_inds):
        filename = filenames[i]
        img_shape = read_image_shape_fast(os.path.join(project_path, *filename))
        joints = dict()
        has_data = False
        for n, xy in enumerate(array[i]):
            # Drop missing body parts
            xy = xy[~np.isnan(xy).any(axis=1)]
            # Drop points lying outside the image
            inside = np.logical_and.reduce(
                (
                    xy[:, 1] < img_shape[2],
                    xy[:, 1] > 0,
                    xy[:, 2] < img_shape[1],
                    xy[:, 2] > 0,
                )
            )
            xy = xy[inside]
            if xy.size:
                has_data = True
                joints[n] = xy

        if has_data:
            data = {
                "image": filename,
                "size": np.asarray(img_shape),
                "joints": joints,
            }
            train_data.append(data)

    return train_data


def create_multianimaltraining_dataset(
    config,
    num_shuffles=1,
    Shuffles=None,
    windows2linux=False,
    net_type=None,
    detector_type=None,
    numdigits=2,
    crop_size=(400, 400),
    crop_sampling="hybrid",
    paf_graph=None,
    trainIndices=None,
    testIndices=None,
    n_edges_threshold=105,
    paf_graph_degree=6,
    userfeedback: bool = True,
    weight_init: WeightInitialization | None = None,
    engine: Engine | None = None,
):
    """
    Creates a training dataset for multi-animal datasets. Labels from all the extracted
    frames are merged into a single .h5 file.\n
    Only the videos included in the config file are used to create this dataset.\n
    [OPTIONAL] Use the function 'add_new_videos' at any stage of the project to add more
    videos to the project.

    Important differences to standard:
     - stores coordinates with numdigits as many digits

    Parameter
    ----------
    config : string
        Full path of the config.yaml file as a string.

    num_shuffles : int, optional
        Number of shuffles of training dataset to create, i.e. [1,2,3] for num_shuffles=3. Default is set to 1.

    Shuffles: list of shuffles.
        Alternatively the user can also give a list of shuffles (integers!).

    net_type: string
        Type of networks. The options available depend on which engine is used. See
        Lauer et al. 2021 https://www.biorxiv.org/content/10.1101/2021.04.30.442096v1
        Currently supported options are:
            TensorFlow
                * ``resnet_50``
                * ``resnet_101``
                * ``resnet_152``
                * ``efficientnet-b0``
                * ``efficientnet-b1``
                * ``efficientnet-b2``
                * ``efficientnet-b3``
                * ``efficientnet-b4``
                * ``efficientnet-b5``
                * ``efficientnet-b6``
            PyTorch (call ``deeplabcut.pose_estimation_pytorch.available_models()`` for
            a complete list)
                * ``resnet_50``
                * ``resnet_101``
                * ``dekr_w18``
                * ``dekr_w32``
                * ``dekr_w48``
                * ``top_down_resnet_50``
                * ``top_down_resnet_101``
                * ``top_down_hrnet_w18``
                * ``top_down_hrnet_w32``
                * ``top_down_hrnet_w48``
                * ``animaltokenpose_base``

    detector_type: string, optional, default=None
        Only for the PyTorch engine.
        When passing creating shuffles for top-down models, you can specify which
        detector you want. If the detector_type is None, the ```ssdlite``` will be used.
        The list of all available detectors can be obtained by calling
        ``deeplabcut.pose_estimation_pytorch.available_detectors()``. Supported options:
            * ``ssdlite``
            * ``fasterrcnn_mobilenet_v3_large_fpn``
            * ``fasterrcnn_resnet50_fpn_v2``

    numdigits: int, optional

    crop_size: tuple of int, optional
        Only for the TensorFlow engine.
        Dimensions (width, height) of the crops for data augmentation.
        Default is 400x400.

    crop_sampling: str, optional
        Only for the TensorFlow engine.
        Crop centers sampling method. Must be either:
        "uniform" (randomly over the image),
        "keypoints" (randomly over the annotated keypoints),
        "density" (weighing preferentially dense regions of keypoints),
        or "hybrid" (alternating randomly between "uniform" and "density").
        Default is "hybrid".

    paf_graph: list of lists, or "config" optional (default=None)
        Only for the TensorFlow engine.
        If not None, overwrite the default complete graph. This is useful for advanced users who
        already know a good graph, or simply want to use a specific one. Note that, in that case,
        the data-driven selection procedure upon model evaluation will be skipped.

        "config" will use the skeleton defined in the config file.

    trainIndices: list of lists, optional (default=None)
        List of one or multiple lists containing train indexes.
        A list containing two lists of training indexes will produce two splits.

    testIndices: list of lists, optional (default=None)
        List of one or multiple lists containing test indexes.

    n_edges_threshold: int, optional (default=105)
        Only for the TensorFlow engine.
        Number of edges above which the graph is automatically pruned.

    paf_graph_degree: int, optional (default=6)
        Only for the TensorFlow engine.
        Degree of paf_graph when automatically pruning it (before training).

    userfeedback: bool, optional, default=True
        If ``False``, all requested train/test splits are created (no matter if they
        already exist). If you want to assure that previous splits etc. are not
        overwritten, set this to ``True`` and you will be asked for each split.

    weight_init: WeightInitialisation, optional, default=None
        PyTorch engine only. Specify how model weights should be initialized. The
        default mode uses transfer learning from ImageNet weights.

    engine: Engine, optional
        Whether to create a pose config for a Tensorflow or PyTorch model. Defaults to
        the value specified in the project configuration file. If no engine is specified
        for the project, defaults to ``deeplabcut.compat.DEFAULT_ENGINE``.

    Example
    --------
    >>> deeplabcut.create_multianimaltraining_dataset('/analysis/project/reaching-task/config.yaml',num_shuffles=1)

    >>> deeplabcut.create_multianimaltraining_dataset('/analysis/project/reaching-task/config.yaml', Shuffles=[0,1,2], trainIndices=[trainInd1, trainInd2, trainInd3], testIndices=[testInd1, testInd2, testInd3])

    Windows:
    >>> deeplabcut.create_multianimaltraining_dataset(r'C:\\Users\\Ulf\\looming-task\\config.yaml',Shuffles=[3,17,5])
    --------
    """
    if windows2linux:
        warnings.warn(
            "`windows2linux` has no effect since 2.2.0.4 and will be removed in 2.2.1.",
            FutureWarning,
        )

    if len(crop_size) != 2 or not all(isinstance(v, int) for v in crop_size):
        raise ValueError("Crop size must be a tuple of two integers (width, height).")

    if crop_sampling not in ("uniform", "keypoints", "density", "hybrid"):
        raise ValueError(
            f"Invalid sampling {crop_sampling}. Must be "
            f"either 'uniform', 'keypoints', 'density', or 'hybrid."
        )

    # Loading metadata from config file:
    cfg = auxiliaryfunctions.read_config(config)
    scorer = cfg["scorer"]
    project_path = cfg["project_path"]
    # Create path for training sets & store data there
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    full_training_path = Path(project_path, trainingsetfolder)
    auxiliaryfunctions.attempt_to_make_folder(full_training_path, recursive=True)

    # Create the trainset metadata file, if it doesn't yet exist
    if not metadata.TrainingDatasetMetadata.path(cfg).exists():
        trainset_metadata = metadata.TrainingDatasetMetadata.create(cfg)
        trainset_metadata.save()

    Data = merge_annotateddatasets(cfg, full_training_path)
    if Data is None:
        return
    Data = Data[scorer]

    if net_type is None:  # loading & linking pretrained models
        net_type = cfg.get("default_net_type", "dlcrnet_ms5")

    # load the engine to use to create the shuffle
    if engine is None:
        engine = compat.get_project_engine(cfg)

    if not (
        any(net in net_type for net in ("resnet", "eff", "dlc", "mob"))
        or engine == Engine.PYTORCH
    ):
        raise ValueError(f"Unsupported network {net_type} for engine {engine}.")

    multi_stage = False
    ### dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module
    ### dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module
    if all(net in net_type for net in ("dlcr", "_ms5")) and engine != Engine.PYTORCH:
        num_layers = re.findall("dlcr([0-9]*)", net_type)[0]
        if num_layers == "":
            num_layers = 50
        net_type = "resnet_{}".format(num_layers)
        multi_stage = True

    dataset_type = "multi-animal-imgaug"
    (
        individuals,
        uniquebodyparts,
        multianimalbodyparts,
    ) = auxfun_multianimal.extractindividualsandbodyparts(cfg)

    if paf_graph is None:  # Automatically form a complete PAF graph
        n_bpts = len(multianimalbodyparts)
        partaffinityfield_graph = [
            list(edge) for edge in combinations(range(n_bpts), 2)
        ]
        n_edges_orig = len(partaffinityfield_graph)
        # If the graph is unnecessarily large (with 15+ keypoints by default),
        # we randomly prune it to a size guaranteeing an average node degree of 6;
        # see Suppl. Fig S9c in Lauer et al., 2022.
        if n_edges_orig >= n_edges_threshold:
            partaffinityfield_graph = auxfun_multianimal.prune_paf_graph(
                partaffinityfield_graph,
                average_degree=paf_graph_degree,
            )
    else:
        if paf_graph == "config":
            # Use the skeleton defined in the config file
            skeleton = cfg["skeleton"]
            paf_graph = [
                sorted(
                    (multianimalbodyparts.index(bpt1), multianimalbodyparts.index(bpt2))
                )
                for bpt1, bpt2 in skeleton
            ]
            print(
                "Using `skeleton` from the config file as a paf_graph. Data-driven skeleton will not be computed."
            )

        # Ignore possible connections between 'multi' and 'unique' body parts;
        # one can never be too careful...
        to_ignore = auxfun_multianimal.filter_unwanted_paf_connections(cfg, paf_graph)
        partaffinityfield_graph = [
            edge for i, edge in enumerate(paf_graph) if i not in to_ignore
        ]
        auxfun_multianimal.validate_paf_graph(cfg, partaffinityfield_graph)

    print("Utilizing the following graph:", partaffinityfield_graph)
    # Disable the prediction of PAFs if the graph is empty
    partaffinityfield_predict = bool(partaffinityfield_graph)

    # Loading the encoder (if necessary downloading from TF)
    dlcparent_path = auxiliaryfunctions.get_deeplabcut_path()
    defaultconfigfile = os.path.join(dlcparent_path, "pose_cfg.yaml")

    if engine == Engine.PYTORCH:
        model_path = dlcparent_path
    else:
        model_path = auxfun_models.check_for_weights(net_type, Path(dlcparent_path))

    Shuffles = validate_shuffles(cfg, Shuffles, num_shuffles, userfeedback)

    # print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)
    if trainIndices is None and testIndices is None:
        splits = []
        for shuffle in Shuffles:  # Creating shuffles starting from 1
            for train_frac in cfg["TrainingFraction"]:
                train_inds, test_inds = SplitTrials(range(len(Data)), train_frac)
                splits.append((train_frac, shuffle, (train_inds, test_inds)))
    else:
        if len(trainIndices) != len(testIndices) != len(Shuffles):
            raise ValueError(
                "Number of Shuffles and train and test indexes should be equal."
            )
        splits = []
        for shuffle, (train_inds, test_inds) in enumerate(
            zip(trainIndices, testIndices)
        ):
            trainFraction = round(
                len(train_inds) * 1.0 / (len(train_inds) + len(test_inds)), 2
            )
            print(
                f"You passed a split with the following fraction: {int(100 * trainFraction)}%"
            )
            # Now that the training fraction is guaranteed to be correct,
            # the values added to pad the indices are removed.
            train_inds = np.asarray(train_inds)
            train_inds = train_inds[train_inds != -1]
            test_inds = np.asarray(test_inds)
            test_inds = test_inds[test_inds != -1]
            splits.append((trainFraction, Shuffles[shuffle], (train_inds, test_inds)))

    top_down = False
    if engine == Engine.PYTORCH and net_type.startswith("top_down_"):
        top_down = True
        net_type = net_type[len("top_down_") :]

    for trainFraction, shuffle, (trainIndices, testIndices) in splits:
        ####################################################
        # Generating data structure with labeled information & frame metadata (for deep cut)
        ####################################################
        print(
            "Creating training data for: Shuffle:",
            shuffle,
            "TrainFraction: ",
            trainFraction,
        )

        # Make training file!
        data = format_multianimal_training_data(
            Data,
            trainIndices,
            cfg["project_path"],
            numdigits,
        )

        if len(trainIndices) > 0:
            (
                datafilename,
                metadatafilename,
            ) = auxiliaryfunctions.get_data_and_metadata_filenames(
                trainingsetfolder, trainFraction, shuffle, cfg
            )
            ################################################################################
            # Saving metadata and data file (Pickle file)
            ################################################################################
            auxiliaryfunctions.save_metadata(
                os.path.join(project_path, metadatafilename),
                data,
                trainIndices,
                testIndices,
                trainFraction,
            )
            metadata.update_metadata(
                cfg=cfg,
                train_fraction=trainFraction,
                shuffle=shuffle,
                engine=engine,
                train_indices=trainIndices,
                test_indices=testIndices,
                overwrite=not userfeedback,
            )

            datafilename = datafilename.split(".mat")[0] + ".pickle"
            import pickle

            with open(os.path.join(project_path, datafilename), "wb") as f:
                # Pickle the 'labeled-data' dictionary using the highest protocol available.
                pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)

            ################################################################################
            # Creating file structure for training &
            # Test files as well as pose_yaml files (containing training and testing information)
            #################################################################################

            modelfoldername = auxiliaryfunctions.get_model_folder(
                trainFraction,
                shuffle,
                cfg,
                engine=engine,
            )
            auxiliaryfunctions.attempt_to_make_folder(
                Path(config).parents[0] / modelfoldername, recursive=True
            )
            auxiliaryfunctions.attempt_to_make_folder(
                str(Path(config).parents[0] / modelfoldername / "train")
            )
            auxiliaryfunctions.attempt_to_make_folder(
                str(Path(config).parents[0] / modelfoldername / "test")
            )

            path_train_config = str(
                os.path.join(
                    cfg["project_path"],
                    Path(modelfoldername),
                    "train",
                    "pose_cfg.yaml",
                )
            )
            path_test_config = str(
                os.path.join(
                    cfg["project_path"],
                    Path(modelfoldername),
                    "test",
                    "pose_cfg.yaml",
                )
            )
            path_inference_config = str(
                os.path.join(
                    cfg["project_path"],
                    Path(modelfoldername),
                    "test",
                    "inference_cfg.yaml",
                )
            )

            if engine == Engine.TF:
                jointnames = [str(bpt) for bpt in multianimalbodyparts]
                jointnames.extend([str(bpt) for bpt in uniquebodyparts])
                items2change = {
                    "dataset": datafilename,
                    "engine": engine.aliases[0],
                    "metadataset": metadatafilename,
                    "num_joints": len(multianimalbodyparts)
                    + len(uniquebodyparts),  # cfg["uniquebodyparts"]),
                    "all_joints": [
                        [i]
                        for i in range(len(multianimalbodyparts) + len(uniquebodyparts))
                    ],  # cfg["uniquebodyparts"]))],
                    "all_joints_names": jointnames,
                    "init_weights": str(model_path),
                    "project_path": str(cfg["project_path"]),
                    "net_type": net_type,
                    "multi_stage": multi_stage,
                    "pairwise_loss_weight": 0.1,
                    "pafwidth": 20,
                    "partaffinityfield_graph": partaffinityfield_graph,
                    "partaffinityfield_predict": partaffinityfield_predict,
                    "weigh_only_present_joints": False,
                    "num_limbs": len(partaffinityfield_graph),
                    "dataset_type": dataset_type,
                    "optimizer": "adam",
                    "batch_size": 8,
                    "multi_step": [[1e-4, 7500], [5 * 1e-5, 12000], [1e-5, 200000]],
                    "save_iters": 10000,
                    "display_iters": 500,
                    "num_idchannel": (
                        len(cfg["individuals"]) if cfg.get("identity", False) else 0
                    ),
                    "crop_size": list(crop_size),
                    "crop_sampling": crop_sampling,
                }

                trainingdata = MakeTrain_pose_yaml(
                    items2change,
                    path_train_config,
                    defaultconfigfile,
                    save=(engine == Engine.TF),
                )
                keys2save = [
                    "dataset",
                    "num_joints",
                    "all_joints",
                    "all_joints_names",
                    "net_type",
                    "multi_stage",
                    "init_weights",
                    "global_scale",
                    "location_refinement",
                    "locref_stdev",
                    "dataset_type",
                    "partaffinityfield_predict",
                    "pairwise_predict",
                    "partaffinityfield_graph",
                    "num_limbs",
                    "dataset_type",
                    "num_idchannel",
                ]

                MakeTest_pose_yaml(
                    trainingdata,
                    keys2save,
                    path_test_config,
                    nmsradius=5.0,
                    minconfidence=0.01,
                    sigma=1,
                    locref_smooth=False,
                )  # setting important def. values for inference
            elif engine == Engine.PYTORCH:
                from deeplabcut.pose_estimation_pytorch.config.make_pose_config import (
                    make_pytorch_pose_config,
                    make_pytorch_test_config,
                )
                from deeplabcut.pose_estimation_pytorch.modelzoo.config import (
                    make_super_animal_finetune_config,
                )

                # backwards compatibility with version 2.X
                if net_type == "dlcrnet_ms5":
                    net_type = "dlcrnet_stride16_ms5"

                config_path = Path(path_train_config).with_name(engine.pose_cfg_name)
                if weight_init is not None and weight_init.with_decoder:
                    pytorch_cfg = make_super_animal_finetune_config(
                        project_config=cfg,
                        pose_config_path=config_path,
                        model_name=net_type,
                        detector_name=detector_type,
                        weight_init=weight_init,
                        save=True,
                    )
                else:
                    pytorch_cfg = make_pytorch_pose_config(
                        project_config=cfg,
                        pose_config_path=config_path,
                        net_type=net_type,
                        top_down=top_down,
                        detector_type=detector_type,
                        weight_init=weight_init,
                        save=True,
                    )

                make_pytorch_test_config(pytorch_cfg, path_test_config, save=True)

            # Setting inference cfg file:
            default_inf_path = Path(dlcparent_path) / "inference_cfg.yaml"
            inf_updates = dict(
                minimalnumberofconnections=int(len(cfg["multianimalbodyparts"]) / 2),
                topktoretain=len(cfg["individuals"]),
                withid=cfg.get("identity", False),
            )
            MakeInference_yaml(inf_updates, path_inference_config, default_inf_path)

            print(
                "The training dataset is successfully created. Use the function "
                "'train_network' to start training. Happy training!"
            )
        else:
            pass


def convert_cropped_to_standard_dataset(
    config_path,
    recreate_datasets=True,
    delete_crops=True,
    back_up=True,
):
    import pandas as pd
    import pickle
    import shutil
    from deeplabcut.generate_training_dataset import trainingsetmanipulation
    from deeplabcut.utils import read_plainconfig, write_config

    cfg = auxiliaryfunctions.read_config(config_path)
    videos_orig = cfg.pop("video_sets_original")
    is_cropped = cfg.pop("croppedtraining")
    if videos_orig is None or not is_cropped:
        print(
            "Labeled data do not appear to be cropped. "
            "Project will remain unchanged..."
        )
        return

    project_path = cfg["project_path"]

    if back_up:
        print("Backing up project...")
        shutil.copytree(project_path, project_path + "_bak", symlinks=True)

    if delete_crops:
        print("Deleting crops...")
        data_path = os.path.join(project_path, "labeled-data")
        for video in cfg["video_sets"]:
            _, filename, _ = trainingsetmanipulation._robust_path_split(video)
            if "_cropped" in video:  # One can never be too safe...
                shutil.rmtree(os.path.join(data_path, filename), ignore_errors=True)

    cfg["video_sets"] = videos_orig
    write_config(config_path, cfg)

    if not recreate_datasets:
        return

    datasets_folder = os.path.join(
        project_path,
        auxiliaryfunctions.get_training_set_folder(cfg),
    )
    df_old = pd.read_hdf(
        os.path.join(datasets_folder, "CollectedData_" + cfg["scorer"] + ".h5"),
    )

    def strip_cropped_image_name(path):
        head, filename = os.path.split(path)
        head = head.replace("_cropped", "")
        file, ext = filename.split(".")
        file = file.split("c")[0]
        return os.path.join(head, file + "." + ext)

    img_names_old = np.asarray(
        [strip_cropped_image_name(img) for img in df_old.index.to_list()]
    )
    df = merge_annotateddatasets(cfg, datasets_folder)
    img_names = df.index.to_numpy()
    train_idx = []
    test_idx = []
    pickle_files = []
    for filename in os.listdir(datasets_folder):
        if filename.endswith("pickle"):
            pickle_file = os.path.join(datasets_folder, filename)
            pickle_files.append(pickle_file)
            if filename.startswith("Docu"):
                with open(pickle_file, "rb") as f:
                    _, train_inds, test_inds, train_frac = pickle.load(f)
                    train_inds_temp = np.flatnonzero(
                        np.isin(img_names, img_names_old[train_inds])
                    )
                    test_inds_temp = np.flatnonzero(
                        np.isin(img_names, img_names_old[test_inds])
                    )
                    train_inds, test_inds = pad_train_test_indices(
                        train_inds_temp, test_inds_temp, train_frac
                    )
                    train_idx.append(train_inds)
                    test_idx.append(test_inds)

    # Search a pose_config.yaml file to parse missing information
    pose_config_path = ""
    for dirpath, _, filenames in os.walk(os.path.join(project_path, "dlc-models")):
        for file in filenames:
            if file.endswith("pose_cfg.yaml"):
                pose_config_path = os.path.join(dirpath, file)
                break
    pose_cfg = read_plainconfig(pose_config_path)
    net_type = pose_cfg["net_type"]
    if net_type == "resnet_50" and pose_cfg.get("multi_stage", False):
        net_type = "dlcrnet_ms5"

    # Clean the training-datasets folder prior to recreating the data pickles
    shuffle_inds = set()
    for file in pickle_files:
        os.remove(file)
        shuffle_inds.add(int(re.findall(r"shuffle(\d+)", file)[0]))
    create_multianimaltraining_dataset(
        config_path,
        trainIndices=train_idx,
        testIndices=test_idx,
        Shuffles=sorted(shuffle_inds),
        net_type=net_type,
        paf_graph=pose_cfg["partaffinityfield_graph"],
        crop_size=pose_cfg.get("crop_size", [400, 400]),
        crop_sampling=pose_cfg.get("crop_sampling", "hybrid"),
    )


--- File: deeplabcut/generate_training_dataset/trainingsetmanipulation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import math
import logging
import os
import os.path
import warnings

from functools import lru_cache
from pathlib import Path
from PIL import Image
from typing import List

import numpy as np
import pandas as pd
import yaml

import deeplabcut.compat as compat
import deeplabcut.generate_training_dataset.metadata as metadata
from deeplabcut.core.engine import Engine
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.utils import (
    auxiliaryfunctions,
    conversioncode,
    auxfun_models,
    auxfun_multianimal,
)
from deeplabcut.utils.auxfun_videos import VideoReader


def comparevideolistsanddatafolders(config):
    """
    Auxiliary function that compares the folders in labeled-data and the ones listed under video_sets (in the config file).

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.

    """
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [Path(i).stem for i in videos]
    alldatafolders = [
        fn
        for fn in os.listdir(Path(config).parent / "labeled-data")
        if "_labeled" not in fn
    ]

    print("Config file contains:", len(video_names))
    print("Labeled-data contains:", len(alldatafolders))

    for vn in video_names:
        if vn not in alldatafolders:
            print(vn, " is missing as a folder!")

    for vn in alldatafolders:
        if vn not in video_names:
            print(vn, " is missing in config file!")


def adddatasetstovideolistandviceversa(config):
    """
    First run comparevideolistsanddatafolders(config) to compare the folders in labeled-data and the ones listed under video_sets (in the config file).
    If you detect differences this function can be used to maker sure each folder has a video entry & vice versa.

    It corrects this problem in the following way:

    If a video entry in the config file does not contain a folder in labeled-data, then the entry is removed.
    If a folder in labeled-data does not contain a video entry in the config file then the prefix path will be added in front of the name of the labeled-data folder and combined
    with the suffix variable as an ending. Width and height will be added as cropping variables as passed on.

    Handle with care!

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.
    """
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"]
    video_names = [Path(i).stem for i in videos]

    alldatafolders = [
        fn
        for fn in os.listdir(Path(config).parent / "labeled-data")
        if "_labeled" not in fn and not fn.startswith(".")
    ]

    print("Config file contains:", len(video_names))
    print("Labeled-data contains:", len(alldatafolders))

    toberemoved = []
    for vn in video_names:
        if vn not in alldatafolders:
            print(vn, " is missing as a labeled folder >> removing key!")
            for fullvideo in videos:
                if vn in fullvideo:
                    toberemoved.append(fullvideo)

    for vid in toberemoved:
        del videos[vid]

    # Load updated lists:
    video_names = [Path(i).stem for i in videos]
    for vn in alldatafolders:
        if vn not in video_names:
            print(vn, " is missing in config file >> adding it!")
            # Find the corresponding video file
            found = False
            for file in os.listdir(os.path.join(cfg["project_path"], "videos")):
                if os.path.splitext(file)[0] == vn:
                    found = True
                    break
            if found:
                video_path = os.path.join(cfg["project_path"], "videos", file)
                clip = VideoReader(video_path)
                videos.update(
                    {video_path: {"crop": ", ".join(map(str, clip.get_bbox()))}}
                )

    auxiliaryfunctions.write_config(config, cfg)


def dropduplicatesinannotatinfiles(config):
    """

    Drop duplicate entries (of images) in annotation files (this should no longer happen, but might be useful).

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.

    """
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / "labeled-data" / Path(i) for i in video_names]

    for folder in folders:
        try:
            fn = os.path.join(str(folder), "CollectedData_" + cfg["scorer"] + ".h5")
            DC = pd.read_hdf(fn)
            numimages = len(DC.index)
            DC = DC[~DC.index.duplicated(keep="first")]
            if len(DC.index) < numimages:
                print("Dropped", numimages - len(DC.index))
                DC.to_hdf(fn, key="df_with_missing", mode="w")
                DC.to_csv(
                    os.path.join(str(folder), "CollectedData_" + cfg["scorer"] + ".csv")
                )

        except FileNotFoundError:
            print("Attention:", folder, "does not appear to have labeled data!")


def dropannotationfileentriesduetodeletedimages(config):
    """
    Drop entries for all deleted images in annotation files, i.e. for folders of the type: /labeled-data/*folder*/CollectedData_*scorer*.h5
    Will be carried out iteratively for all *folders* in labeled-data.

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.

    """
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / "labeled-data" / Path(i) for i in video_names]

    for folder in folders:
        fn = os.path.join(str(folder), "CollectedData_" + cfg["scorer"] + ".h5")
        try:
            DC = pd.read_hdf(fn)
        except FileNotFoundError:
            print("Attention:", folder, "does not appear to have labeled data!")
            continue
        dropped = False
        for imagename in DC.index:
            if os.path.isfile(os.path.join(cfg["project_path"], *imagename)):
                pass
            else:
                print("Dropping...", imagename)
                DC = DC.drop(imagename)
                dropped = True
        if dropped == True:
            DC.to_hdf(fn, key="df_with_missing", mode="w")
            DC.to_csv(
                os.path.join(str(folder), "CollectedData_" + cfg["scorer"] + ".csv")
            )


def dropimagesduetolackofannotation(config):
    """
    Drop images from corresponding folder for not annotated images: /labeled-data/*folder*/CollectedData_*scorer*.h5
    Will be carried out iteratively for all *folders* in labeled-data.

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.
    """
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / "labeled-data" / Path(i) for i in video_names]

    for folder in folders:
        h5file = os.path.join(str(folder), "CollectedData_" + cfg["scorer"] + ".h5")
        try:
            DC = pd.read_hdf(h5file)
        except FileNotFoundError:
            print("Attention:", folder, "does not appear to have labeled data!")
            continue
        conversioncode.guarantee_multiindex_rows(DC)
        annotatedimages = [fn[-1] for fn in DC.index]
        imagelist = [fns for fns in os.listdir(str(folder)) if ".png" in fns]
        print("Annotated images: ", len(annotatedimages), " In folder:", len(imagelist))
        for imagename in imagelist:
            if imagename in annotatedimages:
                pass
            else:
                fullpath = os.path.join(
                    cfg["project_path"], "labeled-data", folder, imagename
                )
                if os.path.isfile(fullpath):
                    print("Deleting", fullpath)
                    os.remove(fullpath)

        annotatedimages = [fn[-1] for fn in DC.index]
        imagelist = [fns for fns in os.listdir(str(folder)) if ".png" in fns]
        print(
            "PROCESSED:",
            folder,
            " now # of annotated images: ",
            len(annotatedimages),
            " in folder:",
            len(imagelist),
        )


def dropunlabeledframes(config):
    """
    Drop entries such that all the bodyparts are not labeled from the annotation files, i.e. h5 and csv files
    Will be carried out iteratively for all *folders* in labeled-data.

    Parameter
    ----------
    config : string
        String containing the full path of the config file in the project.

    """
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / "labeled-data" / Path(i) for i in video_names]

    for folder in folders:
        h5file = os.path.join(str(folder), "CollectedData_" + cfg["scorer"] + ".h5")
        try:
            DC = pd.read_hdf(h5file)
        except FileNotFoundError:
            print("Skipping ", folder, "...")
            continue
        before_len = len(DC.index)
        DC = DC.dropna(how="all")  # drop rows where all values are missing(NaN)
        after_len = len(DC.index)
        dropped = before_len - after_len
        if dropped:
            DC.to_hdf(h5file, key="df_with_missing", mode="w")
            DC.to_csv(
                os.path.join(str(folder), "CollectedData_" + cfg["scorer"] + ".csv")
            )

            print("Dropped ", dropped, "entries in ", folder)

    print("Done.")


def check_labels(
    config,
    Labels=["+", ".", "x"],
    scale=1,
    dpi=100,
    draw_skeleton=True,
    visualizeindividuals=True,
):
    """Check the labeled frames.

    Double check if the labels were at the correct locations and stored in the proper
    file format.

    This creates a new subdirectory for each video under the 'labeled-data' and all the
    frames are plotted with the labels.

    Make sure that these labels are fine.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    Labels: list, default='+'
        List of at least 3 matplotlib markers. The first one will be used to indicate
        the human ground truth location (Default: +)

    scale : float, default=1
        Change the relative size of the output images.

    dpi : int, optional, default=100
        Output resolution in dpi.

    draw_skeleton: bool, default=True
        Plot skeleton overlaid over body parts.

    visualizeindividuals: bool, default: True.
        For a multianimal project, if True, the different individuals have different
        colors (and all bodyparts the same). If False, the colors change over bodyparts
        rather than individuals.

    Returns
    -------
    None

    Examples
    --------
    >>> deeplabcut.check_labels('/analysis/project/reaching-task/config.yaml')
    """

    from deeplabcut.utils import visualization

    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [_robust_path_split(video)[1] for video in videos]

    folders = [
        os.path.join(cfg["project_path"], "labeled-data", str(Path(i)))
        for i in video_names
    ]
    print("Creating images with labels by %s." % cfg["scorer"])
    for folder in folders:
        try:
            DataCombined = pd.read_hdf(
                os.path.join(str(folder), "CollectedData_" + cfg["scorer"] + ".h5")
            )
            conversioncode.guarantee_multiindex_rows(DataCombined)
            if cfg.get("multianimalproject", False):
                color_by = "individual" if visualizeindividuals else "bodypart"
            else:  # for single animal projects
                color_by = "bodypart"

            visualization.make_labeled_images_from_dataframe(
                DataCombined,
                cfg,
                folder,
                scale,
                dpi=dpi,
                keypoint=Labels[0],
                draw_skeleton=draw_skeleton,
                color_by=color_by,
            )
        except FileNotFoundError:
            print("Attention:", folder, "does not appear to have labeled data!")

    print(
        "If all the labels are ok, then use the function 'create_training_dataset' to create the training dataset!"
    )


def boxitintoacell(joints):
    """Auxiliary function for creating matfile."""
    outer = np.array([[None]], dtype=object)
    outer[0, 0] = np.array(joints, dtype="int64")
    return outer


def ParseYaml(configfile):
    raw = open(configfile).read()
    docs = []
    for raw_doc in raw.split("\n---"):
        try:
            docs.append(yaml.load(raw_doc, Loader=yaml.SafeLoader))
        except SyntaxError:
            docs.append(raw_doc)
    return docs


def MakeTrain_pose_yaml(
    itemstochange,
    saveasconfigfile,
    defaultconfigfile,
    items2drop: dict | None = None,
    save: bool = True,
):
    if items2drop is None:
        items2drop = {}

    docs = ParseYaml(defaultconfigfile)
    for key in items2drop.keys():
        if key in docs[0].keys():
            docs[0].pop(key)

    for key in itemstochange.keys():
        docs[0][key] = itemstochange[key]

    if save:
        with open(saveasconfigfile, "w") as f:
            yaml.dump(docs[0], f)

    return docs[0]


def MakeTest_pose_yaml(
    dictionary,
    keys2save,
    saveasfile,
    nmsradius=None,
    minconfidence=None,
    sigma=None,
    locref_smooth=None,
):
    dict_test = {}
    for key in keys2save:
        dict_test[key] = dictionary[key]

    # adding important values for multianiaml project:
    if nmsradius is not None:
        dict_test["nmsradius"] = nmsradius
    if minconfidence is not None:
        dict_test["minconfidence"] = minconfidence
    if sigma is not None:
        dict_test["sigma"] = sigma
    if locref_smooth is not None:
        dict_test["locref_smooth"] = locref_smooth

    dict_test["scoremap_dir"] = "test"
    with open(saveasfile, "w") as f:
        yaml.dump(dict_test, f)


def MakeInference_yaml(itemstochange, saveasconfigfile, defaultconfigfile):
    docs = ParseYaml(defaultconfigfile)
    for key in itemstochange.keys():
        docs[0][key] = itemstochange[key]

    with open(saveasconfigfile, "w") as f:
        yaml.dump(docs[0], f)
    return docs[0]


def _robust_path_split(path):
    sep = "\\" if "\\" in path else "/"
    splits = path.rsplit(sep, 1)
    if len(splits) == 1:
        parent = "."
        file = splits[0]
    elif len(splits) == 2:
        parent, file = splits
    else:
        raise ("Unknown filepath split for path {}".format(path))
    filename, ext = os.path.splitext(file)
    return parent, filename, ext


def parse_video_filenames(videos: List[str]) -> List[str]:
    """Parses the names of all videos listed in a project's ``config.yaml`` file

    Goes through the paths all videos listed for a project, and removes entries with a
    duplicate video name (e.g. if a video is listed twice, once with the path
    ``/data/video-1.mov`` and once with the path ``/my-dlc-project/videos/video-1.mov``,
    then ``video-1`` will only be returned once). The order of videos listed is
    preserved.

    This prevents the same labeled-data to be added multiple times when merging
    annotated datasets.

    Prints a warning for each filename with duplicate video paths.

    Args:
        videos: the videos listed in the project's config.yaml file

    Returns:
        the filenames of videos listed in the project's config.yaml file, with duplicate
        entries removed
    """
    filenames = []
    filename_to_videos = {}
    for video in videos:
        _, filename, _ = _robust_path_split(video)
        videos_with_filename = filename_to_videos.get(filename, [])
        if len(videos_with_filename) == 0:
            filenames.append(filename)

        videos_with_filename.append(video)
        filename_to_videos[filename] = videos_with_filename

    for filename, videos in filename_to_videos.items():
        if len(videos) > 1:
            video_str = "\n  * " + "\n  * ".join(videos)
            logging.warning(
                f"Found multiple videos with the same filename (``{filename}``). To "
                f"avoid issues, please edit your project's `config.yaml` file to have "
                f"each video added only once.\nDuplicate entries: {video_str}"
            )

    return filenames


def merge_annotateddatasets(cfg, trainingsetfolder_full):
    """
    Merges all the h5 files for all labeled-datasets (from individual videos).

    This is a bit of a mess because of cross platform compatibility.

    Within platform comp. is straightforward. But if someone labels on windows and wants to train on a unix cluster or colab...
    """
    AnnotationData = []
    data_path = Path(os.path.join(cfg["project_path"], "labeled-data"))
    videos = cfg["video_sets"].keys()
    video_filenames = parse_video_filenames(videos)
    for filename in video_filenames:
        file_path = os.path.join(
            data_path / filename, f'CollectedData_{cfg["scorer"]}.h5'
        )
        try:
            data = pd.read_hdf(file_path)
            conversioncode.guarantee_multiindex_rows(data)
            if data.columns.levels[0][0] != cfg["scorer"]:
                print(
                    f"{file_path} labeled by a different scorer. This data will not be utilized in training dataset creation. If you need to merge datasets across scorers, see https://github.com/DeepLabCut/DeepLabCut/wiki/Using-labeled-data-in-DeepLabCut-that-was-annotated-elsewhere-(or-merge-across-labelers)"
                )
                continue
            AnnotationData.append(data)
        except FileNotFoundError:
            print(file_path, " not found (perhaps not annotated).")

    if not len(AnnotationData):
        print(
            "Annotation data was not found by splitting video paths (from config['video_sets']). An alternative route is taken..."
        )
        AnnotationData = conversioncode.merge_windowsannotationdataONlinuxsystem(cfg)
        if not len(AnnotationData):
            print("No data was found!")
            return

    AnnotationData = pd.concat(AnnotationData).sort_index()
    # When concatenating DataFrames with misaligned column labels,
    # all sorts of reordering may happen (mainly depending on 'sort' and 'join')
    # Ensure the 'bodyparts' level agrees with the order in the config file.
    if cfg.get("multianimalproject", False):
        (
            _,
            uniquebodyparts,
            multianimalbodyparts,
        ) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
        bodyparts = multianimalbodyparts + uniquebodyparts
    else:
        bodyparts = cfg["bodyparts"]
    AnnotationData = AnnotationData.reindex(
        bodyparts, axis=1, level=AnnotationData.columns.names.index("bodyparts")
    )
    filename = os.path.join(trainingsetfolder_full, f'CollectedData_{cfg["scorer"]}')
    AnnotationData.to_hdf(filename + ".h5", key="df_with_missing", mode="w")
    AnnotationData.to_csv(filename + ".csv")  # human readable.
    return AnnotationData


def SplitTrials(
    trialindex,
    trainFraction=0.8,
    enforce_train_fraction=False,
):
    """Split a trial index into train and test sets. Also checks that the trainFraction is a two digit number between 0 an 1. The reason
    is that the folders contain the trainfraction as int(100*trainFraction).
    If enforce_train_fraction is True, train and test indices are padded with -1
    such that the ratio of their lengths is exactly the desired train fraction.
    """
    if trainFraction > 1 or trainFraction < 0:
        print(
            "The training fraction should be a two digit number between 0 and 1; i.e. 0.95. Please change accordingly."
        )
        return ([], [])

    if abs(trainFraction - round(trainFraction, 2)) > 0:
        print(
            "The training fraction should be a two digit number between 0 and 1; i.e. 0.95. Please change accordingly."
        )
        return ([], [])
    else:
        index_len = len(trialindex)
        train_fraction = round(trainFraction, 2)
        train_size = index_len * train_fraction
        shuffle = np.random.permutation(trialindex)
        test_indices = shuffle[int(train_size) :]
        train_indices = shuffle[: int(train_size)]
        if enforce_train_fraction and not train_size.is_integer():
            train_indices, test_indices = pad_train_test_indices(
                train_indices,
                test_indices,
                train_fraction,
            )

        return train_indices, test_indices


def pad_train_test_indices(train_inds, test_inds, train_fraction):
    n_train_inds = len(train_inds)
    n_test_inds = len(test_inds)
    index_len = n_train_inds + n_test_inds
    if n_train_inds / index_len == train_fraction:
        return

    # Determine the index length required to guarantee
    # the train–test ratio is exactly the desired one.
    min_length_req = int(100 / math.gcd(100, int(round(100 * train_fraction))))
    min_n_train = int(round(min_length_req * train_fraction))
    min_n_test = min_length_req - min_n_train
    mult = max(
        math.ceil(n_train_inds / min_n_train),
        math.ceil(n_test_inds / min_n_test),
    )
    n_train = mult * min_n_train
    n_test = mult * min_n_test
    # Pad indices so lengths agree
    train_inds = np.append(train_inds, [-1] * (n_train - n_train_inds))
    test_inds = np.append(test_inds, [-1] * (n_test - n_test_inds))
    return train_inds, test_inds


def mergeandsplit(config, trainindex=0, uniform=True):
    """
    This function allows additional control over "create_training_dataset".

    Merge annotated data sets (from different folders) and split data in a specific way, returns the split variables (train/test indices).
    Importantly, this allows one to freeze a split.

    One can also either create a uniform split (uniform = True; thereby indexing TrainingFraction in config file) or leave-one-folder out split
    by passing the index of the corresponding video from the config.yaml file as variable trainindex.

    Parameter
    ----------
    config : string
        Full path of the config.yaml file as a string.

    trainindex: int, optional
        Either (in case uniform = True) indexes which element of TrainingFraction in the config file should be used (note it is a list!).
        Alternatively (uniform = False) indexes which folder is dropped, i.e. the first if trainindex=0, the second if trainindex =1, etc.

    uniform: bool, optional
        Perform uniform split (disregarding folder structure in labeled data), or (if False) leave one folder out.

    Examples
    --------
    To create a leave-one-folder-out model:
    >>> trainIndices, testIndices=deeplabcut.mergeandsplit(config,trainindex=0,uniform=False)
    returns the indices for the first video folder (as defined in config file) as testIndices and all others as trainIndices.
    You can then create the training set by calling (e.g. defining it as Shuffle 3):
    >>> deeplabcut.create_training_dataset(config,Shuffles=[3],trainIndices=trainIndices,testIndices=testIndices)

    To freeze a (uniform) split (i.e. iid sampled from all the data):
    >>> trainIndices, testIndices=deeplabcut.mergeandsplit(config,trainindex=0,uniform=True)

    You can then create two model instances that have the identical trainingset. Thereby you can assess the role of various parameters on the performance of DLC.
    >>> deeplabcut.create_training_dataset(config,Shuffles=[0,1],trainIndices=[trainIndices, trainIndices],testIndices=[testIndices, testIndices])
    --------

    """
    # Loading metadata from config file:
    cfg = auxiliaryfunctions.read_config(config)
    scorer = cfg["scorer"]
    project_path = cfg["project_path"]
    # Create path for training sets & store data there
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(
        cfg
    )  # Path concatenation OS platform independent
    auxiliaryfunctions.attempt_to_make_folder(
        Path(os.path.join(project_path, str(trainingsetfolder))), recursive=True
    )
    fn = os.path.join(project_path, trainingsetfolder, "CollectedData_" + cfg["scorer"])

    try:
        Data = pd.read_hdf(fn + ".h5")
    except FileNotFoundError:
        Data = merge_annotateddatasets(
            cfg,
            Path(os.path.join(project_path, trainingsetfolder)),
        )
        if Data is None:
            return [], []

    conversioncode.guarantee_multiindex_rows(Data)
    Data = Data[scorer]  # extract labeled data

    if uniform == True:
        TrainingFraction = cfg["TrainingFraction"]
        trainFraction = TrainingFraction[trainindex]
        trainIndices, testIndices = SplitTrials(
            range(len(Data.index)),
            trainFraction,
            True,
        )
    else:  # leave one folder out split
        videos = cfg["video_sets"].keys()
        test_video_name = [Path(i).stem for i in videos][trainindex]
        print("Excluding the following folder (from training):", test_video_name)
        trainIndices, testIndices = [], []
        for index, name in enumerate(Data.index):
            if test_video_name == name[1]:  # this is the video name
                # print(name,test_video_name)
                testIndices.append(index)
            else:
                trainIndices.append(index)

    return trainIndices, testIndices


@lru_cache(maxsize=None)
def read_image_shape_fast(path):
    # Blazing fast and does not load the image into memory
    with Image.open(path) as img:
        width, height = img.size
        return len(img.getbands()), height, width


def format_training_data(df, train_inds, nbodyparts, project_path):
    train_data = []
    matlab_data = []

    def to_matlab_cell(array):
        outer = np.array([[None]], dtype=object)
        outer[0, 0] = array.astype("int64")
        return outer

    for i in train_inds:
        data = dict()
        filename = df.index[i]
        data["image"] = filename
        img_shape = read_image_shape_fast(os.path.join(project_path, *filename))
        data["size"] = img_shape
        temp = df.iloc[i].values.reshape(-1, 2)
        joints = np.c_[range(nbodyparts), temp]
        joints = joints[~np.isnan(joints).any(axis=1)].astype(int)
        # Check that points lie within the image
        inside = np.logical_and(
            np.logical_and(joints[:, 1] < img_shape[2], joints[:, 1] > 0),
            np.logical_and(joints[:, 2] < img_shape[1], joints[:, 2] > 0),
        )
        if not all(inside):
            joints = joints[inside]
        if joints.size:  # Exclude images without labels
            data["joints"] = joints
            train_data.append(data)
            matlab_data.append(
                (
                    np.array([data["image"]], dtype="U"),
                    np.array([data["size"]]),
                    to_matlab_cell(data["joints"]),
                )
            )
    matlab_data = np.asarray(
        matlab_data, dtype=[("image", "O"), ("size", "O"), ("joints", "O")]
    )
    return train_data, matlab_data


def create_training_dataset(
    config,
    num_shuffles=1,
    Shuffles=None,
    windows2linux=False,
    userfeedback=True,
    trainIndices=None,
    testIndices=None,
    net_type=None,
    detector_type=None,
    augmenter_type=None,
    posecfg_template=None,
    superanimal_name="",
    weight_init: WeightInitialization | None = None,
    engine: Engine | None = None,
):
    """Creates a training dataset.

    Labels from all the extracted frames are merged into a single .h5 file.
    Only the videos included in the config file are used to create this dataset.

    Parameters
    ----------
    config : string
        Full path of the ``config.yaml`` file as a string.

    num_shuffles : int, optional, default=1
        Number of shuffles of training dataset to create, i.e. ``[1,2,3]`` for
        ``num_shuffles=3``.

    Shuffles: list[int], optional
        Alternatively the user can also give a list of shuffles.

    userfeedback: bool, optional, default=True
        If ``False``, all requested train/test splits are created (no matter if they
        already exist). If you want to assure that previous splits etc. are not
        overwritten, set this to ``True`` and you will be asked for each split.

    trainIndices: list of lists, optional, default=None
        List of one or multiple lists containing train indexes.
        A list containing two lists of training indexes will produce two splits.

    testIndices: list of lists, optional, default=None
        List of one or multiple lists containing test indexes.

    net_type: list, optional, default=None
        Type of networks. The options available depend on which engine is used.
        Currently supported options are:
            TensorFlow
                * ``resnet_50``
                * ``resnet_101``
                * ``resnet_152``
                * ``mobilenet_v2_1.0``
                * ``mobilenet_v2_0.75``
                * ``mobilenet_v2_0.5``
                * ``mobilenet_v2_0.35``
                * ``efficientnet-b0``
                * ``efficientnet-b1``
                * ``efficientnet-b2``
                * ``efficientnet-b3``
                * ``efficientnet-b4``
                * ``efficientnet-b5``
                * ``efficientnet-b6``
            PyTorch (call ``deeplabcut.pose_estimation_pytorch.available_models()`` for
            a complete list)
                * ``resnet_50``
                * ``resnet_101``
                * ``hrnet_w18``
                * ``hrnet_w32``
                * ``hrnet_w48``
                * ``dekr_w18``
                * ``dekr_w32``
                * ``dekr_w48``
                * ``top_down_resnet_50``
                * ``top_down_resnet_101``
                * ``top_down_hrnet_w18``
                * ``top_down_hrnet_w32``
                * ``top_down_hrnet_w48``
                * ``animaltokenpose_base``

    detector_type: string, optional, default=None
        Only for the PyTorch engine.
        When passing creating shuffles for top-down models, you can specify which
        detector you want. If the detector_type is None, the ```ssdlite``` will be used.
        The list of all available detectors can be obtained by calling
        ``deeplabcut.pose_estimation_pytorch.available_detectors()``. Supported options:
            * ``ssdlite``
            * ``fasterrcnn_mobilenet_v3_large_fpn``
            * ``fasterrcnn_resnet50_fpn_v2``

    augmenter_type: string, optional, default=None
        Type of augmenter. The options available depend on which engine is used.
        Currently supported options are:
            TensorFlow
                * ``default``
                * ``scalecrop``
                * ``imgaug``
                * ``tensorpack``
                * ``deterministic``
            PyTorch
                * ``albumentations``

    posecfg_template: string, optional, default=None
        Only for the TensorFlow engine.
        Path to a ``pose_cfg.yaml`` file to use as a template for generating the new
        one for the current iteration. Useful if you would like to start with the same
        parameters a previous training iteration. None uses the default
        ``pose_cfg.yaml``.

    superanimal_name: string, optional, default=""
        Only for the TensorFlow engine. For the PyTorch engine, use the ``weight_init``
        parameter.
        Specify the superanimal name is transfer learning with superanimal is desired.
        This makes sure the pose config template uses superanimal configs as template.

    weight_init: WeightInitialisation, optional, default=None
        PyTorch engine only. Specify how model weights should be initialized. The
        default mode uses transfer learning from ImageNet weights.

    engine: Engine, optional
        Whether to create a pose config for a Tensorflow or PyTorch model. Defaults to
        the value specified in the project configuration file. If no engine is specified
        for the project, defaults to ``deeplabcut.compat.DEFAULT_ENGINE``.

    Returns
    -------
    list(tuple) or None
        If training dataset was successfully created, a list of tuples is returned.
        The first two elements in each tuple represent the training fraction and the
        shuffle value. The last two elements in each tuple are arrays of integers
        representing the training and test indices.

        Returns None if training dataset could not be created.

    Notes
    -----
    Use the function ``add_new_videos`` at any stage of the project to add more videos
    to the project.

    Examples
    --------

    Linux/MacOS:
    >>> deeplabcut.create_training_dataset(
            '/analysis/project/reaching-task/config.yaml', num_shuffles=1,
        )

    >>> deeplabcut.create_training_dataset(
            '/analysis/project/reaching-task/config.yaml', Shuffles=[2], engine=deeplabcut.Engine.TF,
        )

    Windows:
    >>> deeplabcut.create_training_dataset(
            'C:\\Users\\Ulf\\looming-task\\config.yaml', Shuffles=[3,17,5],
        )
    """
    import scipy.io as sio

    if windows2linux:
        # DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__
        warnings.warn(
            "`windows2linux` has no effect since 2.2.0.4 and will be removed in 2.2.1.",
            FutureWarning,
        )

    # Loading metadata from config file:
    cfg = auxiliaryfunctions.read_config(config)
    dlc_root_path = auxiliaryfunctions.get_deeplabcut_path()

    if superanimal_name != "":
        # FIXME(niels): this is deprecated
        supermodels = parse_available_supermodels()
        posecfg_template = os.path.join(
            dlc_root_path,
            "pose_estimation_tensorflow",
            "superanimal_configs",
            supermodels[superanimal_name],
        )

    if posecfg_template:
        if (
            not posecfg_template.endswith("pose_cfg.yaml")
            and not posecfg_template.endswith("superquadruped.yaml")
            and not posecfg_template.endswith("supertopview.yaml")
        ):
            raise ValueError(
                "posecfg_template argument must contain path to a pose_cfg.yaml file"
            )
        else:
            print("Reloading pose_cfg parameters from " + posecfg_template + "\n")
            from deeplabcut.utils.auxiliaryfunctions import read_plainconfig

        prior_cfg = read_plainconfig(posecfg_template)
    if cfg.get("multianimalproject", False):
        from deeplabcut.generate_training_dataset.multiple_individuals_trainingsetmanipulation import (
            create_multianimaltraining_dataset,
        )

        create_multianimaltraining_dataset(
            config,
            num_shuffles,
            Shuffles,
            net_type=net_type,
            detector_type=detector_type,
            trainIndices=trainIndices,
            testIndices=testIndices,
            userfeedback=userfeedback,
            engine=engine,
            weight_init=weight_init,
        )
    else:
        scorer = cfg["scorer"]
        project_path = cfg["project_path"]
        if engine is None:
            engine = compat.get_project_engine(cfg)

        # Create path for training sets & store data there
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(
            cfg
        )  # Path concatenation OS platform independent
        auxiliaryfunctions.attempt_to_make_folder(
            Path(os.path.join(project_path, str(trainingsetfolder))), recursive=True
        )

        # Create the trainset metadata file, if it doesn't yet exist
        if not metadata.TrainingDatasetMetadata.path(cfg).exists():
            trainset_metadata = metadata.TrainingDatasetMetadata.create(cfg)
            trainset_metadata.save()

        Data = merge_annotateddatasets(
            cfg,
            Path(os.path.join(project_path, trainingsetfolder)),
        )
        if Data is None:
            return
        Data = Data[scorer]  # extract labeled data

        # loading & linking pretrained models
        if net_type is None:  # loading & linking pretrained models
            net_type = cfg.get("default_net_type", "resnet_50")
        elif engine == Engine.PYTORCH:
            pass
        else:
            if (
                "resnet" in net_type
                or "mobilenet" in net_type
                or "efficientnet" in net_type
                or "dlcrnet" in net_type
            ):
                pass
            else:
                raise ValueError("Invalid network type:", net_type)

        top_down = False
        if engine == Engine.PYTORCH:
            if net_type.startswith("top_down_"):
                top_down = True
                net_type = net_type[len("top_down_") :]

        augmenters = compat.get_available_aug_methods(engine)
        default_augmenter = augmenters[0]
        if augmenter_type is None:
            augmenter_type = cfg.get("default_augmenter", default_augmenter)

            if augmenter_type is None:  # this could be in config.yaml for old projects!
                # updating variable if null/None! #backwardscompatability
                augmenter_type = default_augmenter
                auxiliaryfunctions.edit_config(
                    config, {"default_augmenter": augmenter_type}
                )
            elif augmenter_type not in augmenters:
                # as the default augmenter might not be available for the given engine
                augmenter_type = default_augmenter
                logging.info(
                    f"Default augmenter {augmenter_type} not available for engine "
                    f"{engine}: using {default_augmenter} instead"
                )

        if augmenter_type not in augmenters:
            if engine != Engine.PYTORCH:
                raise ValueError(
                    f"Invalid augmenter type: {augmenter_type} (available: for "
                    f"engine={engine}: {augmenters})"
                )

            logging.info(f"Switching augmentation to {default_augmenter} for PyTorch")
            augmenter_type = default_augmenter

        if posecfg_template:
            if net_type != prior_cfg["net_type"]:
                print(
                    "WARNING: Specified net_type does not match net_type from posecfg_template path entered. Proceed with caution."
                )
            if augmenter_type != prior_cfg["dataset_type"]:
                print(
                    "WARNING: Specified augmenter_type does not match dataset_type from posecfg_template path entered. Proceed with caution."
                )

        # Loading the encoder (if necessary downloading from TF)
        dlcparent_path = auxiliaryfunctions.get_deeplabcut_path()
        if not posecfg_template:
            defaultconfigfile = os.path.join(dlcparent_path, "pose_cfg.yaml")
        elif posecfg_template:
            defaultconfigfile = posecfg_template

        if engine == Engine.PYTORCH:
            model_path = dlcparent_path
        else:
            model_path = auxfun_models.check_for_weights(net_type, Path(dlcparent_path))

        Shuffles = validate_shuffles(cfg, Shuffles, num_shuffles, userfeedback)

        # print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)
        if trainIndices is None and testIndices is None:
            splits = [
                (
                    trainFraction,
                    shuffle,
                    SplitTrials(range(len(Data.index)), trainFraction),
                )
                for trainFraction in cfg["TrainingFraction"]
                for shuffle in Shuffles
            ]
        else:
            if len(trainIndices) != len(testIndices) != len(Shuffles):
                raise ValueError(
                    "Number of Shuffles and train and test indexes should be equal."
                )
            splits = []
            for shuffle, (train_inds, test_inds) in enumerate(
                zip(trainIndices, testIndices)
            ):
                trainFraction = round(
                    len(train_inds) * 1.0 / (len(train_inds) + len(test_inds)), 2
                )
                print(
                    f"You passed a split with the following fraction: {int(100 * trainFraction)}%"
                )
                # Now that the training fraction is guaranteed to be correct,
                # the values added to pad the indices are removed.
                train_inds = np.asarray(train_inds)
                train_inds = train_inds[train_inds != -1]
                test_inds = np.asarray(test_inds)
                test_inds = test_inds[test_inds != -1]
                splits.append(
                    (trainFraction, Shuffles[shuffle], (train_inds, test_inds))
                )

        bodyparts = auxiliaryfunctions.get_bodyparts(cfg)
        nbodyparts = len(bodyparts)
        for trainFraction, shuffle, (trainIndices, testIndices) in splits:
            if len(trainIndices) > 0:
                if userfeedback:
                    trainposeconfigfile, _, _ = compat.return_train_network_path(
                        config,
                        shuffle=shuffle,
                        trainingsetindex=cfg["TrainingFraction"].index(trainFraction),
                        engine=engine,
                    )
                    if trainposeconfigfile.is_file():
                        askuser = input(
                            "The model folder is already present. If you continue, it will overwrite the existing model (split). Do you want to continue?(yes/no): "
                        )
                        if (
                            askuser == "no"
                            or askuser == "No"
                            or askuser == "N"
                            or askuser == "No"
                        ):
                            raise Exception(
                                "Use the Shuffles argument as a list to specify a different shuffle index. Check out the help for more details."
                            )

                ####################################################
                # Generating data structure with labeled information & frame metadata (for deep cut)
                ####################################################
                # Make training file!
                (
                    datafilename,
                    metadatafilename,
                ) = auxiliaryfunctions.get_data_and_metadata_filenames(
                    trainingsetfolder, trainFraction, shuffle, cfg
                )

                ################################################################################
                # Saving data file (convert to training file for deeper cut (*.mat))
                ################################################################################
                data, MatlabData = format_training_data(
                    Data, trainIndices, nbodyparts, project_path
                )
                sio.savemat(
                    os.path.join(project_path, datafilename), {"dataset": MatlabData}
                )

                ################################################################################
                # Saving metadata (Pickle file)
                ################################################################################
                auxiliaryfunctions.save_metadata(
                    os.path.join(project_path, metadatafilename),
                    data,
                    trainIndices,
                    testIndices,
                    trainFraction,
                )
                metadata.update_metadata(
                    cfg=cfg,
                    train_fraction=trainFraction,
                    shuffle=shuffle,
                    engine=engine,
                    train_indices=trainIndices,
                    test_indices=testIndices,
                    overwrite=not userfeedback,
                )

                ################################################################################
                # Creating file structure for training &
                # Test files as well as pose_yaml files (containing training and testing information)
                #################################################################################
                modelfoldername = auxiliaryfunctions.get_model_folder(
                    trainFraction,
                    shuffle,
                    cfg,
                    engine=engine,
                )
                auxiliaryfunctions.attempt_to_make_folder(
                    Path(config).parents[0] / modelfoldername, recursive=True
                )
                auxiliaryfunctions.attempt_to_make_folder(
                    str(Path(config).parents[0] / modelfoldername) + "/train"
                )
                auxiliaryfunctions.attempt_to_make_folder(
                    str(Path(config).parents[0] / modelfoldername) + "/test"
                )

                path_train_config = str(
                    os.path.join(
                        cfg["project_path"],
                        Path(modelfoldername),
                        "train",
                        engine.pose_cfg_name,
                    )
                )
                path_test_config = str(
                    os.path.join(
                        cfg["project_path"],
                        Path(modelfoldername),
                        "test",
                        "pose_cfg.yaml",
                    )
                )
                if engine == Engine.TF:
                    items2change = {
                        "dataset": datafilename,
                        "engine": engine.aliases[0],
                        "metadataset": metadatafilename,
                        "num_joints": len(bodyparts),
                        "all_joints": [[i] for i in range(len(bodyparts))],
                        "all_joints_names": [str(bpt) for bpt in bodyparts],
                        "init_weights": model_path,
                        "project_path": str(cfg["project_path"]),
                        "net_type": net_type,
                        "dataset_type": augmenter_type,
                    }

                    items2drop = {}
                    if augmenter_type == "scalecrop":
                        # these values are dropped as scalecrop
                        # doesn't have rotation implemented
                        items2drop = {"rotation": 0, "rotratio": 0.0}
                    # Also drop maDLC smart cropping augmentation parameters
                    for key in [
                        "pre_resize",
                        "crop_size",
                        "max_shift",
                        "crop_sampling",
                    ]:
                        items2drop[key] = None

                    trainingdata = MakeTrain_pose_yaml(
                        items2change,
                        path_train_config,
                        defaultconfigfile,
                        items2drop,
                        save=(engine == Engine.TF),
                    )

                    keys2save = [
                        "dataset",
                        "num_joints",
                        "all_joints",
                        "all_joints_names",
                        "net_type",
                        "init_weights",
                        "global_scale",
                        "location_refinement",
                        "locref_stdev",
                    ]
                    MakeTest_pose_yaml(trainingdata, keys2save, path_test_config)
                    print(
                        "The training dataset is successfully created. Use the function"
                        "'train_network' to start training. Happy training!"
                    )
                elif engine == Engine.PYTORCH:
                    from deeplabcut.pose_estimation_pytorch.config.make_pose_config import (
                        make_pytorch_pose_config,
                        make_pytorch_test_config,
                    )
                    from deeplabcut.pose_estimation_pytorch.modelzoo.config import (
                        make_super_animal_finetune_config,
                    )

                    if weight_init is not None and weight_init.with_decoder:
                        pytorch_cfg = make_super_animal_finetune_config(
                            project_config=cfg,
                            pose_config_path=path_train_config,
                            model_name=net_type,
                            detector_name=detector_type,
                            weight_init=weight_init,
                            save=True,
                        )
                    else:
                        pytorch_cfg = make_pytorch_pose_config(
                            project_config=cfg,
                            pose_config_path=path_train_config,
                            net_type=net_type,
                            top_down=top_down,
                            detector_type=detector_type,
                            weight_init=weight_init,
                            save=True,
                        )

                    make_pytorch_test_config(pytorch_cfg, path_test_config, save=True)

        return splits


def get_largestshuffle_index(config):
    """Returns the largest shuffle for all dlc-models in the current iteration."""
    shuffle_indices = get_existing_shuffle_indices(config)
    if len(shuffle_indices) > 0:
        return shuffle_indices[-1]

    return None


def get_existing_shuffle_indices(
    cfg: dict | str | Path,
    train_fraction: float | None = None,
    engine: Engine | None = None,
) -> List[int]:
    """
    Args:
        cfg: The content of a project configuration file, or the path to the project
            configuration file.
        train_fraction: If defined, only get the indices of shuffles with this train
            fraction.
        engine: If specified, returns only the shuffle indices that were created with
            the given engine. Can only be used when train_fraction is also defined.

    Returns:
        the indices of existing shuffles for this iteration of the project, sorted by
        ascending index
    """

    def is_valid_data_stem(stem: str) -> bool:
        if len(stem) == 0:
            return False
        suffix = stem.split("_")[-1]
        if len(suffix) == 0:
            return False
        info = suffix.split("shuffle")
        if len(info) != 2:
            return False
        train_frac, idx = info
        return (
            train_frac.isdigit()
            and idx.isdigit()
            and (train_fraction is None or int(train_frac) == int(100 * train_fraction))
        )

    if isinstance(cfg, (str, Path)):
        cfg = auxiliaryfunctions.read_config(cfg)

    project = Path(cfg["project_path"])
    trainset_folder = project / auxiliaryfunctions.get_training_set_folder(cfg)
    if not trainset_folder.exists():
        return []

    shuffle_indices = [
        int(p.stem.split("shuffle")[-1])
        for p in trainset_folder.iterdir()
        if (
            p.stem.startswith("Documentation_data")
            and p.suffix == ".pickle"
            and is_valid_data_stem(p.stem)
        )
    ]
    if engine is not None:
        if train_fraction is None:
            raise ValueError(
                f"Must select {train_fraction} to filter shuffles by engine"
            )

        shuffle_indices = [
            idx
            for idx in shuffle_indices
            if (
                project
                / auxiliaryfunctions.get_model_folder(
                    trainFraction=train_fraction,
                    shuffle=idx,
                    cfg=cfg,
                    engine=engine,
                )
            ).exists()
        ]

    return sorted(shuffle_indices)


def validate_shuffles(
    cfg: dict,
    shuffles: list[int] | None,
    num_shuffles: int | None,
    userfeedback: bool,
) -> list[int]:
    existing_shuffles = get_existing_shuffle_indices(cfg)
    if shuffles is None:
        first_index = 1
        if len(existing_shuffles) > 0:
            first_index = existing_shuffles[-1] + 1

        shuffles = range(first_index, num_shuffles + first_index)
    else:
        shuffles = [i for i in shuffles if isinstance(i, int)]
        for shuffle_idx in shuffles:
            if userfeedback and shuffle_idx in existing_shuffles:
                raise ValueError(
                    f"Cannot create shuffle {shuffle_idx} as it already exists - "
                    f"you must either create the dataset with `userfeedback=False` "
                    f"or delete the shuffle with index {shuffle_idx} manually (in "
                    f"`dlc-models`/`dlc-models-pytorch` and in the "
                    f"`training-datasets` folder) if you want to create a new "
                    f"shuffle with that index. You can otherwise create a shuffle "
                    f"with a new index. Existing indices are {existing_shuffles}."
                )

    return shuffles


def create_training_model_comparison(
    config,
    trainindex=0,
    num_shuffles=1,
    net_types=["resnet_50"],
    augmenter_types=["imgaug"],
    userfeedback=False,
    windows2linux=False,
):
    """Creates a training dataset to compare networks and augmentation types.

    The datasets are created such that the shuffles have same training and testing
    indices. Therefore, this function is useful for benchmarking the performance of
    different network and augmentation types on the same training/testdata.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    trainindex: int, optional, default=0
        Either (in case uniform = True) indexes which element of TrainingFraction in
        the config file should be used (note it is a list!).
        Alternatively (uniform = False) indexes which folder is dropped, i.e. the first
        if trainindex=0, the second if trainindex=1, etc.

    num_shuffles : int, optional, default=1
        Number of shuffles of training dataset to create,
        i.e. [1,2,3] for num_shuffles=3.

    net_types: list[str], optional, default=["resnet_50"]
        Currently supported networks are

        * ``"resnet_50"``
        * ``"resnet_101"``
        * ``"resnet_152"``
        * ``"mobilenet_v2_1.0"``
        * ``"mobilenet_v2_0.75"``
        * ``"mobilenet_v2_0.5"``
        * ``"mobilenet_v2_0.35"``
        * ``"efficientnet-b0"``
        * ``"efficientnet-b1"``
        * ``"efficientnet-b2"``
        * ``"efficientnet-b3"``
        * ``"efficientnet-b4"``
        * ``"efficientnet-b5"``
        * ``"efficientnet-b6"``

    augmenter_types: list[str], optional, default=["imgaug"]
        Currently supported augmenters are

        * ``"default"``
        * ``"imgaug"``
        * ``"tensorpack"``
        * ``"deterministic"``

    userfeedback: bool, optional, default=False
        If ``False``, then all requested train/test splits are created, no matter if
        they already exist. If you want to assure that previous splits etc. are not
        overwritten, then set this to True and you will be asked for each split.

    windows2linux

        ..deprecated::
            Has no effect since 2.2.0.4 and will be removed in 2.2.1.

    Returns
    -------
    shuffle_list: list
        List of indices corresponding to the trainingsplits/models that were created.

    Examples
    --------
    On Linux/MacOS

    >>> shuffle_list = deeplabcut.create_training_model_comparison(
            '/analysis/project/reaching-task/config.yaml',
            num_shuffles=1,
            net_types=['resnet_50','resnet_152'],
            augmenter_types=['tensorpack','deterministic'],
        )

    On Windows

    >>> shuffle_list = deeplabcut.create_training_model_comparison(
            'C:\\Users\\Ulf\\looming-task\\config.yaml',
            num_shuffles=1,
            net_types=['resnet_50','resnet_152'],
            augmenter_types=['tensorpack','deterministic'],
        )

    See ``examples/testscript_openfielddata_augmentationcomparison.py`` for an example
    of how to use ``shuffle_list``.
    """
    # read cfg file
    cfg = auxiliaryfunctions.read_config(config)

    if windows2linux:
        warnings.warn(
            "`windows2linux` has no effect since 2.2.0.4 and will be removed in 2.2.1.",
            FutureWarning,
        )

    # create log file
    log_file_name = os.path.join(cfg["project_path"], "training_model_comparison.log")
    logger = logging.getLogger("training_model_comparison")
    if not logger.handlers:
        logger = logging.getLogger("training_model_comparison")
        hdlr = logging.FileHandler(log_file_name)
        formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")
        hdlr.setFormatter(formatter)
        logger.addHandler(hdlr)
        logger.setLevel(logging.INFO)
    else:
        pass

    largestshuffleindex = get_existing_shuffle_indices(cfg)[-1] + 1

    shuffle_list = []
    for shuffle in range(num_shuffles):
        trainIndices, testIndices = mergeandsplit(
            config, trainindex=trainindex, uniform=True
        )
        for idx_net, net in enumerate(net_types):
            for idx_aug, aug in enumerate(augmenter_types):
                get_max_shuffle_idx = (
                    largestshuffleindex
                    + idx_aug
                    + idx_net * len(augmenter_types)
                    + shuffle * len(augmenter_types) * len(net_types)
                )

                shuffle_list.append(get_max_shuffle_idx)
                log_info = str(
                    "Shuffle index:"
                    + str(get_max_shuffle_idx)
                    + ", net_type:"
                    + net
                    + ", augmenter_type:"
                    + aug
                    + ", trainsetindex:"
                    + str(trainindex)
                    + ", frozen shuffle ID:"
                    + str(shuffle)
                )
                create_training_dataset(
                    config,
                    Shuffles=[get_max_shuffle_idx],
                    net_type=net,
                    trainIndices=[trainIndices],
                    testIndices=[testIndices],
                    augmenter_type=aug,
                    userfeedback=userfeedback,
                )
                logger.info(log_info)

    return shuffle_list


def create_training_dataset_from_existing_split(
    config: str,
    from_shuffle: int,
    from_trainsetindex: int = 0,
    num_shuffles: int = 1,
    shuffles: list[int] | None = None,
    userfeedback: bool = True,
    net_type: str | None = None,
    detector_type: str | None = None,
    augmenter_type: str | None = None,
    posecfg_template: dict | None = None,
    superanimal_name: str = "",
    weight_init: WeightInitialization | None = None,
    engine: Engine | None = None,
) -> None | list[int]:
    """
    Labels from all the extracted frames are merged into a single .h5 file.
    Only the videos included in the config file are used to create this dataset.

    Args:
        config: Full path of the ``config.yaml`` file as a string.

        from_shuffle: The index of the shuffle from which to copy the train/test split.

        from_trainsetindex: The trainset index of the shuffle from which to use the data
            split. Default is 0.

        num_shuffles: Number of shuffles of training dataset to create, used if
            ``shuffles`` is None.

        shuffles: If defined, ``num_shuffles`` is ignored and a shuffle is created for
            each index given in the list.

        userfeedback: If ``False``, all requested train/test splits are created (no
            matter if they already exist). If you want to assure that previous splits
            etc. are not overwritten, set this to ``True`` and you will be asked for
            each existing split if you want to overwrite it.

        net_type: The type of network to create the shuffle for. Currently supported
            options for engine=Engine.TF are:
                * ``resnet_50``
                * ``resnet_101``
                * ``resnet_152``
                * ``mobilenet_v2_1.0``
                * ``mobilenet_v2_0.75``
                * ``mobilenet_v2_0.5``
                * ``mobilenet_v2_0.35``
                * ``efficientnet-b0``
                * ``efficientnet-b1``
                * ``efficientnet-b2``
                * ``efficientnet-b3``
                * ``efficientnet-b4``
                * ``efficientnet-b5``
                * ``efficientnet-b6``
            Currently supported  options for engine=Engine.TF can be obtained by calling
            ``deeplabcut.pose_estimation_pytorch.available_models()``.

        detector_type: string, optional, default=None
            Only for the PyTorch engine.
            When passing creating shuffles for top-down models, you can specify which
            detector you want. If the detector_type is None, the ```ssdlite``` will be
            used. The list of all available detectors can be obtained by calling
            ``deeplabcut.pose_estimation_pytorch.available_detectors()``. Supported
            options:
                * ``ssdlite``
                * ``fasterrcnn_mobilenet_v3_large_fpn``
                * ``fasterrcnn_resnet50_fpn_v2``

        augmenter_type: Type of augmenter. Currently supported augmenters for
            engine=Engine.TF are
                * ``default``
                * ``scalecrop``
                * ``imgaug``
                * ``tensorpack``
                * ``deterministic``
            The only supported augmenter for Engine.PYTORCH is ``albumentations``.

        posecfg_template: Only for Engine.TF. Path to a ``pose_cfg.yaml`` file to use as
            a template for generating the new one for the current iteration. Useful if
            you would like to start with the same parameters a previous training
            iteration. None uses the default ``pose_cfg.yaml``.

        superanimal_name: Specify the superanimal name is transfer learning with
            superanimal is desired. This makes sure the pose config template uses
            superanimal configs as template.

        weight_init: Only for Engine.PYTORCH. Specify how model weights should be
            initialized. The default mode uses transfer learning from ImageNet weights.

        engine: Whether to create a pose config for a Tensorflow or PyTorch model.
            Defaults to the value specified in the project configuration file. If no
            engine is specified for the project, defaults to
            ``deeplabcut.compat.DEFAULT_ENGINE``.

    Returns:
        If training dataset was successfully created, a list of tuples is returned.
        The first two elements in each tuple represent the training fraction and the
        shuffle value. The last two elements in each tuple are arrays of integers
        representing the training and test indices.

        Returns None if training dataset could not be created.

    Raises:
        ValueError: If the shuffle from which to copy the data split doesn't exist.
    """
    cfg = auxiliaryfunctions.read_config(config)
    trainset_meta_path = metadata.TrainingDatasetMetadata.path(cfg)
    if not trainset_meta_path.exists():
        meta = metadata.TrainingDatasetMetadata.create(cfg)
        meta.save()
    else:
        meta = metadata.TrainingDatasetMetadata.load(cfg, load_splits=False)

    shuffle = meta.get(trainset_index=from_trainsetindex, index=from_shuffle)
    shuffle = shuffle.load_split(cfg, trainset_path=trainset_meta_path.parent)

    num_copies = num_shuffles
    if shuffles is not None:
        num_copies = len(shuffles)

    # pad the train and test indices with -1s so the training fraction is exact
    train_idx = list(shuffle.split.train_indices)
    test_idx = list(shuffle.split.test_indices)
    n_train, n_test = len(train_idx), len(test_idx)

    train_fraction = round(cfg["TrainingFraction"][from_trainsetindex], 2)
    if round(n_train / (n_train + n_test), 2) != train_fraction:
        train_padding, test_padding = _compute_padding(train_fraction, n_train, n_test)
        train_idx = train_idx + (train_padding * [-1])
        test_idx = test_idx + (test_padding * [-1])

    return create_training_dataset(
        config=config,
        num_shuffles=num_shuffles,
        Shuffles=shuffles,
        userfeedback=userfeedback,
        trainIndices=[train_idx for _ in range(num_copies)],
        testIndices=[test_idx for _ in range(num_copies)],
        net_type=net_type,
        detector_type=detector_type,
        augmenter_type=augmenter_type,
        posecfg_template=posecfg_template,
        superanimal_name=superanimal_name,
        weight_init=weight_init,
        engine=engine,
    )


def _compute_padding(
    train_fraction: float,
    num_train: int,
    num_test: int,
) -> tuple[int, int]:
    """
    Computes the amount of padding to add to train/test indices such that
    train_fraction = num_train / (num_train + num_test).

    Returns:
        the number of padding indices to add to the train indices
        the number of padding indices to add to the test indices
    """
    if train_fraction <= 0 or train_fraction >= 1:
        raise ValueError(
            f"The training fraction must satisfy 0 < TrainingFraction < 1, but "
            f"{train_fraction} was found"
        )

    base_images = 100
    train_step = int(round(round(train_fraction, 2) * base_images))
    test_step = base_images - train_step

    tgt_train = train_step
    tgt_test = test_step
    while tgt_train < num_train or tgt_test < num_test:
        tgt_train += train_step
        tgt_test += test_step

    return (tgt_train - num_train), (tgt_test - num_test)


--- File: deeplabcut/core/conversion_table.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Defines conversion tables mapping DeepLabCut project bodyparts to SA bodyparts"""
from __future__ import annotations

from dataclasses import dataclass

import numpy as np


@dataclass
class ConversionTable:
    """Maps DLC project bodyparts to the corresponding SuperAnimal bodyparts

    The conversion table must satisfy the following conditions (checked by validate):
        - All SuperAnimal bodyparts must be valid (defined for the SuperAnimal model)
        - All project bodyparts must be valid (defined for the DLC project)
    """

    super_animal: str
    project_bodyparts: list[str]
    super_animal_bodyparts: list[str]
    table: dict[str, str]

    def __post_init__(self):
        """Validates the table"""
        self.validate()

    def to_array(self) -> np.ndarray:
        """
        Returns:
            An array mapping the indices of SuperAnimal bodyparts

        Raises:
            ValueError: If the conversion table is misconfigured.
        """
        self.validate()
        sa_indices = {sa_bpt: i for i, sa_bpt in enumerate(self.super_animal_bodyparts)}
        sa_bpt_ordering = [self.table[bpt] for bpt in self.converted_bodyparts()]
        return np.array([sa_indices[sa_bpt] for sa_bpt in sa_bpt_ordering])

    def converted_bodyparts(self) -> list[str]:
        """Returns: The project bodyparts included in this ordered"""
        return [bpt for bpt in self.project_bodyparts if bpt in self.table]

    def validate(self) -> None:
        """
        Raises:
            ValueError: If the conversion table is misconfigured.
        """
        project_bpts = set(self.project_bodyparts)
        sa_bpts = set(self.super_animal_bodyparts)

        mapped_sa = set(self.table.values())
        mapped_project = set(self.table.keys())

        # check all mapped SuperAnimal bodyparts are in the config
        if len(mapped_sa.difference(sa_bpts)) != 0:
            extra_bodyparts = set(mapped_sa).difference(sa_bpts)
            raise ValueError(
                f"Some bodyparts in your mapping are not in the {self.super_animal} "
                f"model: {extra_bodyparts}. Available bodyparts are {' '.join(sa_bpts)}"
            )

        # check all given bodyparts are in the project configuration
        if len(mapped_project.difference(project_bpts)) != 0:
            extra_bodyparts = mapped_project.difference(project_bpts)
            raise ValueError(
                "Some bodyparts in your mapping are not in your project configuration: "
                f"{extra_bodyparts}. Defined bodyparts are {' '.join(project_bpts)}"
            )


--- File: deeplabcut/core/config.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Simple helper methods related to configuration files stored in yaml files"""
from __future__ import annotations

from pathlib import Path
from typing import Callable

from ruamel.yaml import YAML


def read_config_as_dict(config_path: str | Path) -> dict:
    """
    Args:
        config_path: the path to the configuration file to load

    Returns:
        The configuration file with pure Python classes
    """
    with open(config_path, "r") as f:
        cfg = YAML(typ="safe", pure=True).load(f)

    return cfg


def write_config(config_path: str | Path, config: dict, overwrite: bool = True) -> None:
    """Writes a pose configuration file to disk

    Args:
        config_path: the path where the config should be saved
        config: the config to save
        overwrite: whether to overwrite the file if it already exists

    Raises:
        FileExistsError if overwrite=True and the file already exists
    """
    if not overwrite and Path(config_path).exists():
        raise FileExistsError(
            f"Cannot write to {config_path} - set overwrite=True to force"
        )

    with open(config_path, "w") as file:
        YAML().dump(config, file)


def pretty_print(
    config: dict,
    indent: int = 0,
    print_fn: Callable[[str], None] | None = None,
) -> None:
    """Prints a model configuration in a pretty and readable way

    Args:
        config: the config to print
        indent: the base indent on all keys
        print_fn: custom function to call (simply calls ``print`` if None)
    """
    if print_fn is None:
        print_fn = print

    for k, v in config.items():
        if isinstance(v, dict):
            print_fn(f"{indent * ' '}{k}:")
            pretty_print(v, indent + 2, print_fn=print_fn)
        else:
            print_fn(f"{indent * ' '}{k}: {v}")


--- File: deeplabcut/core/inferenceutils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import heapq
import itertools
import multiprocessing
import operator
import pickle
import warnings
from collections import defaultdict
from dataclasses import dataclass
from math import erf, sqrt
from typing import Any, Iterable, Tuple

import networkx as nx
import numpy as np
import pandas as pd
from scipy.optimize import linear_sum_assignment
from scipy.spatial import cKDTree
from scipy.spatial.distance import cdist, pdist
from scipy.special import softmax
from scipy.stats import chi2, gaussian_kde
from tqdm import tqdm


def _conv_square_to_condensed_indices(ind_row, ind_col, n):
    if ind_row == ind_col:
        raise ValueError("There are no diagonal elements in condensed matrices.")

    if ind_row < ind_col:
        ind_row, ind_col = ind_col, ind_row
    return n * ind_col - ind_col * (ind_col + 1) // 2 + ind_row - 1 - ind_col


Position = Tuple[float, float]


@dataclass(frozen=True)
class Joint:
    pos: Position
    confidence: float = 1.0
    label: int = None
    idx: int = None
    group: int = -1


class Link:
    def __init__(self, j1, j2, affinity=1):
        self.j1 = j1
        self.j2 = j2
        self.affinity = affinity
        self._length = sqrt((j1.pos[0] - j2.pos[0]) ** 2 + (j1.pos[1] - j2.pos[1]) ** 2)

    def __repr__(self):
        return (
            f"Link {self.idx}, affinity={self.affinity:.2f}, length={self.length:.2f}"
        )

    @property
    def confidence(self):
        return self.j1.confidence * self.j2.confidence

    @property
    def idx(self):
        return self.j1.idx, self.j2.idx

    @property
    def length(self):
        return self._length

    @length.setter
    def length(self, length):
        self._length = length

    def to_vector(self):
        return [*self.j1.pos, *self.j2.pos]


class Assembly:
    def __init__(self, size):
        self.data = np.full((size, 4), np.nan)
        self.confidence = 0  # 0 by default, overwritten otherwise with `add_joint`
        self._affinity = 0
        self._links = []
        self._visible = set()
        self._idx = set()
        self._dict = dict()

    def __len__(self):
        return len(self._visible)

    def __contains__(self, assembly):
        return bool(self._visible.intersection(assembly._visible))

    def __add__(self, other):
        if other in self:
            raise ValueError("Assemblies contain shared joints.")

        assembly = Assembly(self.data.shape[0])
        for link in self._links + other._links:
            assembly.add_link(link)
        return assembly

    @classmethod
    def from_array(cls, array):
        n_bpts, n_cols = array.shape

        # if a single coordinate is NaN for a bodypart, set all to NaN
        array[np.isnan(array).any(axis=-1)] = np.nan

        ass = cls(size=n_bpts)
        ass.data[:, :n_cols] = array
        visible = np.flatnonzero(~np.isnan(array).any(axis=1))
        if n_cols < 3:  # Only xy coordinates are being set
            ass.data[visible, 2] = 1  # Set detection confidence to 1
        ass._visible.update(visible)
        return ass

    @property
    def xy(self):
        return self.data[:, :2]

    @property
    def extent(self):
        bbox = np.empty(4)
        bbox[:2] = np.nanmin(self.xy, axis=0)
        bbox[2:] = np.nanmax(self.xy, axis=0)
        return bbox

    @property
    def area(self):
        x1, y1, x2, y2 = self.extent
        return (x2 - x1) * (y2 - y1)

    @property
    def confidence(self):
        return np.nanmean(self.data[:, 2])

    @confidence.setter
    def confidence(self, confidence):
        self.data[:, 2] = confidence

    @property
    def soft_identity(self):
        data = self.data[~np.isnan(self.data).any(axis=1)]
        unq, idx, cnt = np.unique(data[:, 3], return_inverse=True, return_counts=True)
        avg = np.bincount(idx, weights=data[:, 2]) / cnt
        soft = softmax(avg)
        return dict(zip(unq.astype(int), soft))

    @property
    def affinity(self):
        n_links = self.n_links
        if not n_links:
            return 0
        return self._affinity / n_links

    @property
    def n_links(self):
        return len(self._links)

    def intersection_with(self, other):
        x11, y11, x21, y21 = self.extent
        x12, y12, x22, y22 = other.extent
        x1 = max(x11, x12)
        y1 = max(y11, y12)
        x2 = min(x21, x22)
        y2 = min(y21, y22)
        if x2 < x1 or y2 < y1:
            return 0
        ll = np.array([x1, y1])
        ur = np.array([x2, y2])
        xy1 = self.xy[~np.isnan(self.xy).any(axis=1)]
        xy2 = other.xy[~np.isnan(other.xy).any(axis=1)]
        in1 = np.all((xy1 >= ll) & (xy1 <= ur), axis=1).sum()
        in2 = np.all((xy2 >= ll) & (xy2 <= ur), axis=1).sum()
        return min(in1 / len(self), in2 / len(other))

    def add_joint(self, joint):
        if joint.label in self._visible or joint.label is None:
            return False
        self.data[joint.label] = *joint.pos, joint.confidence, joint.group
        self._visible.add(joint.label)
        self._idx.add(joint.idx)
        return True

    def remove_joint(self, joint):
        if joint.label not in self._visible:
            return False
        self.data[joint.label] = np.nan
        self._visible.remove(joint.label)
        self._idx.remove(joint.idx)
        return True

    def add_link(self, link, store_dict=False):
        if store_dict:
            # Selective copy; deepcopy is >5x slower
            self._dict = {
                "data": self.data.copy(),
                "_affinity": self._affinity,
                "_links": self._links.copy(),
                "_visible": self._visible.copy(),
                "_idx": self._idx.copy(),
            }
        i1, i2 = link.idx
        if i1 in self._idx and i2 in self._idx:
            self._affinity += link.affinity
            self._links.append(link)
            return False
        if link.j1.label in self._visible and link.j2.label in self._visible:
            return False
        self.add_joint(link.j1)
        self.add_joint(link.j2)
        self._affinity += link.affinity
        self._links.append(link)
        return True

    def calc_pairwise_distances(self):
        return pdist(self.xy, metric="sqeuclidean")


class Assembler:
    def __init__(
        self,
        data,
        *,
        max_n_individuals,
        n_multibodyparts,
        graph=None,
        paf_inds=None,
        greedy=False,
        pcutoff=0.1,
        min_affinity=0.05,
        min_n_links=2,
        max_overlap=0.8,
        identity_only=False,
        nan_policy="little",
        force_fusion=False,
        add_discarded=False,
        window_size=0,
        method="m1",
    ):
        self.data = data
        self.metadata = self.parse_metadata(self.data)
        self.max_n_individuals = max_n_individuals
        self.n_multibodyparts = n_multibodyparts
        self.n_uniquebodyparts = self.n_keypoints - n_multibodyparts
        self.greedy = greedy
        self.pcutoff = pcutoff
        self.min_affinity = min_affinity
        self.min_n_links = min_n_links
        self.max_overlap = max_overlap
        self._has_identity = "identity" in self[0]
        if identity_only and not self._has_identity:
            warnings.warn(
                "The network was not trained with identity; setting `identity_only` to False."
            )
        self.identity_only = identity_only & self._has_identity
        self.nan_policy = nan_policy
        self.force_fusion = force_fusion
        self.add_discarded = add_discarded
        self.window_size = window_size
        self.method = method
        self.graph = graph or self.metadata["paf_graph"]
        self.paf_inds = paf_inds or self.metadata["paf"]
        self._gamma = 0.01
        self._trees = dict()
        self.safe_edge = False
        self._kde = None
        self.assemblies = dict()
        self.unique = dict()

    def __getitem__(self, item):
        return self.data[self.metadata["imnames"][item]]

    @classmethod
    def empty(
        cls,
        max_n_individuals,
        n_multibodyparts,
        n_uniquebodyparts,
        graph,
        paf_inds,
        greedy=False,
        pcutoff=0.1,
        min_affinity=0.05,
        min_n_links=2,
        max_overlap=0.8,
        identity_only=False,
        nan_policy="little",
        force_fusion=False,
        add_discarded=False,
        window_size=0,
        method="m1",
    ):
        # Dummy data
        n_bodyparts = n_multibodyparts + n_uniquebodyparts
        data = {
            "metadata": {
                "all_joints_names": ["" for _ in range(n_bodyparts)],
                "PAFgraph": graph,
                "PAFinds": paf_inds,
            },
            "0": {},
        }
        return cls(
            data,
            max_n_individuals=max_n_individuals,
            n_multibodyparts=n_multibodyparts,
            graph=graph,
            paf_inds=paf_inds,
            greedy=greedy,
            pcutoff=pcutoff,
            min_affinity=min_affinity,
            min_n_links=min_n_links,
            max_overlap=max_overlap,
            identity_only=identity_only,
            nan_policy=nan_policy,
            force_fusion=force_fusion,
            add_discarded=add_discarded,
            window_size=window_size,
            method=method,
        )

    @property
    def n_keypoints(self):
        return self.metadata["num_joints"]

    def calibrate(self, train_data_file):
        df = pd.read_hdf(train_data_file)
        try:
            df.drop("single", level="individuals", axis=1, inplace=True)
        except KeyError:
            pass
        n_bpts = len(df.columns.get_level_values("bodyparts").unique())
        if n_bpts == 1:
            warnings.warn("There is only one keypoint; skipping calibration...")
            return

        xy = df.to_numpy().reshape((-1, n_bpts, 2))
        frac_valid = np.mean(~np.isnan(xy), axis=(1, 2))
        # Only keeps skeletons that are more than 90% complete
        xy = xy[frac_valid >= 0.9]
        if not xy.size:
            warnings.warn("No complete poses were found. Skipping calibration...")
            return

        # TODO Normalize dists by longest length?
        # TODO Smarter imputation technique (Bayesian? Grassmann averages?)
        dists = np.vstack([pdist(data, "sqeuclidean") for data in xy])
        mu = np.nanmean(dists, axis=0)
        missing = np.isnan(dists)
        dists = np.where(missing, mu, dists)
        try:
            kde = gaussian_kde(dists.T)
            kde.mean = mu
            self._kde = kde
            self.safe_edge = True
        except np.linalg.LinAlgError:
            # Covariance matrix estimation fails due to numerical singularities
            warnings.warn(
                "The assembler could not be robustly calibrated. Continuing without it..."
            )

    def calc_assembly_mahalanobis_dist(
        self, assembly, return_proba=False, nan_policy="little"
    ):
        if self._kde is None:
            raise ValueError("Assembler should be calibrated first with training data.")

        dists = assembly.calc_pairwise_distances() - self._kde.mean
        mask = np.isnan(dists)
        # Distance is undefined if the assembly is empty
        if not len(assembly) or mask.all():
            if return_proba:
                return np.inf, 0
            return np.inf

        if nan_policy == "little":
            inds = np.flatnonzero(~mask)
            dists = dists[inds]
            inv_cov = self._kde.inv_cov[np.ix_(inds, inds)]
            # Correct distance to account for missing observations
            factor = self._kde.d / len(inds)
        else:
            # Alternatively, reduce contribution of missing values to the Mahalanobis
            # distance to zero by substituting the corresponding means.
            dists[mask] = 0
            mask.fill(False)
            inv_cov = self._kde.inv_cov
            factor = 1
        dot = dists @ inv_cov
        mahal = factor * sqrt(np.sum((dot * dists), axis=-1))
        if return_proba:
            proba = 1 - chi2.cdf(mahal, np.sum(~mask))
            return mahal, proba
        return mahal

    def calc_link_probability(self, link):
        if self._kde is None:
            raise ValueError("Assembler should be calibrated first with training data.")

        i = link.j1.label
        j = link.j2.label
        ind = _conv_square_to_condensed_indices(i, j, self.n_multibodyparts)
        mu = self._kde.mean[ind]
        sigma = self._kde.covariance[ind, ind]
        z = (link.length**2 - mu) / sigma
        return 2 * (1 - 0.5 * (1 + erf(abs(z) / sqrt(2))))

    @staticmethod
    def _flatten_detections(data_dict):
        ind = 0
        coordinates = data_dict["coordinates"][0]
        confidence = data_dict["confidence"]
        ids = data_dict.get("identity", None)
        if ids is None:
            ids = [np.ones(len(arr), dtype=int) * -1 for arr in confidence]
        else:
            ids = [arr.argmax(axis=1) for arr in ids]
        for i, (coords, conf, id_) in enumerate(zip(coordinates, confidence, ids)):
            if not np.any(coords):
                continue
            for xy, p, g in zip(coords, conf, id_):
                joint = Joint(tuple(xy), p.item(), i, ind, g)
                ind += 1
                yield joint

    def extract_best_links(self, joints_dict, costs, trees=None):
        links = []
        for ind in self.paf_inds:
            s, t = self.graph[ind]
            dets_s = joints_dict.get(s, None)
            dets_t = joints_dict.get(t, None)
            if dets_s is None or dets_t is None:
                continue
            if ind not in costs:
                continue
            lengths = costs[ind]["distance"]
            if np.isinf(lengths).all():
                continue
            aff = costs[ind][self.method].copy()
            aff[np.isnan(aff)] = 0

            if trees:
                vecs = np.vstack(
                    [[*det_s.pos, *det_t.pos] for det_s in dets_s for det_t in dets_t]
                )
                dists = []
                for n, tree in enumerate(trees, start=1):
                    d, _ = tree.query(vecs)
                    dists.append(np.exp(-self._gamma * n * d))
                w = np.mean(dists, axis=0)
                aff *= w.reshape(aff.shape)

            if self.greedy:
                conf = np.asarray(
                    [
                        [det_s.confidence * det_t.confidence for det_t in dets_t]
                        for det_s in dets_s
                    ]
                )
                rows, cols = np.where(
                    (conf >= self.pcutoff * self.pcutoff) & (aff >= self.min_affinity)
                )
                candidates = sorted(
                    zip(rows, cols, aff[rows, cols], lengths[rows, cols]),
                    key=lambda x: x[2],
                    reverse=True,
                )
                i_seen = set()
                j_seen = set()
                for i, j, w, l in candidates:
                    if i not in i_seen and j not in j_seen:
                        i_seen.add(i)
                        j_seen.add(j)
                        links.append(Link(dets_s[i], dets_t[j], w))
                        if len(i_seen) == self.max_n_individuals:
                            break
            else:  # Optimal keypoint pairing
                inds_s = sorted(
                    range(len(dets_s)), key=lambda x: dets_s[x].confidence, reverse=True
                )[: self.max_n_individuals]
                inds_t = sorted(
                    range(len(dets_t)), key=lambda x: dets_t[x].confidence, reverse=True
                )[: self.max_n_individuals]
                keep_s = [
                    ind for ind in inds_s if dets_s[ind].confidence >= self.pcutoff
                ]
                keep_t = [
                    ind for ind in inds_t if dets_t[ind].confidence >= self.pcutoff
                ]
                aff = aff[np.ix_(keep_s, keep_t)]
                rows, cols = linear_sum_assignment(aff, maximize=True)
                for row, col in zip(rows, cols):
                    w = aff[row, col]
                    if w >= self.min_affinity:
                        links.append(Link(dets_s[keep_s[row]], dets_t[keep_t[col]], w))
        return links

    def _fill_assembly(self, assembly, lookup, assembled, safe_edge, nan_policy):
        stack = []
        visited = set()
        tabu = []
        counter = itertools.count()

        def push_to_stack(i):
            for j, link in lookup[i].items():
                if j in assembly._idx:
                    continue
                if link.idx in visited:
                    continue
                heapq.heappush(stack, (-link.affinity, next(counter), link))
                visited.add(link.idx)

        for idx in assembly._idx:
            push_to_stack(idx)

        while stack and len(assembly) < self.n_multibodyparts:
            _, _, best = heapq.heappop(stack)
            i, j = best.idx
            if i in assembly._idx:
                new_ind = j
            elif j in assembly._idx:
                new_ind = i
            else:
                continue
            if new_ind in assembled:
                continue
            if safe_edge:
                d_old = self.calc_assembly_mahalanobis_dist(
                    assembly, nan_policy=nan_policy
                )
                success = assembly.add_link(best, store_dict=True)
                if not success:
                    assembly._dict = dict()
                    continue
                d = self.calc_assembly_mahalanobis_dist(assembly, nan_policy=nan_policy)
                if d < d_old:
                    push_to_stack(new_ind)
                    try:
                        _, _, link = heapq.heappop(tabu)
                        heapq.heappush(stack, (-link.affinity, next(counter), link))
                    except IndexError:
                        pass
                else:
                    heapq.heappush(tabu, (d - d_old, next(counter), best))
                    assembly.__dict__.update(assembly._dict)
                assembly._dict = dict()
            else:
                assembly.add_link(best)
                push_to_stack(new_ind)

    def build_assemblies(self, links):
        lookup = defaultdict(dict)
        for link in links:
            i, j = link.idx
            lookup[i][j] = link
            lookup[j][i] = link

        assemblies = []
        assembled = set()

        # Fill the subsets with unambiguous, complete individuals
        G = nx.Graph([link.idx for link in links])
        for chain in nx.connected_components(G):
            if len(chain) == self.n_multibodyparts:
                edges = [tuple(sorted(edge)) for edge in G.edges(chain)]
                assembly = Assembly(self.n_multibodyparts)
                for link in links:
                    i, j = link.idx
                    if (i, j) in edges:
                        success = assembly.add_link(link)
                        if success:
                            lookup[i].pop(j)
                            lookup[j].pop(i)
                assembled.update(assembly._idx)
                assemblies.append(assembly)

        if len(assemblies) == self.max_n_individuals:
            return assemblies, assembled

        for link in sorted(links, key=lambda x: x.affinity, reverse=True):
            if any(i in assembled for i in link.idx):
                continue
            assembly = Assembly(self.n_multibodyparts)
            assembly.add_link(link)
            self._fill_assembly(
                assembly, lookup, assembled, self.safe_edge, self.nan_policy
            )
            for link in assembly._links:
                i, j = link.idx
                lookup[i].pop(j)
                lookup[j].pop(i)
            assembled.update(assembly._idx)
            assemblies.append(assembly)

        # Fuse superfluous assemblies
        n_extra = len(assemblies) - self.max_n_individuals
        if n_extra > 0:
            if self.safe_edge:
                ds_old = [
                    self.calc_assembly_mahalanobis_dist(assembly)
                    for assembly in assemblies
                ]
                while len(assemblies) > self.max_n_individuals:
                    ds = []
                    for i, j in itertools.combinations(range(len(assemblies)), 2):
                        if assemblies[j] not in assemblies[i]:
                            temp = assemblies[i] + assemblies[j]
                            d = self.calc_assembly_mahalanobis_dist(temp)
                            delta = d - max(ds_old[i], ds_old[j])
                            ds.append((i, j, delta, d, temp))
                    if not ds:
                        break
                    min_ = sorted(ds, key=lambda x: x[2])
                    i, j, delta, d, new = min_[0]
                    if delta < 0 or len(min_) == 1:
                        assemblies[i] = new
                        assemblies.pop(j)
                        ds_old[i] = d
                        ds_old.pop(j)
                    else:
                        break
            elif self.force_fusion:
                assemblies = sorted(assemblies, key=len)
                for nrow in range(n_extra):
                    assembly = assemblies[nrow]
                    candidates = [a for a in assemblies[nrow:] if assembly not in a]
                    if not candidates:
                        continue
                    if len(candidates) == 1:
                        candidate = candidates[0]
                    else:
                        dists = []
                        for cand in candidates:
                            d = cdist(assembly.xy, cand.xy)
                            dists.append(np.nanmin(d))
                        candidate = candidates[np.argmin(dists)]
                    ind = assemblies.index(candidate)
                    assemblies[ind] += assembly
            else:
                store = dict()
                for assembly in assemblies:
                    if len(assembly) != self.n_multibodyparts:
                        for i in assembly._idx:
                            store[i] = assembly
                used = [link for assembly in assemblies for link in assembly._links]
                unconnected = [link for link in links if link not in used]
                for link in unconnected:
                    i, j = link.idx
                    try:
                        if store[j] not in store[i]:
                            temp = store[i] + store[j]
                            store[i].__dict__.update(temp.__dict__)
                            assemblies.remove(store[j])
                            for idx in store[j]._idx:
                                store[idx] = store[i]
                    except KeyError:
                        pass

        # Second pass without edge safety
        for assembly in assemblies:
            if len(assembly) != self.n_multibodyparts:
                self._fill_assembly(assembly, lookup, assembled, False, "")
                assembled.update(assembly._idx)

        return assemblies, assembled

    def _assemble(self, data_dict, ind_frame):
        joints = list(self._flatten_detections(data_dict))
        if not joints:
            return None, None

        bag = defaultdict(list)
        for joint in joints:
            bag[joint.label].append(joint)

        assembled = set()

        if self.n_uniquebodyparts:
            unique = np.full((self.n_uniquebodyparts, 3), np.nan)
            for n, ind in enumerate(range(self.n_multibodyparts, self.n_keypoints)):
                dets = bag[ind]
                if not dets:
                    continue
                if len(dets) > 1:
                    det = max(dets, key=lambda x: x.confidence)
                else:
                    det = dets[0]
                # Mark the unique body parts as assembled anyway so
                # they are not used later on to fill assemblies.
                assembled.update(d.idx for d in dets)
                if det.confidence <= self.pcutoff and not self.add_discarded:
                    continue
                unique[n] = *det.pos, det.confidence
            if np.isnan(unique).all():
                unique = None
        else:
            unique = None

        if not any(i in bag for i in range(self.n_multibodyparts)):
            return None, unique

        if self.n_multibodyparts == 1:
            assemblies = []
            for joint in bag[0]:
                if joint.confidence >= self.pcutoff:
                    ass = Assembly(self.n_multibodyparts)
                    ass.add_joint(joint)
                    assemblies.append(ass)
            return assemblies, unique

        if self.max_n_individuals == 1:
            get_attr = operator.attrgetter("confidence")
            ass = Assembly(self.n_multibodyparts)
            for ind in range(self.n_multibodyparts):
                joints = bag[ind]
                if not joints:
                    continue
                ass.add_joint(max(joints, key=get_attr))
            return [ass], unique

        if self.identity_only:
            assemblies = []
            get_attr = operator.attrgetter("group")
            temp = sorted(
                (joint for joint in joints if np.isfinite(joint.confidence)),
                key=get_attr,
            )
            groups = itertools.groupby(temp, get_attr)
            for _, group in groups:
                ass = Assembly(self.n_multibodyparts)
                for joint in sorted(group, key=lambda x: x.confidence, reverse=True):
                    if (
                        joint.confidence >= self.pcutoff
                        and joint.label < self.n_multibodyparts
                    ):
                        ass.add_joint(joint)
                if len(ass):
                    assemblies.append(ass)
                    assembled.update(ass._idx)
        else:
            trees = []
            for j in range(1, self.window_size + 1):
                tree = self._trees.get(ind_frame - j, None)
                if tree is not None:
                    trees.append(tree)

            links = self.extract_best_links(bag, data_dict["costs"], trees)
            if self._kde:
                for link in links[::-1]:
                    p = max(self.calc_link_probability(link), 0.001)
                    link.affinity *= p
                    if link.affinity < self.min_affinity:
                        links.remove(link)

            if self.window_size >= 1 and links:
                # Store selected edges for subsequent frames
                vecs = np.vstack([link.to_vector() for link in links])
                self._trees[ind_frame] = cKDTree(vecs)

            assemblies, assembled_ = self.build_assemblies(links)
            assembled.update(assembled_)

        # Remove invalid assemblies
        discarded = set(
            joint
            for joint in joints
            if joint.idx not in assembled and np.isfinite(joint.confidence)
        )
        for assembly in assemblies[::-1]:
            if 0 < assembly.n_links < self.min_n_links or not len(assembly):
                for link in assembly._links:
                    discarded.update((link.j1, link.j2))
                assemblies.remove(assembly)
        if 0 < self.max_overlap < 1:  # Non-maximum pose suppression
            if self._kde is not None:
                scores = [
                    -self.calc_assembly_mahalanobis_dist(ass) for ass in assemblies
                ]
            else:
                scores = [ass._affinity for ass in assemblies]
            lst = list(zip(scores, assemblies))
            assemblies = []
            while lst:
                temp = max(lst, key=lambda x: x[0])
                lst.remove(temp)
                assemblies.append(temp[1])
                for pair in lst[::-1]:
                    if temp[1].intersection_with(pair[1]) >= self.max_overlap:
                        lst.remove(pair)
        if len(assemblies) > self.max_n_individuals:
            assemblies = sorted(assemblies, key=len, reverse=True)
            for assembly in assemblies[self.max_n_individuals :]:
                for link in assembly._links:
                    discarded.update((link.j1, link.j2))
            assemblies = assemblies[: self.max_n_individuals]

        if self.add_discarded and discarded:
            # Fill assemblies with unconnected body parts
            for joint in sorted(discarded, key=lambda x: x.confidence, reverse=True):
                if self.safe_edge:
                    for assembly in assemblies:
                        if joint.label in assembly._visible:
                            continue
                        d_old = self.calc_assembly_mahalanobis_dist(assembly)
                        assembly.add_joint(joint)
                        d = self.calc_assembly_mahalanobis_dist(assembly)
                        if d < d_old:
                            break
                        assembly.remove_joint(joint)
                else:
                    dists = []
                    for i, assembly in enumerate(assemblies):
                        if joint.label in assembly._visible:
                            continue
                        d = cdist(assembly.xy, np.atleast_2d(joint.pos))
                        dists.append((i, np.nanmin(d)))
                    if not dists:
                        continue
                    min_ = sorted(dists, key=lambda x: x[1])
                    ind, _ = min_[0]
                    assemblies[ind].add_joint(joint)

        return assemblies, unique

    def assemble(self, chunk_size=1, n_processes=None):
        self.assemblies = dict()
        self.unique = dict()
        # Spawning (rather than forking) multiple processes does not
        # work nicely with the GUI or interactive sessions.
        # In that case, we fall back to the serial assembly.
        if chunk_size == 0 or multiprocessing.get_start_method() == "spawn":

            for i, data_dict in enumerate(tqdm(self)):
                assemblies, unique = self._assemble(data_dict, i)
                if assemblies:
                    self.assemblies[i] = assemblies
                if unique is not None:
                    self.unique[i] = unique
        else:
            global wrapped  # Hack to make the function pickable

            def wrapped(i):
                return i, self._assemble(self[i], i)

            n_frames = len(self.metadata["imnames"])
            with multiprocessing.Pool(n_processes) as p:
                with tqdm(total=n_frames) as pbar:
                    for i, (assemblies, unique) in p.imap_unordered(
                        wrapped, range(n_frames), chunksize=chunk_size
                    ):
                        if assemblies:
                            self.assemblies[i] = assemblies
                        if unique is not None:
                            self.unique[i] = unique
                        pbar.update()

    def from_pickle(self, pickle_path):
        with open(pickle_path, "rb") as file:
            data = pickle.load(file)
        self.unique = data.pop("single", {})
        self.assemblies = data

    @staticmethod
    def parse_metadata(data):
        params = dict()
        params["joint_names"] = data["metadata"]["all_joints_names"]
        params["num_joints"] = len(params["joint_names"])
        params["paf_graph"] = data["metadata"]["PAFgraph"]
        params["paf"] = data["metadata"].get(
            "PAFinds", np.arange(len(params["joint_names"]))
        )
        params["bpts"] = params["ibpts"] = range(params["num_joints"])
        params["imnames"] = [fn for fn in list(data) if fn != "metadata"]
        return params

    def to_h5(self, output_name):
        data = np.full(
            (
                len(self.metadata["imnames"]),
                self.max_n_individuals,
                self.n_multibodyparts,
                4,
            ),
            fill_value=np.nan,
        )
        for ind, assemblies in self.assemblies.items():
            for n, assembly in enumerate(assemblies):
                data[ind, n] = assembly.data
        index = pd.MultiIndex.from_product(
            [
                ["scorer"],
                map(str, range(self.max_n_individuals)),
                map(str, range(self.n_multibodyparts)),
                ["x", "y", "likelihood"],
            ],
            names=["scorer", "individuals", "bodyparts", "coords"],
        )
        temp = data[..., :3].reshape((data.shape[0], -1))
        df = pd.DataFrame(temp, columns=index)
        df.to_hdf(output_name, key="ass")

    def to_pickle(self, output_name):
        data = dict()
        for ind, assemblies in self.assemblies.items():
            data[ind] = [ass.data for ass in assemblies]
        if self.unique:
            data["single"] = self.unique
        with open(output_name, "wb") as file:
            pickle.dump(data, file, pickle.HIGHEST_PROTOCOL)


@dataclass
class MatchedPrediction:
    """A match between a prediction and a ground truth assembly

    The ground truth assembly should be None f the prediction was not matched to any GT,
    and the OKS should be 0.

    Attributes:
        prediction: A prediction made by a pose model.
        score: The confidence score for the prediction.
        ground_truth: If None, then this prediction is not matched to any ground truth
            (this can happen when there are more predicted individuals than GT).
            Otherwise, the ground truth assembly to which this prediction is matched.
        oks: The OKS score between the prediction and the ground truth pose.
    """

    prediction: Assembly
    score: float
    ground_truth: Assembly | None
    oks: float


def calc_object_keypoint_similarity(
    xy_pred,
    xy_true,
    sigma,
    margin=0,
    symmetric_kpts=None,
):
    visible_gt = ~np.isnan(xy_true).all(axis=1)
    if visible_gt.sum() < 2:  # At least 2 points needed to calculate scale
        return np.nan

    true = xy_true[visible_gt]
    scale_squared = np.prod(np.ptp(true, axis=0) + np.spacing(1) + margin * 2)
    if np.isclose(scale_squared, 0):
        return np.nan

    k_squared = (2 * sigma) ** 2
    denom = 2 * scale_squared * k_squared
    if symmetric_kpts is None:
        pred = xy_pred[visible_gt]
        pred[np.isnan(pred)] = np.inf
        dist_squared = np.sum((pred - true) ** 2, axis=1)
        oks = np.exp(-dist_squared / denom)
        return np.mean(oks)
    else:
        oks = []
        xy_preds = [xy_pred]
        combos = (
            pair
            for l in range(len(symmetric_kpts))
            for pair in itertools.combinations(symmetric_kpts, l + 1)
        )
        for pairs in combos:
            # Swap corresponding keypoints
            tmp = xy_pred.copy()
            for pair in pairs:
                tmp[pair, :] = tmp[pair[::-1], :]
            xy_preds.append(tmp)
        for xy_pred in xy_preds:
            pred = xy_pred[visible_gt]
            pred[np.isnan(pred)] = np.inf
            dist_squared = np.sum((pred - true) ** 2, axis=1)
            oks.append(np.mean(np.exp(-dist_squared / denom)))
        return max(oks)


def match_assemblies(
    predictions: list[Assembly],
    ground_truth: list[Assembly],
    sigma: float,
    margin: int = 0,
    symmetric_kpts: list[tuple[int, int]] | None = None,
    greedy_matching: bool = False,
    greedy_oks_threshold: float = 0.0,
) -> tuple[int, list[MatchedPrediction]]:
    """Matches assemblies to ground truth predictions

    Returns:
        int: the total number of valid ground truth assemblies
        list[MatchedPrediction]: a list containing all valid predictions, potentially
            matched to ground truth assemblies.
    """
    # Only consider assemblies of at least two keypoints
    predictions = [a for a in predictions if len(a) > 1]
    ground_truth = [a for a in ground_truth if len(a) > 1]
    num_ground_truth = len(ground_truth)

    # Sort predictions by score
    inds_pred = np.argsort(
        [ins.affinity if ins.n_links else ins.confidence for ins in predictions]
    )[::-1]
    predictions = np.asarray(predictions)[inds_pred]

    # indices of unmatched ground truth assemblies
    matched = [
        MatchedPrediction(
            prediction=p,
            score=(p.affinity if p.n_links else p.confidence),
            ground_truth=None,
            oks=0.0,
        )
        for p in predictions
    ]

    # Greedy assembly matching like in pycocotools
    if greedy_matching:
        matched_gt_indices = set()
        for idx, pred in enumerate(predictions):
            oks = [
                calc_object_keypoint_similarity(
                    pred.xy,
                    gt.xy,
                    sigma,
                    margin,
                    symmetric_kpts,
                )
                for gt in ground_truth
            ]
            if np.all(np.isnan(oks)):
                continue

            ind_best = np.nanargmax(oks)

            # if this gt already matched, and not a crowd, continue
            if ind_best in matched_gt_indices:
                continue

            # Only match the pred to the GT if the OKS value is above a given threshold
            if oks[ind_best] < greedy_oks_threshold:
                continue

            matched_gt_indices.add(ind_best)
            matched[idx].ground_truth = ground_truth[ind_best]
            matched[idx].oks = oks[ind_best]

    # Global rather than greedy assembly matching
    else:
        inds_true = list(range(len(ground_truth)))
        mat = np.zeros((len(predictions), len(ground_truth)))
        for i, a_pred in enumerate(predictions):
            for j, a_true in enumerate(ground_truth):
                oks = calc_object_keypoint_similarity(
                    a_pred.xy,
                    a_true.xy,
                    sigma,
                    margin,
                    symmetric_kpts,
                )
                if ~np.isnan(oks):
                    mat[i, j] = oks
        rows, cols = linear_sum_assignment(mat, maximize=True)
        for row, col in zip(rows, cols):
            matched[row].ground_truth = ground_truth[col]
            matched[row].oks = mat[row, col]
            _ = inds_true.remove(col)

    return num_ground_truth, matched


def parse_ground_truth_data_file(h5_file):
    df = pd.read_hdf(h5_file)
    try:
        df.drop("single", axis=1, level="individuals", inplace=True)
    except KeyError:
        pass
    # Cast columns of dtype 'object' to float to avoid TypeError
    # further down in _parse_ground_truth_data.
    cols = df.select_dtypes(include="object").columns
    if cols.to_list():
        df[cols] = df[cols].astype("float")
    n_individuals = len(df.columns.get_level_values("individuals").unique())
    n_bodyparts = len(df.columns.get_level_values("bodyparts").unique())
    data = df.to_numpy().reshape((df.shape[0], n_individuals, n_bodyparts, -1))
    return _parse_ground_truth_data(data)


def _parse_ground_truth_data(data):
    gt = dict()
    for i, arr in enumerate(data):
        temp = []
        for row in arr:
            if np.isnan(row[:, :2]).all():
                continue
            ass = Assembly.from_array(row)
            temp.append(ass)
        if not temp:
            continue
        gt[i] = temp
    return gt


def find_outlier_assemblies(dict_of_assemblies, criterion="area", qs=(5, 95)):
    if not hasattr(Assembly, criterion):
        raise ValueError(f"Invalid criterion {criterion}.")

    if len(qs) != 2:
        raise ValueError(
            "Two percentiles (for lower and upper bounds) should be given."
        )

    tuples = []
    for frame_ind, assemblies in dict_of_assemblies.items():
        for assembly in assemblies:
            tuples.append((frame_ind, getattr(assembly, criterion)))
    frame_inds, vals = zip(*tuples)
    vals = np.asarray(vals)
    lo, up = np.percentile(vals, qs, interpolation="nearest")
    inds = np.flatnonzero((vals < lo) | (vals > up)).tolist()
    return list(set(frame_inds[i] for i in inds))


def _compute_precision_and_recall(
    num_gt_assemblies: int,
    oks_values: np.ndarray,
    oks_threshold: float,
    recall_thresholds: np.ndarray,
) -> tuple[np.ndarray, np.ndarray]:
    """Computes the precision and recall scores at a given OKS threshold

    Args:
        num_gt_assemblies: the number of ground truth assemblies (used to compute false
            negatives + true positives).
        oks_values: the OKS value to the matched GT assembly for each prediction
        oks_threshold: the OKS threshold at which recall and precision are being
            computed
        recall_thresholds: the recall thresholds to use to compute scores

    Returns:
        The precision and recall arrays at each recall threshold
    """
    tp = np.cumsum(oks_values >= oks_threshold)
    fp = np.cumsum(oks_values < oks_threshold)
    rc = tp / num_gt_assemblies
    pr = tp / (fp + tp + np.spacing(1))
    recall = rc[-1]

    # Guarantee precision decreases monotonically, see
    # https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173
    for i in range(len(pr) - 1, 0, -1):
        if pr[i] > pr[i - 1]:
            pr[i - 1] = pr[i]

    inds_rc = np.searchsorted(rc, recall_thresholds, side="left")
    precision = np.zeros(inds_rc.shape)
    valid = inds_rc < len(pr)
    precision[valid] = pr[inds_rc[valid]]
    return precision, recall


def evaluate_assembly_greedy(
    assemblies_gt: dict[Any, list[Assembly]],
    assemblies_pred: dict[Any, list[Assembly]],
    oks_sigma: float,
    oks_thresholds: Iterable[float],
    margin: int | float = 0,
    symmetric_kpts: list[tuple[int, int]] | None = None,
) -> dict:
    """Runs greedy mAP evaluation, as done by pycocotools

    Args:
        assemblies_gt: A dictionary mapping image ID (e.g. filepath) to ground truth
            assemblies. Should contain all the same keys as ``assemblies_pred``.
        assemblies_pred: A dictionary mapping image ID (e.g. filepath) to predicted
            assemblies. Should contain all the same keys as ``assemblies_gt``.
        oks_sigma: The sigma to use to compute OKS values for keypoints .
        oks_thresholds: The OKS thresholds at which to compute precision & recall.
        margin: The margin to use to compute bounding boxes from keypoints.
        symmetric_kpts: The symmetric keypoints in the dataset.
    """
    recall_thresholds = np.linspace(  # np.linspace(0, 1, 101)
        start=0.0, stop=1.00, num=int(np.round((1.00 - 0.0) / 0.01)) + 1, endpoint=True
    )
    precisions = []
    recalls = []
    for oks_t in oks_thresholds:
        all_matched = []
        total_gt_assemblies = 0
        for ind, gt_assembly in assemblies_gt.items():
            pred_assemblies = assemblies_pred.get(ind, [])
            num_gt_assemblies, matched = match_assemblies(
                pred_assemblies,
                gt_assembly,
                oks_sigma,
                margin,
                symmetric_kpts,
                greedy_matching=True,
                greedy_oks_threshold=oks_t,
            )
            all_matched.extend(matched)
            total_gt_assemblies += num_gt_assemblies

        if len(all_matched) == 0:
            precisions.append(0.0)
            recalls.append(0.0)
            continue

        # Global sort of assemblies (across all images) by score
        scores = np.asarray([-m.score for m in all_matched])
        sorted_pred_indices = np.argsort(scores, kind="mergesort")
        oks = np.asarray([match.oks for match in all_matched])[sorted_pred_indices]

        # Compute prediction and recall
        p, r = _compute_precision_and_recall(
            total_gt_assemblies, oks, oks_t, recall_thresholds
        )
        precisions.append(p)
        recalls.append(r)

    precisions = np.asarray(precisions)
    recalls = np.asarray(recalls)
    return {
        "precisions": precisions,
        "recalls": recalls,
        "mAP": precisions.mean(),
        "mAR": recalls.mean(),
    }


def evaluate_assembly(
    ass_pred_dict,
    ass_true_dict,
    oks_sigma=0.072,
    oks_thresholds=np.linspace(0.5, 0.95, 10),
    margin=0,
    symmetric_kpts=None,
    greedy_matching=False,
    with_tqdm: bool = True,
):
    if greedy_matching:
        return evaluate_assembly_greedy(
            ass_true_dict,
            ass_pred_dict,
            oks_sigma=oks_sigma,
            oks_thresholds=oks_thresholds,
            margin=margin,
            symmetric_kpts=symmetric_kpts,
        )

    # sigma is taken as the median of all COCO keypoint standard deviations
    all_matched = []
    total_gt_assemblies = 0

    gt_assemblies = ass_true_dict.items()
    if with_tqdm:
        gt_assemblies = tqdm(gt_assemblies)

    for ind, gt_assembly in gt_assemblies:
        pred_assemblies = ass_pred_dict.get(ind, [])
        num_gt, matched = match_assemblies(
            pred_assemblies,
            gt_assembly,
            oks_sigma,
            margin,
            symmetric_kpts,
            greedy_matching,
        )
        all_matched.extend(matched)
        total_gt_assemblies += num_gt

    if not all_matched:
        return {
            "precisions": np.array([]),
            "recalls": np.array([]),
            "mAP": 0.0,
            "mAR": 0.0,
        }

    conf_pred = np.asarray([match.score for match in all_matched])
    idx = np.argsort(-conf_pred, kind="mergesort")
    # Sort matching score (OKS) in descending order of assembly affinity
    oks = np.asarray([match.oks for match in all_matched])[idx]
    recall_thresholds = np.linspace(0, 1, 101)
    precisions = []
    recalls = []
    for t in oks_thresholds:
        p, r = _compute_precision_and_recall(
            total_gt_assemblies, oks, t, recall_thresholds
        )
        precisions.append(p)
        recalls.append(r)

    precisions = np.asarray(precisions)
    recalls = np.asarray(recalls)
    return {
        "precisions": precisions,
        "recalls": recalls,
        "mAP": precisions.mean(),
        "mAR": recalls.mean(),
    }


--- File: deeplabcut/core/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/core/visualization.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Visualization methods for """
from __future__ import annotations

from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np


def form_figure(nx, ny) -> tuple[plt.Figure, plt.Axes]:
    """Forms a figure on which to plot images"""
    fig, ax = plt.subplots(frameon=False)
    ax.set_xlim(0, nx)
    ax.set_ylim(0, ny)
    ax.axis("off")
    ax.invert_yaxis()
    fig.tight_layout()
    return fig, ax


def visualize_scoremaps(
    image: np.ndarray,
    scmap: np.ndarray,
) -> tuple[plt.Figure, plt.Axes]:
    """Plots scoremaps as an image overlay.

    Args:
        image: An image as a numpy array of shape (h, w, channels)
        scmap: A scoremap of shape (h, w)

    Returns:
        The figure and axis on which the image scoremap was plot.
    """
    ny, nx = np.shape(image)[:2]
    fig, ax = form_figure(nx, ny)
    ax.imshow(image)
    ax.imshow(scmap, alpha=0.5)
    return fig, ax


def visualize_locrefs(
    image: np.ndarray,
    scmap: np.ndarray,
    locref_x: np.ndarray,
    locref_y: np.ndarray,
    step: int = 5,
    zoom_width: int = 0,
) -> tuple[plt.Figure, plt.Axes]:
    """Plots a scoremap and the corresponding location refinement field on an image.

    Args:
        image: An image as a numpy array of shape (h, w, channels)
        scmap: A scoremap of shape (h, w)
        locref_x: The x-coordinate of the location refinement field, of shape (h, w)
        locref_y: The y-coordinate of the location refinement field, of shape (h, w)
        step: The step with which to plot the location refinement field.
        zoom_width: The zoom width with which to plot the scoremaps.

    Returns:
        The figure and axis on which the image scoremap and locref field were plot.
    """
    fig, ax = visualize_scoremaps(image, scmap)
    X, Y = np.meshgrid(np.arange(locref_x.shape[1]), np.arange(locref_x.shape[0]))
    M = np.zeros(locref_x.shape, dtype=bool)
    M[scmap < 0.5] = True
    U = np.ma.masked_array(locref_x, mask=M)
    V = np.ma.masked_array(locref_y, mask=M)
    ax.quiver(
        X[::step, ::step],
        Y[::step, ::step],
        U[::step, ::step],
        V[::step, ::step],
        color="r",
        units="x",
        scale_units="xy",
        scale=1,
        angles="xy",
    )
    if zoom_width > 0:
        maxloc = np.unravel_index(np.argmax(scmap), scmap.shape)
        ax.set_xlim(maxloc[1] - zoom_width, maxloc[1] + zoom_width)
        ax.set_ylim(maxloc[0] + zoom_width, maxloc[0] - zoom_width)
    return fig, ax


def visualize_paf(
    image: np.ndarray,
    paf: np.ndarray,
    step: int = 5,
    colors: list | None = None,
) -> tuple[plt.Figure, plt.Axes]:
    """Plots the PAF on top of the image.

    Args:
        image: Shape (height, width, channels). The image on which the model was run.
        paf: Shape (height, width, 2 * len(paf_graph)). The PAF output by the model.
        step: The step with which to plot the scoremaps.
        colors: The colormap to use.

    Returns:
        The figure and axis on which the image PAF was plot.
    """
    ny, nx = np.shape(image)[:2]
    fig, ax = form_figure(nx, ny)
    ax.imshow(image)
    n_fields = paf.shape[2]
    if colors is None:
        colors = ["r"] * n_fields
    for n in range(n_fields):
        U = paf[:, :, n, 0]
        V = paf[:, :, n, 1]
        X, Y = np.meshgrid(np.arange(U.shape[1]), np.arange(U.shape[0]))
        M = np.zeros(U.shape, dtype=bool)
        M[U**2 + V**2 < 0.5 * 0.5**2] = True
        U = np.ma.masked_array(U, mask=M)
        V = np.ma.masked_array(V, mask=M)
        ax.quiver(
            X[::step, ::step],
            Y[::step, ::step],
            U[::step, ::step],
            V[::step, ::step],
            scale=50,
            headaxislength=4,
            alpha=1,
            width=0.002,
            color=colors[n],
            angles="xy",
        )
    return fig, ax


def generate_model_output_plots(
    output_folder: Path,
    image_name: str,
    bodypart_names: list[str],
    bodyparts_to_plot: list[str],
    image: np.ndarray,
    scmap: np.ndarray,
    locref: np.ndarray | None = None,
    paf: np.ndarray | None = None,
    paf_graph: list[tuple[int, int]] | None = None,
    paf_all_in_one: bool = True,
    paf_colormap: str = "rainbow",
    output_suffix: str = "",
) -> None:
    """Generates model output plots (maps) for an image and saves them to disk.

    Args:
        output_folder: The folder in which the plots should be saved.
        image_name: The name of the image for which the plots were generated.
        bodypart_names: The names of bodyparts the model outputs.
        bodyparts_to_plot: The names of bodyparts that should be plot.
        image: Shape (height, width, channels). The image on which the model was run.
        scmap: Shape (height, width, num_bodyparts). The scoremaps output by the model.
        locref: Shape (height, width, num_bodyparts, 2). Optionally, the location
            refinement fields output by the model.
        paf: Shape (height, width, 2 * len(paf_graph)). Optionally, the part-affinity
            fields output by the model.
        paf_graph: Must be set if paf is not None. The PAF graph used to assemble.
        paf_all_in_one: Whether to plot all PAFs in a single image.
        paf_colormap: The colormap to use for the PAF maps.
        output_suffix: The filename suffix for the maps to output.
    """
    def _filename(map_name) -> str:
        return f"{image_name}_{map_name}_{output_suffix}.png"

    to_plot = [
        i for i, bpt in enumerate(bodypart_names) if bpt in bodyparts_to_plot
    ]
    if len(to_plot) > 1:
        map_ = scmap[:, :, to_plot].sum(axis=2)
    elif len(to_plot) == 1 and len(bodypart_names) > 1:
        map_ = scmap[:, :, to_plot[0]]
    else:
        map_ = scmap[..., 0]

    fig1, _ = visualize_scoremaps(image, map_)
    fig1.savefig(output_folder / _filename("scmap"))

    if locref is not None:
        if len(to_plot) > 1:
            map_ = scmap[:, :, to_plot]
            locref_x_ = locref[:, :, to_plot, 0]
            locref_y_ = locref[:, :, to_plot, 1]
            # only get the locref fields around their respective detections
            locref_x_[map_ < 0.5] = 0
            locref_y_[map_ < 0.5] = 0
            # combine locrefs
            map_ = map_.sum(axis=2)
            locref_x_ = locref_x_.sum(axis=2)
            locref_y_ = locref_y_.sum(axis=2)
        elif len(to_plot) == 1 and len(bodypart_names) > 1:
            locref_x_ = locref[:, :, to_plot[0], 0]
            locref_y_ = locref[:, :, to_plot[0], 1]
        else:
            locref_x_ = locref[..., 0]
            locref_y_ = locref[..., 1]

        fig2, _ = visualize_locrefs(image, map_, locref_x_, locref_y_)
        fig2.savefig(output_folder / _filename("locref"))

    if paf is not None:
        if paf_graph is None:
            raise ValueError(f"When plotting the PAF, you must pass the ``paf_graph``")

        edge_list = []
        for n, edge in enumerate(paf_graph):
            if any(ind in to_plot for ind in edge):
                e0, e1 = edge
                edge_list.append(
                    [(2 * n, 2 * n + 1), (bodypart_names[e0], bodypart_names[e1])]
                )

        if paf_all_in_one:
            inds = [elem[0] for elem in edge_list]
            n_inds = len(inds)
            cmap = plt.cm.get_cmap(paf_colormap, n_inds)
            colors = cmap(range(n_inds))
            fig3, _ = visualize_paf(image, paf[:, :, inds], colors=colors)
            fig3.savefig(output_folder / _filename("paf"))
        else:
            for inds, names in edge_list:
                fig3, _ = visualize_paf(image, paf[:, :, [inds]])
                fig3.savefig(output_folder / _filename(f"paf_{'_'.join(names)}"))

    plt.close("all")


--- File: deeplabcut/core/engine.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Defines the deep learning frameworks available"""
from __future__ import annotations

from dataclasses import dataclass
from enum import Enum


@dataclass(frozen=True)
class EngineDataMixin:
    aliases: tuple[str]
    model_folder_name: str
    pose_cfg_name: str
    results_folder_name: str


class Engine(EngineDataMixin, Enum):
    PYTORCH = (
        ("pytorch", "torch"),
        "dlc-models-pytorch",
        "pytorch_config.yaml",
        "evaluation-results-pytorch",
    )
    TF = (
        ("tensorflow", "tf"),
        "dlc-models",
        "pose_cfg.yaml",
        "evaluation-results",
    )

    @classmethod
    def _missing_(cls, value):
        if isinstance(value, str):
            for member in cls:
                if value.lower() in member.aliases:
                    return member
        return None

    def __repr__(self) -> str:
        return f"Engine.{self.name}"


--- File: deeplabcut/core/crossvalutils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import os
import pickle
import shutil
from collections import defaultdict
from copy import deepcopy

import networkx as nx
import numpy as np
import pandas as pd
from scipy.spatial import cKDTree
from sklearn.metrics.cluster import contingency_matrix
from tqdm import tqdm

from deeplabcut.core.inferenceutils import (
    _parse_ground_truth_data,
    Assembler,
    evaluate_assembly,
)
from deeplabcut.utils import auxfun_multianimal, auxiliaryfunctions


def _set_up_evaluation(data):
    params = dict()
    params["joint_names"] = data["metadata"]["all_joints_names"]
    params["num_joints"] = len(params["joint_names"])
    partaffinityfield_graph = data["metadata"]["PAFgraph"]
    params["paf"] = np.arange(len(partaffinityfield_graph))
    params["paf_graph"] = params["paf_links"] = [
        partaffinityfield_graph[l] for l in params["paf"]
    ]
    params["bpts"] = params["ibpts"] = range(params["num_joints"])
    params["imnames"] = [fn for fn in list(data) if fn != "metadata"]
    return params


def _form_original_path(path):
    root, filename = os.path.split(path)
    base, ext = os.path.splitext(filename)
    return os.path.join(root, filename.split("c")[0] + ext)


def _unsorted_unique(array):
    _, inds = np.unique(array, return_index=True)
    return np.asarray(array)[np.sort(inds)]


def find_closest_neighbors(
    query: np.ndarray, ref: np.ndarray, k: int = 3
) -> np.ndarray:
    """Greedy matching of predicted keypoints to ground truth keypoints

    Args:
        query: the query keypoints
        ref: the reference keypoints
        k: The list of k-th nearest neighbors to return.

    Returns:
        an array of shape (len(query), ) containing the index of the closest
        reference keypoint for each query keypoint
    """
    n_preds = ref.shape[0]
    tree = cKDTree(ref)
    dist, inds = tree.query(query, k=k)
    idx = np.argsort(dist[:, 0])
    neighbors = np.full(len(query), -1, dtype=int)
    picked = {tree.n}
    for i, ind in enumerate(inds[idx]):
        for j in ind:
            if j not in picked:
                picked.add(j)
                neighbors[idx[i]] = j
                break
        if len(picked) == (n_preds + 1):
            break
    return neighbors


def _calc_separability(
    vals_left, vals_right, n_bins=101, metric="jeffries", max_sensitivity=False
):
    if metric not in ("jeffries", "auc"):
        raise ValueError("`metric` should be either 'jeffries' or 'auc'.")

    bins = np.linspace(0, 1, n_bins)
    hist_left = np.histogram(vals_left, bins=bins)[0]
    hist_left = hist_left / hist_left.sum()
    hist_right = np.histogram(vals_right, bins=bins)[0]
    hist_right = hist_right / hist_right.sum()
    tpr = np.cumsum(hist_right)
    if metric == "jeffries":
        sep = np.sqrt(
            2 * (1 - np.sum(np.sqrt(hist_left * hist_right)))
        )  # Jeffries-Matusita distance
    else:
        sep = np.trapz(np.cumsum(hist_left), tpr)
    if max_sensitivity:
        threshold = bins[max(1, np.argmax(tpr > 0))]
    else:
        threshold = bins[np.argmin(1 - np.cumsum(hist_left) + tpr)]
    return sep, threshold


def _calc_within_between_pafs(
    data,
    metadata,
    per_edge=True,
    train_set_only=True,
):
    data = deepcopy(data)
    train_inds = set(metadata["data"]["trainIndices"])
    graph = data["metadata"]["PAFgraph"]
    within_train = defaultdict(list)
    within_test = defaultdict(list)
    between_train = defaultdict(list)
    between_test = defaultdict(list)
    for i, (key, dict_) in enumerate(data.items()):
        if key == "metadata":
            continue

        is_train = i in train_inds
        if train_set_only and not is_train:
            continue

        df = dict_["groundtruth"][2]
        try:
            df.drop("single", level="individuals", inplace=True)
        except KeyError:
            pass
        bpts = df.index.get_level_values("bodyparts").unique().to_list()
        coords_gt = (
            df.unstack(["individuals", "coords"])
            .reindex(bpts, level="bodyparts")
            .to_numpy()
            .reshape((len(bpts), -1, 2))
        )
        if np.isnan(coords_gt).all():
            continue

        coords = dict_["prediction"]["coordinates"][0]
        # Get animal IDs and corresponding indices in the arrays of detections
        lookup = dict()
        for i, (coord, coord_gt) in enumerate(zip(coords, coords_gt)):
            inds = np.flatnonzero(np.all(~np.isnan(coord), axis=1))
            inds_gt = np.flatnonzero(np.all(~np.isnan(coord_gt), axis=1))
            if inds.size and inds_gt.size:
                neighbors = find_closest_neighbors(coord_gt[inds_gt], coord[inds], k=3)
                found = neighbors != -1
                lookup[i] = dict(zip(inds_gt[found], inds[neighbors[found]]))

        costs = dict_["prediction"]["costs"]
        for k, v in costs.items():
            paf = v["m1"]
            mask_within = np.zeros(paf.shape, dtype=bool)
            s, t = graph[k]
            if s not in lookup or t not in lookup:
                continue
            lu_s = lookup[s]
            lu_t = lookup[t]
            common_id = set(lu_s).intersection(lu_t)
            for id_ in common_id:
                mask_within[lu_s[id_], lu_t[id_]] = True
            within_vals = paf[mask_within]
            between_vals = paf[~mask_within]
            if is_train:
                within_train[k].extend(within_vals)
                between_train[k].extend(between_vals)
            else:
                within_test[k].extend(within_vals)
                between_test[k].extend(between_vals)
    if not per_edge:
        within_train = np.concatenate([*within_train.values()])
        within_test = np.concatenate([*within_test.values()])
        between_train = np.concatenate([*between_train.values()])
        between_test = np.concatenate([*between_test.values()])
    return (within_train, within_test), (between_train, between_test)


def _benchmark_paf_graphs(
    config,
    inference_cfg,
    data,
    paf_inds,
    greedy=False,
    add_discarded=True,
    identity_only=False,
    calibration_file="",
    oks_sigma=0.1,
    margin=0,
    symmetric_kpts=None,
    split_inds=None,
):
    metadata = data.pop("metadata")
    multi_bpts_orig = auxfun_multianimal.extractindividualsandbodyparts(config)[2]
    multi_bpts = [j for j in metadata["all_joints_names"] if j in multi_bpts_orig]
    n_multi = len(multi_bpts)
    data_ = {"metadata": metadata}
    for k, v in data.items():
        data_[k] = v["prediction"]
    ass = Assembler(
        data_,
        max_n_individuals=inference_cfg["topktoretain"],
        n_multibodyparts=n_multi,
        greedy=greedy,
        pcutoff=inference_cfg.get("pcutoff", 0.1),
        min_affinity=inference_cfg.get("pafthreshold", 0.1),
        add_discarded=add_discarded,
        identity_only=identity_only,
    )
    if calibration_file:
        ass.calibrate(calibration_file)

    params = ass.metadata
    image_paths = params["imnames"]
    bodyparts = params["joint_names"]
    idx = (
        data[image_paths[0]]["groundtruth"][2]
        .unstack("coords")
        .reindex(bodyparts, level="bodyparts")
        .index
    )
    mask_multi = idx.get_level_values("individuals") != "single"
    if not mask_multi.all():
        idx = idx.drop("single", level="individuals")
    individuals = idx.get_level_values("individuals").unique()
    n_individuals = len(individuals)
    map_ = dict(zip(individuals, range(n_individuals)))

    # Form ground truth beforehand
    ground_truth = []
    for i, imname in enumerate(image_paths):
        temp = data[imname]["groundtruth"][2].reindex(multi_bpts, level="bodyparts")
        ground_truth.append(temp.to_numpy().reshape((-1, 2)))
    ground_truth = np.stack(ground_truth)
    temp = np.ones((*ground_truth.shape[:2], 3))
    temp[..., :2] = ground_truth
    temp = temp.reshape((temp.shape[0], n_individuals, -1, 3))
    ass_true_dict = _parse_ground_truth_data(temp)
    ids = np.vectorize(map_.get)(idx.get_level_values("individuals").to_numpy())
    ground_truth = np.insert(ground_truth, 2, ids, axis=2)

    # Assemble animals on the full set of detections
    paf_inds = sorted(paf_inds, key=len)
    n_graphs = len(paf_inds)
    all_scores = []
    all_metrics = []
    all_assemblies = []
    for j, paf in enumerate(paf_inds, start=1):
        print(f"Graph {j}|{n_graphs}")
        ass.paf_inds = paf
        ass.assemble()
        all_assemblies.append((ass.assemblies, ass.unique, ass.metadata["imnames"]))
        if split_inds is not None:
            oks = []

            # get the indices of the images in the training set
            dataset_idx = [data[image_name]["index"] for image_name in image_paths]
            for inds in split_inds:
                ass_gt = {
                    k: v for k, v in ass_true_dict.items() if dataset_idx[k] in inds
                }
                ass_pred = {
                    k: v for k, v in ass.assemblies.items() if dataset_idx[k] in inds
                }

                oks.append(
                    evaluate_assembly(
                        ass_pred,
                        ass_gt,
                        oks_sigma,
                        margin=margin,
                        symmetric_kpts=symmetric_kpts,
                        greedy_matching=inference_cfg.get("greedy_oks", False),
                    )
                )
        else:
            oks = evaluate_assembly(
                ass.assemblies,
                ass_true_dict,
                oks_sigma,
                margin=margin,
                symmetric_kpts=symmetric_kpts,
                greedy_matching=inference_cfg.get("greedy_oks", False),
            )
        all_metrics.append(oks)
        scores = np.full((len(image_paths), 2), np.nan)
        for i, imname in enumerate(tqdm(image_paths)):
            gt = ground_truth[i]
            gt = gt[~np.isnan(gt).any(axis=1)]
            if len(np.unique(gt[:, 2])) < 2:  # Only consider frames with 2+ animals
                continue

            # Count the number of unassembled bodyparts
            n_dets = len(gt)
            animals = ass.assemblies.get(i)
            if animals is None:
                if n_dets:
                    scores[i, 0] = 1
            else:
                animals = [
                    np.c_[animal.data, np.ones(animal.data.shape[0]) * n]
                    for n, animal in enumerate(animals)
                ]
                hyp = np.concatenate(animals)
                hyp = hyp[~np.isnan(hyp).any(axis=1)]
                scores[i, 0] = max(0, (n_dets - hyp.shape[0]) / n_dets)
                neighbors = find_closest_neighbors(gt[:, :2], hyp[:, :2])
                valid = neighbors != -1
                id_gt = gt[valid, 2]
                id_hyp = hyp[neighbors[valid], -1]
                mat = contingency_matrix(id_gt, id_hyp)
                purity = mat.max(axis=0).sum() / mat.sum()
                scores[i, 1] = purity
        all_scores.append((scores, paf))

    dfs = []
    for score, inds in all_scores:
        df = pd.DataFrame(score, columns=["miss", "purity"])
        df["ngraph"] = len(inds)
        dfs.append(df)
    big_df = pd.concat(dfs)
    group = big_df.groupby("ngraph")
    return (all_scores, group.agg(["mean", "std"]).T, all_metrics, all_assemblies)


def _get_n_best_paf_graphs(
    data,
    metadata,
    full_graph,
    n_graphs=10,
    root=None,
    which="best",
    ignore_inds=None,
    metric="auc",
):
    if which not in ("best", "worst"):
        raise ValueError('`which` must be either "best" or "worst"')

    (within_train, _), (between_train, _) = _calc_within_between_pafs(
        data,
        metadata,
        train_set_only=True,
    )
    # Handle unlabeled bodyparts...
    existing_edges = set(k for k, v in within_train.items() if v)
    if ignore_inds is not None:
        existing_edges = existing_edges.difference(ignore_inds)
    existing_edges = list(existing_edges)

    if not any(between_train.values()):
        # Only 1 animal, let us return the full graph indices only
        return ([existing_edges], dict(zip(existing_edges, [0] * len(existing_edges))))

    scores, _ = zip(
        *[
            _calc_separability(between_train[n], within_train[n], metric=metric)
            for n in existing_edges
        ]
    )

    # Find minimal skeleton
    G = nx.Graph()
    for edge, score in zip(existing_edges, scores):
        if np.isfinite(score):
            G.add_edge(*full_graph[edge], weight=score)
    if which == "best":
        order = np.asarray(existing_edges)[np.argsort(scores)[::-1]]
        if root is None:
            root = []
            for edge in nx.maximum_spanning_edges(G, data=False):
                root.append(full_graph.index(sorted(edge)))
    else:
        order = np.asarray(existing_edges)[np.argsort(scores)]
        if root is None:
            root = []
            for edge in nx.minimum_spanning_edges(G, data=False):
                root.append(full_graph.index(sorted(edge)))

    n_edges = len(existing_edges) - len(root)
    lengths = np.linspace(0, n_edges, min(n_graphs, n_edges + 1), dtype=int)[1:]
    order = order[np.isin(order, root, invert=True)]
    paf_inds = [root]
    for length in lengths:
        paf_inds.append(root + list(order[:length]))
    return paf_inds, dict(zip(existing_edges, scores))


def cross_validate_paf_graphs(
    config,
    inference_config,
    full_data_file,
    metadata_file,
    output_name="",
    pcutoff=0.1,
    oks_sigma=0.1,
    margin=0,
    greedy=False,
    add_discarded=True,
    calibrate=False,
    overwrite_config=True,
    n_graphs=10,
    paf_inds=None,
    symmetric_kpts=None,
):
    cfg = auxiliaryfunctions.read_config(config)
    inf_cfg = auxiliaryfunctions.read_plainconfig(inference_config)
    inf_cfg_temp = inf_cfg.copy()
    inf_cfg_temp["pcutoff"] = pcutoff

    with open(full_data_file, "rb") as file:
        data = pickle.load(file)
    with open(metadata_file, "rb") as file:
        metadata = pickle.load(file)

    params = _set_up_evaluation(data)
    to_ignore = auxfun_multianimal.filter_unwanted_paf_connections(
        cfg, params["paf_graph"]
    )
    best_graphs = _get_n_best_paf_graphs(
        data,
        metadata,
        params["paf_graph"],
        ignore_inds=to_ignore,
        n_graphs=n_graphs,
    )
    paf_scores = best_graphs[1]
    if paf_inds is None:
        paf_inds = best_graphs[0]

    if calibrate:
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
        calibration_file = os.path.join(
            cfg["project_path"],
            str(trainingsetfolder),
            "CollectedData_" + cfg["scorer"] + ".h5",
        )
    else:
        calibration_file = ""

    results = _benchmark_paf_graphs(
        cfg,
        inf_cfg_temp,
        data,
        paf_inds,
        greedy,
        add_discarded,
        oks_sigma=oks_sigma,
        margin=margin,
        symmetric_kpts=symmetric_kpts,
        calibration_file=calibration_file,
        split_inds=[
            metadata["data"]["trainIndices"],
            metadata["data"]["testIndices"],
        ],
    )
    # Select optimal PAF graph
    df = results[1]
    size_opt = np.argmax((1 - df.loc["miss", "mean"]) * df.loc["purity", "mean"])
    pose_config = inference_config.replace("inference_cfg", "pose_cfg")
    if not overwrite_config:
        shutil.copy(pose_config, pose_config.replace(".yaml", "_old.yaml"))
    inds = list(paf_inds[size_opt])
    auxiliaryfunctions.edit_config(
        pose_config, {"paf_best": [int(ind) for ind in inds]}
    )
    if output_name:
        with open(output_name, "wb") as file:
            pickle.dump([results], file)
    return results[:3], paf_scores, results[3][size_opt]


# Backwards compatibility
_find_closest_neighbors = find_closest_neighbors


--- File: deeplabcut/core/trackingutils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import abc
import math
import warnings
from collections import defaultdict

import numpy as np
from filterpy.common import kinematic_kf
from filterpy.kalman import KalmanFilter
from matplotlib import patches
from numba import jit
from numba.core.errors import NumbaPerformanceWarning
from scipy.optimize import linear_sum_assignment
from scipy.stats import mode
from tqdm import tqdm

warnings.simplefilter("ignore", category=NumbaPerformanceWarning)

TRACK_METHODS = {
    "box": "_bx",
    "skeleton": "_sk",
    "ellipse": "_el",
    "transformer": "_tr",
}


def calc_iou(bbox1, bbox2):
    x1 = max(bbox1[0], bbox2[0])
    y1 = max(bbox1[1], bbox2[1])
    x2 = min(bbox1[2], bbox2[2])
    y2 = min(bbox1[3], bbox2[3])
    w = max(0, x2 - x1)
    h = max(0, y2 - y1)
    wh = w * h
    return wh / (
        (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])
        + (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])
        - wh
    )


class BaseTracker:
    """Base class for a constant-velocity Kalman filter-based tracker."""

    n_trackers = 0

    def __init__(self, dim, dim_z):
        self.kf = kinematic_kf(
            dim,
            1,
            dim_z=dim_z,
            order_by_dim=False,
        )
        self.id = self.__class__.n_trackers
        self.__class__.n_trackers += 1
        self.time_since_update = 0
        self.age = 0
        self.hits = 0
        self.hit_streak = 0

    def update(self, z):
        self.time_since_update = 0
        self.hits += 1
        self.hit_streak += 1
        self.kf.update(z)

    def predict(self):
        self.kf.predict()
        self.age += 1
        if self.time_since_update > 0:
            self.hit_streak = 0
        self.time_since_update += 1
        return self.state

    @property
    def state(self):
        return self.kf.x.squeeze()[: self.kf.dim_z]

    @state.setter
    def state(self, state):
        self.kf.x[: self.kf.dim_z] = state


class Ellipse:
    def __init__(self, x, y, width, height, theta):
        self.x = x
        self.y = y
        self.width = width
        self.height = height
        self.theta = theta  # in radians
        self._geometry = None

    @property
    def parameters(self):
        return self.x, self.y, self.width, self.height, self.theta

    @property
    def aspect_ratio(self):
        return max(self.width, self.height) / min(self.width, self.height)

    def calc_similarity_with(self, other_ellipse):
        max_dist = max(
            self.height, self.width, other_ellipse.height, other_ellipse.width
        )
        dist = math.sqrt(
            (self.x - other_ellipse.x) ** 2 + (self.y - other_ellipse.y) ** 2
        )

        if max_dist == 0:
            max_dist = 1

        cost1 = 1 - min(dist / max_dist, 1)
        cost2 = abs(math.cos(self.theta - other_ellipse.theta))
        return 0.8 * cost1 + 0.2 * cost2 * cost1

    def contains_points(self, xy, tol=0.1):
        ca = math.cos(self.theta)
        sa = math.sin(self.theta)
        x_demean = xy[:, 0] - self.x
        y_demean = xy[:, 1] - self.y
        return (
            ((ca * x_demean + sa * y_demean) ** 2 / (0.5 * self.width) ** 2)
            + ((sa * x_demean - ca * y_demean) ** 2 / (0.5 * self.height) ** 2)
        ) <= 1 + tol

    def draw(self, show_axes=True, ax=None, **kwargs):
        import matplotlib.pyplot as plt
        from matplotlib.lines import Line2D
        from matplotlib.transforms import Affine2D

        if ax is None:
            ax = plt.subplot(111, aspect="equal")
        el = patches.Ellipse(
            xy=(self.x, self.y),
            width=self.width,
            height=self.height,
            angle=np.rad2deg(self.theta),
            **kwargs,
        )
        ax.add_patch(el)
        if show_axes:
            major = Line2D([-self.width / 2, self.width / 2], [0, 0], lw=3, zorder=3)
            minor = Line2D([0, 0], [-self.height / 2, self.height / 2], lw=3, zorder=3)
            trans = (
                Affine2D().rotate(self.theta).translate(self.x, self.y) + ax.transData
            )
            major.set_transform(trans)
            minor.set_transform(trans)
            ax.add_artist(major)
            ax.add_artist(minor)


class EllipseFitter:
    def __init__(self, sd=2):
        self.sd = sd
        self.x = None
        self.y = None
        self.params = None
        self._coeffs = None

    def fit(self, xy):
        self.x, self.y = xy[np.isfinite(xy).all(axis=1)].T
        if len(self.x) < 3:
            return None
        if self.sd:
            self.params = self._fit_error(self.x, self.y, self.sd)
        else:
            self._coeffs = self._fit(self.x, self.y)
            self.params = self.calc_parameters(self._coeffs)
        if not np.isnan(self.params).any():
            return Ellipse(*self.params)
        return None

    @staticmethod
    @jit(nopython=True)
    def _fit(x, y):
        """
        Least Squares ellipse fitting algorithm
        Fit an ellipse to a set of X- and Y-coordinates.
        See Halir and Flusser, 1998 for implementation details

        :param x: ndarray, 1D trajectory
        :param y: ndarray, 1D trajectory
        :return: 1D ndarray of 6 coefficients of the general quadratic curve:
            ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0
        """
        D1 = np.vstack((x * x, x * y, y * y))
        D2 = np.vstack((x, y, np.ones_like(x)))
        S1 = D1 @ D1.T
        S2 = D1 @ D2.T
        S3 = D2 @ D2.T
        T = -np.linalg.inv(S3) @ S2.T
        temp = S1 + S2 @ T
        M = np.zeros_like(temp)
        M[0] = temp[2] * 0.5
        M[1] = -temp[1]
        M[2] = temp[0] * 0.5
        E, V = np.linalg.eig(M)
        cond = 4 * V[0] * V[2] - V[1] ** 2
        a1 = V[:, cond > 0][:, 0]
        a2 = T @ a1
        return np.hstack((a1, a2))

    @staticmethod
    @jit(nopython=True)
    def _fit_error(x, y, sd):
        """
        Fit a sd-sigma covariance error ellipse to the data.

        :param x: ndarray, 1D input of X coordinates
        :param y: ndarray, 1D input of Y coordinates
        :param sd: int, size of the error ellipse in 'standard deviation'
        :return: ellipse center, semi-axes length, angle to the X-axis
        """
        cov = np.cov(x, y)
        E, V = np.linalg.eigh(cov)  # Returns the eigenvalues in ascending order
        # r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)
        # height, width = np.sqrt(E * r2)
        height, width = 2 * sd * np.sqrt(E)
        a, b = V[:, 1]
        rotation = math.atan2(b, a) % np.pi
        return [np.mean(x), np.mean(y), width, height, rotation]

    @staticmethod
    @jit(nopython=True)
    def calc_parameters(coeffs):
        """
        Calculate ellipse center coordinates, semi-axes lengths, and
        the counterclockwise angle of rotation from the x-axis to the ellipse major axis.
        Visit http://mathworld.wolfram.com/Ellipse.html
        for how to estimate ellipse parameters.

        :param coeffs: list of fitting coefficients
        :return: center: 1D ndarray, semi-axes: 1D ndarray, angle: float
        """
        # The general quadratic curve has the form:
        # ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0
        a, b, c, d, f, g = coeffs
        b *= 0.5
        d *= 0.5
        f *= 0.5

        # Ellipse center coordinates
        x0 = (c * d - b * f) / (b * b - a * c)
        y0 = (a * f - b * d) / (b * b - a * c)

        # Semi-axes lengths
        num = 2 * (a * f * f + c * d * d + g * b * b - 2 * b * d * f - a * c * g)
        den1 = (b * b - a * c) * (np.sqrt((a - c) ** 2 + 4 * b * b) - (a + c))
        den2 = (b * b - a * c) * (-np.sqrt((a - c) ** 2 + 4 * b * b) - (a + c))
        major = np.sqrt(num / den1)
        minor = np.sqrt(num / den2)

        # Angle to the horizontal
        if b == 0:
            if a < c:
                phi = 0
            else:
                phi = np.pi / 2
        else:
            if a < c:
                phi = np.arctan(2 * b / (a - c)) / 2
            else:
                phi = np.pi / 2 + np.arctan(2 * b / (a - c)) / 2

        return [x0, y0, 2 * major, 2 * minor, phi]


class EllipseTracker(BaseTracker):
    def __init__(self, params):
        super().__init__(dim=5, dim_z=5)
        self.kf.R[2:, 2:] *= 10.0
        # High uncertainty to the unobservable initial velocities
        self.kf.P[5:, 5:] *= 1000.0
        self.kf.P *= 10.0
        self.kf.Q[5:, 5:] *= 0.01
        self.state = params

    @BaseTracker.state.setter
    def state(self, params):
        state = np.asarray(params).reshape((-1, 1))
        super(EllipseTracker, type(self)).state.fset(self, state)


class SkeletonTracker(BaseTracker):
    def __init__(self, n_bodyparts):
        super().__init__(dim=n_bodyparts * 2, dim_z=n_bodyparts)
        self.kf.Q[self.kf.dim_z :, self.kf.dim_z :] *= 10
        self.kf.R[self.kf.dim_z :, self.kf.dim_z :] *= 0.01
        self.kf.P[self.kf.dim_z :, self.kf.dim_z :] *= 1000

    def update(self, pose):
        flat = pose.reshape((-1, 1))
        empty = np.isnan(flat).squeeze()
        if empty.any():
            H = self.kf.H.copy()
            H[empty] = 0
            flat[empty] = 0
            self.kf.update(flat, H=H)
        else:
            super().update(flat)

    @BaseTracker.state.setter
    def state(self, pose):
        curr_pose = pose.copy()
        empty = np.isnan(curr_pose).all(axis=1)
        if empty.any():
            fill = np.nanmean(pose, axis=0)
            curr_pose[empty] = fill
        super(SkeletonTracker, type(self)).state.fset(self, curr_pose.reshape((-1, 1)))


class BoxTracker(BaseTracker):
    def __init__(self, bbox):
        super().__init__(dim=4, dim_z=4)
        self.kf = KalmanFilter(dim_x=7, dim_z=4)
        self.kf.F = np.array(
            [
                [1, 0, 0, 0, 1, 0, 0],
                [0, 1, 0, 0, 0, 1, 0],
                [0, 0, 1, 0, 0, 0, 1],
                [0, 0, 0, 1, 0, 0, 0],
                [0, 0, 0, 0, 1, 0, 0],
                [0, 0, 0, 0, 0, 1, 0],
                [0, 0, 0, 0, 0, 0, 1],
            ]
        )
        self.kf.H = np.array(
            [
                [1, 0, 0, 0, 0, 0, 0],
                [0, 1, 0, 0, 0, 0, 0],
                [0, 0, 1, 0, 0, 0, 0],
                [0, 0, 0, 1, 0, 0, 0],
            ]
        )
        self.kf.R[2:, 2:] *= 10.0
        # Give high uncertainty to the unobservable initial velocities
        self.kf.P[4:, 4:] *= 1000.0
        self.kf.P *= 10.0
        self.kf.Q[-1, -1] *= 0.01
        self.kf.Q[4:, 4:] *= 0.01
        self.state = bbox

    def update(self, bbox):
        super().update(self.convert_bbox_to_z(bbox))

    def predict(self):
        if (self.kf.x[6] + self.kf.x[2]) <= 0:
            self.kf.x[6] *= 0.0
        return super().predict()

    @property
    def state(self):
        return self.convert_x_to_bbox(self.kf.x)[0]

    @state.setter
    def state(self, bbox):
        state = self.convert_bbox_to_z(bbox)
        super(BoxTracker, type(self)).state.fset(self, state)

    @staticmethod
    def convert_x_to_bbox(x, score=None):
        """
        Takes a bounding box in the centre form [x,y,s,r] and returns it in the form
        [x1,y1,x2,y2] where x1,y1 is the top left and x2,y2 is the bottom right
        """
        w = np.sqrt(x[2] * x[3])
        h = x[2] / w
        if score is None:
            return np.array(
                [x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0]
            ).reshape((1, 4))
        else:
            return np.array(
                [x[0] - w / 2.0, x[1] - h / 2.0, x[0] + w / 2.0, x[1] + h / 2.0, score]
            ).reshape((1, 5))

    @staticmethod
    def convert_bbox_to_z(bbox):
        """
        Takes a bounding box in the form [x1,y1,x2,y2] and returns z in the form
        [x,y,s,r] where x,y is the centre of the box and s is the scale/area and r is
        the aspect ratio
        """
        w = bbox[2] - bbox[0]
        h = bbox[3] - bbox[1]
        x = bbox[0] + w / 2.0
        y = bbox[1] + h / 2.0
        s = w * h  # scale is just area
        r = w / float(h)
        return np.array([x, y, s, r]).reshape((4, 1))


class SORTBase(metaclass=abc.ABCMeta):
    def __init__(self):
        self.n_frames = 0
        self.trackers = []

    @abc.abstractmethod
    def track(self):
        pass


class SORTEllipse(SORTBase):
    def __init__(self, max_age, min_hits, iou_threshold, sd=2):
        self.max_age = max_age
        self.min_hits = min_hits
        self.iou_threshold = iou_threshold
        self.fitter = EllipseFitter(sd)
        EllipseTracker.n_trackers = 0
        super().__init__()

    def track(self, poses, identities=None):
        self.n_frames += 1

        trackers = np.zeros((len(self.trackers), 6))
        for i in range(len(trackers)):
            trackers[i, :5] = self.trackers[i].predict()
        empty = np.isnan(trackers).any(axis=1)
        trackers = trackers[~empty]
        for ind in np.flatnonzero(empty)[::-1]:
            self.trackers.pop(ind)

        ellipses = []
        pred_ids = []
        for i, pose in enumerate(poses):
            el = self.fitter.fit(pose)
            if el is not None:
                ellipses.append(el)
                if identities is not None:
                    pred_ids.append(mode(identities[i])[0][0])
        if not len(trackers):
            matches = np.empty((0, 2), dtype=int)
            unmatched_detections = np.arange(len(ellipses))
            unmatched_trackers = np.empty((0, 6), dtype=int)
        else:
            ellipses_trackers = [Ellipse(*t[:5]) for t in trackers]
            cost_matrix = np.zeros((len(ellipses), len(ellipses_trackers)))
            for i, el in enumerate(ellipses):
                for j, el_track in enumerate(ellipses_trackers):
                    cost = el.calc_similarity_with(el_track)
                    if identities is not None:
                        match = 2 if pred_ids[i] == self.trackers[j].id_ else 1
                        cost *= match
                    cost_matrix[i, j] = cost
            row_indices, col_indices = linear_sum_assignment(cost_matrix, maximize=True)
            unmatched_detections = [
                i for i, _ in enumerate(ellipses) if i not in row_indices
            ]
            unmatched_trackers = [
                j for j, _ in enumerate(trackers) if j not in col_indices
            ]
            matches = []
            for row, col in zip(row_indices, col_indices):
                val = cost_matrix[row, col]
                # diff = val - cost_matrix
                # diff[row, col] += val
                # if (
                #         val < self.iou_threshold
                #         or np.any(diff[row] <= 0.2)
                #         or np.any(diff[:, col] <= 0.2)
                # ):
                if val < self.iou_threshold:
                    unmatched_detections.append(row)
                    unmatched_trackers.append(col)
                else:
                    matches.append([row, col])
            if not len(matches):
                matches = np.empty((0, 2), dtype=int)
            else:
                matches = np.stack(matches)
            unmatched_trackers = np.asarray(unmatched_trackers)
            unmatched_detections = np.asarray(unmatched_detections)

        animalindex = []
        for t, tracker in enumerate(self.trackers):
            if t not in unmatched_trackers:
                ind = matches[matches[:, 1] == t, 0][0]
                animalindex.append(ind)
                tracker.update(ellipses[ind].parameters)
            else:
                animalindex.append(-1)

        for i in unmatched_detections:
            trk = EllipseTracker(ellipses[i].parameters)
            if identities is not None:
                trk.id_ = mode(identities[i])[0][0]
            self.trackers.append(trk)
            animalindex.append(i)

        i = len(self.trackers)
        ret = []
        for trk in reversed(self.trackers):
            d = trk.state
            if (trk.time_since_update < 1) and (
                trk.hit_streak >= self.min_hits or self.n_frames <= self.min_hits
            ):
                ret.append(
                    np.concatenate((d, [trk.id, int(animalindex[i - 1])])).reshape(
                        1, -1
                    )
                )  # for DLC we also return the original animalid
                # +1 as MOT benchmark requires positive >> this is removed for DLC!
            i -= 1
            # remove dead tracklet
            if trk.time_since_update > self.max_age:
                self.trackers.pop(i)

        if len(ret) > 0:
            return np.concatenate(ret)
        return np.empty((0, 7))


class SORTSkeleton(SORTBase):
    def __init__(self, n_bodyparts, max_age=20, min_hits=3, oks_threshold=0.5):
        self.n_bodyparts = n_bodyparts
        self.max_age = max_age
        self.min_hits = min_hits
        self.oks_threshold = oks_threshold
        SkeletonTracker.n_trackers = 0
        super().__init__()

    @staticmethod
    def weighted_hausdorff(x, y):
        # Modified from scipy source code:
        # - to restrict its use to 2D
        # - to get rid of shuffling (since arrays are only (nbodyparts * 3) element long)
        # TODO - factor in keypoint confidence (and weight by # of observations??)
        cmax = 0
        for i in range(x.shape[0]):
            no_break_occurred = True
            cmin = np.inf
            for j in range(y.shape[0]):
                d = (x[i, 0] - y[j, 0]) ** 2 + (x[i, 1] - y[j, 1]) ** 2
                if d < cmax:
                    no_break_occurred = False
                    break
                if d < cmin:
                    cmin = d
            if cmin != np.inf and cmin > cmax and no_break_occurred:
                cmax = cmin
        return np.sqrt(cmax)

    @staticmethod
    def object_keypoint_similarity(x, y):
        mask = ~np.isnan(x * y).all(axis=1)  # Intersection visible keypoints
        xx = x[mask]
        yy = y[mask]
        dist = np.linalg.norm(xx - yy, axis=1)
        scale = np.sqrt(
            np.product(np.ptp(yy, axis=0))
        )  # square root of bounding box area
        oks = np.exp(-0.5 * (dist / (0.05 * scale)) ** 2)
        return np.mean(oks)

    def calc_pairwise_hausdorff_dist(self, poses, poses_ref):
        mat = np.zeros((len(poses), len(poses_ref)))
        for i, pose in enumerate(poses):
            for j, pose_ref in enumerate(poses_ref):
                mat[i, j] = self.weighted_hausdorff(pose, pose_ref)
        return mat

    def calc_pairwise_oks(self, poses, poses_ref):
        mat = np.zeros((len(poses), len(poses_ref)))
        for i, pose in enumerate(poses):
            for j, pose_ref in enumerate(poses_ref):
                mat[i, j] = self.object_keypoint_similarity(pose, pose_ref)
        return mat

    def track(self, poses):
        self.n_frames += 1

        if not len(self.trackers):
            for pose in poses:
                tracker = SkeletonTracker(self.n_bodyparts)
                tracker.state = pose
                self.trackers.append(tracker)

        poses_ref = []
        for i, tracker in enumerate(self.trackers):
            pose_ref = tracker.predict()
            poses_ref.append(pose_ref.reshape((-1, 2)))

        # mat = self.calc_pairwise_oks(poses, poses_ref)
        mat = self.calc_pairwise_hausdorff_dist(poses, poses_ref)
        row_indices, col_indices = linear_sum_assignment(mat, maximize=False)

        unmatched_poses = [p for p, _ in enumerate(poses) if p not in row_indices]
        unmatched_trackers = [
            t for t, _ in enumerate(poses_ref) if t not in col_indices
        ]
        # Remove matched detections with low OKS
        # matches = []
        # for row, col in zip(row_indices, col_indices):
        #     if mat[row, col] < self.oks_threshold:
        #         unmatched_poses.append(row)
        #         unmatched_trackers.append(col)
        #     else:
        #         matches.append([row, col])
        # if not len(matches):
        #     matches = np.empty((0, 2), dtype=int)
        # else:
        #     matches = np.stack(matches)
        matches = np.c_[row_indices, col_indices]

        animalindex = []
        for t, tracker in enumerate(self.trackers):
            if t not in unmatched_trackers:
                ind = matches[matches[:, 1] == t, 0][0]
                animalindex.append(ind)
                tracker.update(poses[ind])
            else:
                animalindex.append(-1)

        for i in unmatched_poses:
            tracker = SkeletonTracker(self.n_bodyparts)
            tracker.state = poses[i]
            self.trackers.append(tracker)
            animalindex.append(i)

        states = []
        i = len(self.trackers)
        for tracker in reversed(self.trackers):
            i -= 1
            if tracker.time_since_update > self.max_age:
                self.trackers.pop()
                continue
            state = tracker.predict()
            states.append(np.r_[state, [tracker.id, int(animalindex[i])]])
        if len(states) > 0:
            return np.stack(states)
        return np.empty((0, self.n_bodyparts * 2 + 2))


class SORTBox(SORTBase):
    def __init__(self, max_age, min_hits, iou_threshold):
        self.max_age = max_age
        self.min_hits = min_hits
        self.iou_threshold = iou_threshold
        BoxTracker.n_trackers = 0
        super().__init__()

    def track(self, dets):
        self.n_frames += 1

        trackers = np.zeros((len(self.trackers), 5))
        for i in range(len(trackers)):
            trackers[i, :4] = self.trackers[i].predict()
        empty = np.isnan(trackers).any(axis=1)
        trackers = trackers[~empty]
        for ind in np.flatnonzero(empty)[::-1]:
            self.trackers.pop(ind)

        matched, unmatched_dets, unmatched_trks = self.match_detections_to_trackers(
            dets, trackers, self.iou_threshold
        )

        # update matched trackers with assigned detections
        animalindex = []
        for t, trk in enumerate(self.trackers):
            if t not in unmatched_trks:
                d = matched[np.where(matched[:, 1] == t)[0], 0]
                animalindex.append(d[0])
                trk.update(dets[d, :][0])  # update coordinates
            else:
                animalindex.append("nix")  # lost trk!

        # create and initialise new trackers for unmatched detections
        for i in unmatched_dets:
            trk = BoxTracker(dets[i, :])
            self.trackers.append(trk)
            animalindex.append(i)

        i = len(self.trackers)
        ret = []
        for trk in reversed(self.trackers):
            d = trk.state
            if (trk.time_since_update < 1) and (
                trk.hit_streak >= self.min_hits or self.n_frames <= self.min_hits
            ):
                ret.append(
                    np.concatenate((d, [trk.id, int(animalindex[i - 1])])).reshape(
                        1, -1
                    )
                )  # for DLC we also return the original animalid
                # +1 as MOT benchmark requires positive >> this is removed for DLC!
            i -= 1
            # remove dead tracklet
            if trk.time_since_update > self.max_age:
                self.trackers.pop(i)

        if len(ret) > 0:
            return np.concatenate(ret)
        return np.empty((0, 5))

    @staticmethod
    def match_detections_to_trackers(detections, trackers, iou_threshold):
        """
        Assigns detections to tracked object (both represented as bounding boxes)

        Returns 3 lists of matches, unmatched_detections and unmatched_trackers
        """
        if not len(trackers):
            return (
                np.empty((0, 2), dtype=int),
                np.arange(len(detections)),
                np.empty((0, 5), dtype=int),
            )
        iou_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)

        for d, det in enumerate(detections):
            for t, trk in enumerate(trackers):
                iou_matrix[d, t] = calc_iou(det, trk)
        row_indices, col_indices = linear_sum_assignment(-iou_matrix)

        unmatched_detections = []
        for d, det in enumerate(detections):
            if d not in row_indices:
                unmatched_detections.append(d)
        unmatched_trackers = []
        for t, trk in enumerate(trackers):
            if t not in col_indices:
                unmatched_trackers.append(t)

        # filter out matched with low IOU
        matches = []
        for row, col in zip(row_indices, col_indices):
            if iou_matrix[row, col] < iou_threshold:
                unmatched_detections.append(row)
                unmatched_trackers.append(col)
            else:
                matches.append([row, col])
        if not len(matches):
            matches = np.empty((0, 2), dtype=int)
        else:
            matches = np.stack(matches)
        return matches, np.array(unmatched_detections), np.array(unmatched_trackers)


def fill_tracklets(tracklets, trackers, animals, imname):
    for content in trackers:
        tracklet_id, pred_id = content[-2:].astype(int)
        if tracklet_id not in tracklets:
            tracklets[tracklet_id] = {}
        if pred_id != -1:
            tracklets[tracklet_id][imname] = np.asarray(animals[pred_id])
        else:  # Resort to the tracker prediction
            xy = np.asarray(content[:-2])
            pred = np.insert(xy, range(2, len(xy) + 1, 2), 1)
            tracklets[tracklet_id][imname] = np.asarray(pred)


def calc_bboxes_from_keypoints(data, slack=0, offset=0):
    data = np.asarray(data)
    if data.shape[-1] < 3:
        raise ValueError("Data should be of shape (n_animals, n_bodyparts, 3)")

    if data.ndim != 3:
        data = np.expand_dims(data, axis=0)
    bboxes = np.full((data.shape[0], 5), np.nan)
    bboxes[:, :2] = np.nanmin(data[..., :2], axis=1) - slack  # X1, Y1
    bboxes[:, 2:4] = np.nanmax(data[..., :2], axis=1) + slack  # X2, Y2
    bboxes[:, -1] = np.nanmean(data[..., 2], axis=1)  # Average confidence
    bboxes[:, [0, 2]] += offset
    return bboxes


def reconstruct_all_ellipses(data, sd):
    """
    Reconstructs ellipses for multiple individuals based on their body part coordinates
    across multiple frames. Each ellipse is fitted to the coordinates using an `EllipseFitter`.

    Parameters
    ----------
    data : pandas.DataFrame
        A multi-level DataFrame containing body part coordinates and likelihood values.
        The index represents frames, and the columns follow a multi-level structure:
        - Level 0: Scorer
        - Level 1: Individuals
        - Level 2: Body parts
        - Level 3: Coordinates ("x" and "y") and "likelihood".
    sd : float
        The standard deviation used by the `EllipseFitter` for fitting ellipses.

    Returns
    -------
    numpy.ndarray
        A 3D array of shape (A, F, 5), where:
        - A is the number of individuals (excluding "single" if present).
        - F is the number of frames.
        - Each row contains ellipse parameters [cx, cy, width, height, angle].

    Notes
    -----
    - The method drops the "likelihood" column from the input DataFrame as it is not
      relevant for ellipse fitting.
    - If the "single" individual is present, it is excluded from the reconstruction process.
    - The `EllipseFitter` is used to fit ellipses to the body part coordinates for each
      individual in each frame.
    - NaN values are assigned when no valid ellipse can be fitted.
    """
    xy = data.droplevel("scorer", axis=1).drop("likelihood", axis=1, level=-1)
    if "single" in xy:
        xy.drop("single", axis=1, level="individuals", inplace=True)
    animals = xy.columns.get_level_values("individuals").unique()
    nrows = xy.shape[0]
    ellipses = np.full((len(animals), nrows, 5), np.nan)
    fitter = EllipseFitter(sd)
    for n, animal in enumerate(animals):
        data = xy.xs(animal, axis=1, level="individuals").values.reshape((nrows, -1, 2))
        for i, coords in enumerate(tqdm(data)):
            el = fitter.fit(coords.astype(np.float64))
            if el is not None:
                ellipses[n, i] = el.parameters
    return ellipses


def _track_individuals(
    individuals, min_hits=1, max_age=5, similarity_threshold=0.6, track_method="ellipse"
):
    if track_method not in TRACK_METHODS:
        raise ValueError(f"Unknown {track_method} tracker.")

    if track_method == "ellipse":
        tracker = SORTEllipse(max_age, min_hits, similarity_threshold)
    elif track_method == "box":
        tracker = SORTBox(max_age, min_hits, similarity_threshold)
    else:
        n_bodyparts = individuals[0][0].shape[0]
        tracker = SORTSkeleton(n_bodyparts, max_age, min_hits, similarity_threshold)

    tracklets = defaultdict(dict)
    all_hyps = dict()
    for i, (multi, single) in enumerate(tqdm(individuals)):
        if single is not None:
            tracklets["single"][i] = single
        if multi is None:
            continue
        if track_method == "box":
            # TODO: get cropping parameters and utilize!
            xy = calc_bboxes_from_keypoints(multi)
        else:
            xy = multi[..., :2]
        hyps = tracker.track(xy)
        all_hyps[i] = hyps
        for hyp in hyps:
            tracklet_id, pred_id = hyp[-2:].astype(int)
            if pred_id != -1:
                tracklets[tracklet_id][i] = multi[pred_id]
    return tracklets, all_hyps


--- File: deeplabcut/core/weight_init.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Classes to configure how to initialize model weights"""
from __future__ import annotations

import warnings
from dataclasses import dataclass
from pathlib import Path

import numpy as np


@dataclass
class WeightInitialization:
    """Configures weights initialization when transfer learning or fine-tuning models

    Args:
        snapshot_path: The path to the snapshot used to initialize pose model weights
            when training a model.
        detector_snapshot_path: The path to the snapshot used to initialize detector
            weights when training a model.
        dataset: Optionally, the dataset on which the snapshots were trained. Required
            when fine-tuning SuperAnimal models.
        with_decoder: Whether to load the decoder weights as well.
        memory_replay: Only when ``with_decoder=True``. Whether to train the model with
            memory replay, so that it predicts all SuperAnimal (or previous project)
            bodyparts.
        conversion_array: The mapping from SuperAnimal (or other project, on which the
            weights were trained) to project bodyparts. Required when
            `with_decoder=True`.
            An array [7, 0, 1] means the project has 3 bodyparts, where the 1st bodypart
            corresponds to the 8th bodypart in the pretrained model, the 2nd to the 1st
            and the 3rd to the 2nd (as arrays are 0-indexed).
        bodyparts: Optionally, the name of each bodypart entry in the conversion array.
    """

    snapshot_path: Path
    detector_snapshot_path: Path | None = None
    dataset: str | None = None
    with_decoder: bool = False
    memory_replay: bool = False
    conversion_array: np.ndarray | None = None
    bodyparts: list[str] | None = None

    def __post_init__(self):
        if self.memory_replay and not self.with_decoder:
            raise ValueError(
                "You cannot train a model with memory replay if you do not keep the "
                "decoder layers (``with_decoder=True``), but you passed "
                "`memory_replay=True` and `with_decoder=False`. Please change your "
                "WeightInitialization parameters."
            )

        if self.with_decoder and self.conversion_array is None:
            raise ValueError(
                f"You must specify a conversion_array to initialize decoder weights "
                f"(``with_decoder=True``)."
            )

        if self.bodyparts is not None and self.conversion_array is None:
            raise ValueError(
                f"Specifying bodyparts should only be done when `with_decoder=True` and"
                f" the conversion array is specified."
            )

        if self.conversion_array is not None and self.bodyparts is not None:
            if not len(self.conversion_array) == len(self.bodyparts):
                raise ValueError(
                    f"There must be the same number of elements in the bodyparts list "
                    "and conv. array; found {self.bodyparts}, {self.conversion_array}"
                )

    def to_dict(self) -> dict:
        """Returns: the weight initialization as a dict"""
        data = dict()
        if self.dataset is not None:
            data["dataset"] = self.dataset

        data["snapshot_path"] = str(self.snapshot_path)
        if self.detector_snapshot_path is not None:
            data["detector_snapshot_path"] = str(self.detector_snapshot_path)

        data["with_decoder"] = self.with_decoder
        data["memory_replay"] = self.memory_replay

        if self.conversion_array is not None:
            data["conversion_array"] = self.conversion_array.tolist()

        if self.bodyparts is not None:
            data["bodyparts"] = self.bodyparts

        return data

    @staticmethod
    def from_dict(data: dict) -> "WeightInitialization":
        if "snapshot_path" not in data:
            return WeightInitialization.from_dict_legacy(data)

        detector_snapshot_path = data.get("detector_snapshot_path")
        if detector_snapshot_path is not None:
            detector_snapshot_path = Path(detector_snapshot_path)

        conversion_array = data.get("conversion_array")
        if conversion_array is not None:
            conversion_array = np.array(conversion_array, dtype=int)

        return WeightInitialization(
            snapshot_path=Path(data["snapshot_path"]),
            detector_snapshot_path=detector_snapshot_path,
            dataset=data.get("dataset"),
            with_decoder=data["with_decoder"],
            memory_replay=data["memory_replay"],
            conversion_array=conversion_array,
            bodyparts=data.get("bodyparts"),
        )

    @staticmethod
    def from_dict_legacy(data: dict) -> "WeightInitialization":
        """Deals with weight initialization that were created before 3.0.0rc5"""
        import deeplabcut.pose_estimation_pytorch.modelzoo.utils as utils

        conversion_array = data.get("conversion_array")
        if conversion_array is not None:
            conversion_array = np.array(conversion_array, dtype=int)

        return WeightInitialization(
            snapshot_path=utils.get_super_animal_snapshot_path(
                dataset=data["dataset"],
                model_name="hrnet_w32",
            ),
            detector_snapshot_path=utils.get_super_animal_snapshot_path(
                dataset=data["dataset"],
                model_name="fasterrcnn_resnet50_fpn_v2",
            ),
            with_decoder=data["with_decoder"],
            memory_replay=data["memory_replay"],
            conversion_array=conversion_array,
            bodyparts=data.get("bodyparts"),
        )

    @staticmethod
    def build(
        cfg: dict,
        super_animal: str,
        model_name: str = "hrnet_w32",
        detector_name: str = "fasterrcnn_resnet50_fpn_v2",
        with_decoder: bool = False,
        memory_replay: bool = False,
        customized_pose_checkpoint: str | None = None,
        customized_detector_checkpoint: str | None = None,
    ) -> "WeightInitialization":
        """Builds a WeightInitialization for a project

        `WeightInitialization.build` is deprecated and will be removed in a future
        version of DeepLabCut. Please use `build_weight_init` from `deeplabcut.modelzoo`
        instead.

        Args:
            cfg: The project's configuration.
            super_animal: The SuperAnimal model with which to initialize weights.
            model_name: The name of the model architecture for which to load the weights
                (defaults to "hrnet_w32" for backwards compatibility).
            detector_name: The name of the detector architecture for which to load the
                weights (defaults to "fasterrcnn_resnet50_fpn_v2" for backwards
                compatibility).
            with_decoder: Whether to load the decoder weights as well. If this is true,
                a conversion table must be specified for the given SuperAnimal in the
                project configuration file. See
                ``deeplabcut.modelzoo.utils.create_conversion_table`` to create a
                conversion table.
            memory_replay: Only when ``with_decoder=True``. Whether to train the model
                with memory replay, so that it predicts all SuperAnimal bodyparts.
            customized_pose_checkpoint: A customized SuperAnimal pose checkpoint, as an
                alternative to the Hugging Face one
            customized_detector_checkpoint: A customized SuperAnimal detector
                checkpoint, as an alternative to the Hugging Face one

        Returns:
            The built WeightInitialization.
        """
        from deeplabcut.modelzoo import build_weight_init
        deprecation_warning = (
            "The `WeightInitialization.build` is deprecated and will be removed in a "
            "future version of DeepLabCut. Please use `build_weight_init` from "
            "`deeplabcut.modelzoo` instead."
        )
        warnings.warn(deprecation_warning, DeprecationWarning)

        return build_weight_init(
            cfg,
            super_animal,
            model_name,
            detector_name,
            with_decoder,
            memory_replay,
            customized_pose_checkpoint,
            customized_detector_checkpoint,
        )


--- File: deeplabcut/core/metrics/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from .api import compute_metrics, prepare_evaluation_data
from .bbox import compute_bbox_metrics
from .identity import compute_identity_scores


--- File: deeplabcut/core/metrics/distance_metrics.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Implementations of methods to compute distance metrics such as RMSE or OKS"""
from __future__ import annotations

import numpy as np

import deeplabcut.core.metrics.matching as matching
from deeplabcut.core.crossvalutils import find_closest_neighbors
from deeplabcut.core.inferenceutils import calc_object_keypoint_similarity


def compute_oks_matrix(
    ground_truth: np.ndarray,
    predictions: np.ndarray,
    oks_sigma: float,
    oks_bbox_margin: float = 0.0,
) -> np.ndarray:
    """Computes the OKS score for each (prediction, gt) pair in an image

    Args:
        ground_truth: The GT poses for an image, shape (n_individuals, n_kpts, 2)
        predictions: The predicted poses in the image, shape (n_pred, n_kpts, 2)
        oks_sigma: The sigma value to use to compute OKS
        oks_bbox_margin: The margin to add around keypoints when computing the area.
            FIXME(niels) We should allow the use of ground truth bboxes to get area

    Returns:
        A matrix of shape (n_pred, n_kpts) where entry (i, j) is the OKS between
        prediction i and ground truth j.
    """
    oks_matrix = np.zeros((len(predictions), len(ground_truth)))
    for pred_idx, pred in enumerate(predictions):
        for gt_idx, gt in enumerate(ground_truth):
            oks_matrix[pred_idx, gt_idx] = calc_object_keypoint_similarity(
                pred[:, :2],
                gt[:, :2],
                sigma=oks_sigma,
                margin=oks_bbox_margin,
            )

    return oks_matrix


def compute_oks(
    data: list[tuple[np.ndarray, np.ndarray]],
    oks_bbox_margin: float = 0.0,
    oks_sigma: float = 0.1,
    oks_thresholds: np.ndarray | None = None,
    oks_recall_thresholds: np.ndarray | None = None,
) -> dict[str, float]:
    """Computes the OKS for pose at different thresholds.

    Args:
        data: The data for which to compute OKS mAP: a list containing (gt_poses,
            predicted_poses) tuples, where gt_pose is an array of shape
            (num_gt_individuals, num_bpts, 3) and predicted_poses is an array of shape
            (num_predictions, num_bpts, 3). For the GT, the 3 coordinates are (x, y,
            visibility) while for the pose they are (x, y, confidence score).
        oks_sigma: The OKS sigma to use to compute pose.
        oks_bbox_margin: The margin to add around keypoints to compute the area for OKS
            computation.
        oks_thresholds: The OKS thresholds at which to compute AP. If None, defaults to
            (0.5, 0.55, 0.6, ..., 0.9, 0.95).
        oks_recall_thresholds: The recall thresholds to use to compute mAP. If None,
            defaults to the same default values used in pycocotools.

    Returns:
        A dictionary containing mAP and mAR scores.
    """
    if oks_thresholds is None:
        oks_thresholds = np.linspace(0.5, 0.95, 10)

    if oks_recall_thresholds is None:
        oks_recall_thresholds = np.linspace(
            start=0.0,
            stop=1.00,
            num=int(np.round((1.00 - 0.0) / 0.01)) + 1,
            endpoint=True,
        )

    total_gt = 0
    pose_data = []
    for gt, pred in data:
        # filter data to only keep individuals with at least 2 valid keypoints
        gt = gt[np.sum(np.all(~np.isnan(gt), axis=-1), axis=-1) > 1]
        pred = pred[np.sum(np.all(~np.isnan(pred), axis=-1), axis=-1) > 1]

        oks_matrix = compute_oks_matrix(
            gt[:, :, :2],
            pred[:, :, :2],
            oks_sigma=oks_sigma,
            oks_bbox_margin=oks_bbox_margin,
        )

        total_gt += len(gt)
        pose_data.append((gt, pred, oks_matrix))

    precisions, recalls = [], []
    for oks_threshold in oks_thresholds:
        matches = []
        for gt, pred, oks_matrix in pose_data:
            image_matches = matching.match_greedy_oks(
                gt,
                pred,
                oks_matrix=oks_matrix,
                oks_threshold=oks_threshold,
            )
            matches.extend(image_matches)

        if len(matches) == 0:  # no predictions -> precision 0, recall 0
            return {"mAP": 0, "mAR": 0}

        scores = np.asarray([m.score for m in matches])
        match_order = np.argsort(-scores, kind="mergesort")
        oks_values = np.asarray([m.oks for m in matches])
        oks_values = oks_values[match_order]

        tp = np.cumsum(oks_values >= oks_threshold)
        fp = np.cumsum(oks_values < oks_threshold)
        rc = tp / total_gt
        pr = tp / (fp + tp + np.spacing(1))
        recall = rc[-1]

        # Guarantee precision decreases monotonically, see
        # https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173
        for i in range(len(pr) - 1, 0, -1):
            if pr[i] > pr[i - 1]:
                pr[i - 1] = pr[i]

        inds_rc = np.searchsorted(rc, oks_recall_thresholds, side="left")
        precision = np.zeros(inds_rc.shape)
        valid = inds_rc < len(pr)
        precision[valid] = pr[inds_rc[valid]]

        precisions.append(precision)
        recalls.append(recall)

    precisions = np.asarray(precisions)
    recalls = np.asarray(recalls)
    return {
        "mAP": 100 * precisions.mean().item(),
        "mAR": 100 * recalls.mean().item(),
    }


def match_predictions_for_rmse(
    data: list[tuple[np.ndarray, np.ndarray]],
    single_animal: bool,
    oks_bbox_margin: float = 0.0,
) -> list[matching.PotentialMatch]:
    """Matches GT keypoints to predictions to compute RMSE.

    Single animal RMSE is computed by simply calculating the distance between each
    ground truth keypoint and the corresponding prediction.

    Multi-animal RMSE is computed differently: predictions are first matched to ground
    truth individuals using greedy OKS matching. RMSE is then computed only between
    predictions and the ground truth pose they are matched to, only when the OKS is
    non-zero (greater than a small threshold). Predictions that cannot be matched to
    any ground truth with non-zero OKS are not used to compute RMSE.

    Args:
        data: The data for which to compute RMSE. This is a list containing (gt_poses,
            predicted_poses), where gt_pose is an array of shape (num_gt_individuals,
            num_bpts, 3) and predicted_poses is an array of shape (num_predictions,
            num_bpts, 3). For the GT, the 3 coordinates are (x, y, visibility) while for
            the pose they are (x, y, confidence score).
        single_animal: Whether this is a single animal dataset.
        oks_bbox_margin: When single_animal is False, predictions are matched to GT
            using OKS. This is the margin used to apply when computing the bbox from
            the pose to compute OKS.

    Returns:
        A list containing the predictions matched to ground truth.

    Raises:
        ValueError: If `single_animal=True` but more than one ground truth/predicted
        keypoint is found for an entry
    """
    matches = []
    for gt, pred in data:
        if single_animal:
            if gt.shape[0] > 1 or pred.shape[0] > 1:
                raise ValueError(
                    "At most 1 individual and 1 prediction can be given when computing "
                    f"single animal RMSE. Found gt={gt.shape}, pred={pred.shape}"
                )

            image_matches = []
            if gt.shape[0] == 1 and pred.shape[0] == 1:
                match = matching.PotentialMatch.from_pose(pred[0])
                match.match(gt[0], oks=float("nan"))  # OKS not needed for RMSE
                image_matches.append(match)
        else:
            oks_matrix = compute_oks_matrix(
                gt[:, :, :2],
                pred[:, :, :2],
                oks_sigma=0.1,
                oks_bbox_margin=oks_bbox_margin,
            )
            image_matches = matching.match_greedy_oks(
                gt,
                pred,
                oks_matrix=oks_matrix,
                oks_threshold=1e-6,
            )

        matches.extend(image_matches)

    return matches


def compute_rmse(
    data: list[tuple[np.ndarray, np.ndarray]],
    single_animal: bool,
    pcutoff: float | list[float],
    data_unique: list[tuple[np.ndarray, np.ndarray]] | None = None,
    per_keypoint_results: bool = False,
    oks_bbox_margin: float = 0.0,
) -> dict[str, float]:
    """Computes the RMSE for pose predictions.

    Single animal RMSE is computed by simply calculating the distance between each
    ground truth keypoint and the corresponding prediction.

    Multi-animal RMSE is computed differently: predictions are first matched to ground
    truth individuals using greedy OKS matching. RMSE is then computed only between
    predictions and the ground truth pose they are matched to, only when the OKS is
    non-zero (greater than a small threshold). Predictions that cannot be matched to
    any ground truth with non-zero OKS are not used to compute RMSE.

    Args:
        data: The data for which to compute RMSE. This is a list containing (gt_poses,
            predicted_poses), where gt_pose is an array of shape (num_gt_individuals,
            num_bpts, 3) and predicted_poses is an array of shape (num_predictions,
            num_bpts, 3). For the GT, the 3 coordinates are (x, y, visibility) while for
            the pose they are (x, y, confidence score).
        single_animal: Whether this is a single animal dataset.
        pcutoff: The p-cutoff to use to compute RMSE. If a list, the cutoff for each
            bodypart is set individually. The list must have length num_bodyparts +
            num_unique_bodyparts.
        data_unique: Unique bodypart ground truth and predictions to include in RMSE
            computations, if there are any such bodyparts.
        per_keypoint_results: Whether to compute the RMSE for each individual keypoint.
        oks_bbox_margin: When single_animal is False, predictions are matched to GT
            using OKS. This is the margin used to apply when computing the bbox from
            the pose to compute OKS.

    Returns:
        A dictionary matching metric names to values. It will at least have "rmse" and
        "rmse_cutoff" keys. If `per_keypoint_results=True` and there is at least one
        non-NaN pixel error it will also contain "rmse_keypoint_X" and
        "rmse_cutoff_keypoint_X" keys for each bodypart, where X is the index of the
        bodypart.

    Raises:
        ValueError: If `single_animal=True` but more than one ground truth/predicted
            keypoint is found for an entry
    """
    matches = match_predictions_for_rmse(data, single_animal, oks_bbox_margin)
    pixel_errors, keypoint_scores = None, None
    if len(matches) > 0:
        pixel_errors = np.stack([m.pixel_errors() for m in matches])
        keypoint_scores = np.stack([m.keypoint_scores() for m in matches])

    error, support, cutoff_error, cutoff_support = 0, 0, 0, 0
    if pixel_errors is not None:
        bpt_cutoffs = pcutoff
        if not isinstance(pcutoff, (int, float)):
            bpt_cutoffs = pcutoff[:pixel_errors.shape[1]]

        error, support, cutoff_error, cutoff_support = collect_pixel_errors(
            pixel_errors, keypoint_scores, bpt_cutoffs,
        )

    unique_pixel_errors, unique_keypoint_scores = None, None
    if data_unique is not None:
        u_matches = match_predictions_for_rmse(data_unique, single_animal=True)
        if len(u_matches) > 0:
            unique_pixel_errors = np.stack([m.pixel_errors() for m in u_matches])
            unique_keypoint_scores = np.stack([m.keypoint_scores() for m in u_matches])

            bpt_cutoffs = pcutoff
            if not isinstance(pcutoff, (int, float)):
                bpt_cutoffs = pcutoff[-unique_pixel_errors.shape[1]:]
            u_error, u_support, u_cutoff_error, u_cutoff_support = collect_pixel_errors(
                unique_pixel_errors, unique_keypoint_scores, bpt_cutoffs,
            )
            error += u_error
            support += u_support
            cutoff_error += u_cutoff_error
            cutoff_support += u_cutoff_support

    results = dict(rmse=float("nan"), rmse_pcutoff=float("nan"))
    if support > 0:
        results["rmse"] = float(error / support)
    if cutoff_support > 0:
        results["rmse_pcutoff"] = float(cutoff_error / cutoff_support)

    if per_keypoint_results:
        bodypart_errors = [("rmse_keypoint", pixel_errors)]
        if unique_pixel_errors is not None:
            bodypart_errors.append(("rmse_unique_keypoint", unique_pixel_errors))

        for key_prefix, bpt_errors in bodypart_errors:
            for idx, keypoint_error in enumerate(bpt_errors.T):
                rmse = float("nan")
                if np.any(~np.isnan(keypoint_error)):
                    rmse = np.nanmean(keypoint_error).item()
                results[f"{key_prefix}_{idx}"] = float(rmse)

    return results


def compute_detection_rmse(
    data: list[tuple[np.ndarray, np.ndarray]],
    pcutoff: float | list[float],
    data_unique: list[tuple[np.ndarray, np.ndarray]] | None = None,
) -> tuple[float, float]:
    """Computes the detection RMSE for pose predictions.

    The detection RMSE score does not take individual assemblies into account. It only
    judges the performance of the detections, matching each predicted keypoint to the
    closest ground truth for each bodypart.

    This is the same way multi-animal RMSE was computed in DeepLabCut 2.X.

    Args:
        data: The data for which to compute RMSE. This is a list containing (gt_poses,
            predicted_poses), where gt_pose is an array of shape (num_gt_individuals,
            num_bpts, 3) and predicted_poses is an array of shape (num_predictions,
            num_bpts, 3). For the GT, the 3 coordinates are (x, y, visibility) while for
            the pose they are (x, y, confidence score).
        pcutoff: The p-cutoff to use to compute RMSE. If a list, the cutoff for each
            bodypart is set individually. The list must have length num_bodyparts +
            num_unique_bodyparts.
        data_unique: Unique bodypart ground truth and predictions to include in RMSE
            computations, if there are any such bodyparts.

    Returns:
        The detection RMSE and detection RMSE after removing all detections with a
        score below the pcutoff.
    """
    distances = []
    distances_cutoff = []
    for image_gt, image_pred in data:
        image_gt = image_gt.transpose((1, 0, 2))  # to (num_bpts, num_gt_individuals, 3)
        image_pred = image_pred.transpose((1, 0, 2))  # to (num_bpts, num_pred, 3)

        for bpt_index, (bpt_gt, bpt_pred) in enumerate(zip(image_gt, image_pred)):
            # filter NaNs and invalid values
            bpt_gt = bpt_gt[~np.any(np.isnan(bpt_gt), axis=1)]
            bpt_pred = bpt_pred[~np.any(np.isnan(bpt_pred), axis=1)]
            if len(bpt_gt) == 0 or len(bpt_pred) == 0:
                continue

            if isinstance(pcutoff, (int, float)):
                bpt_pcutoff = pcutoff
            else:
                bpt_pcutoff = pcutoff[bpt_index]

            # assignment of predicted bodyparts to ground truth
            neighbors = find_closest_neighbors(bpt_gt, bpt_pred, k=3)
            for gt_index, pred_index in enumerate(neighbors):
                if pred_index != -1:
                    gt = bpt_gt[gt_index]
                    pred = bpt_pred[pred_index]
                    dist = np.linalg.norm(gt[:2] - pred[:2])
                    distances.append(dist)

                    score = bpt_pred[pred_index, 2]
                    if score >= bpt_pcutoff:
                        distances_cutoff.append(dist)

    if data_unique is not None:
        for image_gt, image_pred in data_unique:
            assert len(image_gt) <= 1 and len(image_pred) <= 1, (
                f"Unique GT an predictions must have length 0 or 1! Found {image_gt.shape}, "
                f"{image_pred.shape}."
            )

            if len(image_gt) == 1 and len(image_pred) == 1:
                unique_gt, unique_pred = image_gt[0], image_pred[0]
                num_unique = unique_gt.shape[0]
                unique_cutoffs = pcutoff
                if not isinstance(pcutoff, (int, float)):
                    unique_cutoffs = pcutoff[-num_unique:]

                for bpt_index, (gt, pred) in enumerate(zip(unique_gt, unique_pred)):
                    dist = np.linalg.norm(gt[:2] - pred[:2])
                    distances.append(dist)

                    score = pred[2]
                    if isinstance(pcutoff, (int, float)):
                        bpt_pcutoff = unique_cutoffs
                    else:
                        bpt_pcutoff = unique_cutoffs[bpt_index]

                    if score >= bpt_pcutoff:
                        distances_cutoff.append(dist)

    rmse, rmse_cutoff = float("nan"), float("nan")
    if len(distances) == 0:
        return rmse, rmse_cutoff

    distances = np.stack(distances)
    if np.any(~np.isnan(distances)):
        rmse = float(np.nanmean(distances).item())

        if len(distances_cutoff) > 0:
            distances_cutoff = np.stack(distances_cutoff)
            if np.any(~np.isnan(distances_cutoff)):
                rmse_cutoff = float(np.nanmean(distances_cutoff).item())

    return rmse, rmse_cutoff


def collect_pixel_errors(
    pixel_errors: np.ndarray,
    keypoint_scores: np.ndarray,
    pcutoff: float,
) -> tuple[float, int, float, int]:
    """Collects pixel errors for RMSE computation

    Args:
        pixel_errors: The pixel errors to collect, of shape (num_matches, num_bodyparts)
        keypoint_scores: The scores corresponding to the pixel errors, of shape
            (num_matches, num_bodyparts).
        pcutoff: The pcutoff to use when computing cutoff RMSE.

    Returns: error, support, cutoff_error, support_cutoff
        error: The sum of all pixel errors.
        support: The number of valid pixel errors.
        cutoff_error: The sum of all pixel errors with score > pcutoff.
        support_cutoff: The number of valid pixel errors with score > pcutoff.
    """
    error = 0.0
    cutoff_error = 0.0
    support = np.sum(~np.isnan(pixel_errors)).item()
    support_cutoff = 0
    if support > 0:
        error += np.nansum(pixel_errors).item()

        cutoff_mask = keypoint_scores >= pcutoff
        cutoff_pixel_errors = pixel_errors[cutoff_mask]
        support_cutoff = np.sum(~np.isnan(cutoff_pixel_errors)).item()
        if support_cutoff > 0:
            cutoff_error = np.nansum(cutoff_pixel_errors).item()

    return error, support, cutoff_error, support_cutoff


--- File: deeplabcut/core/metrics/matching.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Algorithms to match predictions to ground truth labels"""
from __future__ import annotations

from dataclasses import dataclass

import numpy as np


@dataclass
class PotentialMatch:
    """A potential match between predicted pose and ground truth pose.

    Args:
        pose: An array of shape (num_bodyparts, 3)
        score: The score for the prediction. This could be the mean of the confidence
            score for each bodypart, or another value representing how confident the
            model is that this assembly is correct.
        gt: None if no ground truth pose was matched to the prediction. If defined, the
            ground truth to which the prediction is matched. It should be of shape
            (num_bodyparts, 3), where the 3 values are x, y and visibility.
        oks: The OKS score between the pose and the ground truth.
    """

    pose: np.ndarray
    score: float
    gt: np.ndarray | None = None
    oks: float = 0.0

    def keypoint_scores(self) -> np.ndarray:
        """Returns: The confidence score for each bodypart in the predicted pose."""
        return self.pose[:, 2].copy()

    def pixel_errors(self) -> np.ndarray:
        """
        Returns:
            The distance (in pixels) between each predicted and ground truth bodypart.
            If this prediction is unmatched, returns an array of length num_bodyparts
            containing all NaNs.
        """
        if self.gt is None:
            return np.full(len(self.pose), np.nan)

        return np.linalg.norm(self.pose[:, :2] - self.gt[:, :2], axis=1)

    def match(self, gt: np.ndarray, oks: float) -> None:
        """Adds a ground truth match to this PotentialMatch

        Args:
            gt: The ground truth to which the prediction is matched. The ground truth
                pose should be of shape (num_bodyparts, 3), where the 3 values are x, y
                and visibility.
            oks: The OKS similarity between the ground truth and this.
        """
        self.gt = gt
        self.oks = oks

    @classmethod
    def from_pose(cls, pose: np.ndarray) -> "PotentialMatch":
        assert len(pose.shape) == 2  # Must be pose for a single individual
        scores = pose[:, 2]
        if np.all(np.isnan(scores)):
            raise ValueError(
                "Cannot create a Match from a pose prediction where all scores are nan "
                f"(pose={pose})"
            )

        return PotentialMatch(pose=pose, score=np.nanmean(scores).item())


def match_greedy_oks(
    ground_truth: np.ndarray,
    predictions: np.ndarray,
    oks_matrix: np.ndarray,
    oks_threshold: float = 0.0,
) -> list[PotentialMatch]:
    """Greedy matching of ground truth individuals to predicted individuals using OKS

    This is done in the same way as done in pycocotools. The predictions must be sorted
    by score before being passed to this function.

    Args:
        ground_truth: The ground truth labels for an image, of shape (n_idv, n_bpt, 2)
        predictions: The predictions for an image, of shape (n_idv, n_bpt, 2)
        oks_matrix: A matrix of shape (n_pred, n_kpts) where entry (i, j) is the OKS
            between prediction i and ground truth j.
        oks_threshold: The min. OKS for a prediction to be matched to a GT pose

    Returns:
        A list containing a PotentialMatch for each predicted pose in the given
        predictions.
    """
    matches = [PotentialMatch.from_pose(pose=pred) for pred in predictions]
    matched_gt_indices = set()
    for idx, pred in enumerate(predictions):
        oks = oks_matrix[idx]
        if np.all(np.isnan(oks)):
            continue

        ind_best = np.nanargmax(oks)

        # if this gt already matched, continue
        if ind_best in matched_gt_indices:
            continue

        # Only match the pred to the GT if the OKS value is above a given threshold
        if oks[ind_best] < oks_threshold:
            continue

        matched_gt_indices.add(ind_best)
        matches[idx].match(gt=ground_truth[ind_best], oks=oks[ind_best])

    return matches


def match_greedy_rmse(
    ground_truth: np.ndarray,
    predictions: np.ndarray,
    keep_assemblies: bool = True,
) -> list[PotentialMatch]:
    """Greedy matching of ground truth individuals to predicted individuals using RMSE

    The predictions must be sorted by score before being passed to this function.

    Args:
        ground_truth: The ground truth labels for an image, of shape (n_idv, n_bpt, 2)
        predictions: The predictions for an image, of shape (n_idv, n_bpt, 2)
        keep_assemblies: Whether to match predicted keypoints to ground truth keypoints
            while enforcing that all bodyparts for a predicted individual are matched
            to bodyparts from the same ground truth assembly. When set to False, this
            corresponds to detection RMSE score.

    Returns:
        A list containing a PotentialMatch for each predicted pose in the given
        predictions.
    """
    if not keep_assemblies:
        raise NotImplementedError()

    matches = [PotentialMatch.from_pose(pose=pred) for pred in predictions]
    matched_gt_indices = set()
    for idx, pred in enumerate(predictions):
        bpt_distances = np.linalg.norm(pred[:, :2] - ground_truth[:, :, :2], axis=-1)
        if np.all(np.isnan(bpt_distances)):
            continue

        distances = np.nanmean(bpt_distances, axis=-1)
        ind_best = np.nanargmin(distances)

        # if this gt already matched, continue
        if ind_best in matched_gt_indices:
            continue

        matched_gt_indices.add(ind_best)
        matches[idx].match(
            gt=ground_truth[ind_best],
            oks=float("nan"),  # don't compute OKS here
        )

    return matches


--- File: deeplabcut/core/metrics/api.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""API methods to get metrics for deep learning models"""
from __future__ import annotations

import numpy as np

import deeplabcut.core.metrics.distance_metrics as distance_metrics


def compute_metrics(
    ground_truth: dict[str, np.ndarray],
    predictions: dict[str, np.ndarray],
    single_animal: bool = False,
    unique_bodypart_gt: dict[str, np.ndarray] | None = None,
    unique_bodypart_poses: dict[str, np.ndarray] | None = None,
    pcutoff: float = -1,
    oks_bbox_margin: int = 0,
    oks_sigma: float = 0.1,
    per_keypoint_rmse: bool = False,
    compute_detection_rmse: bool = True,
) -> dict:
    """Computes pose estimation performance metrics

    Given ground truth pose labels and predictions on a dataset, computes RMSE and pose
    mAP/mAR using OKS.

    The image paths in the ground_truth dict must be the same as the ones in the
    predictions dict.

    Single animal RMSE is computed by simply calculating the Euclidean distance between
    each ground truth keypoint and the corresponding prediction.

    Multi-animal RMSE is computed differently: predictions are first matched to ground
    truth individuals using greedy OKS matching. OKS (or object keypoint similarity) is
    a similarity metric for keypoints (you can read more about it and its definition
    here: https://cocodataset.org/#keypoints-eval). RMSE is then computed only between
    predictions and the ground truth pose they are matched to, only when the OKS is
    greater than a small threshold. Predictions that cannot be matched to any ground
    truth with non-zero OKS are not used to compute RMSE.

    Args:
        ground_truth: The ground truth pose for which to compute metrics in the dataset.
            This should be a dictionary mapping strings (image UIDs, such as image
            paths) to ground truth pose for the image. The pose arrays should be
            in the format (num_individuals, num_bodyparts, 3), where the 3 values are
            x, y and visibility. The ``num_individuals`` corresponds to the number of
            individuals labeled in each image.
        predictions: The predicted poses for which to compute metrics in the dataset.
            This should be a dictionary mapping strings (image UIDs, such as image
            paths) to pose predictions for the image. The pose arrays should be
            in the format (num_predictions, num_bodyparts, 3), where the 3 values are
            x, y and score. The number of predictions can be different to the number of
            ground truth individuals labeled for an image.
        single_animal: Whether the metrics are being computed on a single-animal or
            multi-animal dataset. This has an impact on RMSE computation.
        unique_bodypart_gt: If unique bodyparts are defined for the dataset, they should
            be contained in this dict in the same format as the ``ground_truth`` dict.
        unique_bodypart_poses: If unique bodyparts are defined for the dataset, the
            predictions should be contained in this dict in the same format as the
            ``predictions`` dict.
        pcutoff: The threshold to compute the "rmse_cutoff" score (RMSE of all
            predictions with score above the cutoff).
        oks_bbox_margin: The margin to add around keypoints to compute the area for OKS
            computation.
        oks_sigma: The OKS sigma to use to compute pose.
        per_keypoint_rmse: Compute per-keypoint RMSE values.
        compute_detection_rmse: Computes detection RMSE (without animal assembly) if the
            predictions are from a multi-animal model.

    Returns:
        A dictionary containing keys "rmse", "rmse_cutoff", "mAP" and "mAR" mapping
        to those metrics on the given dataset.

        If unique bodyparts are given, two extra keys "rmse_unique_bodyparts" and
        "rmse_pcutoff_unique_bodyparts" are also returned, containing the metrics for
        the unique bodyparts head.

        If `per_keypoint_evaluation=True`, "keypoint_rmse", "keypoint_rmse_cutoff" (and
        optionally "unique_keypoint_rmse" and "unique_keypoint_rmse_cutoff") keys are
        added, containing a list of floats representing the RMSE for each keypoint.

    Examples:
        >>> # Define the p-cutoff, prediction, and target DataFrames
        >>> pcutoff = 0.5
        >>> ground_truth = {"img0": np.array([[[1.0, 1.0, 2.0], ...], ...]), ...}
        >>> predictions = {"img0": np.array([[[2.0, 1.0, 0.4], ...], ...]), ...}
        >>> scores = compute_metrics(ground_truth, predictions, pcutoff=pcutoff)
        >>> print(scores)
        {
            "rmse": 1.0,
            "rmse_pcutoff": 0.0,
            'mAP': 84.2,
            'mAR': 74.5
        }  # Sample output scores
    """
    data = prepare_evaluation_data(ground_truth, predictions)
    oks_scores = distance_metrics.compute_oks(
        data=data,
        oks_sigma=oks_sigma,
        oks_bbox_margin=oks_bbox_margin,
    )

    data_unique = None
    if unique_bodypart_gt is not None:
        assert unique_bodypart_poses is not None
        data_unique = prepare_evaluation_data(unique_bodypart_gt, unique_bodypart_poses)

    rmse_scores = distance_metrics.compute_rmse(
        data,
        single_animal,
        pcutoff,
        data_unique=data_unique,
        per_keypoint_results=per_keypoint_rmse,
    )
    results = dict(**rmse_scores, **oks_scores)

    if compute_detection_rmse and not single_animal:
        det_rmse, det_rmse_p = distance_metrics.compute_detection_rmse(
            data, pcutoff, data_unique=data_unique,
        )
        results["rmse_detections"] = det_rmse
        results["rmse_detections_pcutoff"] = det_rmse_p

    return results


def prepare_evaluation_data(
    ground_truth: dict[str, np.ndarray],
    predictions: dict[str, np.ndarray],
) -> list[tuple[np.ndarray, np.ndarray]]:
    """Prepares predictions and ground truth pose to compute metrics.

    Only keeps ground truth and predicted assemblies with at least 2 valid keypoints.
    Sets the coordinates for all keypoints that aren't visible (for ground truth,
    visibility <= 0 and for predictions score <= 0) to ``np.nan``.

    Sorts valid predictions by score.

    Args:
        ground_truth: For each image, the GT of shape (n_idv, n_bpt, 3).
        predictions: For each image, the pose predictions of shape (n_pred, n_bpt, 3).

    Returns:
        A list containing (ground truth pose, predicted pose) for each image in the
        dataset, where the predicted pose is sorted from highest to lowest score.
    """
    pose_data = []
    for image, gt in ground_truth.items():
        gt = gt.copy()
        gt[gt[..., 2] <= 0] = np.nan

        # only keep ground truth pose with at least one keypoint
        gt_mask = np.any(np.all(~np.isnan(gt), axis=-1), axis=-1)
        gt = gt[gt_mask]

        pred = predictions[image][..., :3].copy()  # PAF have 5 values; keep xy + score
        pred[pred[..., 2] < 0] = np.nan

        # only keep predicted pose with at least two keypoints
        pred_mask = np.any(np.all(~np.isnan(pred), axis=-1), axis=-1)
        pred = pred[pred_mask]

        scores = np.nanmean(pred[:, :, 2], axis=-1)
        pred_order = np.argsort(-scores, kind="mergesort")
        pose_data.append((gt, pred[pred_order]))

    return pose_data


--- File: deeplabcut/core/metrics/identity.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Implementations of methods to compute identity prediction accuracy"""
from __future__ import annotations

import numpy as np
from sklearn.metrics import accuracy_score

from deeplabcut.core.crossvalutils import find_closest_neighbors


def compute_identity_scores(
    individuals: list[str],
    bodyparts: list[str],
    predictions: dict[str, np.ndarray],
    identity_scores: dict[str, np.ndarray],
    ground_truth: dict[str, np.ndarray],
) -> dict[str, float]:
    """
    FIXME: With DLCRNet all heatmap "peaks" above 0.01 were kept, with 1 keypoint and
     1 identity score map per peak. Then, for each ground truth keypoint, we selected
     the prediction closest to it, and evaluated the identity score in that position.
     This is no longer the case, as we're now evaluating after assembly. So we only
     have num_individuals assemblies.

    Args:
        individuals:
        bodyparts:
        predictions: (num_assemblies, num_bodyparts, 3)
        identity_scores: (num_assemblies, num_bodyparts, num_individuals)
        ground_truth: (num_individuals, num_bodyparts, 3)

    Returns:

    """
    if not len(predictions) == len(ground_truth):
        raise ValueError("Mismatch between number of predictions and ground truth")

    all_bpts = np.asarray(len(individuals) * bodyparts)
    ids = np.full((len(predictions), len(all_bpts), 2), np.nan)
    for i, (image, pred) in enumerate(predictions.items()):
        for j in range(len(individuals)):
            for k in range(len(bodyparts)):
                bpt_idx = len(bodyparts) * j + k
                ids[i, bpt_idx, 0] = j

        # set keypoints that aren't visible to NaN
        gt = ground_truth[image].copy()
        gt[gt[..., 2] <= 0, :2] = np.nan
        gt = gt[..., :2]

        id_scores = identity_scores[image]

        # reorder to (bodypart, individual, ...)
        gt = gt.transpose((1, 0, 2))
        pred = pred.transpose((1, 0, 2))[..., :2]
        id_scores = id_scores.transpose((1, 0, 2))
        for bpt, bpt_gt, bpt_pred, bpt_id_scores in zip(bodyparts, gt, pred, id_scores):
            # assign ground truth keypoints to the closest prediction, so the ID score
            # is the closest possible to the ID score computed with "ground truth"
            indices_gt = np.flatnonzero(np.all(~np.isnan(bpt_gt), axis=1))

            # Remove NaN predictions from the bodypart predictions
            indices_pred = np.all(np.isfinite(bpt_pred), axis=1)
            bpt_pred = bpt_pred[indices_pred]
            bpt_id_scores = bpt_id_scores[indices_pred]

            neighbors = find_closest_neighbors(bpt_gt[indices_gt], bpt_pred, k=3)
            found = neighbors != -1
            indices = np.flatnonzero(all_bpts == bpt)
            # Get the predicted identity of each bodypart by taking the argmax
            ids[i, indices[indices_gt[found]], 1] = np.argmax(
                bpt_id_scores[neighbors[found]], axis=1
            )

    ids = ids.reshape((len(predictions), len(individuals), len(bodyparts), 2))
    results = {}
    for i, bpt in enumerate(bodyparts):
        temp = ids[:, :, i].reshape((-1, 2))
        valid = np.isfinite(temp).all(axis=1)
        y_true, y_pred = temp[valid].T
        results[f"{bpt}_accuracy"] = accuracy_score(y_true, y_pred)

    return results


--- File: deeplabcut/core/metrics/bbox.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Bounding box metrics

Metrics are currently computed using pycocotools, which can be installed with `pypi`
(see https://github.com/ppwwyyxx/cocoapi/tree/master).
"""
from __future__ import annotations

from unittest.mock import Mock, patch

import numpy as np

try:
    from pycocotools.coco import COCO
    from pycocotools.cocoeval import COCOeval

    with_pycocotools = True
except ModuleNotFoundError as err:
    with_pycocotools = False


@patch("pycocotools.coco.print", Mock())
@patch("pycocotools.cocoeval.print", Mock())
def compute_bbox_metrics(
    ground_truth: dict[str, dict],
    detections: dict[str, dict],
) -> dict[str, float]:
    """Computes bbox mAP and mAR metrics for bounding boxes.

    Args:
        ground_truth: A dictionary mapping image UIDs (such as image paths or filenames)
            to a ground truth labels dict. The labels dict should contain the keys
            "width" (image width), "height" (image height) and "bboxes" (a numpy array
            of shape (num_gt_bboxes, 4) containing the ground truth bounding boxes in
            format xywh).
        detections: A dictionary mapping image UIDs (such as image paths or filenames)
            to a predicted bounding box dict. The detections dict should contain the
            keys "bboxes" (a numpy array of shape (num_detected_bboxes, 4) containing
            the predicted bounding boxes in format xywh) and "scores" (a numpy array of
            length num_detected_bboxes containing the confidence score for each
            predicted bounding box).

    Returns:
        The bounding box mAP/mAR metrics in a dictionary.

    Raises:
        ModuleNotFoundError: if ``pycocotools`` is not installed
        ValueError: if there are mismatches in the keys of ground_truth and detections
    """
    if not with_pycocotools:
        raise ModuleNotFoundError("pycocotools not installed! can't compute bbox mAP")

    if len(detections) != len(ground_truth):
        raise ValueError()

    coco = COCO()
    coco.dataset["annotations"] = []
    coco.dataset["categories"] = [{"id": 1, "name": "animals", "supercategory": "obj"}]
    coco.dataset["images"] = []
    predictions = []
    for idx, (img, gt) in enumerate(ground_truth.items()):
        img_id = idx + 1
        coco.dataset["images"].append(
            {
                "id": img_id,
                "file_name": img,
                "width": gt["width"],
                "height": gt["height"],
            }
        )
        for bbox in gt["bboxes"][:, :4]:
            ann_id = len(coco.dataset["annotations"]) + 1
            coco.dataset["annotations"].append(
                {
                    "id": ann_id,
                    "image_id": img_id,
                    "category_id": 1,
                    "area": max(1, (bbox[2] * bbox[3]).item()),
                    "bbox": bbox,
                    "iscrowd": 0,
                }
            )

        for bbox, score in zip(detections[img]["bboxes"], detections[img]["scores"]):
            predictions.append(np.array([img_id, *bbox, score, 1]))

    if len(predictions) == 0:
        return {
            "mAP@50:95": 0.0,
            "mAP@50": 0.0,
            "mAP@75": 0.0,
            "mAR@50:95": 0.0,
            "mAR@50": 0.0,
            "mAR@75": 0.0,
        }

    predictions = np.stack(predictions, axis=0)
    coco.createIndex()
    coco_det = coco.loadRes(predictions)
    coco_eval = COCOeval(coco, coco_det, iouType="bbox")
    coco_eval.evaluate()
    coco_eval.accumulate()
    return {
        name: val
        for name, val in [
            _get_metric(coco_eval, recall=False),
            _get_metric(coco_eval, recall=False, iou_threshold=0.5),
            _get_metric(coco_eval, recall=False, iou_threshold=0.75),
            _get_metric(coco_eval, recall=True),
            _get_metric(coco_eval, recall=True, iou_threshold=0.5),
            _get_metric(coco_eval, recall=True, iou_threshold=0.75),
        ]
    }


def _get_metric(
    coco_eval: COCOeval,
    recall: bool = False,
    iou_threshold: float | None = None,
    area_rng: str = "all",
    max_dets: int = 100,
) -> tuple[str, float]:
    metric_name = "mAR" if recall else "mAP"
    if iou_threshold is not None:
        thresh = f"{int(100 * iou_threshold)}"
    else:
        low, high = coco_eval.params.iouThrs[0], coco_eval.params.iouThrs[-1]
        thresh = f"{int(100 * low)}:{int(100 * high)}"

    aind = [i for i, aRng in enumerate(coco_eval.params.areaRngLbl) if aRng == area_rng]
    mind = [i for i, mDet in enumerate(coco_eval.params.maxDets) if mDet == max_dets]
    if recall:
        s = coco_eval.eval["recall"]
        if iou_threshold is not None:
            t = np.where(iou_threshold == coco_eval.params.iouThrs)[0]
            s = s[t]
        s = s[:, :, aind, mind]
    else:
        s = coco_eval.eval["precision"]
        if iou_threshold is not None:
            t = np.where(iou_threshold == coco_eval.params.iouThrs)[0]
            s = s[t]
        s = s[:, :, :, aind, mind]

    if len(s[s > -1]) == 0:
        mean_s = -1
    else:
        mean_s = 100 * np.mean(s[s > -1]).item()

    return f"{metric_name}@{thresh}", mean_s


--- File: deeplabcut/pose_estimation_pytorch/task.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Types of tasks that can be run by DeepLabCut pose estimation models"""
from __future__ import annotations

from dataclasses import dataclass
from enum import Enum


@dataclass(frozen=True)
class TaskDataMixin:
    aliases: tuple[str]
    snapshot_prefix: str


class Task(TaskDataMixin, Enum):
    """A task to solve"""

    BOTTOM_UP = ("BU", "BottomUp"), "snapshot"
    DETECT = ("DT", "Detect"), "snapshot-detector"
    TOP_DOWN = ("TD", "TopDown"), "snapshot"

    @classmethod
    def _missing_(cls, value):
        if isinstance(value, str):
            value = value.upper()
            for member in cls:
                if value in member.aliases:
                    return member
        return None

    def __repr__(self) -> str:
        return f"Task.{self.name}"


--- File: deeplabcut/pose_estimation_pytorch/registry.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import inspect
from functools import partial
from typing import Any, Dict, Optional


def build_from_cfg(
    cfg: Dict, registry: "Registry", default_args: Optional[Dict] = None
) -> Any:
    """Builds a module from the configuration dictionary when it represents a class configuration,
    or call a function from the configuration dictionary when it represents a function configuration.

    Args:
        cfg: Configuration dictionary. It should at least contain the key "type".
        registry: The registry to search the type from.
        default_args: Default initialization arguments.
                      Defaults to None.

    Returns:
        Any: The constructed object.

    Example:
        >>> from deeplabcut.pose_estimation_pytorch.registry import Registry, build_from_cfg
        >>> class Model:
        >>>     def __init__(self, param):
        >>>         self.param = param
        >>> cfg = {"type": "Model", "param": 10}
        >>> registry = Registry("models")
        >>> registry.register_module(Model)
        >>> obj = build_from_cfg(cfg, registry)
        >>> assert isinstance(obj, Model)
        >>> assert obj.param == 10
    """

    args = cfg.copy()

    if default_args is not None:
        for name, value in default_args.items():
            args.setdefault(name, value)

    obj_type = args.pop("type")
    if isinstance(obj_type, str):
        obj_cls = registry.get(obj_type)
        if obj_cls is None:
            raise KeyError(f"{obj_type} is not in the {registry.name} registry")
    elif inspect.isclass(obj_type) or inspect.isfunction(obj_type):
        obj_cls = obj_type
    else:
        raise TypeError(f"type must be a str or valid type, but got {type(obj_type)}")
    try:
        return obj_cls(**args)
    except Exception as e:
        # Normal TypeError does not print class name.
        raise type(e)(f"{obj_cls.__name__}: {e}")


class Registry:
    """A registry to map strings to classes or functions.
    Registered objects could be built from the registry. Meanwhile, registered
    functions could be called from the registry.

    Args:
        name: Registry name.
        build_func: Builds function to construct an instance from
                    the Registry. If neither ``parent`` nor
                    ``build_func`` is specified, the ``build_from_cfg``
                    function is used. If ``parent`` is specified and
                    ``build_func`` is not given,  ``build_func`` will be
                    inherited from ``parent``. Default: None.
        parent: Parent registry. The class registered in
                children's registry could be built from the parent.
                Default: None.
        scope: The scope of the registry. It is the key to search
               for children's registry. If not specified, scope will be the
               name of the package where the class is defined, e.g. mmdet, mmcls, mmseg.
               Default: None.

    Attributes:
        name: Registry name.
        module_dict: The dictionary containing registered modules.
        children: The dictionary containing children registries.
        scope: The scope of the registry.
    """

    def __init__(self, name, build_func=None, parent=None, scope=None):
        self._name = name
        self._module_dict = dict()
        self._children = dict()
        self._scope = "."

        if build_func is None:
            if parent is not None:
                self.build_func = parent.build_func
            else:
                self.build_func = build_from_cfg
        else:
            self.build_func = build_func
        if parent is not None:
            assert isinstance(parent, Registry)
            parent._add_children(self)
            self.parent = parent
        else:
            self.parent = None

    def __len__(self):
        return len(self._module_dict)

    def __contains__(self, key):
        return self.get(key) is not None

    def __repr__(self):
        format_str = (
            self.__class__.__name__ + f"(name={self._name}, "
            f"items={self._module_dict})"
        )
        return format_str

    @staticmethod
    def split_scope_key(key):
        """Split scope and key.
        The first scope will be split from key.
        Examples:
            >>> Registry.split_scope_key('mmdet.ResNet')
            'mmdet', 'ResNet'
            >>> Registry.split_scope_key('ResNet')
            None, 'ResNet'
        Return:
            tuple[str | None, str]: The former element is the first scope of
            the key, which can be ``None``. The latter is the remaining key.
        """
        split_index = key.find(".")
        if split_index != -1:
            return key[:split_index], key[split_index + 1 :]
        else:
            return None, key

    @property
    def name(self):
        return self._name

    @property
    def scope(self):
        return self._scope

    @property
    def module_dict(self):
        return self._module_dict

    @property
    def children(self):
        return self._children

    def get(self, key):
        """Get the registry record.

        Args:
            key: The class name in string format.

        Returns:
            class: The corresponding class.

        Example:
            >>> from deeplabcut.pose_estimation_pytorch.registry import Registry
            >>> registry = Registry("models")
            >>> class Model:
            >>>     pass
            >>> registry.register_module(Model, "Model")
            >>> assert registry.get("Model") == Model
        """
        scope, real_key = self.split_scope_key(key)
        if scope is None or scope == self._scope:
            # get from self
            if real_key in self._module_dict:
                return self._module_dict[real_key]
        else:
            # get from self._children
            if scope in self._children:
                return self._children[scope].get(real_key)
            else:
                # goto root
                parent = self.parent
                while parent.parent is not None:
                    parent = parent.parent
                return parent.get(key)

    def build(self, *args, **kwargs):
        """Builds an instance from the registry.

        Args:
            *args: Arguments passed to the build function.
            **kwargs: Keyword arguments passed to the build function.

        Returns:
            Any: The constructed object.

        Example:
            >>> from deeplabcut.pose_estimation_pytorch.registry import Registry, build_from_cfg
            >>> class Model:
            >>>     def __init__(self, param):
            >>>         self.param = param
            >>> cfg = {"type": "Model", "param": 10}
            >>> registry = Registry("models")
            >>> registry.register_module(Model)
            >>> obj = registry.build(cfg, param=20)
            >>> assert isinstance(obj, Model)
            >>> assert obj.param == 20
        """
        return self.build_func(*args, **kwargs, registry=self)

    def _add_children(self, registry):
        """Add children for a registry.

        Args:
            registry: The registry to be added as children based on its scope.

        Returns:
            None

        Example:
            >>> from deeplabcut.pose_estimation_pytorch.registry import Registry
            >>> models = Registry('models')
            >>> mmdet_models = Registry('models', parent=models)
            >>> class Model:
            >>>     pass
            >>> mmdet_models.register_module(Model)
            >>> obj = models.build(dict(type='mmdet.Model'))
            >>> assert isinstance(obj, Model)
        """
        assert isinstance(registry, Registry)
        assert registry.scope is not None
        assert (
            registry.scope not in self.children
        ), f"scope {registry.scope} exists in {self.name} registry"
        self.children[registry.scope] = registry

    def _register_module(self, module, module_name=None, force=False):
        """Register a module.

        Args:
            module: Module class or function to be registered.
            module_name: The module name(s) to be registered.
                                                     If not specified, the class name will be used.
            force: Whether to override an existing class with the same name.
                                    Default: False.

        Returns:
            None

        Example:
            >>> from deeplabcut.pose_estimation_pytorch.registry import Registry
            >>> registry = Registry("models")
            >>> class Model:
            >>>     pass
            >>> registry._register_module(Model, "Model")
            >>> assert registry.get("Model") == Model
        """
        if not inspect.isclass(module) and not inspect.isfunction(module):
            raise TypeError(
                "module must be a class or a function, " f"but got {type(module)}"
            )

        if module_name is None:
            module_name = module.__name__
        if isinstance(module_name, str):
            module_name = [module_name]
        for name in module_name:
            if not force and name in self._module_dict:
                raise KeyError(f"{name} is already registered " f"in {self.name}")
            self._module_dict[name] = module

    def deprecated_register_module(self, cls=None, force=False):
        """Decorator to register a class in the registry.

        Args:
            cls: The class to be registered.
            force: Whether to override an existing class with the same name.
                                    Default: False.

        Returns:
            type: The input class.

        Example:
            >>> from deeplabcut.pose_estimation_pytorch.registry import Registry
            >>> registry = Registry("models")
            >>> @registry.deprecated_register_module()
            >>> class Model:
            >>>     pass
            >>> assert registry.get("Model") == Model
        """
        if cls is None:
            return partial(self.deprecated_register_module, force=force)
        self._register_module(cls, force=force)
        return cls

    def register_module(self, name=None, force=False, module=None):
        """Register a module.
        A record will be added to `self._module_dict`, whose key is the class
        name or the specified name, and value is the class itself.
        It can be used as a decorator or a normal function.
        Args:
            name: The module name to be registered. If not
                  specified, the class name will be used.
            force: Whether to override an existing class with
                   the same name. Default: False.
            module: Module class or function to be registered.
        """
        if not isinstance(force, bool):
            raise TypeError(f"force must be a boolean, but got {type(force)}")
        # NOTE: This is a walkaround to be compatible with the old api,
        # while it may introduce unexpected bugs.
        if isinstance(name, type):
            return self.deprecated_register_module(name, force=force)

        # use it as a normal method: x.register_module(module=SomeClass)
        if module is not None:
            self._register_module(module=module, module_name=name, force=force)
            return module

        # use it as a decorator: @x.register_module()
        def _register(module):
            self._register_module(module=module, module_name=name, force=force)
            return module

        return


--- File: deeplabcut/pose_estimation_pytorch/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import deeplabcut.pose_estimation_pytorch.config as config
from deeplabcut.pose_estimation_pytorch.apis import (
    analyze_image_folder,
    analyze_images,
    analyze_videos,
    build_predictions_dataframe,
    create_labeled_images,
    create_tracking_dataset,
    convert_detections2tracklets,
    evaluate,
    evaluate_network,
    extract_maps,
    extract_save_all_maps,
    get_detector_inference_runner,
    get_pose_inference_runner,
    predict,
    superanimal_analyze_images,
    train,
    train_network,
    video_inference,
    VideoIterator,
    visualize_predictions,
)
from deeplabcut.pose_estimation_pytorch.config import (
    available_detectors,
    available_models,
)
from deeplabcut.pose_estimation_pytorch.data import (
    build_transforms,
    COCOLoader,
    COLLATE_FUNCTIONS,
    DLCLoader,
    Loader,
    PoseDataset,
    PoseDatasetParameters,
)
from deeplabcut.pose_estimation_pytorch.runners import (
    build_inference_runner,
    build_training_runner,
    DetectorInferenceRunner,
    DetectorTrainingRunner,
    get_load_weights_only,
    InferenceRunner,
    PoseInferenceRunner,
    PoseTrainingRunner,
    set_load_weights_only,
    TorchSnapshotManager,
    TrainingRunner,
)
from deeplabcut.pose_estimation_pytorch.task import Task
from deeplabcut.pose_estimation_pytorch.utils import fix_seeds


--- File: deeplabcut/pose_estimation_pytorch/README.md ---
# PyTorch DeepLabCut API

This overview is primarily written for maintainers and expert users. 

Here we detail the logic and structure for the DLC3.* PyTorch code. Furthermore, we
provide many practical examples to illustrate the usage of the code for developers. 

## Structure of the PyTorch DLC code

[API](#API)

[Models](#models)

[Data](#data)

[Runners](#runners)

### API

High-level API methods are implemented in `deeplabcut.pose_estimations_pytorch.apis`.
This folder includes methods to train and evaluate models on DeepLabCut projects, and
analyze videos or folders (of images). While some of the methods are implemented to work
directly from DeepLabCut projects (i.e. by specifying the path to the project config
file and the shuffle number), internally they call methods that allow more flexibility.
Thus, they are also ideally suited for developers.

### Models

We provide state-of-the-art pose estimation models such as DLCRNet, HRNet, DEKR, BUCTD
and more are coming! Object detection models are also available (and implemented in 
`deeplabcut.pose_estimations_pytorch.models.detectors`).

The `deeplabcut.pose_estimations_pytorch.models` package contains all components related
to building a model. Models are flexibly build from modular components: `backbone`, 
`neck` (optional) and `head` (as discussed below). 

You can check available models by running:

```python
import deeplabcut.pose_estimation_pytorch

# Available pose estimation models
print(deeplabcut.pose_estimation_pytorch.available_models())

# Available object detection models
print(deeplabcut.pose_estimation_pytorch.available_detectors())
```

#### Model Configuration Files

Model architectures are built according to a configuration specified in a `yaml` file.
This file (named `pytorch_cfg.yaml`) describes the architecture of the model you want to
train (but also hyperparameters, optimizer, ...). All code to manipulate PyTorch 
configuration files is in `deeplabcut.pose_estimations_pytorch.config`.

To generate a model configuration, you can call `make_pytorch_pose_config`. Note that 
this does not save the configuration to a given filepath - it just returns it as a 
dictionary. However, you can save it with `write_config`. 

During a typical DeepLabCut project management workflow, these methods don't need to be 
called, as `create_training_dataset` will create this configuration file and save it to 
disk.

```python
from pathlib import Path

import deeplabcut.pose_estimation_pytorch as dlc_torch

project_cfg = { "Task": "mice", ... }  # the configuration for your DLC project
pose_config_path = Path("/path/to/my/config/pytorch_cfg.yaml")
model_cfg = dlc_torch.config.make_pytorch_pose_config(
    project_config=project_cfg,
    pose_config_path=pose_config_path,
    net_type="hrnet_w32",
    top_down=True,
    save=True,
)
```

#### Adding Models

If you want to add a novel model, you'll ideally build them from the following
implemented parts:

- a backbone (such as a ResNet or HRNet)
- a head (such as a HeatmapHead)
- a predictor (transforming model outputs into keypoint locations)
- a target generator (creating the targets for your head outputs from your labels)

Some models can also define a neck (model components between the backbone and the head).
You'll also need some loss criterions, but usually you'll be able to use existing ones.

You can either use existing classes and only replace some elements, or rewrite
everything you need for your model. We use Model Registries to simplify the process of
adding models.

#### Model Registry

Registries are created for all model building blocks to make it easy to add new models.
All you need to do is add the decorator `REGISTRY.register_module` to be able to load 
your model from a configuration file. Available registries are `BACKBONES`, `NECKS`,
`HEADS`, `PREDICTORS` and `TARGET_GENERATORS`. Each building block has a base class
that should be inherited by the class added to the model registry (`BaseBackbone`,
`BaseNeck`, `BaseHead`, `BasePredictor` and `BaseGenerator` respectively).

Let's illustrate that with a small example. We'll create a dummy backbone, which simply
applies a max-pool to the input:

```python
import torch
import torch.nn.functional as F

from deeplabcut.pose_estimation_pytorch.models.backbones import BACKBONES, BaseBackbone


@BACKBONES.register_module
class DummyBackbone(BaseBackbone):
    """A dummy backbone, simply max-pooling the input"""
    
    def __init__(self, kernel_size: int = 2):
        super().__init__(stride=kernel_size)
        self.kernel_size = kernel_size
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return F.max_pool2d(x, kernel_size=self.kernel_size)


backbone_config = dict(type="DummyBackbone", kernel_size=3)
backbone = BACKBONES.build(backbone_config)  # will create a DummyBackbone
```

Another example would be creating a custom head for our model. In this case, let's make
a head which takes as input the output of a backbone (which has shape `(num_channels,
H', W')`) and put it through a kernel-size 1 convolution, simply changing the number of
channels.

Heads can output multiple tensors (such as heatmaps and location refinement fields). 
Therefore, their `forward(...)` method outputs a dictionary mapping strings to tensors.
Here, we return the `heatmap` and `locref` tensors.

A head must contain different: a `target_generator` to generate targets for
its outputs and a `predictor` to convert model outputs to pose. Make sure that the keys
output by the `target_generator` and the `head` match! Some `criterion` also needs to be
defined to compute the loss between the outputs and targets. When more than one output 
is specified (such as in this case, where we're generating heatmaps and location 
refinement fields), a loss aggregator must also be given to combine all losses into one
(this should simply be a `WeightedLossAggregator`, indicating the weight for each loss).

```python
import torch
import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.models.criterions import (
    BaseCriterion,
    BaseLossAggregator,
    WeightedHuberCriterion,
    WeightedLossAggregator,
    WeightedMSECriterion,
)
from deeplabcut.pose_estimation_pytorch.models.heads import HEADS, BaseHead
from deeplabcut.pose_estimation_pytorch.models.predictors import (
    BasePredictor,
    HeatmapPredictor,
)
from deeplabcut.pose_estimation_pytorch.models.target_generators import (
    BaseGenerator,
    HeatmapGaussianGenerator,
)


@HEADS.register_module
class DummyHead(BaseHead):
    """A dummy backbone, simply max-pooling the input"""
    
    def __init__(
        self,
        num_input_channels: int,
        num_bodyparts: int,
        predictor: BasePredictor,
        target_generator: BaseGenerator,
        criterion: dict[str, BaseCriterion],
        aggregator: BaseLossAggregator,
    ):
        super().__init__(
            stride=1,
            predictor=predictor,
            target_generator=target_generator,
            criterion=criterion,
            aggregator=aggregator
        )
        self.conv_heatmap = nn.Conv2d(
            in_channels=num_input_channels,
            out_channels=num_bodyparts,
            kernel_size=1,
            stride=1,
        )
        self.locref_heatmap = nn.Conv2d(
            in_channels=num_input_channels,
            out_channels=2 * num_bodyparts,
            kernel_size=1,
            stride=1,
        )

    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:
        return {
            "heatmap": self.conv_heatmap(x),
            "locref": self.locref_heatmap(x),
        }


head_config = dict(
    type="DummyHead",
    num_input_channels=2048,
    num_bodyparts=5,
    predictor=HeatmapPredictor(location_refinement=True, locref_std= 7.2801),
    target_generator=HeatmapGaussianGenerator(
        num_heatmaps=5,
        pos_dist_thresh=17,
        heatmap_mode=HeatmapGaussianGenerator.Mode.KEYPOINT,
        generate_locref=True,
    ),
    criterion={
        "heatmap": WeightedMSECriterion(),
        "locref": WeightedHuberCriterion(),
    },
    aggregator=WeightedLossAggregator(weights={"heatmap": 1, "locref": 0.05}),
)
head = HEADS.build(head_config)
```

### Data

The `deeplabcut.pose_estimations_pytorch.data` package contains all code for PyTorch
dataset creation and test/train splitting. The `DLCLoader` class is used to load the
labeled data for a specific shuffle.

```python3
import deeplabcut.pose_estimation_pytorch as dlc_torch

loader = dlc_torch.DLCLoader(
    config="/path/to/my/project/config.yaml",
    trainset_index=0,
    shuffle=1,
)

# print the path to the model folder (where the config file is stored)
print(loader.model_folder)
# print the path to the evaluation folder
print(loader.evaluation_folder)

# display the DataFrame containing the dataset
print(loader.df)

# display the DataFrames containing the train/test data respectively
print(loader.df_train)
print(loader.df_test)
```

The `PoseDataset` class is an instance of 
[torch.utils.Dataset](https://pytorch.org/docs/stable/data.html), which converts raw 
images and keypoints to a tensor dataset for training and evaluation. You can generate 
an instance of training/test dataset with your `DLCLoader`:

```python3
import deeplabcut.pose_estimation_pytorch as dlc_torch

loader = dlc_torch.DLCLoader(
    config="/path/to/my/project/config.yaml",
    trainset_index=0,
    shuffle=1,
)
train_dataset = loader.create_dataset(
    transform=dlc_torch.build_transforms(loader.model_cfg["data"]["train"]),
    mode="train",
    task=loader.pose_task,
)
valid_dataset = loader.create_dataset(
    transform=dlc_torch.build_transforms(loader.model_cfg["data"]["train"]),
    mode="test",
    task=loader.pose_task,
)
```

A `COCOLoader` is also available, and allows you train models in DeepLabCut on 
[COCO-format](https://medium.com/@manuktiwary/coco-format-what-and-how-5c7d22cf5301)
datasets. This essentially consists of having a folder containing your dataset in the 
format:

```
COCOProject
└───annotations
│   │   train.json
│   │   test.json
│   
└───images
    │   img0000.png
    │   img0001.png
    │           ...
```

In your `train.json` and `test.json` files, you can either specify your image 
`"file_name"` with a relative path or with an absolute path. If a relative path is 
used (e.g. `img0000.png` or `subfolder/img0000.png`), it will be resolved to the 
`images` folder in your project (i.e. `/path/to/COCOProject/images/img0000.png` or 
`/path/to/COCOProject/images/subfolder/img0000.png`).

If you specify an absolute path, the path to the image will not be resolved, and the 
image will be loaded from the specified path. This allows you to keep data on different
disks, or reuse the same images in different projects without having to duplicate them.

To train a DeepLabCut model on a COCO-format dataset, you'll need to specify a model 
configuration file (as described in [#model_configuration_files]).

```python3
from pathlib import Path

import deeplabcut.pose_estimation_pytorch as dlc_torch

# Specify project paths
project_root = Path("/path/to/my/COCOProject")
train_json_filename = "train.json"
test_json_filename = "test.json"

# Parse information about the project
train_dict = dlc_torch.COCOLoader.load_json(project_root, filename=train_json_filename)
max_num_individuals, bodyparts = dlc_torch.COCOLoader.get_project_parameters(train_dict)

# Generate a configuration file for your PyTorch model
# In this case, it's for a Top-Down HRNet_w32
experiment_path = project_root / "experiments" / "hrnet_w32"
model_cfg_path = experiment_path / "train" / "pytorch_cfg.yaml"
model_cfg = dlc_torch.config.make_pytorch_pose_config(
    project_config=dlc_torch.config.make_basic_project_config(
        dataset_path=str(project_root.resolve()),
        bodyparts=bodyparts,
        max_individuals=max_num_individuals,
        multi_animal=True,
    ),
    pose_config_path=experiment_path,
    net_type="hrnet_w32",
    top_down=True,
    save=True,
)

# Create the loader for the COCO dataset
loader = dlc_torch.COCOLoader(
    project_root=project_root,
    model_config_path="/path/to/my/project/experiments/pytorch_config.yaml",
    train_json_filename=train_json_filename,
    test_json_filename=test_json_filename,
)
train_dataset = loader.create_dataset(
    transform=dlc_torch.build_transforms(loader.model_cfg["data"]["train"]),
    mode="train",
    task=loader.pose_task,
)
valid_dataset = loader.create_dataset(
    transform=dlc_torch.build_transforms(loader.model_cfg["data"]["train"]),
    mode="test",
    task=loader.pose_task,
)
```

### Runners

The `deeplabcut.pose_estimations_pytorch.runners` contains code to get models, load 
pretrained weights, and either train them or run inference with them.

## Code Examples

### Training a Model on a COCO Dataset

```python
from pathlib import Path

import deeplabcut.pose_estimation_pytorch as dlc_torch

# Specify project paths
project_root = Path("/path/to/my/COCOProject")
train_json_filename = "train.json"
test_json_filename = "test.json"

loader = dlc_torch.COCOLoader(
    project_root=project_root,
    model_config_path="/path/to/my/project/experiments/pytorch_config.yaml",
    train_json_filename=train_json_filename,
    test_json_filename=test_json_filename,
)
dlc_torch.train(
    loader=loader,
    run_config=loader.model_cfg,
    task=dlc_torch.Task(loader.model_cfg["method"]),
    device="cuda:2",
    logger_config=dict(
        type="WandbLogger",
        project_name="MyWandbProject",
        tags=["model=hrnet_w32"],
    ),
    snapshot_path=None,
)
```

### Running Video Analysis outside a DeepLabCut Project

DeepLabCut provides high-level APIs (via the GUI or the python package) to analyze your
data. The usage of this API assumes the existence of a DLC project (with `config.yaml`
file, etc.).

Sometimes it might be more convenient to just run a model on your data via a low-level
API. We also use this API under the hood, in particular for the Model Zoo. Check out the
example below:

```python
from deeplabcut.core.config import read_config_as_dict
from pathlib import Path

import deeplabcut.pose_estimation_pytorch as dlc_torch

train_dir = Path("/Users/Jaylen/my-dlc-models/train")
pytorch_config_path = train_dir / "pytorch_config.yaml"
snapshot_path = train_dir / "snapshot-100.pt"

# for top-down models, otherwise None
detector_snapshot_path = train_dir / "detector-snapshot-100.pt"

# video and inference parameters
video_path = Path("/Users/Jaylen/my-dlc-models/videos/test-video.mp4")
max_num_animals = 5
batch_size = 16
detector_batch_size = 8

# read model configuration
model_cfg = read_config_as_dict(pytorch_config_path)
pose_task = dlc_torch.Task(model_cfg["method"])
pose_runner = dlc_torch.get_pose_inference_runner(
    model_config=model_cfg,
    snapshot_path=snapshot_path,
    max_individuals=max_num_animals,
    batch_size=batch_size,
)

detector_runner = None
if pose_task == dlc_torch.Task.TOP_DOWN:
    detector_runner = dlc_torch.get_detector_inference_runner(
        model_config=model_cfg,
        snapshot_path=detector_snapshot_path,
        max_individuals=max_num_animals,
        batch_size=detector_batch_size,
    )

predictions = dlc_torch.video_inference(
    video=video_path,
    pose_runner=pose_runner,
    detector_runner=detector_runner,
)
```


### Running Top-Down Video Analysis with Existing Bounding Boxes

When `deeplabcut.pose_estimation_pytorch.apis.videos.video_inference` is called
with a top-down model, it is assumed that a detector snapshot is given as well to obtain
bounding boxes with which to run pose estimation. It's possible that you've already 
obtained bounding boxes for your video (with another object detector or through some 
other means), and you want to reuse those bounding boxes instead of running an object
detector again.

You can easily do so by writing a bit of custom code, as shown in the example below:

```python
from deeplabcut.core.config import read_config_as_dict
from pathlib import Path

import numpy as np
import deeplabcut.pose_estimation_pytorch as dlc_torch
from tqdm import tqdm

# create an iterator for your video
video = dlc_torch.VideoIterator("/Users/Jayson/my-cool-video.mp4")

# dummy bboxes - you can load yours from a file or in another way
#  the bboxes should be in `xywh` format, i.e. (x_top_left, y_top_left, width, height)
bounding_boxes = [
    dict(  # frame 0 bounding boxes
        bboxes=np.array([[12, 37, 120, 78]]),
    ),
    dict(  # frame 1 bounding boxes
        bboxes=np.array([[17, 45, 128, 73], [532, 34, 117, 87]]),
    ),
    # ...
    dict(  # frame N bboxes -> must be equal to the number of frames in the video!
        bboxes=np.array([[17, 45, 128, 73], [532, 34, 117, 87]]),
    ),
]
video.set_context(bounding_boxes)
max_individuals = np.max([len(context["bboxes"]) for context in bounding_boxes])

# run inference!
model_cfg = read_config_as_dict("/Users/Jayson/pytorch_config.yaml")
pose_runner = dlc_torch.get_pose_inference_runner(
    model_config=model_cfg,
    snapshot_path=Path("/Users/Jayson/model-snapshot.pt"),
    max_individuals=max_individuals,
    batch_size=32,
)

# your predictions will be a list, containing the predictions made for each frame
#  as a dict (with keys for "bodyparts" but also "bboxes")!
predictions = pose_runner.inference(images=tqdm(video))
```


--- File: deeplabcut/pose_estimation_pytorch/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import os
import random
from pathlib import Path

import numpy as np
import torch

from deeplabcut.utils.auxiliaryfunctions import read_plainconfig


def create_folder(path_to_folder):
    """Creates all folders contained in the path.

    Args:
        path_to_folder: Path to the folder that should be created
    """
    if not os.path.exists(path_to_folder):
        os.makedirs(path_to_folder)


def fix_seeds(seed: int) -> None:
    """
    Fixes the random seed for python, numpy and pytorch

    Args:
        seed: the seed to set
    """
    random.seed(seed)
    torch.manual_seed(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def resolve_device(model_config: dict) -> str:
    """Determines which device should be used from the model config

    When the device is set to 'auto':
        If an Nvidia GPU is available, selects the device as cuda:0.
        Selects 'mps' if available (on macOS) and the net type is compatible.
        Otherwise, returns 'cpu'.
    Otherwise, simply returns the selected device

    Args:
        model_config: the configuration for the pose model

    Returns:
        the device on which training should be run
    """
    device = model_config["device"]
    supports_mps = "resnet" in model_config.get("net_type", "resnet")

    if device == "auto":
        if torch.cuda.is_available():
            return "cuda"
        elif supports_mps and torch.backends.mps.is_available():
            return "mps"
        return "cpu"
    return device


--- File: deeplabcut/pose_estimation_pytorch/benchmark/profile_HRNetCoAM.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

# Script for reproducing results in Zhou* & Stoffl* et al. for BUCTD with CoAM

# path=datapath
# results=resultspath or put numbers

# train model

# evaluate and
# check if predicted is close to result


--- File: deeplabcut/pose_estimation_pytorch/benchmark/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_estimation_pytorch/metrics/scoring.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import numpy as np
import pickle
from sklearn.metrics import accuracy_score

from deeplabcut.core.crossvalutils import find_closest_neighbors
from deeplabcut.utils.auxiliaryfunctions import read_config


def _match_identity_preds_to_gt(
    config_path: str, full_pickle_path: str
) -> tuple[np.ndarray, list]:
    with open(full_pickle_path, "rb") as f:
        data = pickle.load(f)
    metadata = data.pop("metadata")
    cfg = read_config(config_path)
    all_ids = cfg["individuals"].copy()
    all_bpts = cfg["multianimalbodyparts"] * len(all_ids)
    n_multibodyparts = len(all_bpts)
    if cfg["uniquebodyparts"]:
        all_ids += ["single"]
        all_bpts += cfg["uniquebodyparts"]
    all_bpts = np.asarray(all_bpts)
    joints = metadata["all_joints_names"]
    ids = np.full((len(data), len(all_bpts), 2), np.nan)
    for i, dict_ in enumerate(data.values()):
        id_gt, _, df_gt = dict_["groundtruth"]
        for j, id_ in enumerate(id_gt):
            if id_.size:
                ids[i, j, 0] = all_ids.index(id_)

        df = df_gt.unstack("coords").reindex(joints, level="bodyparts")
        xy_pred = dict_["prediction"]["coordinates"][0]
        for bpt, xy_gt in df.groupby(level="bodyparts"):
            inds_gt = np.flatnonzero(np.all(~np.isnan(xy_gt), axis=1))
            n_joint = joints.index(bpt)
            xy = xy_pred[n_joint]
            if inds_gt.size and xy.size:
                # Pick the predictions closest to ground truth,
                # rather than the ones the model has most confident in
                xy_gt_values = xy_gt.iloc[inds_gt].values
                neighbors = find_closest_neighbors(xy_gt_values, xy, k=3)
                found = neighbors != -1
                inds = np.flatnonzero(all_bpts == bpt)
                id_ = dict_["prediction"]["identity"][n_joint]
                ids[i, inds[inds_gt[found]], 1] = np.argmax(
                    id_[neighbors[found]], axis=1
                )
    ids = ids[:, :n_multibodyparts].reshape((len(data), len(cfg["individuals"]), -1, 2))
    return ids, list(data)


def compute_id_accuracy(ids: np.ndarray, mask_test: np.ndarray) -> np.ndarray:
    nbpts = ids.shape[2]  # ids shape is (n_images, n_individuals, n_bodyparts, 2)
    accu = np.empty((nbpts, 2))
    for i in range(nbpts):
        temp = ids[:, :, i].reshape((-1, 2))
        valid = np.isfinite(temp).all(axis=1)
        y_true, y_pred = temp[valid].T
        mask = np.repeat(mask_test, ids.shape[1])[valid]
        ac_train = accuracy_score(y_true[~mask], y_pred[~mask])
        ac_test = accuracy_score(y_true[mask], y_pred[mask])
        accu[i] = ac_train, ac_test
    return accu


--- File: deeplabcut/pose_estimation_pytorch/metrics/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_estimation_pytorch/post_processing/match_predictions_to_gt.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import numpy as np
from scipy.optimize import linear_sum_assignment

from deeplabcut.core.inferenceutils import (
    calc_object_keypoint_similarity,
)


def rmse_match_prediction_to_gt(
    pred_kpts: np.ndarray, gt_kpts: np.ndarray
) -> np.ndarray:
    """
    Hungarian algorithm predicted individuals to ground truth ones, using root mean
    squared error (rmse). The function provides a way to match predicted individuals to
    ground truth individuals based on the rmse distance between their corresponding
    keypoints. This algorithm is used to find the optimal matching, taking into account
    the potential missing animal.

    Raises:
        ValueError: if `gt_kpts.shape != pred_kpts.shape`

    Args:
        pred_kpts: shape (num_predictions, num_keypoints, 3), ground truth keypoints for
            an image, where the 3 values are (x,y,score) for each keypoint
        gt_kpts: shape (num_individuals, num_keypoints, 3), ground truth keypoints for
            an image, where the 3 values are (x,y,visibility) for each keypoint

    Returns:
        col_ind: array of the individuals indices for prediction
    """
    num_pred, num_keypoints, _ = pred_kpts.shape
    num_idv, num_keypoints_gt, _ = gt_kpts.shape
    if num_keypoints + 1 == num_keypoints_gt:
        gt_kpts = gt_kpts[:, :-1, :].copy()
    elif num_keypoints == num_keypoints_gt:
        gt_kpts = gt_kpts.copy()
    else:
        raise ValueError("Shape mismatch between ground truth and predictions")

    valid_gt = np.any(gt_kpts[..., 2] > 0, axis=1)
    valid_gt_indices = np.nonzero(valid_gt)[0]
    if len(valid_gt_indices) == 0:
        return np.arange(num_idv)

    valid_pred = np.any(pred_kpts[..., 2] > 0, axis=1)
    valid_pred_indices = np.nonzero(valid_pred)[0]
    if len(valid_pred_indices) == 0:
        return np.arange(num_idv)

    distance_matrix = np.full((len(valid_gt_indices), len(valid_pred_indices)), np.nan)
    for i, gt_idx in enumerate(valid_gt_indices):
        gt_idv = gt_kpts[gt_idx]
        mask = gt_idv[:, 2] > 0
        for j, pred_idx in enumerate(valid_pred_indices):
            pred_idv = pred_kpts[pred_idx]
            d = (gt_idv[mask, :2] - pred_idv[mask, :2]) ** 2
            if np.any(~np.isnan(d)):
                distance_matrix[i, j] = np.nanmean(d)

    if np.all(np.isnan(distance_matrix)):
        return np.arange(num_idv)

    # np.inf and np.nan in linear_sum_assigment raises error; so when a prediction
    # cannot be assigned to a ground truth (e.g. with PAFs, where predicted bodyparts
    # can be NaN) set the distance to a distance greater than the maximum distance
    max_dist = np.nanmax(distance_matrix)
    distance_matrix = np.nan_to_num(distance_matrix, nan=100 * max_dist)
    _, col_ind = linear_sum_assignment(distance_matrix)  # len == len(valid_gt_indices)

    gt_idx_to_pred_idx = {
        valid_gt_indices[valid_gt_index]: valid_pred_indices[valid_pred_index]
        for valid_gt_index, valid_pred_index in enumerate(col_ind)
    }
    matched_pred = {valid_pred_indices[i] for i in col_ind}
    unmatched_pred = [i for i in range(num_idv) if i not in matched_pred]
    next_unmatched = 0
    col_ind = []
    for gt_index in range(num_idv):
        if gt_index in gt_idx_to_pred_idx:
            col_ind.append(gt_idx_to_pred_idx[gt_index])
        else:
            col_ind.append(unmatched_pred[next_unmatched])
            next_unmatched += 1

    return np.array(col_ind)


def oks_match_prediction_to_gt(
    pred_kpts: np.array, gt_kpts: np.array, individual_names: list
) -> np.array:
    """Summary:
    Hungarian algorithm predicted individuals to ground truth ones, using object keypoint similarity (oks).
    Oks measures the accuracy of predicted keypoints compared to ground truth keypoints.
    More information about oks can be found in cocodataset (https://cocodataset.org/#keypoints-eval).

    Args:
        pred_kpts: Predicted keypoints for each animal. The shape of the array is (num_animals, num_keypoints, 3):
            num_animals: Number of animals.
            num_keypoints: Number of keypoints.
            3: (x, y, score) coordinates of each keypoint.
        gt_kpts: Ground truth keypoints for each animal. The shape of the array is (num_animals, num_keypoints(+1 if with center), 2):
            num_animals: Number of animals.
            num_keypoints: Number of keypoints.
        individual_names: names of individuals

    Returns:
        col_ind: Array of the individual indexes for prediction.

    Examples:
        input:
            pred_kpts = np.array(...)
            gt_kpts = np.array(...)
            individual_names = [...]
        output:
            col_ind = np.array([...])
    """

    num_animals, num_keypoints, _ = pred_kpts.shape
    if num_keypoints + 1 == gt_kpts.shape[1]:
        gt_kpts_without_ctr = gt_kpts[:, :-1, :].copy()
    elif num_keypoints == gt_kpts.shape[1]:
        gt_kpts_without_ctr = gt_kpts.copy()
    else:
        raise ValueError("Shape mismatch between ground truth and predictions")

    # Computation of the number of annotated animals in the ground truth
    num_animals_gt = num_animals
    for animal_index in range(num_animals):
        if (gt_kpts_without_ctr[animal_index] < 0).all():
            num_animals_gt -= 1

    oks_matrix = np.zeros((num_animals_gt, num_animals))
    gt_kpts_without_ctr[
        gt_kpts_without_ctr < 0
    ] = np.nan  # non visible keypoints should be nan to use calc_oks
    idx_gt = -1
    for g in range(num_animals):
        if np.isnan(gt_kpts_without_ctr[g]).all():
            continue
        else:
            idx_gt += 1
        for p in range(num_animals):
            oks_matrix[idx_gt, p] = calc_object_keypoint_similarity(
                pred_kpts[p, :, :2],
                gt_kpts_without_ctr[g],
                0.1,
                margin=0,
                symmetric_kpts=None,  # TODO take into account symmetric keypoints
            )

    row_ind, col_ind = linear_sum_assignment(oks_matrix, maximize=True)
    # if animals are missing in the frame, the predictions corresponding to nothing are not shuffled
    col_ind = extend_col_ind(col_ind, num_animals)

    return col_ind


def extend_col_ind(col_ind: np.array, num_animals: int) -> np.array:
    """Summary:
    Extends the column indices of a 1D array, col_ind, by adding any missing column indices from 0 to num_animals-1.

    Args:
        col_ind: 1D array of column indices
        num_animals: total number of animals

    Returns:
        extended_array: extended 1D array of column indices

    Examples:
        input:
            col_ind =
            num_animals = 5
        output:
            extended_array =
    """
    existing_cols = set(col_ind)  # Convert the array to a set for faster lookup
    missing_cols = [num for num in range(num_animals) if num not in existing_cols]
    extended_array = np.concatenate((col_ind, missing_cols)).astype(int)
    return extended_array


--- File: deeplabcut/pose_estimation_pytorch/post_processing/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.post_processing.match_predictions_to_gt import (
    oks_match_prediction_to_gt,
    rmse_match_prediction_to_gt,
)


--- File: deeplabcut/pose_estimation_pytorch/post_processing/identity.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Functions to assign identity to predictions from an identity head"""
from __future__ import annotations

import numpy as np
from scipy.optimize import linear_sum_assignment


def assign_identity(
    predictions: np.ndarray, identity_scores: np.ndarray
) -> np.ndarray:
    """
    Args:
        predictions: Pose predictions for an image, with shape (num_individuals,
            num_bodyparts, 3)
        identity_scores: Identity predictions for keypoints in an image, of shape
            (num_individuals, num_bodyparts, num_individuals).

    Returns:
        The ordering to use to match predictions to identities.
    """
    if not len(predictions) == len(identity_scores):
        raise ValueError(
            "There are not the same number of predictions as identity scores"
            f" ({len(predictions)} != {len(identity_scores)}"
        )

    # average of ID scores, weighted by keypoint confidence
    pose_conf = predictions[:, :, 2:3]
    cost_matrix = np.mean(pose_conf * identity_scores, axis=1)

    row_ind, col_ind = linear_sum_assignment(cost_matrix, maximize=True)
    new_order = np.zeros_like(row_ind)
    for old_pos, new_pos in zip(row_ind, col_ind):
        new_order[new_pos] = old_pos

    return new_order


--- File: deeplabcut/pose_estimation_pytorch/apis/tracklets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import pickle
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Union

import numpy as np
import pandas as pd
from scipy.optimize import linear_sum_assignment
from scipy.special import softmax
from tqdm import tqdm

import deeplabcut.utils.auxiliaryfunctions as auxiliaryfunctions
import deeplabcut.utils.auxfun_multianimal as auxfun_multianimal
from deeplabcut.core import trackingutils
from deeplabcut.core.engine import Engine
from deeplabcut.core.inferenceutils import Assembly
from deeplabcut.pose_estimation_pytorch.apis.utils import (
    get_scorer_name,
    list_videos_in_folder,
)


def convert_detections2tracklets(
    config: str,
    videos: Union[str, List[str]],
    videotype: Optional[str] = None,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    overwrite: bool = False,
    destfolder: Optional[str] = None,
    ignore_bodyparts: Optional[List[str]] = None,
    inferencecfg: Optional[dict] = None,
    modelprefix="",
    greedy: bool = False,  # TODO(niels): implement greedy assembly during video analysis
    calibrate: bool = False,  # TODO(niels): implement assembly calibration during video analysis
    window_size: int = 0,  # TODO(niels): implement window size selection for assembly during video analysis
    identity_only=False,
    track_method="",
):
    """TODO: Documentation, clean & remove code duplication (with analyze video)"""
    cfg = auxiliaryfunctions.read_config(config)
    inference_cfg = inferencecfg
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)

    if len(cfg["multianimalbodyparts"]) == 1 and track_method != "box":
        warnings.warn("Switching to `box` tracker for single point tracking...")
        track_method = "box"
        cfg["default_track_method"] = track_method
        auxiliaryfunctions.write_config(config, cfg)

    train_fraction = cfg["TrainingFraction"][trainingsetindex]
    start_path = os.getcwd()  # record cwd to return to this directory in the end

    # TODO: add cropping as in video analysis!
    # if cropping is not None:
    #    cfg['cropping']=True
    #    cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping
    #    print("Overwriting cropping parameters:", cropping)
    #    print("These are used for all videos, but won't be save to the cfg file.")

    rel_model_dir = auxiliaryfunctions.get_model_folder(
        train_fraction,
        shuffle,
        cfg,
        modelprefix=modelprefix,
        engine=Engine.PYTORCH,
    )
    model_dir = Path(cfg["project_path"]) / rel_model_dir
    path_test_config = model_dir / "test" / "pose_cfg.yaml"
    dlc_cfg = auxiliaryfunctions.read_plainconfig(str(path_test_config))

    if "multi-animal" not in dlc_cfg["dataset_type"]:
        raise ValueError("This function is only required for multianimal projects!")

    if inference_cfg is None:
        inference_cfg = auxfun_multianimal.read_inferencecfg(
            model_dir / "test" / "inference_cfg.yaml", cfg
        )
    auxfun_multianimal.check_inferencecfg_sanity(cfg, inference_cfg)

    if len(cfg["multianimalbodyparts"]) == 1 and track_method != "box":
        warnings.warn("Switching to `box` tracker for single point tracking...")
        track_method = "box"
        # Also ensure `boundingboxslack` is greater than zero, otherwise overlap
        # between trackers cannot be evaluated, resulting in empty tracklets.
        inference_cfg["boundingboxslack"] = max(inference_cfg["boundingboxslack"], 40)

    dlc_scorer = get_scorer_name(
        cfg,
        shuffle,
        train_fraction,
        snapshot_index=None,
        detector_index=None,
        modelprefix=modelprefix,
    )

    videos = list_videos_in_folder(videos, videotype)
    if len(videos) == 0:
        print(f"No videos were found in {videos}")
        return

    for video in videos:
        print("Processing... ", video)
        if destfolder is None:
            output_path = video.parent
        else:
            output_path = Path(destfolder)
            output_path.mkdir(exist_ok=True, parents=True)

        video_name = video.stem

        data_prefix = video_name + dlc_scorer
        data_filename = output_path / (data_prefix + ".h5")
        print(f"Loading From {data_filename}")
        data, metadata = auxfun_multianimal.LoadFullMultiAnimalData(str(data_filename))
        if track_method == "ellipse":
            method = "el"
        elif track_method == "box":
            method = "bx"
        else:
            method = "sk"

        track_filename = output_path / (data_prefix + f"_{method}.pickle")
        if not overwrite and track_filename.exists():
            # TODO: check if metadata are identical (same parameters!)
            print(f"Tracklets already computed at {track_filename}")
            print("Set overwrite = True to overwrite.")
        else:
            assemblies_path = data_filename.with_stem(
                data_filename.stem + "_assemblies"
            ).with_suffix(".pickle")
            if not assemblies_path.exists():
                raise FileNotFoundError(
                    f"Could not find the assembles file {assemblies_path}. You're "
                    f"converting detections to tracklets using PyTorch, which "
                    "means the assemblies file must be created by the model when "
                    "analyzing the video!"
                )
            assemblies_data = auxiliaryfunctions.read_pickle(assemblies_path)

            tracklets = build_tracklets(
                assemblies_data=assemblies_data,
                track_method=track_method,
                inference_cfg=inference_cfg,
                joints=data["metadata"]["all_joints_names"],
                scorer=metadata["data"]["Scorer"],
                num_frames=data["metadata"]["nframes"],
                ignore_bodyparts=ignore_bodyparts,
                unique_bodyparts=cfg["uniquebodyparts"],
                identity_only=identity_only
            )

            with open(track_filename, "wb") as f:
                pickle.dump(tracklets, f, pickle.HIGHEST_PROTOCOL)

    os.chdir(str(start_path))
    print(
        "The tracklets were created (i.e., under the hood "
        "deeplabcut.convert_detections2tracklets was run). Now you can "
        "'refine_tracklets' in the GUI, or run 'deeplabcut.stitch_tracklets'."
    )


def build_tracklets(
    assemblies_data: dict,
    track_method: str,
    inference_cfg: dict,
    joints: list[str],
    scorer: str,
    num_frames: int,
    ignore_bodyparts: list[str]|None = None,
    unique_bodyparts: list|None = None,
    identity_only: bool = False
) -> dict :

    if track_method == "box":
        mot_tracker = trackingutils.SORTBox(
            inference_cfg["max_age"],
            inference_cfg["min_hits"],
            inference_cfg.get("iou_threshold", 0.3),
        )
    elif track_method == "skeleton":
        mot_tracker = trackingutils.SORTSkeleton(
            len(joints),
            inference_cfg["max_age"],
            inference_cfg["min_hits"],
            inference_cfg.get("oks_threshold", 0.5),
        )
    else:
        mot_tracker = trackingutils.SORTEllipse(
            inference_cfg.get("max_age", 1),
            inference_cfg.get("min_hits", 1),
            inference_cfg.get("iou_threshold", 0.6),
        )

    tracklets = {}

    df_index = _create_tracklets_header(joints, scorer)
    tracklets["header"] = df_index

    # Initialize storage of the 'single' individual track
    if unique_bodyparts:
        tracklets["single"] = {}
        _single = {}
        for index in range(num_frames):
            single_detection = assemblies_data["single"].get(index)
            if single_detection is None:
                continue
            _single[index] = np.asarray(single_detection)
        tracklets["single"].update(_single)

    pcutoff = inference_cfg.get("pcutoff")
    if inference_cfg["topktoretain"] == 1:
        tracklets[0] = {}
        for index in tqdm(range(num_frames)):
            assemblies = assemblies_data.get(index)
            if assemblies is None or len(assemblies) == 0:
                continue

            assembly = np.asarray(assemblies[0].data)
            assembly[assembly[..., 2] < pcutoff] = np.nan
            tracklets[0][index] = assembly
    else:
        multi_bpts = list(set(joints).difference(unique_bodyparts or []))
        keep = set(multi_bpts).difference(ignore_bodyparts or [])
        keep_inds = sorted(multi_bpts.index(bpt) for bpt in keep)
        for index in tqdm(range(num_frames)):
            assemblies = assemblies_data.get(index)
            if assemblies is None or len(assemblies) == 0:
                continue

            animals = np.stack([a for a in assemblies])
            animals[np.any(animals[..., :3] < 0, axis=-1), :2] = np.nan
            animals[animals[..., 2] < pcutoff, :2] = np.nan
            animal_mask = ~np.all(np.isnan(animals[:, :, :2]), axis=(1, 2))
            if ~np.any(animal_mask):
                continue
            animals = animals[animal_mask]

            if identity_only:
                # Optimal identity assignment based on soft voting
                mat = np.zeros((len(animals), inference_cfg["topktoretain"]))
                for row, animal_pose in enumerate(animals):
                    animal_pose = animal_pose[
                        ~np.isnan(animal_pose).any(axis=1)
                    ]
                    unique_ids, idx = np.unique(
                        animal_pose[:, 3], return_inverse=True
                    )
                    total_scores = np.bincount(idx, weights=animal_pose[:, 2])
                    softmax_id_scores = softmax(total_scores)
                    for pred_id, softmax_score in zip(
                            unique_ids.astype(int), softmax_id_scores
                    ):
                        mat[row, pred_id] = softmax_score

                inds = linear_sum_assignment(mat, maximize=True)
                trackers = np.c_[inds][:, ::-1]
            else:
                if track_method == "box":
                    xy = trackingutils.calc_bboxes_from_keypoints(
                        animals[:, keep_inds], inference_cfg["boundingboxslack"]
                    )  # TODO: get cropping parameters and utilize!
                else:
                    xy = animals[:, keep_inds, :2]
                trackers = mot_tracker.track(xy)

            strwidth = int(np.ceil(np.log10(num_frames)))
            imname = "frame" + str(index).zfill(strwidth)
            trackingutils.fill_tracklets(tracklets, trackers, animals, imname)

    return tracklets


def _create_tracklets_header(joints, dlc_scorer):
    bodypart_labels = [bpt for bpt in joints for _ in range(3)]
    scorers = len(bodypart_labels) * [dlc_scorer]
    xyl_value = int(len(bodypart_labels) / 3) * ["x", "y", "likelihood"]
    return pd.MultiIndex.from_arrays(
        np.vstack([scorers, bodypart_labels, xyl_value]),
        names=["scorer", "bodyparts", "coords"],
    )


def _conv_predictions_to_assemblies(
    image_names: List[str], predictions: Dict[str, np.ndarray]
) -> Dict[int, List[Assembly]]:
    """
    Converts predictions to an assemblies dictionary
    predictions shape (num_animals, num_keypoints, 2 or 3)
    """
    assemblies = {}
    if len(predictions) == 0:
        return assemblies

    for image_index, image_name in enumerate(image_names):
        frame_predictions = predictions.get(image_name)
        if frame_predictions is not None:
            num_kpts, num_animals, pred_shape = frame_predictions.shape
            kpt_lst = []
            for i in range(num_animals):
                animal_prediction = frame_predictions[:, i, :]
                ass_prediction = np.ones((num_kpts, 4), dtype=frame_predictions.dtype)
                ass_prediction[:, 3] = -ass_prediction[:, 3]
                ass_prediction[:, :pred_shape] = animal_prediction.copy()
                ass = Assembly.from_array(ass_prediction)
                if len(ass) > 0:
                    kpt_lst.append(ass)

            assemblies[image_index] = kpt_lst

    return assemblies


--- File: deeplabcut/pose_estimation_pytorch/apis/tracking_dataset.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Code to create tracking datasets for ReID model training"""
from pathlib import Path

from tqdm import tqdm

import deeplabcut.pose_estimation_pytorch.apis.utils as utils
import deeplabcut.pose_estimation_pytorch.data as data
import deeplabcut.pose_estimation_pytorch.data.postprocessor as postprocessing
import deeplabcut.pose_estimation_pytorch.models as models
import deeplabcut.pose_estimation_pytorch.runners as runners
import deeplabcut.pose_estimation_pytorch.runners.shelving as shelving
from deeplabcut.core.config import read_config_as_dict
from deeplabcut.pose_estimation_pytorch.apis.videos import VideoIterator
from deeplabcut.pose_estimation_pytorch.task import Task
from deeplabcut.pose_tracking_pytorch import create_triplets_dataset


def build_feature_extraction_runner(
    loader: data.Loader,
    snapshot_path: str | Path,
    device: str,
    batch_size: int = 1,
) -> runners.PoseInferenceRunner:
    """Builds a runner to extract backbone features for poses of individuals

    Args:
        loader: The loader for the model to use.
        snapshot_path: The path of the snapshot to use.
        device: The device on which to run pose estimation.
        batch_size: The batch size to run pose estimation with.

    Returns:
        A PoseInferenceRunner that will return features for extracted pose.
    """
    num_features = loader.model_cfg["model"]["backbone_output_channels"]
    num_bodyparts = len(loader.model_cfg["metadata"]["bodyparts"])
    top_down = loader.pose_task != Task.BOTTOM_UP
    rescale_mode = postprocessing.RescaleAndOffset.Mode.KEYPOINT
    if top_down:
        rescale_mode = postprocessing.RescaleAndOffset.Mode.KEYPOINT_TD
        data_cfg = loader.model_cfg["data"]["inference"]
        crop_cfg = data_cfg.get("top_down_crop", {})
        width, height = crop_cfg.get("width", 256), crop_cfg.get("height", 256)
        preprocessor = data.build_top_down_preprocessor(
            color_mode=loader.model_cfg["data"]["colormode"],
            transform=data.build_transforms(data_cfg),
            top_down_crop_size=(width, height),
            top_down_crop_margin=crop_cfg.get("margin", 0),
        )
    else:
        preprocessor = data.build_bottom_up_preprocessor(
            loader.model_cfg["data"]["colormode"],
            data.build_transforms(loader.model_cfg["data"]["inference"])
        )

    postprocessor = postprocessing.ComposePostprocessor(
        [
            postprocessing.PrepareBackboneFeatures(top_down=top_down),
            postprocessing.ConcatenateOutputs(
                keys_to_concatenate={
                    "bodyparts": ("bodypart", "poses"),
                    "features": ("backbone", "bodypart_features"),
                },
                empty_shapes={
                    "bodyparts": (num_bodyparts, 3),
                    "features": (num_bodyparts, num_features),
                },
                create_empty_outputs=True,
            ),
            postprocessing.RescaleAndOffset(["bodyparts"], rescale_mode),
        ]
    )

    runner = runners.build_inference_runner(
        task=loader.pose_task,
        model=models.PoseModel.build(loader.model_cfg["model"]),
        device=device,
        snapshot_path=snapshot_path,
        batch_size=batch_size,
        preprocessor=preprocessor,
        postprocessor=postprocessor,
        load_weights_only=loader.model_cfg["runner"].get("load_weights_only", None),
    )
    assert isinstance(runner, runners.PoseInferenceRunner), (
        f"Failed to build inference runner: got type {type(runner)}"
    )

    # Set the model to output backbone features
    runner.model.output_features = True

    return runner


def extract_features_for_video(
    runner: runners.PoseInferenceRunner,
    video: VideoIterator,
    shelf_writer: shelving.FeatureShelfWriter,
    detector_runner: runners.DetectorInferenceRunner | None = None,
) -> None:
    """Extracts backbone features for predicted keypoints in a video.

    Args:
        video: The video for which to extract backbone features.
        runner: The inference runner with which to extract backbone features.
        shelf_writer: The ShelfWriter used to extract features.
        detector_runner: For top-down models, the detector to use to predict bboxes.
    """
    if detector_runner is not None:
        print(f"Running detector with batch size {detector_runner.batch_size}")
        bbox_predictions = detector_runner.inference(images=tqdm(video))
        video.set_context(bbox_predictions)

    shelf_writer.open()
    runner.inference(tqdm(video), shelf_writer=shelf_writer)
    shelf_writer.close()


def create_tracking_dataset(
    config: str,
    videos: list[str] | list[Path],
    track_method: str,
    videotype: str = "",
    shuffle: int = 1,
    trainingsetindex: int = 0,
    destfolder: str | None = None,
    batch_size: int | None = None,
    detector_batch_size: int | None = None,
    cropping: list[int] | None = None,
    modelprefix: str = "",
    robust_nframes: bool = False,
    n_triplets: int = 1000,
) -> str:
    """Creates a tracking dataset to train a ReID tracklet stitcher.

    Args:
        config: Full path of the config.yaml file for the project
        videos: A str (or list of strings) containing the full paths to videos from
            which to create the tracking dataset or a path to the directory, where all
            the videos with same extension are stored.
        track_method: Specifies the tracker used to generate the pose estimation data.
            Must be either 'box', 'skeleton', or 'ellipse'.
        videotype: Checks for the extension of the video in case the input to the video
            is a directory. Only videos with this extension are analyzed. If left
            unspecified, keeps videos with extensions ('avi', 'mp4', 'mov', 'mpeg',
            'mkv').
        shuffle: An integer specifying the shuffle index of the training dataset used
            for training the network.
        trainingsetindex: Integer specifying which TrainingsetFraction to use.
        destfolder: Specifies the destination folder for the tracking data. If ``None``,
            the path of the video is used. Note that for subsequent analysis this
            folder also needs to be passed.
        batch_size: The batch size to use for inference. Takes the value from the
            project config as a default.
        detector_batch_size: The batch size to use for detector inference. Takes the
            value from the project config as a default.
        cropping: List of cropping coordinates as [x1, x2, y1, y2]. Note that the same
            cropping parameters will then be used for all videos. If different video
            crops are desired, run ``analyze_videos`` on individual videos with the
            corresponding cropping coordinates.
        modelprefix: Directory containing the deeplabcut models to use when evaluating
            the network. By default, they are assumed to exist in the project folder.
        robust_nframes: Evaluate a video's number of frames in a robust manner. This
            option is slower (as the whole video is read frame-by-frame), but does not
            rely on metadata, hence its robustness against file corruption.
        n_triplets: The number of triplets to extract for the dataset.

    Returns:
        The scorer used to analyze the videos.
    """
    loader = data.DLCLoader(
        config,
        trainset_index=trainingsetindex,
        shuffle=shuffle,
        modelprefix=modelprefix,
    )
    test_cfg_path = loader.model_folder.parent / "test" / "pose_cfg.yaml"
    test_cfg = read_config_as_dict(test_cfg_path)

    snapshot_index, detector_snapshot_index = utils.parse_snapshot_index_for_analysis(
        loader.project_cfg, loader.model_cfg, None, None,
    )
    snapshot = utils.get_model_snapshots(
        snapshot_index, loader.model_folder, loader.pose_task,
    )[0]

    if cropping is None and loader.project_cfg.get("cropping", False):
        cropping = (
            loader.project_cfg["x1"],
            loader.project_cfg["x2"],
            loader.project_cfg["y1"],
            loader.project_cfg["y2"],
        )

    output_folder = None
    if destfolder is not None and destfolder != "":
        output_folder = Path(destfolder)

    if batch_size is None:
        batch_size = loader.project_cfg["batch_size"]

    device = utils.resolve_device(loader.model_cfg)
    runner = build_feature_extraction_runner(
        loader, snapshot.path, device, batch_size=batch_size
    )

    detector_runner = None
    detector_snapshot = None
    if loader.pose_task == Task.TOP_DOWN:
        if detector_batch_size is None:
            detector_batch_size = loader.project_cfg.get("detector_batch_size", 1)

        detector_snapshot = utils.get_model_snapshots(
            detector_snapshot_index, loader.model_folder, Task.DETECT,
        )[0]
        detector_runner = utils.get_detector_inference_runner(
            model_config=loader.model_cfg,
            snapshot_path=detector_snapshot.path,
            batch_size=detector_batch_size,
            device=device,
        )

    dlc_scorer = utils.get_scorer_name(
        loader.project_cfg,
        shuffle,
        loader.train_fraction,
        snapshot_uid=utils.get_scorer_uid(snapshot, detector_snapshot),
        modelprefix=modelprefix,
    )

    videos = utils.list_videos_in_folder(videos, videotype)
    for video_path in videos:
        print(f"Loading {video_path}")
        video = VideoIterator(video_path, cropping=cropping)

        nx, ny = video.dimensions
        nframes = video.get_n_frames(robust=robust_nframes)
        duration = video.calc_duration(robust=robust_nframes)
        fps = video.fps
        if robust_nframes:
            fps = nframes / duration

        print(f"Duration of video [s]: {duration:.2f}, recorded with {fps:.2f} fps!")
        print(f"Overall # of frames: {nframes} found with (before cropping)")
        print(f"Frame dimensions: {nx} x {ny}")

        if output_folder is None:
            output_folder = Path(video.video_path).parent
        output_folder.mkdir(parents=True, exist_ok=True)
        output_prefix = Path(video_path).stem + dlc_scorer
        output_filepath = output_folder / f"{output_prefix}_bpt_features.pickle"

        shelf_writer = shelving.FeatureShelfWriter(
            test_cfg,
            output_filepath,
            num_frames=video.get_n_frames(robust=robust_nframes),
        )
        extract_features_for_video(
            runner, video, shelf_writer, detector_runner=detector_runner
        )

    create_triplets_dataset(
        videos,
        dlc_scorer,
        track_method,
        n_triplets=n_triplets,
        destfolder=destfolder,
    )
    return dlc_scorer


--- File: deeplabcut/pose_estimation_pytorch/apis/evaluation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import argparse
from pathlib import Path
from typing import Iterable

import albumentations as A
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from tqdm import tqdm

import deeplabcut.core.metrics as metrics
import deeplabcut.pose_estimation_pytorch.apis.prune_paf_graph as prune_paf_graph
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.pose_estimation_pytorch import utils
from deeplabcut.pose_estimation_pytorch.apis.utils import (
    build_predictions_dataframe,
    ensure_multianimal_df_format,
    get_inference_runners,
    get_model_snapshots,
    get_scorer_name,
    get_scorer_uid,
    build_bboxes_dict_for_dataframe,
)
from deeplabcut.pose_estimation_pytorch.data import DLCLoader, Loader
from deeplabcut.pose_estimation_pytorch.data.dataset import PoseDatasetParameters
from deeplabcut.pose_estimation_pytorch.runners import InferenceRunner
from deeplabcut.pose_estimation_pytorch.runners.snapshots import Snapshot
from deeplabcut.pose_estimation_pytorch.task import Task
from deeplabcut.utils import auxfun_videos, auxiliaryfunctions
from deeplabcut.utils.visualization import (
    create_minimal_figure,
    erase_artists,
    get_cmap,
    make_multianimal_labeled_image,
    plot_evaluation_results,
    save_labeled_frame,
)


def predict(
    pose_runner: InferenceRunner,
    loader: Loader,
    mode: str,
    detector_runner: InferenceRunner | None = None,
) -> dict[str, dict[str, np.ndarray]]:
    """Predicts poses on data contained in a loader

    Args:
        pose_runner: The runner to use for pose estimation
        loader: The loader containing the data to predict poses on
        mode: {"train", "test"} The mode to predict on
        detector_runner: If the loader's `pose_task` is "TD", a detector runner can be
            given to detect individuals in the images. If no detector is given, ground
            truth bounding boxes will be used to crop individuals before pose estimation

    Returns:
        The paths of images for which predictions were computed mapping to the
        different predictions made by each model head
    """
    image_paths = loader.image_filenames(mode)
    context = None

    if loader.pose_task == Task.TOP_DOWN:
        # Get bounding boxes for context
        if detector_runner is not None:
            bbox_predictions = detector_runner.inference(images=tqdm(image_paths))
            context = bbox_predictions
        else:
            ground_truth_bboxes = loader.ground_truth_bboxes(mode=mode)
            context = [
                {"bboxes": ground_truth_bboxes[image]["bboxes"]}
                for image in image_paths
            ]

    images_with_context = image_paths
    if context is not None:
        if len(context) != len(image_paths):
            raise ValueError(
                f"Missing context for some images: {len(context)} != {len(image_paths)}"
            )
        images_with_context = list(zip(image_paths, context))

    predictions = pose_runner.inference(images=tqdm(images_with_context))
    return {
        image_path: image_predictions
        for image_path, image_predictions in zip(image_paths, predictions)
    }


def evaluate(
    pose_runner: InferenceRunner,
    loader: Loader,
    mode: str,
    detector_runner: InferenceRunner | None = None,
    parameters: PoseDatasetParameters | None = None,
    comparison_bodyparts: str | list[str] | None = None,
    per_keypoint_evaluation: bool = False,
    pcutoff: float | list[float] = 0.6,
) -> tuple[dict[str, float], dict[str, dict[str, np.ndarray]]]:
    """
    Args:
        pose_runner: The runner for pose estimation
        loader: The loader containing the data to evaluate
        mode: Either 'train' or 'test'
        detector_runner: If the loader's `pose_task` is "TD", a detector can be given to
            compute bounding boxes for pose estimation. If no detector is given, ground
            truth bounding boxes are used.
        parameters: PoseDatasetParameters to use. If None, the parameters will be
            obtained from the given Loader. This can be used to change the names of
            bodyparts, e.g. when a model is trained with memory replay.
        comparison_bodyparts: A subset of the bodyparts for which to compute the
            evaluation metrics. Passing "all" or None evaluates on all bodyparts.
        per_keypoint_evaluation: Compute the train and test RMSE for each keypoint, and
            save the results to a {model_name}-keypoint-results.csv in the
            evaluation-results-pytorch folder.
        pcutoff: Confidence threshold for RMSE computation. If a list is provided,
            there should be one value for each bodypart and one value for each unique
            bodypart (if there are any).

    Returns:
        A dict containing the evaluation results
        A dict mapping the paths of images for which predictions were computed to the
            different predictions made by each model head
    """
    predictions = predict(pose_runner, loader, mode, detector_runner=detector_runner)

    # For models trained with memory-replay from SuperAnimal, keep project bodyparts
    if weight_init_cfg := loader.model_cfg["train_settings"].get("weight_init"):
        weight_init = WeightInitialization.from_dict(weight_init_cfg)
        if weight_init.memory_replay:
            for _, pred in predictions.items():
                pred["bodyparts"] = pred["bodyparts"][:, weight_init.conversion_array]

    if parameters is None:
        parameters = loader.get_dataset_parameters()

    gt_pose = loader.ground_truth_keypoints(mode)
    pred_pose = {filename: pred["bodyparts"] for filename, pred in predictions.items()}
    kpt_idx = _get_keypoints_to_use(parameters.bodyparts, comparison_bodyparts)

    gt_unique, pred_unique, unique_idx = None, None, None
    if parameters.num_unique_bpts >= 1:
        gt_unique = loader.ground_truth_keypoints(mode, unique_bodypart=True)
        pred_unique = {
            filename: pred["unique_bodyparts"] for filename, pred in predictions.items()
        }
        unique_idx = _get_keypoints_to_use(parameters.unique_bpts, comparison_bodyparts)

    # When `comparison_bodyparts` is used, check that the bodyparts used for evaluation
    # make sense; If only unique bodyparts are being evaluated, set them as bodyparts
    if kpt_idx is not None and unique_idx is not None:
        if len(kpt_idx) == 0 and len(unique_idx) == 0:
            unique_err = ""
            if len(parameters.unique_bpts) > 0:
                unique_err = f" and the unique_bodyparts are {parameters.unique_bpts}"
            raise ValueError(
                f"No bodyparts left when comparison_bodyparts={comparison_bodyparts}! "
                f"The project bodyparts are {parameters.bodyparts}{unique_err}! Set "
                f"comparison_bodyparts to `None` or `'all'` to evaluate on all of them,"
                f" or select a subset of them to evaluate."
            )
        elif len(kpt_idx) == 0 and len(unique_idx) > 0:
            gt_pose, pred_pose, kpt_idx = gt_unique, pred_unique, unique_idx
            parameters = PoseDatasetParameters(
                bodyparts=parameters.unique_bpts,
                unique_bpts=[],
                individuals=["animal"],
            )
            gt_unique, pred_unique, unique_idx = None, None, None

    if kpt_idx is not None:
        gt_pose = {img: kpts[:, kpt_idx] for img, kpts in gt_pose.items()}
        pred_pose = {img: kpts[:, kpt_idx] for img, kpts in pred_pose.items()}

    if unique_idx is not None:
        gt_unique = {img: kpts[:, unique_idx] for img, kpts in gt_unique.items()}
        pred_unique = {img: kpts[:, unique_idx] for img, kpts in pred_unique.items()}

    bodyparts = _get_subset_bodyparts(parameters.bodyparts, comparison_bodyparts)
    unique_bpts = _get_subset_bodyparts(parameters.unique_bpts, comparison_bodyparts)
    _validate_pcutoff(bodyparts, unique_bpts, pcutoff)

    results = metrics.compute_metrics(
        gt_pose,
        pred_pose,
        single_animal=parameters.max_num_animals == 1,
        pcutoff=pcutoff,
        unique_bodypart_poses=pred_unique,
        unique_bodypart_gt=gt_unique,
        per_keypoint_rmse=per_keypoint_evaluation,
        compute_detection_rmse=False,
    )

    if loader.model_cfg["metadata"]["with_identity"]:
        pred_id_scores = {
            filename: pred["identity_scores"] for filename, pred in predictions.items()
        }
        id_scores = metrics.compute_identity_scores(
            individuals=parameters.individuals,
            bodyparts=parameters.bodyparts,
            predictions=pred_pose,
            identity_scores=pred_id_scores,
            ground_truth=gt_pose,
        )
        for name, score in id_scores.items():
            results[f"id_head_{name}"] = score

    # Updating poses to be aligned and padded
    for image, pose in pred_pose.items():
        predictions[image]["bodyparts"] = pose

    return results, predictions


def visualize_predictions(
    predictions: dict,
    ground_truth: dict,
    output_dir: str | Path | None = None,
    num_samples: int | None = None,
    random_select: bool = False,
    show_ground_truth: bool = True,
    plot_bboxes: bool = True,
) -> None:
    """Visualize model predictions alongside ground truth keypoints.

    This function processes keypoint predictions and ground truth data, applies
    visibility masks, and generates visualization plots. It supports random or
    sequential sampling of images for visualization.

    Args:
        predictions: Dictionary mapping image paths to prediction data.
            Each prediction contains:
            - bodyparts: array of shape [N, num_keypoints, 3] where 3 represents
                (x, y, confidence)
            - bboxes: array of shape [N, 4] for bounding boxes (optional)
            - bbox_scores: array of shape [N,] for bbox confidences (optional)
        ground_truth: Dictionary mapping image paths to ground truth keypoints.
            Each value has shape [N, num_keypoints, 3] where 3 represents
                (x, y, visibility)
        output_dir: Path to save visualization outputs.
            Defaults to "predictions_visualizations"
        num_samples: Number of images to visualize. If None, processes all images
        random_select: If True, randomly samples images; if False, uses first N images
        show_ground_truth: If True, displays ground truth poses alongside predictions.
            If False, only shows predictions but uses GT visibility mask
        plot_bboxes: If True and the model is a top-down model, predicted bboxes will
            be shown in the images as well
    """
    # Setup output directory
    output_dir = Path(output_dir or "predictions_visualizations")
    output_dir.mkdir(exist_ok=True)

    # Select images to process
    image_paths = list(predictions.keys())
    if num_samples and num_samples < len(image_paths):
        if random_select:
            image_paths = np.random.choice(
                image_paths, num_samples, replace=False
            ).tolist()
        else:
            image_paths = image_paths[:num_samples]

    # Process each selected image
    for image_path in image_paths:
        # Get prediction and ground truth data
        pred_data = predictions[image_path]
        gt_keypoints = ground_truth[image_path]  # Shape: [N, num_keypoints, 3]

        # Create visibility mask from first GT sample. This mask will be applied to all samples for consistency
        vis_mask = gt_keypoints[0, :, 2] > 0

        # Process ground truth keypoints if showing GT
        if show_ground_truth:
            visible_gt = []
            for gt in gt_keypoints:
                visible_points = gt[vis_mask, :2]  # Keep only x,y for visible joints
                visible_gt.append(visible_points)
            visible_gt = np.stack(visible_gt)  # Shape: [N, num_visible_joints, 2]
        else:
            visible_gt = None

        # Process predicted keypoints
        pred_keypoints = pred_data["bodyparts"]  # Shape: [N, num_keypoints, 3]
        visible_pred = []
        for pred in pred_keypoints:
            visible_points = pred[vis_mask]  # Keep only visible joint predictions
            visible_pred.append(visible_points)
        visible_pred = np.stack(visible_pred)  # Shape: [N, num_visible_joints, 3]

        if plot_bboxes:
            bboxes = predictions[image_path].get("bboxes", None)
            bbox_scores = predictions[image_path].get("bbox_scores", None)
            bounding_boxes = (
                (bboxes, bbox_scores)
                if bboxes is not None and bbox_scores is not None
                else None
            )
        else:
            bounding_boxes = None

        # Generate and save visualization
        try:
            plot_gt_and_predictions(
                image_path=image_path,
                output_dir=output_dir,
                gt_bodyparts=visible_gt,
                pred_bodyparts=visible_pred,
                bounding_boxes=bounding_boxes,
            )
            print(f"Successfully plotted predictions for {image_path}")
        except Exception as e:
            print(f"Error plotting predictions for {image_path}: {str(e)}")


def plot_gt_and_predictions(
    image_path: str | Path,
    output_dir: str | Path,
    gt_bodyparts: np.ndarray,
    pred_bodyparts: np.ndarray,
    gt_unique_bodyparts: np.ndarray | None = None,
    pred_unique_bodyparts: np.ndarray | None = None,
    mode: str = "bodypart",
    colormap: str = "rainbow",
    dot_size: int = 12,
    alpha_value: float = 0.7,
    p_cutoff: float | list[float] = 0.6,
    bounding_boxes: tuple[np.ndarray, np.ndarray] | None = None,
    bboxes_pcutoff: float = 0.6,
    bounding_boxes_color: str = "auto",
):
    """Plot ground truth and predictions on an image.

    Args:
        image_path: Path to the image
        gt_bodyparts: Ground truth keypoints array (num_animals, num_keypoints, 3)
        pred_bodyparts: Predicted keypoints array (num_animals, num_keypoints, 3)
        output_dir: Directory where labeled images will be saved
        gt_unique_bodyparts: Ground truth unique bodyparts if any
        pred_unique_bodyparts: Predicted unique bodyparts if any
        mode: How to color the points ("bodypart" or "individual")
        colormap: Matplotlib colormap name
        dot_size: Size of the plotted points
        alpha_value: Transparency of the points
        p_cutoff: Confidence threshold for showing predictions. If a list is provided,
            there should be one value for each bodypart and one value for each unique
            bodypart (if there are any).
        bounding_boxes:  bounding boxes (top-left corner, size) and their respective
            confidence levels,
        bboxes_pcutoff: bounding boxes confidence cutoff threshold.
        bounding_boxes_color: If plotting bounding boxes, this is the color that will be
            used for bounding boxes. If set to "auto" (default value):
                - if mode is "bodypart", the bbox color will be a default color
                - if mode is "individual", each individual's color will be used for its
                    bounding box
    """
    # Ensure output directory exists
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Read the image
    frame = auxfun_videos.imread(str(image_path), mode="skimage")
    num_pred, num_keypoints = pred_bodyparts.shape[:2]

    # Create figure and set dimensions
    fig, ax = create_minimal_figure()
    h, w, _ = np.shape(frame)
    fig.set_size_inches(w / 100, h / 100)
    ax.set_xlim(0, w)
    ax.set_ylim(0, h)
    ax.invert_yaxis()
    ax.imshow(frame, "gray")

    # Set up colors based on mode
    if mode == "bodypart":
        num_colors = num_keypoints
        if pred_unique_bodyparts is not None:
            num_colors += pred_unique_bodyparts.shape[1]
        colors = get_cmap(num_colors, name=colormap)

        predictions = pred_bodyparts.swapaxes(0, 1)
        ground_truth = gt_bodyparts.swapaxes(0, 1)
    elif mode == "individual":
        colors = get_cmap(num_pred + 1, name=colormap)
        predictions = pred_bodyparts
        ground_truth = gt_bodyparts
    else:
        raise ValueError(f"Invalid mode: {mode}")

    if bounding_boxes_color == "auto":
        if mode == "bodypart":
            bboxes_color = None
        elif mode == "individual":
            bboxes_color = get_cmap(num_pred + 1, name=colormap)
        else:
            raise ValueError(f"Invalid mode: {mode}")
    else:
        bboxes_color = bounding_boxes_color

    # Plot regular bodyparts
    ax = make_multianimal_labeled_image(
        frame,
        ground_truth,
        predictions[:, :, :2],
        predictions[:, :, 2:],
        colors,
        dot_size,
        alpha_value,
        p_cutoff,
        ax=ax,
        bounding_boxes=bounding_boxes,
        bboxes_cutoff=bboxes_pcutoff,
        bboxes_color=bboxes_color,
    )

    # Plot unique bodyparts if present
    if pred_unique_bodyparts is not None and gt_unique_bodyparts is not None:
        if mode == "bodypart":
            unique_predictions = pred_unique_bodyparts.swapaxes(0, 1)
            unique_ground_truth = gt_unique_bodyparts.swapaxes(0, 1)
        else:
            unique_predictions = pred_unique_bodyparts
            unique_ground_truth = gt_unique_bodyparts

        ax = make_multianimal_labeled_image(
            frame,
            unique_ground_truth,
            unique_predictions[:, :, :2],
            unique_predictions[:, :, 2:],
            colors[num_keypoints:],
            dot_size,
            alpha_value,
            p_cutoff,
            ax=ax,
        )

    # Save the labeled image
    save_labeled_frame(
        fig,
        str(image_path),
        str(output_dir),
        belongs_to_train=False,
    )
    erase_artists(ax)
    plt.close()


def evaluate_snapshot(
    cfg: dict,
    loader: DLCLoader,
    snapshot: Snapshot,
    scorer: str,
    transform: A.Compose | None = None,
    plotting: bool | str = False,
    show_errors: bool = True,
    comparison_bodyparts: str | list[str] | None = None,
    per_keypoint_evaluation: bool = False,
    detector_snapshot: Snapshot | None = None,
    pcutoff: float | list[float] | dict[str, float] | None = None,
) -> pd.DataFrame:
    """Evaluates a snapshot.
    The evaluation results are stored in the .h5 and .csv file under the subdirectory
    'evaluation_results'.

    Args:
        cfg: the content of the project's config file
        loader: the loader for the shuffle to evaluate
        snapshot: the snapshot to evaluate
        scorer: the scorer name to use for the snapshot
        transform: transformation pipeline for evaluation
            ** Should normalise the data the same way it was normalised during training **
        plotting: Plots the predictions on the train and test images. If provided it must
            be either ``True``, ``False``, ``"bodypart"``, or ``"individual"``. Setting
            to ``True`` defaults as ``"bodypart"`` for multi-animal projects.
        show_errors: whether to compare predictions and ground truth
        comparison_bodyparts: A subset of the bodyparts for which to compute the
            evaluation metrics.
        per_keypoint_evaluation: Compute the train and test RMSE for each keypoint, and
            save the results to a {model_name}-keypoint-results.csv in the
            evaluation-results-pytorch folder.
        detector_snapshot: Only for TD models. If defined, evaluation metrics are
            computed using the detections made by this snapshot
        pcutoff: The cutoff to use for computing evaluation metrics. When `None`, the
            cutoff will be loaded from the project config. If a list is provided, there
            should be one value for each bodypart and one value for each unique bodypart
            (if there are any). If a dict is provided, the keys should be bodyparts
            mapping to pcutoff values for each bodypart. Bodyparts that are not defined
            in the dict will have pcutoff set to 0.6.
    """
    head_type = loader.model_cfg["model"]["heads"]["bodypart"]["type"]
    if head_type == "DLCRNetHead":
        prune_paf_graph.benchmark_paf_graphs(
            loader=loader, snapshot_path=snapshot.path, verbose=False,
        )

    parameters = loader.get_dataset_parameters()

    detector_path = None
    if detector_snapshot is not None:
        detector_path = detector_snapshot.path

    pose_runner, detector_runner = get_inference_runners(
        model_config=loader.model_cfg,
        snapshot_path=snapshot.path,
        max_individuals=parameters.max_num_animals,
        num_bodyparts=parameters.num_joints,
        num_unique_bodyparts=parameters.num_unique_bpts,
        with_identity=loader.model_cfg["metadata"]["with_identity"],
        transform=transform,
        detector_path=detector_path,
    )

    # For memory-replay SuperAnimal models, convert bodyparts to project bodyparts
    if weight_init_cfg := loader.model_cfg["train_settings"].get("weight_init", None):
        weight_init = WeightInitialization.from_dict(weight_init_cfg)
        if weight_init.memory_replay:
            bodyparts = weight_init.bodyparts
            if bodyparts is None:
                bodyparts = auxiliaryfunctions.get_bodyparts(cfg)

            parameters = PoseDatasetParameters(
                bodyparts=bodyparts,
                unique_bpts=parameters.unique_bpts,
                individuals=parameters.individuals,
            )

    # get the names of bodyparts on which the model is evaluated
    eval_parameters = PoseDatasetParameters(
        bodyparts=_get_subset_bodyparts(parameters.bodyparts, comparison_bodyparts),
        unique_bpts=_get_subset_bodyparts(parameters.unique_bpts, comparison_bodyparts),
        individuals=parameters.individuals,
    )

    if pcutoff is None:
        pcutoff = cfg.get("pcutoff", 0.6)
    elif isinstance(pcutoff, dict):
        pcutoff = [
            pcutoff.get(bpt, 0.6)
            for bpt in eval_parameters.bodyparts + eval_parameters.unique_bpts
        ]
    _validate_pcutoff(parameters.bodyparts, parameters.unique_bpts, pcutoff)

    predictions = {}
    rmse_per_bodypart = {}
    bounding_boxes = {}
    scores = {
        "%Training dataset": loader.train_fraction,
        "Shuffle number": loader.shuffle,
        "Training epochs": snapshot.epochs,
        "Detector epochs (TD only)": (
            -1 if detector_snapshot is None else detector_snapshot.epochs
        ),
        "pcutoff": (
            ", ".join([str(v) for v in pcutoff])
            if isinstance(pcutoff, list) else pcutoff
        ),
    }
    for split in ["train", "test"]:
        results, predictions_for_split = evaluate(
            pose_runner=pose_runner,
            loader=loader,
            mode=split,
            pcutoff=pcutoff,
            detector_runner=detector_runner,
            comparison_bodyparts=comparison_bodyparts,
            per_keypoint_evaluation=per_keypoint_evaluation,
            parameters=parameters,
        )
        if per_keypoint_evaluation:
            rmse_per_bodypart[split] = _extract_rmse_per_bodypart(
                results,
                eval_parameters.bodyparts,
                eval_parameters.unique_bpts,
            )

        df_split_predictions = build_predictions_dataframe(
            scorer=scorer,
            predictions=predictions_for_split,
            parameters=eval_parameters,
            image_name_to_index=image_to_dlc_df_index,
        )
        split_bounding_boxes = build_bboxes_dict_for_dataframe(
            predictions=predictions_for_split,
            image_name_to_index=image_to_dlc_df_index,
        )
        predictions[split] = df_split_predictions
        bounding_boxes[split] = split_bounding_boxes
        for k, v in results.items():
            scores[f"{split} {k}"] = round(v, 2)

    results_filename = f"{scorer}.h5"
    df_predictions = pd.concat(predictions.values(), axis=0)
    df_predictions = df_predictions.reindex(loader.df.index)
    output_filename = loader.evaluation_folder / results_filename
    output_filename.parent.mkdir(parents=True, exist_ok=True)
    df_predictions.to_hdf(output_filename, key="df_with_missing")

    df_scores = pd.DataFrame([scores]).set_index(
        [
            "%Training dataset",
            "Shuffle number",
            "Training epochs",
            "Detector epochs (TD only)",
            "pcutoff",
        ]
    )
    scores_filepath = output_filename.with_suffix(".csv")
    scores_filepath = scores_filepath.with_stem(scores_filepath.stem + "-results")
    save_evaluation_results(df_scores, scores_filepath, show_errors, pcutoff)

    if per_keypoint_evaluation:
        rmse_per_bpt_path = output_filename.with_name(
            output_filename.stem + "-keypoint-results.csv"
        )
        save_rmse_per_bodypart(rmse_per_bodypart, rmse_per_bpt_path, show_errors)

    if plotting:
        folder_name = f"LabeledImages_{scorer}"
        folder_path = loader.evaluation_folder / folder_name
        folder_path.mkdir(parents=True, exist_ok=True)
        if isinstance(plotting, str):
            plot_mode = plotting
        else:
            plot_mode = "bodypart"

        df_ground_truth = ensure_multianimal_df_format(loader.df)

        bboxes_cutoff = (
            loader.model_cfg.get("detector", {})
            .get("model", {})
            .get("box_score_thresh", 0.6)
        )

        for mode in ["train", "test"]:
            df_combined = predictions[mode].merge(
                df_ground_truth, left_index=True, right_index=True
            )
            bboxes_split = bounding_boxes[mode]

            plot_evaluation_results(
                df_combined=df_combined,
                project_root=cfg["project_path"],
                scorer=cfg["scorer"],
                model_name=scorer,
                output_folder=str(folder_path),
                in_train_set=mode == "train",
                plot_unique_bodyparts=eval_parameters.num_unique_bpts > 0,
                mode=plot_mode,
                colormap=cfg["colormap"],
                dot_size=cfg["dotsize"],
                alpha_value=cfg["alphavalue"],
                p_cutoff=cfg["pcutoff"],
                bounding_boxes=bboxes_split,
                bboxes_cutoff=bboxes_cutoff,
            )

    return df_predictions


def evaluate_network(
    config: str | Path,
    shuffles: Iterable[int] = (1,),
    trainingsetindex: int | str = 0,
    snapshotindex: int | str | None = None,
    device: str | None = None,
    plotting: bool | str = False,
    show_errors: bool = True,
    transform: A.Compose = None,
    snapshots_to_evaluate: list[str] | None = None,
    comparison_bodyparts: str | list[str] | None = None,
    per_keypoint_evaluation: bool = False,
    modelprefix: str = "",
    detector_snapshot_index: int | None = None,
    pcutoff: float | list[float] | dict[str, float] | None = None,
) -> None:
    """Evaluates a snapshot.

    The evaluation results are stored in the .h5 and .csv file under the subdirectory
    'evaluation_results'.

    Args:
        config: path to the project's config file
        shuffles: Iterable of integers specifying the shuffle indices to evaluate.
        trainingsetindex: Integer specifying which training set fraction to use.
            Evaluates all fractions if set to "all"
        snapshotindex: index (starting at 0) of the snapshot we want to load. To
            evaluate the last one, use -1. To evaluate all snapshots, use "all". For
            example if we have 3 models saved
                - snapshot-0.pt
                - snapshot-50.pt
                - snapshot-100.pt
            and we want to evaluate snapshot-50.pt, snapshotindex should be 1. If None,
            the snapshotindex is loaded from the project configuration.
        device: the device to run evaluation on
        plotting: Plots the predictions on the train and test images. If provided it must
            be either ``True``, ``False``, ``"bodypart"``, or ``"individual"``. Setting
            to ``True`` defaults as ``"bodypart"`` for multi-animal projects.
        show_errors: display train and test errors.
        transform: transformation pipeline for evaluation
            ** Should normalise the data the same way it was normalised during training **
        snapshots_to_evaluate: List of snapshot names to evaluate (e.g. ["snapshot-50",
            "snapshot-75"]). If defined, `snapshotindex` will be ignored.
        comparison_bodyparts: A subset of the bodyparts for which to compute the
            evaluation metrics.
        per_keypoint_evaluation: Compute the train and test RMSE for each keypoint, and
            save the results to a {model_name}-keypoint-results.csv in the
            evaluation-results-pytorch folder.
        modelprefix: directory containing the deeplabcut models to use when evaluating
            the network. By default, they are assumed to exist in the project folder.
        detector_snapshot_index: Only for TD models. If defined, uses the detector with
            the given index for pose estimation.
        pcutoff: The cutoff to use for computing evaluation metrics. When `None`, the
            cutoff will be loaded from the project config. If a list is provided, there
            should be one value for each bodypart and one value for each unique bodypart
            (if there are any). If a dict is provided, the keys should be bodyparts
            mapping to pcutoff values for each bodypart. Bodyparts that are not defined
            in the dict will have pcutoff set to 0.6.

    Examples:
        If you want to evaluate on shuffle 1 without plotting predictions.

        >>> import deeplabcut
        >>> deeplabcut.evaluate_network(
        >>>     '/analysis/project/reaching-task/config.yaml', shuffles=[1],
        >>> )

        If you want to evaluate shuffles 0 and 1 and plot the predictions.

        >>> deeplabcut.evaluate_network(
        >>>     '/analysis/project/reaching-task/config.yaml',
        >>>     shuffles=[0, 1],
        >>>     plotting=True,
        >>> )

        If you want to plot assemblies for a maDLC project

        >>> deeplabcut.evaluate_network(
        >>>     '/analysis/project/reaching-task/config.yaml',
        >>>     shuffles=[1],
        >>>     plotting="individual",
        >>> )
    """
    cfg = auxiliaryfunctions.read_config(config)

    if isinstance(trainingsetindex, int):
        train_set_indices = [trainingsetindex]
    elif isinstance(trainingsetindex, str) and trainingsetindex.lower() == "all":
        train_set_indices = list(range(len(cfg["TrainingFraction"])))
    else:
        raise ValueError(f"Invalid trainingsetindex: {trainingsetindex}")

    if snapshotindex is None:
        snapshotindex = cfg["snapshotindex"]

    if detector_snapshot_index is None:
        detector_snapshot_index = cfg["detector_snapshotindex"]

    for train_set_index in train_set_indices:
        for shuffle in shuffles:
            loader = DLCLoader(
                config=config,
                shuffle=shuffle,
                trainset_index=train_set_index,
                modelprefix=modelprefix,
            )
            loader.evaluation_folder.mkdir(exist_ok=True, parents=True)

            if device is not None:
                loader.model_cfg["device"] = device
            loader.model_cfg["device"] = utils.resolve_device(loader.model_cfg)

            snapshots = get_model_snapshots(
                snapshotindex,
                model_folder=loader.model_folder,
                task=loader.pose_task,
                snapshot_filter=snapshots_to_evaluate,
            )

            detector_snapshots = [None]
            if loader.pose_task == Task.TOP_DOWN:
                if detector_snapshot_index is not None:
                    det_snapshots = get_model_snapshots(
                        "all", loader.model_folder, Task.DETECT
                    )
                    if len(det_snapshots) == 0:
                        print(
                            "The detector_snapshot_index was set to "
                            f"{detector_snapshot_index} but no detector snapshots were "
                            f"found in {loader.model_folder}. Using ground truth "
                            "bounding boxes to compute metrics.\n"
                            "To analyze videos with a top-down model, you'll need to "
                            "train a detector!"
                        )
                    else:
                        detector_snapshots = get_model_snapshots(
                            detector_snapshot_index,
                            loader.model_folder,
                            Task.DETECT,
                        )
                else:
                    print("Using GT bounding boxes to compute evaluation metrics")

            for detector_snapshot in detector_snapshots:
                for snapshot in snapshots:
                    scorer = get_scorer_name(
                        cfg=cfg,
                        shuffle=shuffle,
                        train_fraction=loader.train_fraction,
                        snapshot_uid=get_scorer_uid(snapshot, detector_snapshot),
                        modelprefix=modelprefix,
                    )
                    evaluate_snapshot(
                        loader=loader,
                        cfg=cfg,
                        scorer=scorer,
                        snapshot=snapshot,
                        transform=transform,
                        plotting=plotting,
                        show_errors=show_errors,
                        comparison_bodyparts=comparison_bodyparts,
                        per_keypoint_evaluation=per_keypoint_evaluation,
                        detector_snapshot=detector_snapshot,
                        pcutoff=pcutoff,
                    )


def image_to_dlc_df_index(image: str) -> tuple[str, ...]:
    """
    Args:
        image: the path of the image to map to a DLC index

    Returns:
        the image index to create a multi-animal DLC dataframe:
            ("labeled-data", video_name, image_name)
    """
    image_path = Path(image)
    if len(image_path.parts) >= 3 and image_path.parts[-3] == "labeled-data":
        return Path(image_path).parts[-3:]

    raise ValueError(f"Unexpected image filepath for a DLC project")


def save_evaluation_results(
    df_scores: pd.DataFrame, scores_path: Path, print_results: bool, pcutoff: float
) -> None:
    """
    Saves the evaluation results to a CSV file. Adds the evaluation results for the
    model to the combined results file, or creates it if it does not yet exist.

    Args:
        df_scores: the scores dataframe for a snapshot
        scores_path: the path where the model scores CSV should be saved
        print_results: whether to print evaluation results to the console
        pcutoff: the pcutoff used to get the evaluation results
    """
    if print_results:
        print(f"Evaluation results for {scores_path.name} (pcutoff: {pcutoff}):")
        print(df_scores.iloc[0])

    # Save scores file
    df_scores.to_csv(scores_path)

    # Update combined results
    combined_scores_path = scores_path.parent.parent / "CombinedEvaluation-results.csv"
    if combined_scores_path.exists():
        df_existing_results = pd.read_csv(
            combined_scores_path, index_col=[0, 1, 2, 3, 4]
        )
        df_scores = df_scores.combine_first(df_existing_results)

    df_scores = df_scores.sort_index()
    df_scores.to_csv(combined_scores_path)


def save_rmse_per_bodypart(
    rmse_per_bodypart: dict[str, dict[str, float]],
    output_path: Path,
    print_results: bool,
) -> None:
    """
    Saves the evaluation results per bodypart to a CSV file.

    Args:
        rmse_per_bodypart: The scores dataframe for a snapshot
        output_path: The path of the file where
        print_results: Whether to print results to the console
    """
    index, data = [], []
    if print_results:
        print(f"Per-bodypart evaluation results ({output_path.stem}):")

    for split, rmse_results in rmse_per_bodypart.items():
        key = split.capitalize() + " error (px)"
        index.append(key)
        data.append(rmse_results)

        if print_results:
            print(f"  {key}")
            bpt_key_length = max([len(k) for k in rmse_results.keys()]) + 4
            for k, v in rmse_results.items():
                key = (k + ":").ljust(bpt_key_length)
                print(f"    {key}{v:3>.2f}px")

    # Save scores file
    df_rmse_per_bodypart = pd.DataFrame(data, index=index)
    df_rmse_per_bodypart.to_csv(output_path)


def _validate_pcutoff(
    bodyparts: list[str],
    unique_bpts: list[str],
    pcutoff: float | list[float],
) -> None:
    """Checks that the given `pcutoff` value has the correct number of elements"""
    if isinstance(pcutoff, (int, float)):
        return

    total_bodyparts = len(bodyparts) + len(unique_bpts)
    if len(pcutoff) != total_bodyparts:
        raise ValueError(
            "When passing the pcutoff as a list, the length of the list should be "
            "equal to the number of bodyparts and the number of unique bpts. "
            f"Found a list containing {len(pcutoff)} elements, but there are "
            f"{total_bodyparts} total bodyparts, which are {bodyparts + unique_bpts}."
        )


def _get_keypoints_to_use(
    bodyparts: list[str],
    bodypart_subset: str | list[str] | None,
) -> list[int] | None:
    """Computes the indices of the keypoints indices to keep based on the given subset.

    Args:
        bodyparts: The bodyparts predicted by the model.
        bodypart_subset: The subset of bodyparts to keep. If None or "all", all
            bodyparts are kept.

    Returns:
        None if all bodyparts should be kept, or bodyparts is an empty list. Otherwise,
        returns a list containing the indices of the bodyparts to keep. If no bodyparts
        should be kept, returns an empty list.
    """
    if len(bodyparts) == 0 or bodypart_subset is None or bodypart_subset == "all":
        return None

    if isinstance(bodypart_subset, str):
        bodypart_subset = [bodypart_subset]

    to_keep = set(bodypart_subset)
    return [i for i, b in enumerate(bodyparts) if b in to_keep]


def _get_subset_bodyparts(
    bodyparts: list[str],
    subset: str | list[str] | None,
) -> list[str]:
    """Gets a subset of bodyparts that were used.

    Args:
        bodyparts: The bodyparts output by the model.
        subset: The subset of bodyparts to keep.

    Returns:
        The bodyparts that were used to evaluate the model.
    """
    if subset is None or subset == "all":
        return bodyparts

    if isinstance(subset, str):
        subset = [subset]

    to_keep = set(subset)
    return [b for b in bodyparts if b in to_keep]


def _extract_rmse_per_bodypart(
    results: dict[str, float],
    bodyparts: list[str],
    unique_bodyparts: list[str],
) -> dict[str, float]:
    """Extracts the RMSE per bodypart metrics from the results dict

    This method modifies the given dict in-place, removing all keys for RMSE per
    bodypart or unique bodypart.

    Args:
        results: The results returned by the evaluation method.
        bodyparts: The bodyparts defined for the project.
        unique_bodyparts: The unique bodyparts defined for the project.

    Returns:
        The per-bodypart RMSE.
    """
    rmse_per_bodypart = {}
    for bpt_idx, bpt in enumerate(bodyparts):
        rmse = results.pop(f"rmse_keypoint_{bpt_idx}", None)
        if rmse is not None:
            rmse_per_bodypart[bpt] = rmse

    for bpt_idx, bpt in enumerate(unique_bodyparts):
        rmse = results.pop(f"rmse_unique_keypoint_{bpt_idx}", None)
        if rmse is not None:
            rmse_per_bodypart[bpt] = rmse

    return rmse_per_bodypart


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str)
    parser.add_argument("--modelprefix", type=str, default="")
    parser.add_argument("--snapshotindex", type=int, default=49)
    parser.add_argument("--plotting", type=bool, default=False)
    parser.add_argument("--show_errors", type=bool, default=True)
    args = parser.parse_args()
    evaluate_network(
        config=args.config,
        modelprefix=args.modelprefix,
        snapshotindex=args.snapshotindex,
        plotting=args.plotting,
        show_errors=args.show_errors,
    )


--- File: deeplabcut/pose_estimation_pytorch/apis/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from deeplabcut.pose_estimation_pytorch.apis.analyze_images import (
    analyze_image_folder,
    analyze_images,
    analyze_image_folder,
    superanimal_analyze_images,
)
from deeplabcut.pose_estimation_pytorch.apis.videos import (
    analyze_videos,
    video_inference,
    VideoIterator,
)
from deeplabcut.pose_estimation_pytorch.apis.tracklets import (
    convert_detections2tracklets,
)
from deeplabcut.pose_estimation_pytorch.apis.evaluation import (
    predict,
    evaluate,
    evaluate_network,
    visualize_predictions,
)
from deeplabcut.pose_estimation_pytorch.apis.export import export_model
from deeplabcut.pose_estimation_pytorch.apis.tracking_dataset import (
    create_tracking_dataset,
)
from deeplabcut.pose_estimation_pytorch.apis.training import (
    train,
    train_network,
)
from deeplabcut.pose_estimation_pytorch.apis.utils import (
    get_detector_inference_runner,
    get_inference_runners,
    get_pose_inference_runner,
)
from deeplabcut.pose_estimation_pytorch.apis.visualization import (
    create_labeled_images,
    extract_maps,
    extract_save_all_maps,
)
from deeplabcut.pose_estimation_pytorch.apis.utils import (
    build_predictions_dataframe,
    get_detector_inference_runner,
    get_pose_inference_runner,
)


--- File: deeplabcut/pose_estimation_pytorch/apis/export.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Code to export DeepLabCut models for DLCLive inference"""
import copy
from pathlib import Path

import torch

import deeplabcut.pose_estimation_pytorch.apis.utils as utils
import deeplabcut.pose_estimation_pytorch.data as dlc3_data
import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.pose_estimation_pytorch.runners.snapshots import Snapshot
from deeplabcut.pose_estimation_pytorch.task import Task


def export_model(
    config: str | Path,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    snapshotindex: int | None = None,
    detector_snapshot_index: int | None = None,
    iteration: int | None = None,
    overwrite: bool = False,
    wipe_paths: bool = False,
    without_detector: bool = False,
    modelprefix: str | None = None,
) -> None:
    """Export DeepLabCut models for live inference.

    Saves the pytorch_config.yaml configuration, snapshot files, of the model to a
    directory named exported-models-pytorch within the project directory.

    Args:
        config: Path of the project configuration file
        shuffle : The shuffle of the model to export.
        trainingsetindex: The index of the training fraction for the model you wish to
            export.
        snapshotindex: The snapshot index for the weights you wish to export. If None,
            uses the snapshotindex as defined in ``config.yaml``.
        detector_snapshot_index: Only for TD models. If defined, uses the detector with
            the given index for pose estimation. If None, uses the snapshotindex as
            defined in the project ``config.yaml``.
        iteration: The project iteration (active learning loop) you wish to export. If
            None, the iteration listed in the project config file is used.
        overwrite : bool, optional
            If the model you wish to export has already been exported, whether to
            overwrite. default = False
        wipe_paths : bool, optional
            Removes the actual path of your project and the init_weights from the
            ``pytorch_config.yaml``.
        without_detector: bool, optional
            Exports top-down models without the detector.
        modelprefix: Directory containing the deeplabcut models to use when evaluating
            the network. By default, the models are assumed to exist in the project
            folder.

    Raises:
        ValueError: If no snapshots could be found for the shuffle.
        ValueError: If a top-down model is exported but no detector snapshots are found.

    Examples:
        Export the last stored snapshot for model trained with shuffle 3:
        >>> import deeplabcut
        >>> deeplabcut.export_model(
        >>>     "/analysis/project/reaching-task/config.yaml",
        >>>     shuffle=3,
        >>>     snapshotindex=-1,
        >>> )
    """
    cfg = af.read_config(str(config))
    if iteration is not None:
        cfg["iteration"] = iteration

    loader = dlc3_data.DLCLoader(
        config=cfg,
        trainset_index=trainingsetindex,
        shuffle=shuffle,
        modelprefix="" if modelprefix is None else modelprefix,
    )

    if snapshotindex is None:
        snapshotindex = loader.project_cfg["snapshotindex"]
    snapshots = utils.get_model_snapshots(
        snapshotindex, loader.model_folder, loader.pose_task
    )

    if len(snapshots) == 0:
        raise ValueError(
            f"Could not find any snapshots to export in ``{loader.model_folder}`` for "
            f"``snapshotindex={snapshotindex}``."
        )

    detector_snapshots = [None]
    if loader.pose_task == Task.TOP_DOWN and not without_detector:
        if detector_snapshot_index is None:
            detector_snapshot_index = loader.project_cfg["detector_snapshotindex"]
        detector_snapshots = utils.get_model_snapshots(
            detector_snapshot_index, loader.model_folder, Task.DETECT
        )

        if len(detector_snapshots) == 0:
            raise ValueError(
                "Attempting to export a top-down pose estimation model but no detector "
                f"snapshots were found in ``{loader.model_folder}`` for "
                f"``detector_snapshot_index={detector_snapshot_index}``. You must "
                f"export a detector snapshot with a top-down pose estimation model."
            )

    export_folder_name = get_export_folder_name(loader)
    export_dir = loader.project_path / "exported-models-pytorch" / export_folder_name
    export_dir.mkdir(exist_ok=True, parents=True)

    load_kwargs = dict(map_location="cpu", weights_only=True)
    for det_snapshot in detector_snapshots:
        detector_weights = None
        if det_snapshot is not None:
            detector_weights = torch.load(det_snapshot.path, **load_kwargs)["model"]

        for snapshot in snapshots:
            export_filename = get_export_filename(loader, snapshot, det_snapshot)
            export_path = export_dir / export_filename
            if export_path.exists() and not overwrite:
                continue

            model_cfg = copy.deepcopy(loader.model_cfg)
            if wipe_paths:
                wipe_paths_from_model_config(model_cfg)

            pose_weights = torch.load(snapshot.path, **load_kwargs)["model"]
            export_dict = dict(config=model_cfg, pose=pose_weights)
            if detector_weights is not None:
                export_dict["detector"] = detector_weights

            torch.save(export_dict, export_path)


def get_export_folder_name(loader: dlc3_data.DLCLoader) -> str:
    """
    Args:
        loader: The loader for the shuffle for which we want to export models.

    Returns:
        The name of the folder in which exported models should be placed for a shuffle.
    """
    return (
        f"DLC_{loader.project_cfg['Task']}_{loader.model_cfg['net_type']}_"
        f"iteration-{loader.project_cfg['iteration']}_shuffle-{loader.shuffle}"
    )


def get_export_filename(
    loader: dlc3_data.DLCLoader,
    snapshot: Snapshot,
    detector_snapshot: Snapshot | None = None,
) -> str:
    """
    Args:
        loader: The loader for the shuffle for which we want to export models.
        snapshot: The pose model snapshot to export.
        detector_snapshot: The detector snapshot to export, for top-down models.

    Returns:
        The name of the file in which the exported model should be stored.
    """
    export_filename = get_export_folder_name(loader)
    if detector_snapshot is not None:
        export_filename += "_snapshot-detector" + detector_snapshot.uid()
    export_filename += "_snapshot-" + snapshot.uid()
    return export_filename + ".pt"


def wipe_paths_from_model_config(model_cfg: dict) -> None:
    """
    Removes all paths from the contents of the ``pytorch_config`` file.

    Args:
        model_cfg: The model configuration to wipe.
    """
    model_cfg["metadata"]["project_path"] = ""
    model_cfg["metadata"]["pose_config_path"] = ""
    if "weight_init" in model_cfg["train_settings"]:
        model_cfg["train_settings"]["weight_init"] = None
    if "resume_training_from" in model_cfg:
        model_cfg["resume_training_from"] = None
    if "resume_training_from" in model_cfg.get("detector", {}):
        model_cfg["detector"]["resume_training_from"] = None


--- File: deeplabcut/pose_estimation_pytorch/apis/visualization.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Methods to help with visualization of model outputs"""
from __future__ import annotations

from pathlib import Path

import cv2
import matplotlib.collections as collections
import matplotlib.colors as colors
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn.functional as F
from PIL import Image
from tqdm import tqdm

import deeplabcut.core.visualization as visualization
import deeplabcut.pose_estimation_pytorch.apis.utils as utils
import deeplabcut.pose_estimation_pytorch.data as data
import deeplabcut.pose_estimation_pytorch.data.preprocessor as preprocessor
import deeplabcut.pose_estimation_pytorch.models as models
from deeplabcut.core.config import read_config_as_dict
from deeplabcut.core.engine import Engine
from deeplabcut.pose_estimation_pytorch.task import Task
from deeplabcut.utils import auxiliaryfunctions


def create_labeled_images(
    predictions: dict[str, dict[str, np.ndarray | np.ndarray]],
    out_folder: str | Path,
    pcutoff: float = 0.6,
    bboxes_pcutoff: float = 0.6,
    mode: str = "bodypart",
    cmap: str | colors.Colormap = "rainbow",
    dot_size: int = 12,
    alpha_value: float = 0.7,
    skeleton: list[tuple[int, int]] | None = None,
    skeleton_color: str = "k",
    close_figure_after_save: bool = True,
):
    """Plots model predictions on images.

    Args:
        predictions: The predictions to plot. A dictionary mapping image paths to
            the predictions made by the model on that image. The predictions should
            contain a "bodyparts" key, mapping to an array of shape (max_individuals,
            num_bodyparts, 3) containing predicted bodyparts. If there are any unique
            bodyparts predicted, then it should also contain a "unique_bodyparts" key,
            mapping to an array of shape (1, num_bodyparts, 3) containing the predicted
            unique bodyparts.
        out_folder: The folder where model predictions should be saved.
        pcutoff: The p-cutoff score above which predicted bodyparts are displayed with
            a "⋅" marker, and below which they are displayed with a "X" marker.
        bboxes_pcutoff: The bounding box cutoff score, below which predicted bounding
            boxes are shown with a dashed line.
        mode: One of "bodypart", "individual". Whether to color predictions by
            bodypart or individual.
        cmap: The colormap to use to plot predictions.
        dot_size: The size of the bodypart prediction markers.
        alpha_value: The transparency value of the bodypart prediction markers.
        skeleton: If skeletons should be plotted, the list of bodyparts that constitute
            the skeletons.
        skeleton_color: The color with which to plot the skeleton, if one is given.
        close_figure_after_save: Whether to close figures after saving the labeled
            images to disk.
    """
    out_folder = Path(out_folder)
    out_folder.mkdir(exist_ok=True)

    color_by_individual = mode == "individual"
    if isinstance(cmap, str):
        cmap = plt.cm.get_cmap(cmap)

    for image_path, image_predictions in predictions.items():
        # Load frame
        frame = Image.open(str(image_path))

        # get pose predictions
        pred = image_predictions["bodyparts"]
        total_idv, total_bodyparts = pred.shape[:2]
        unique_pred = None
        if "unique_bodyparts" in image_predictions:
            unique_pred = image_predictions["unique_bodyparts"][0]
            total_idv += 1
            total_bodyparts += len(unique_pred)

        # create plot
        fig, ax = plt.subplots()
        ax.imshow(frame)

        # plot bodyparts
        for idx, pose in enumerate(pred):
            xy, scores = pose[:, :2], pose[:, 2]
            mask = scores > pcutoff
            if np.sum(pose) < 0 or np.sum(mask) <= 0:
                continue

            bones = []
            if skeleton is not None:
                for idx_1, idx_2 in skeleton:
                    if scores[idx_1] > pcutoff and scores[idx_2] > pcutoff:
                        bones.append(xy[[idx_1, idx_2]])

            kwargs = dict(s=dot_size)
            if color_by_individual:
                kwargs["c"] = cmap(idx / total_idv)
            else:
                c = np.linspace(0, 1, total_bodyparts)[:len(pose)][mask]
                kwargs["c"] = c
                kwargs["cmap"] = cmap

            xy = xy[mask]
            ax.scatter(xy[:, 0], xy[:, 1], **kwargs)
            if len(bones) > 0:
                ax.add_collection(
                    collections.LineCollection(
                        bones, colors=skeleton_color, alpha=alpha_value
                    )
                )

        # plot unique bodyparts
        if unique_pred is not None:
            xy, scores = unique_pred[:, :2], unique_pred[:, 2]
            mask = scores > pcutoff
            if np.sum(mask) <= 0:
                continue

            kwargs = dict(s=dot_size)
            if color_by_individual:
                kwargs["c"] = cmap(1)
            else:
                c = np.linspace(0, 1, total_bodyparts)
                kwargs["c"] = c[-len(unique_pred):][mask]
                kwargs["cmap"] = cmap

            xy = xy[mask]
            ax.scatter(xy[:, 0], xy[:, 1], **kwargs)

        # plot bounding boxes
        if "bboxes" in image_predictions:
            bboxes = image_predictions["bboxes"]
            bbox_scores = image_predictions["bbox_scores"]
            for idx, (bbox, score) in enumerate(zip(bboxes, bbox_scores)):
                if score <= bboxes_pcutoff:
                    continue

                xmin, ymin, w, h = bbox
                rect = plt.Rectangle(
                    (xmin, ymin), w, h, fill=False, edgecolor="green", linewidth=2
                )
                ax.add_patch(rect)

        # save predictions
        output_path = out_folder / f"predictions_{Path(image_path).stem}.png"
        fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
        fig.savefig(output_path)

        if close_figure_after_save:
            plt.close(fig)

    if close_figure_after_save:
        plt.close()


@torch.no_grad()
def extract_model_outputs(
    images: list[str] | list[Path],
    model: models.PoseModel,
    pre_processor: preprocessor.Preprocessor,
    device: str = "auto",
    context: list[dict[str, np.ndarray]] | None = None,
) -> list[dict[str, np.ndarray]]:
    """Obtains the outputs for a model for a list of images

    Args:
        images: List of image paths for which to get model outputs.
        model: The model for which to get model outputs.
        pre_processor: The pre-processor used to prepare the images before giving them
            to the model.
        device: The device on which to run inference.
        context: The context for each image to give to the pre-processor. For top-down
            models, this context should contain the bounding boxes to use for each
            image. This should be in a format:
                [
                    {"bboxes": array of shape (num_bboxes, 4)},  # image 1 bboxes,
                    {"bboxes": array of shape (num_bboxes, 4)},  # image 2 bboxes,
                    ...,
                    {"bboxes": array of shape (num_bboxes, 4)},  # image N bboxes,
                ]

    Returns:
        A list containing a dict for each input image, in the format:
        {
            inputs: a numpy array containing the inputs given to the model for the image
            context: the context given alongside the image
            outputs: a dict containing the model outputs
        }
    """
    if context is not None and len(context) != len(images):
        raise ValueError(
            "When passing context along with the images (e.g. bounding boxes for "
            "top-down models), there should be the same number of elements in the "
            f"context as the number of images. Received {len(images)} images but "
            f"{len(context)} contexts."
        )

    model = model.to(device)
    model = model.eval()

    model_data = []
    for idx, image in enumerate(images):
        image_context = {}
        if context is not None:
            image_context = context[idx]

        inputs, image_context = pre_processor(image, image_context)
        output = model(inputs.to(device))

        for head, head_cfg in model.cfg["heads"].items():
            if (
                head_cfg["predictor"].get("apply_sigmoid", False)
                or head_cfg["predictor"]["type"] == "PartAffinityFieldPredictor"
            ):
                if "heatmap" in output[head]:
                    output[head]["heatmap"] = F.sigmoid(output[head]["heatmap"])

        output = {
            head: {name: output.cpu().numpy() for name, output in head_outputs.items()}
            for head, head_outputs in output.items()
        }
        model_data.append(
            dict(inputs=inputs.cpu().numpy(), context=context, outputs=output)
        )

    return model_data


def extract_maps(
    config,
    shuffle: int = 0,
    trainingsetindex: int | str = 0,
    device: str | None = None,
    rescale: bool = False,
    indices: list[int] | None = None,
    extract_paf: bool = True,
    modelprefix: str | None = "",
    snapshot_index: int | str | None = None,
    detector_snapshot_index: int | str | None = None,
) -> dict:
    """
    Extracts the different maps output by DeepLabCut models, such as scoremaps, location
    refinement fields and part-affinity fields.

    Args:
        config: Full path of the config.yaml file as a string.
        shuffle: Index of the shuffle for which to extract maps
        trainingset_index: Integer specifying which TrainingsetFraction to use. This
            variable can also be set to "all".
        rescale: Evaluate the model at the 'global_scale' variable (as set in the
            test/pose_config.yaml file for a particular project). Every image will be
            resized according to that scale and prediction will be compared to the
            resized ground truth. The error will be reported in pixels at rescaled to
            the *original* size. Example:
                For a [200, 200] pixel image evaluated at ``global_scale=0.5``,
                predictions are calculated on [100, 100] pixel images, compared to
                ``0.5*ground truth`` and this error is then multiplied by 2!. The
                evaluation images are also shown for the original size!
        indices: Optionally, you can only obtain maps for a subset of images in your
            dataset. The indices given here are the indices of the images for which
            maps will be extracted.
        modelprefix: Directory containing the deeplabcut models to use when evaluating
            the network. By default, the models are assumed to exist in the project
            folder.
        snapshot_index: Index (starting at 0) of the snapshot we want to extract maps
            with. To evaluate the last one, use -1. To extract maps for all snapshots,
            use "all".
        detector_snapshot_index: Only for TD models. If defined, uses the detector with
            the given index for pose estimation. To extract maps for all detector
            snapshots, use "all".

    Returns:
        a dict indexed by (trainingset_fraction, snapshot_index, image_index). For each
        key, the item contains a tuple of:
            (img, scmap, locref, paf, bpt_names, paf_graph, img_name, is_train)

    Examples
    --------
        If you want to extract the data for image 0 and 103 (of the training set) for
        model trained with shuffle 0.

        >>> deeplabcut.extract_maps(config, 0, indices=[0, 103])
    """
    cfg = read_config_as_dict(config)

    trainset_indices = [trainingsetindex]
    if trainingsetindex == "all":
        trainset_indices = [i for i in range(len(cfg["TrainingFraction"]))]
    if snapshot_index is None:
        snapshot_index = cfg["snapshotindex"]
    if detector_snapshot_index is None:
        detector_snapshot_index = cfg["detector_snapshotindex"]

    extracted_maps = {}
    for trainset_index in trainset_indices:
        loader = data.DLCLoader(
            config=config,
            shuffle=shuffle,
            trainset_index=trainset_index,
            modelprefix=modelprefix,
        )
        extracted_maps[loader.train_fraction] = {}

        # (img, scmap, locref, paf, bpt_names, paf_graph, img_name, is_train)
        metadata = loader.model_cfg["metadata"]
        bpt_names = metadata["bodyparts"] + metadata["unique_bodyparts"]
        paf_graph = []
        bpt_head_cfg = loader.model_cfg["model"]["heads"]["bodypart"]
        if bpt_head_cfg["type"] == "DLCRNetHead":
            paf_graph = bpt_head_cfg.get("predictor", {}).get("graph")
            paf_indices = bpt_head_cfg.get("predictor", {}).get("edges_to_keep")
            if paf_indices is not None:
                paf_graph = [paf_graph[i] for i in paf_indices]

        if device is not None:
            loader.model_cfg["device"] = device
        loader.model_cfg["device"] = utils.resolve_device(loader.model_cfg)
        device = loader.model_cfg["device"]

        if snapshot_index is None:
            snapshot_index = -1
        snapshots = utils.get_model_snapshots(
            snapshot_index, loader.model_folder, loader.pose_task
        )

        image_paths = loader.df.index
        if indices is not None:
            image_paths = [image_paths[idx] for idx in indices]
        if len(image_paths) > 0 and isinstance(image_paths[0], tuple):
            image_paths = [Path(*img_path) for img_path in image_paths]

        image_paths = [
            (loader.project_path / img_path).resolve() for img_path in image_paths
        ]

        context = _get_context(image_paths, loader, detector_snapshot_index, device)
        train_idx = set(loader.split["train"])
        for snapshot in snapshots:
            snapshot_id = snapshot.path.stem
            extracted_maps[loader.train_fraction][snapshot_id] = {}
            runner = utils.get_pose_inference_runner(
                model_config=loader.model_cfg,
                snapshot_path=snapshot.path,
            )
            results = extract_model_outputs(
                image_paths,
                runner.model,
                runner.preprocessor,
                runner.device,
                context=context,
            )
            for idx, result in enumerate(results):
                image_idx = idx
                if indices is not None:
                    image_idx = indices[idx]

                # key can be just image_idx, or (image_idx, bbox_idx) for TD models
                keys, images, outputs = _collect_model_outputs(
                    loader.pose_task, result, image_idx
                )
                for key, image, output in zip(keys, images, outputs):
                    parsed = _parse_model_outputs(
                        image,
                        output,
                        strides={
                            k: runner.model.get_stride(k)
                            for k in runner.model.heads.keys()
                        },
                        denormalize_image=True,
                    )
                    img_name = image_paths[idx].stem
                    if isinstance(key, tuple):
                        bbox_id = key[1]
                        img_name += f"_bbox{bbox_id:03d}"

                    is_train = image_idx in train_idx
                    extracted_maps[loader.train_fraction][snapshot_id][key] = (
                        *parsed,
                        None,
                        bpt_names,
                        paf_graph,
                        img_name,
                        is_train,
                    )

    # img, scmap, locref, paf, peaks, bpt_names, paf_graph, img_name, is_train
    return extracted_maps


def extract_save_all_maps(
    config: str | Path,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    comparison_bodyparts: str | list[str] = "all",
    extract_paf: bool = True,
    all_paf_in_one: bool = True,
    device: str | None = None,
    rescale: bool = False,
    indices: list[int] | None = None,
    modelprefix: str | None = "",
    snapshot_index: int | str | None = None,
    detector_snapshot_index: int | str | None = None,
    dest_folder: str | Path | None = None,
):
    """
    Extracts the scoremap, location refinement field and part affinity field prediction
    of the model. The maps will be rescaled to the size of the input image and stored
    in the corresponding model folder in /evaluation-results-pytorch.

    Args:
        config: Full path of the config.yaml file as a string.
        shuffle: Index of the shuffle for which to extract maps
        trainingset_index: Integer specifying which TrainingsetFraction to use. This
            variable can also be set to "all".
        comparison_bodyparts: The average error will be computed for those body parts
            only (Has to be a subset of the body parts).
        extract_paf: Extract part affinity fields by default. Note that turning it off
            will make the function much faster.
        all_paf_in_one: By default, all part affinity fields are displayed on a single
            frame. If false, individual fields are shown on separate frames.
        indices: Optionally, you can only obtain maps for a subset of images in your
            dataset. The indices given here are the indices of the images for which
            maps will be extracted.
        modelprefix: Directory containing the deeplabcut models to use when evaluating
            the network. By default, the models are assumed to exist in the project
            folder.
        snapshot_index: Index (starting at 0) of the snapshot we want to extract maps
            with. To evaluate the last one, use -1. To extract maps for all snapshots,
            use "all".
        detector_snapshot_index: Only for TD models. If defined, uses the detector with
            the given index for pose estimation. To extract maps for all detector
            snapshots, use "all".

    Examples
    --------
    Calculated maps for images 0, 1 and 33.
        >>> deeplabcut.extract_save_all_maps(
        >>>     "/analysis/project/reaching-task/config.yaml",
        >>>     shuffle=1,
        >>>     indices=[0, 1, 33]
        >>> )

    """
    cfg = read_config_as_dict(config)
    maps = extract_maps(
        config,
        shuffle=shuffle,
        trainingsetindex=trainingsetindex,
        device=device,
        rescale=rescale,
        indices=indices,
        snapshot_index=snapshot_index,
        detector_snapshot_index=detector_snapshot_index,
        modelprefix=modelprefix,
    )
    bpts_to_plot = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
        cfg, comparison_bodyparts
    )

    print("Saving plots...")
    for frac, values in maps.items():
        dest_folder = _get_maps_folder(cfg, frac, shuffle, modelprefix, dest_folder)
        dest_folder.mkdir(exist_ok=True)
        for snap, maps in values.items():
            for image_idx, image_maps in tqdm(maps.items()):
                (
                    image,
                    scmap,
                    locref,
                    paf,
                    peaks,
                    bpt_names,
                    paf_graph,
                    image_path,
                    training_image,
                ) = image_maps

                if not extract_paf:
                    paf = []

                label = "train" if training_image else "test"
                img_w, img_h = image.shape[1], image.shape[0]
                scmap = _prepare_maps_for_plotting(scmap, (img_w, img_h))
                if scmap is None:
                    raise ValueError("Cannot plot heatmaps - none output by the model")

                locref = _prepare_maps_for_plotting(locref, (img_w, img_h))
                if locref is not None:
                    locref = locref.reshape((img_h, img_w, -1, 2))
                paf = _prepare_maps_for_plotting(paf, (img_w, img_h))

                visualization.generate_model_output_plots(
                    output_folder=dest_folder,
                    image_name=Path(image_path).stem,
                    bodypart_names=bpt_names,
                    bodyparts_to_plot=bpts_to_plot,
                    image=image,
                    scmap=scmap,
                    locref=locref,
                    paf=paf,
                    paf_graph=paf_graph,
                    paf_all_in_one=all_paf_in_one,
                    paf_colormap=cfg["colormap"],
                    output_suffix=f"{label}_{shuffle}_{frac}_{snap}",
                )


def _get_context(
    image_paths: list[Path],
    loader: data.Loader,
    detector_snapshot_index: int | str | None,
    device: str,
) -> list[dict] | None:
    """Gets the context for top-down pose estimation models"""
    if loader.pose_task != Task.TOP_DOWN:
        return None

    det_snapshots = []
    if detector_snapshot_index is not None:
        det_snapshots = utils.get_model_snapshots(
            detector_snapshot_index, loader.model_folder, Task.DETECT
        )

    if detector_snapshot_index is None or len(det_snapshots) == 0:
        if detector_snapshot_index is None:
            print("No ``detector_snapshot_index`` given.")
        else:
            print(f"No detector snapshots found in {loader.model_folder}")
        print("Using GT bboxes to extract maps for this top-down model")

        bboxes_train = loader.ground_truth_bboxes(mode="train")
        bboxes_test = loader.ground_truth_bboxes(mode="test")
        bboxes = {**bboxes_train, **bboxes_test}
        return [
            dict(bboxes=bboxes[str(img_path)]["bboxes"]) for img_path in image_paths
        ]

    detector_runner = utils.get_detector_inference_runner(
        model_config=loader.model_cfg,
        snapshot_path=det_snapshots[-1].path,
        device=device,
    )
    return detector_runner.inference(image_paths)


def _collect_model_outputs(
    task: Task,
    result: dict,
    image_idx: int,
) -> tuple[list, list, list]:
    """Collects the model outputs into data that can be processed.

    Args:
        task: Whether the model is a bottom-up or top-down model.
        result: A result output by ``extract_model_outputs``.
        image_idx: The index of the image

    Returns: keys, images, outputs
        keys: The key for each image to plot.
        images: The images to plot for this input image (a single image for bottom-up
            models, and the number of bounding boxes for top-down models).
        outputs: The model outputs for each image.
    """
    if task == Task.TOP_DOWN:
        keys, images, outputs = [], [], []

        # parse each input individually
        num_bboxes = len(result["inputs"])
        for bbox_idx in range(num_bboxes):
            keys.append((image_idx, bbox_idx))
            images.append(result["inputs"][bbox_idx])
            outputs.append(
                {
                    head: {k: v[bbox_idx] for k, v in head_outputs.items()}
                    for head, head_outputs in result["outputs"].items()
                }
            )
        return keys, images, outputs

    # remove batch dimension
    return (
        [image_idx],
        [result["inputs"][0]],
        [
            {
                head: {k: v[0] for k, v in head_outputs.items()}
                for head, head_outputs in result["outputs"].items()
            }
        ],
    )


def _parse_model_outputs(
    image: np.ndarray,
    outputs: dict[str, dict[str, np.ndarray]],
    strides: dict[str, int],
    denormalize_image: bool = True,
) -> tuple[np.ndarray, list[np.ndarray], list[np.ndarray], list[np.ndarray]]:
    """Parses the model outputs into a format that can easily be plotted.

    Args:
        image: The image used to obtain the outputs.
        outputs: The model outputs.
        strides: The total stride for each model head.
        denormalize_image: Whether the image was normalized and should be de-normalized.

    Returns: (img, scmap, locref, paf)
        img: The (de-normalized) image used as input.
        scmap: The score maps output by the model.
        locref: The locref fields output by the model.
        paf: The part-affinity fields output by the model.
    """
    image = image.transpose((1, 2, 0))
    if denormalize_image:
        image = image * np.array([0.229, 0.224, 0.225])
        image = image + np.array([0.485, 0.456, 0.406])
        image = np.clip(image, 0, 1)

    heatmaps = [h for h in outputs["bodypart"].get("heatmap", [])]
    locrefs = [m * strides["bodypart"] for m in outputs["bodypart"].get("locref", [])]
    paf = [p for p in outputs["bodypart"].get("paf", [])]

    if "unique_bodypart" in outputs:
        heatmaps += [h for h in outputs["unique_bodypart"].get("heatmap", [])]
        locrefs += [
            strides["unique_bodypart"] * m
            for m in outputs["unique_bodypart"].get("locref", [])
        ]

    return image, heatmaps, locrefs, paf


def _prepare_maps_for_plotting(
    maps: list[np.ndarray], image_size: tuple[int, int]
) -> np.ndarray | None:
    """Resizes all maps to the image size and concatenates them into a single array.

    Args:
        maps: The maps that will be shown on the image.
        image_size: The (width, height) of the input image.

    Returns:
        The resized maps, or None if the list of maps was empty.
    """
    if len(maps) == 0:
        return None

    img_w, img_h = image_size
    return np.stack(
        [
            cv2.resize(map_, (img_w, img_h), interpolation=cv2.INTER_LINEAR)
            for map_ in maps
        ],
        axis=-1,
    )


def _get_maps_folder(
    cfg: dict,
    train_frac: float,
    shuffle: int,
    model_prefix: str | None,
    dest_folder: str | Path | None,
) -> Path:
    """Gets the destination folder for output maps"""
    if dest_folder is None:
        project_path = Path(cfg["project_path"])
        eval_folder = auxiliaryfunctions.get_evaluation_folder(
            trainFraction=train_frac,
            shuffle=shuffle,
            cfg=cfg,
            engine=Engine.PYTORCH,
            modelprefix=model_prefix,
        )
        dest_folder = project_path / eval_folder / "maps"

    return Path(dest_folder)


--- File: deeplabcut/pose_estimation_pytorch/apis/analyze_images.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import copy
import glob
import json
import logging
import os
from collections import defaultdict
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

import deeplabcut.core.config as config_utils
import deeplabcut.pose_estimation_pytorch.apis.visualization as visualization
import deeplabcut.pose_estimation_pytorch.data as data
import deeplabcut.pose_estimation_pytorch.modelzoo as modelzoo
from deeplabcut.core.engine import Engine
from deeplabcut.modelzoo.utils import get_superanimal_colormaps
from deeplabcut.pose_estimation_pytorch.apis.utils import (
    get_detector_inference_runner,
    build_predictions_dataframe,
    get_model_snapshots,
    get_pose_inference_runner,
    get_scorer_name,
    get_scorer_uid,
    parse_snapshot_index_for_analysis,
)
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import update_config
from deeplabcut.pose_estimation_pytorch.task import Task
from deeplabcut.pose_estimation_pytorch.utils import resolve_device
from deeplabcut.utils import auxfun_videos, auxiliaryfunctions


def superanimal_analyze_images(
    superanimal_name: str,
    model_name: str,
    detector_name: str,
    images: str | Path | list[str] | list[Path],
    max_individuals: int,
    out_folder: str | Path,
    progress_bar: bool = True,
    device: str | None = None,
    pose_threshold: float = 0.4,
    bbox_threshold: float = 0.6,
    plot_skeleton: bool = True,
    customized_model_config: str | Path | dict | None = None,
    customized_pose_checkpoint: str | Path | None = None,
    customized_detector_checkpoint: str | Path | None = None,
) -> dict[str, dict]:
    """
    This function inferences a superanimal model on a set of images and saves the
    results as labeled images.

    Args:
        superanimal_name: str
            The name of the SuperAnimal to analyze. Supported list:
                - "superanimal_bird"
                - "superanimal_topviewmouse"
                - "superanimal_quadruped"

        model_name: str
            The name of the pose model architecture to use for inference. To get a list
            of available models for a SuperAnimal, call:
                >>> import dlclibrary
                >>> superanimal_name = "superanimal_topviewmouse"
                >>> dlclibrary.get_available_models(superanimal_name)

        detector_name: str
            The name of the detector architecture to use for inference. To get a list
            of available detectors for a SuperAnimal, call:
                >>> import dlclibrary
                >>> superanimal_name = "superanimal_topviewmouse"
                >>> dlclibrary.get_available_detectors(superanimal_name)

        images: str, Path, list[str], list[Path]
            The images to analyze. Can either be a directory containing images, or
            a list of paths of images.

        max_individuals: int
            The maximum number of individuals to detect in each image.

        out_folder: str | Path
            The directory where the labeled images will be saved.

        progress_bar: bool, default=True
            Whether to display a progress bar when running inference.

        device: str | None, default=None
            The device to use to run image analysis.

        pose_threshold: float, default=0.4
            The cutoff score when plotting pose predictions. To note, this is called 
            pcutoff in other parts of the code. Must be in (0, 1).

        bbox_threshold: float, default=0.1
            The minimum confidence score to keep bounding box detections. Must be in
            (0, 1).

        plot_skeleton: bool, default=True
            If a skeleton is defined in the model configuration file, whether to plot
            the skeleton connecting the predicted bodyparts on the images.

        customized_model_config: str | Path | dict | None
            A customized SuperAnimal model config, as an alternative to the default
            SuperAnimal model config. You can get the default SuperAnimal config with:
                >>> import deeplabcut.pose_estimation_pytorch.modelzoo as modelzoo
                >>> config = modelzoo.load_super_animal_config(
                >>>     super_animal, model_name, detector_name,
                >>> )

        customized_pose_checkpoint: str | None
            A customized SuperAnimal pose checkpoint, as an alternative to the
            HuggingFace SuperAnimal models.

        customized_detector_checkpoint: str | None
            A customized SuperAnimal detector checkpoint, as an alternative to the
            HuggingFace SuperAnimal models.

    Returns:
        The predictions made by the model for each image.

    Examples:
        >>> from deeplabcut.pose_estimation_pytorch.apis import (
        >>>     superanimal_analyze_images
        >>> )
        >>> predictions = superanimal_analyze_images(
        >>>     superanimal_name="superanimal_topviewmouse",
        >>>     model_name="resnet_50",
        >>>     detector_name="fasterrcnn_mobilenet_v3_large_fpn",
        >>>     images="test_mouse_images",
        >>>     max_individuals=3,
        >>>     out_folder="test_mouse_images_labeled",
        >>>     device="cuda:0",
        >>>     pose_threshold=0.1,
        >>> )
    """
    out_folder = Path(out_folder)
    out_folder.mkdir(exist_ok=True, parents=True)

    if customized_pose_checkpoint is None:
        snapshot_path = modelzoo.get_super_animal_snapshot_path(
            dataset=superanimal_name,
            model_name=model_name,
        )
    else:
        snapshot_path = Path(customized_pose_checkpoint)

    if customized_detector_checkpoint is None:
        detector_path = modelzoo.get_super_animal_snapshot_path(
            dataset=superanimal_name,
            model_name=detector_name,
        )
    else:
        detector_path = Path(customized_detector_checkpoint)

    if customized_model_config is None:
        config = modelzoo.load_super_animal_config(
            super_animal=superanimal_name,
            model_name=model_name,
            detector_name=detector_name,
        )
    elif isinstance(customized_model_config, (str, Path)):
        config = config_utils.read_config_as_dict(customized_model_config)
    else:
        config = copy.deepcopy(customized_model_config)

    config = update_config(config, max_individuals, device)
    config["metadata"]["individuals"] = [f"animal{i}" for i in range(max_individuals)]
    if "detector" in config:
        config["detector"]["model"]["box_score_thresh"] = bbox_threshold

    predictions = analyze_image_folder(
        model_cfg=config,
        images=images,
        snapshot_path=snapshot_path,
        detector_path=detector_path,
        max_individuals=max_individuals,
        device=device,
        progress_bar=progress_bar,
    )

    skeleton_bodyparts = config.get("skeleton", [])
    skeleton = None
    if plot_skeleton and len(skeleton_bodyparts) > 0:
        skeleton = []
        bodyparts = config["metadata"]["bodyparts"]
        for bpt_0, bpt_1 in skeleton_bodyparts:
            skeleton.append(
                (bodyparts.index(bpt_0), bodyparts.index(bpt_1))
            )

    visualization.create_labeled_images(
        predictions=predictions,
        out_folder=out_folder,
        pcutoff=pose_threshold,
        bboxes_pcutoff=bbox_threshold,
        cmap=get_superanimal_colormaps()[superanimal_name],
        skeleton=skeleton,
        skeleton_color=config.get("skeleton_color", "black"),
        close_figure_after_save=False,
    )

    return predictions


def analyze_images(
    config: str | Path,
    images: str | Path | list[str] | list[Path],
    frame_type: str | None = None,
    output_dir: str | Path | None = None,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    snapshot_index: int | None = None,
    detector_snapshot_index: int | None = None,
    modelprefix: str = "",
    device: str | None = None,
    max_individuals: int | None = None,
    save_as_csv: bool = False,
    progress_bar: bool = True,
    plotting: bool | str = False,
    pcutoff: float | None = None,
    bbox_pcutoff: float | None = None,
    plot_skeleton: bool = True,
) -> dict[str, dict]:
    """Runs analysis on images using a pose model.

    Args:
        config: The project configuration file.
        images: The image(s) to run inference on. Can be the path to an image, the path
            to a directory containing images, or a list of image paths or directories
            containing images.
        frame_type: Filters the images to analyze to only the ones with the given suffix
            (e.g. setting `frame_type`=".png" will only analyze ".png" images). The
            default behavior analyzes all ".jpg", ".jpeg" and ".png" images.
        output_dir: The directory where the predictions will be stored.
        shuffle: The shuffle for which to run image analysis.
        trainingsetindex: The trainingsetindex for which to run image analysis.
        snapshot_index: The index of the snapshot to use. Loaded from the project
            configuration file if None.
        detector_snapshot_index: For top-down models only. The index of the detector
            snapshot to use. Loaded from the project configuration file if None.
        modelprefix: The model prefix used for the shuffle.
        device: The device to use to run image analysis.
        max_individuals: The maximum number of individuals to detect in each image. Set
            to the number of individuals in the project if None.
        save_as_csv: Whether to also save the predictions as a CSV file.
        progress_bar: Whether to display a progress bar when running inference.
        plotting: Whether to plot predictions on images.
        pcutoff: The cutoff score when plotting pose predictions. Must be None or in
            (0, 1). If None, the pcutoff is read from the project configuration file.
        bbox_pcutoff: The cutoff score when plotting bounding box predictions. Must be
            None or in (0, 1). If None, it is read from the project configuration file.
        plot_skeleton: If a skeleton is defined in the model configuration file, whether
            to plot the skeleton connecting the predicted bodyparts on the images.

    Returns:
        A dictionary mapping each image filename to the different types of predictions
        for it (e.g. "bodyparts", "unique_bodyparts", "bboxes", "bbox_scores")
    """
    cfg = auxiliaryfunctions.read_config(config)
    train_frac = cfg["TrainingFraction"][trainingsetindex]
    model_folder = Path(cfg["project_path"]) / auxiliaryfunctions.get_model_folder(
        train_frac,
        shuffle,
        cfg,
        engine=Engine.PYTORCH,
        modelprefix=modelprefix,
    )
    train_folder = model_folder / "train"

    model_cfg_path = train_folder / Engine.PYTORCH.pose_cfg_name
    model_cfg = config_utils.read_config_as_dict(model_cfg_path)
    pose_task = Task(model_cfg["method"])

    # get the snapshots to analyze images with
    snapshot_index, detector_snapshot_index = parse_snapshot_index_for_analysis(
        cfg, model_cfg, snapshot_index, detector_snapshot_index
    )
    snapshot = get_model_snapshots(snapshot_index, train_folder, pose_task)[0]
    detector_snapshot = None
    if detector_snapshot_index is not None:
        detector_snapshot = get_model_snapshots(
            detector_snapshot_index, train_folder, Task.DETECT
        )[0]

    predictions = analyze_image_folder(
        model_cfg=model_cfg,
        images=images,
        snapshot_path=snapshot.path,
        detector_path=None if detector_snapshot is None else detector_snapshot.path,
        frame_type=frame_type,
        device=device,
        max_individuals=max_individuals,
        progress_bar=progress_bar,
    )

    if len(predictions) == 0:
        print(f"Found no images in {images}")
        return {}

    if output_dir is None:
        images = list(predictions.keys())
        output_dir = Path(images[0]).parent.resolve()
        print(f"Setting output directory to {output_dir}")

    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True)

    scorer = get_scorer_name(
        cfg,
        shuffle=shuffle,
        train_fraction=train_frac,
        snapshot_uid=get_scorer_uid(snapshot, detector_snapshot),
        modelprefix=modelprefix,
    )
    individuals = model_cfg["metadata"]["individuals"]
    if max_individuals is not None:
        individuals = [f"individual{i}" for i in range(max_individuals)]

    df_predictions = build_predictions_dataframe(
        scorer=scorer,
        predictions=predictions,
        parameters=data.PoseDatasetParameters(
            bodyparts=model_cfg["metadata"]["bodyparts"],
            unique_bpts=model_cfg["metadata"]["unique_bodyparts"],
            individuals=individuals,
        ),
        image_name_to_index=None,
    )

    output_filepath = output_dir / f"image_predictions_{scorer}.h5"
    print(f"Saving predictions to {output_filepath}")

    df_predictions.to_hdf(output_filepath, key="predictions")
    if save_as_csv:
        print(f"Saving CSV as {output_filepath}")
        df_predictions.to_csv(output_filepath.with_suffix(".csv"))

    if plotting:
        plot_dir = output_dir / f"LabeledImages_{scorer}"
        plot_dir.mkdir(exist_ok=True)

        mode = plotting if isinstance(plotting, str) else "bodypart"

        bodyparts = model_cfg["metadata"]["bodyparts"]
        skeleton = None
        if plot_skeleton and len(cfg.get("skeleton", [])) > 0:
            skeleton = [
                (bodyparts.index(bpt_0), bodyparts.index(bpt_1))
                for bpt_0, bpt_1 in cfg["skeleton"]
            ]

        if pcutoff is None:
            pcutoff = cfg.get("pcutoff", 0.6)
        if bbox_pcutoff is None:
            bbox_pcutoff = cfg.get("bbox_pcutoff", 0.6)

        visualization.create_labeled_images(
            predictions=predictions,
            out_folder=plot_dir,
            pcutoff=pcutoff,
            bboxes_pcutoff=bbox_pcutoff,
            mode=mode,
            cmap=cfg.get("colormap", "rainbow"),
            dot_size=cfg.get("dotsize", 12),
            alpha_value=cfg.get("alphavalue", 12),
            skeleton=skeleton,
            skeleton_color=cfg.get("skeleton_color"),
        )

    return predictions


def analyze_image_folder(
    model_cfg: str | Path | dict,
    images: str | Path | list[str] | list[Path],
    snapshot_path: str | Path,
    detector_path: str | Path | None = None,
    frame_type: str | None = None,
    device: str | None = None,
    max_individuals: int | None = None,
    progress_bar: bool = True,
) -> dict[str, dict[str, np.ndarray | np.ndarray]]:
    """Runs pose inference on a folder of images and returns the predictions

    Args:
        model_cfg: The model config (or its path) used to analyze the images.
        images: The images to analyze. Can either be a directory containing images, or
            a list of paths of images.
        snapshot_path: The path of the snapshot to use to analyze the images.
        detector_path: The path of the detector snapshot to use to analyze the images,
            if a top-down model was used.
        frame_type: Filters the images to analyze to only the ones with the given suffix
            (e.g. setting `frame_type`=".png" will only analyze ".png" images). The
            default behavior analyzes all ".jpg", ".jpeg" and ".png" images.
        device: The device to use to run image analysis.
        max_individuals: The maximum number of individuals to detect in each image. Set
            to the number of individuals in the project if None.
        progress_bar: Whether to display a progress bar when running inference.

    Returns:
        A dictionary mapping each image filename to the different types of predictions
        for it (e.g. "bodyparts", "unique_bodyparts", "bboxes", "bbox_scores")

    Raises:
        ValueError: if the pose model is a top-down model but no detector path is given
    """
    if not isinstance(model_cfg, dict):
        model_cfg = config_utils.read_config_as_dict(model_cfg)

    pose_task = Task(model_cfg["method"])
    if pose_task == Task.TOP_DOWN and detector_path is None:
        raise ValueError(
            "A detector path must be specified for image analysis using top-down models"
            f" Please specify the `detector_path` parameter."
        )

    if max_individuals is None:
        max_individuals = len(model_cfg["metadata"]["individuals"])

    if device is None:
        device = resolve_device(model_cfg)

    pose_runner = get_pose_inference_runner(
        model_config=model_cfg,
        snapshot_path=snapshot_path,
        device=device,
        max_individuals=max_individuals,
    )

    image_suffixes = ".png", ".jpg", ".jpeg"
    if frame_type is not None:
        image_suffixes = (frame_type, )

    image_paths = parse_images_and_image_folders(images, image_suffixes)
    pose_inputs = image_paths
    if detector_path is not None:
        logging.info(f"Running object detection with {detector_path}")
        detector_runner = get_detector_inference_runner(
            model_config=model_cfg,
            snapshot_path=detector_path,
            device=device,
            max_individuals=max_individuals,
        )

        detector_image_paths = image_paths
        if progress_bar:
            detector_image_paths = tqdm(detector_image_paths)
        bbox_predictions = detector_runner.inference(images=detector_image_paths)
        pose_inputs = list(zip(image_paths, bbox_predictions))

    logging.info(f"Running pose estimation with {detector_path}")

    if progress_bar:
        pose_inputs = tqdm(pose_inputs)

    predictions = pose_runner.inference(pose_inputs)

    return {
        image_path: image_predictions
        for image_path, image_predictions in zip(image_paths, predictions)
    }


def plot_images_coco(
    model_cfg: str | Path | dict,
    image_folder: str | Path,
    snapshot_path: str | Path,
    out_path: str = "test_images",
    data_json_path: str = "",
    detector_path: str | Path | None = None,
    device: str | None = None,
    max_individuals: int | None = None,
) -> list[dict]:
    """
    Runs pose inference on a folder of images from a COCO dataset, and plots all
    predicted keypoints and bounding boxes

    Args:
        model_cfg: The model config (or its path) used to analyze the images.
        image_folder: The path to the folder containing the images to analyze.
        snapshot_path: The path of the snapshot to use to analyze the images.
        out_path: The path of the folder where images should be output.
        data_json_path: The path to the JSON file containing ground truth data.
        detector_path: The path of the detector snapshot to use to analyze the images,
            if a top-down model was used.
        device: The device on which to run image inference
        max_individuals: The maximum number of individuals to detect in an image.

    Returns:
        A list of dictionaries containing predictions made on each image.

    Raises:
        ValueError: if a top-down model configuration is given but detector_path is None
    """
    with open(data_json_path, "r") as f:
        obj = json.load(f)

    coco_images = obj["images"]
    coco_annotations = obj["annotations"]

    image_name_to_id = {}
    for image in coco_images:
        # only works with relative path as a test image can be in a different folder
        image_name = image["file_name"].split(os.sep)[-1]
        image_name_to_id[image_name] = image["id"]

    image_id_to_annotations = defaultdict(list)
    image_ids = list(image_name_to_id.values())
    for annotation in coco_annotations:
        image_id = annotation["image_id"]
        if annotation["image_id"] in image_ids:
            image_id_to_annotations[image_id].append(annotation)

    # need to support more image types
    images_in_folder = glob.glob(str(Path(image_folder) / "*.png"))
    corresponded_images = []
    for image in images_in_folder:
        image_path = image
        image_name = image.split(os.sep)[-1]
        if image_name in image_name_to_id:
            corresponded_images.append(image_path)

    images = corresponded_images

    predictions = analyze_image_folder(
        model_cfg=model_cfg,
        images=images,
        snapshot_path=snapshot_path,
        detector_path=detector_path,
        device=device,
        max_individuals=max_individuals,
        progress_bar=True,
    )

    os.makedirs(out_path, exist_ok=True)

    coco_format_predictions = []
    for image_path, prediction in predictions.items():
        image_name = image_path.split(os.sep)[-1]
        coco_prediction = dict(
            image_id=image_name_to_id[image_name],
            gt_annotations=image_id_to_annotations[image_name_to_id[image_name]],
            file_name=image_path,
            bodyparts=prediction["bodyparts"],
        )
        if "unique_bodyparts" in prediction:
            coco_prediction["unique_bodyparts"] = prediction["unique_bodyparts"]
        if "bboxes" in prediction:
            coco_prediction["bboxes"] = prediction["bboxes"]
        if "bbox_scores" in prediction:
            coco_prediction["bbox_scores"] = prediction["bbox_scores"]

        coco_format_predictions.append(coco_prediction)

        frame = auxfun_videos.imread(str(image_path), mode="skimage")
        fig, ax = plt.subplots()
        ax.imshow(frame)

        # TODO: color of keypoints are all red. Need to change to a different colormap
        for pose in prediction["bodyparts"]:
            x, y, confidence = pose[:, 0], pose[:, 1], pose[:, 2]
            mask = confidence > 0.0
            x = x[mask]
            y = y[mask]
            ax.scatter(x, y, color="red")

        bboxes = prediction["bboxes"]
        for bbox in bboxes:
            # Draw bounding boxes around detected objects
            xmin, ymin, w, h = bbox
            rect = plt.Rectangle(
                (xmin, ymin), w, h, fill=False, edgecolor="blue", linewidth=2
            )

        ax.add_patch(rect)
        image_name = image_path.split("/")[-1]
        fig.savefig(os.path.join(out_path, image_name))

    return coco_format_predictions


def parse_images_and_image_folders(
    images: str | Path | list[str] | list[Path],
    image_suffixes: tuple[str] = (".png", ".jpg", ".jpeg"),
) -> list[str]:
    """Parses image paths or directory paths into a single list of image paths.

    Args:
        images: Paths of images or folders containing images.
        image_suffixes: Suffixes used for images.

    Returns:
        The images contained in the folders or directly the paths given as input
    """
    if isinstance(images, (str, Path)):
        path = Path(images)
        if path.is_dir():
            return [str(img) for img in path.iterdir() if img.suffix in image_suffixes]

        return [str(path)]

    image_to_analyze = []
    for file in images:
        image_to_analyze += parse_images_and_image_folders(file)

    return image_to_analyze


--- File: deeplabcut/pose_estimation_pytorch/apis/prune_paf_graph.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from collections import defaultdict
from pathlib import Path

import networkx as nx
import numpy as np
import torch
from tqdm import tqdm

import deeplabcut.core.metrics as metrics
import deeplabcut.pose_estimation_pytorch.apis.utils as utils
import deeplabcut.pose_estimation_pytorch.data as data
import deeplabcut.pose_estimation_pytorch.models.predictors as predictors
import deeplabcut.utils.auxiliaryfunctions as auxiliaryfunctions
from deeplabcut.core.crossvalutils import find_closest_neighbors
from deeplabcut.pose_estimation_pytorch.models import PoseModel
from deeplabcut.pose_estimation_pytorch.models.predictors.paf_predictor import Graph


@torch.no_grad()
def benchmark_paf_graphs(
    loader: data.Loader,
    snapshot_path: Path,
    verbose: bool = False,
    overwrite: bool = False,
    update_config: bool = True,
) -> list[dict]:
    """Prunes the PAF graph to maximize performance

    Args:
        loader: The loader for the model to prune.
        snapshot_path: The path to the snapshot with which to prune the model.
        verbose: Verbose pruning of the model.
        overwrite: Whether to overwrite the graph if it was already pruned.
        update_config: Whether to update the model configuration with the pruned graph.

    Returns:
        A list of dictionaries containing results for each pruned graph.

        If the graph was already pruned, a single element is returned with an
        "edges_to_keep" key,  containing the indices of edges to keep in the graph.

        Otherwise, a list of graphs that were evaluated is returned, with "key_metric",
        "edges_to_keep" and "metrics" keys. The list is sorted by "key_metric" (which
        is pose mAP).
    """
    runner = utils.get_pose_inference_runner(loader.model_cfg, snapshot_path)
    device = runner.device
    preprocessor = runner.preprocessor
    model = runner.model
    predictor = model.heads.bodypart.predictor

    # only benchmark the PAF graph if the PAF indices contain all edges
    if not overwrite and len(predictor.edges_to_keep) < len(predictor.graph):
        return [dict(edges_to_keep=predictor.edges_to_keep)]

    model.to(device)
    model.eval()

    if not isinstance(predictor, predictors.PartAffinityFieldPredictor):
        raise ValueError(f"Predictor should be a PartAffinityFieldPredictor.")

    if verbose:
        print("-------------------------------------------------")
        print("Benchmarking different Part-Affinity Field Graphs")
        print("  (1/3) Obtaining the best graph candidates")

    gt_train = loader.ground_truth_keypoints("train")
    best_paf_edges, _ = get_n_best_paf_graphs(
        model,
        gt_train,
        preprocessor,
        device,
        predictor.graph,
        n_graphs=10,
    )

    if verbose:
        print("  (2/3) Running test inference")

    gt_test = loader.ground_truth_keypoints("test")
    images_test = [img_path for img_path in gt_test]

    predictions = {graph_id: {} for graph_id in range(len(best_paf_edges))}
    with torch.no_grad():
        for image_path in tqdm(images_test):
            image, _ = preprocessor(image_path, {})
            outputs = model(image.to(device))
            for graph_id, edges in enumerate(best_paf_edges):
                predictor.set_paf_edges_to_keep(edges)
                pred_pose = model.get_predictions(outputs)["bodypart"]["poses"]
                predictions[graph_id][image_path] = pred_pose.cpu().numpy()[0]

    if verbose:
        print("  (3/3) Evaluating Graphs")

    results = []
    for graph_id, pred_pose in predictions.items():
        edges_to_keep = [int(i) for i in best_paf_edges[graph_id]]
        graph_metrics = metrics.compute_metrics(
            gt_test,
            pred_pose,
            single_animal=False,
            pcutoff=0.6,
        )
        results.append(
            dict(
                edges_to_keep=edges_to_keep,
                key_metric=graph_metrics["mAP"],
                metrics=graph_metrics,
            )
        )

        if verbose:
            print("    ---")
            print(f"    |Graph {graph_id}: {len(edges_to_keep)} edges")
            print(f"    |   mAP: {graph_metrics['mAP']}")
            print(f"    |   mAR: {graph_metrics['mAR']}")
            print(f"    |   edges: {edges_to_keep}")
            print()

    results = list(sorted(results, key=lambda r: 1 - r["key_metric"]))

    if update_config and len(results) > 0:
        best_results = results[0]
        best_edges = best_results["edges_to_keep"]
        graph_metrics = best_results["metrics"]

        if verbose:
            print("Selecting the following Graph")
            print(60 * "-")
            print(f"|Graph with {len(best_edges)} edges")
            print(f"|   mAP: {graph_metrics['mAP']}")
            print(f"|   mAR: {graph_metrics['mAR']}")
            print(f"|   edges: {best_edges}")
            print()

        # update the edges to keep in the PyTorch configuration file
        loader.update_model_cfg(
            {"model.heads.bodypart.predictor.edges_to_keep": best_edges}
        )

        # update the edges indices
        test_config = loader.model_folder.parent / "test" / "pose_cfg.yaml"
        auxiliaryfunctions.edit_config(str(test_config), dict(paf_best=best_edges))

    return results


def _calc_separability(
    vals_left: np.ndarray,
    vals_right: np.ndarray,
    n_bins: int = 101,
    metric: str = "jeffries",
    max_sensitivity: bool = False,
) -> tuple[float, float]:
    if metric not in ("jeffries", "auc"):
        raise ValueError("`metric` should be either 'jeffries' or 'auc'.")

    bins = np.linspace(0, 1, n_bins)
    hist_left = np.histogram(vals_left, bins=bins)[0]
    hist_left = hist_left / hist_left.sum()
    hist_right = np.histogram(vals_right, bins=bins)[0]
    hist_right = hist_right / hist_right.sum()
    tpr = np.cumsum(hist_right)
    if metric == "jeffries":
        sep = np.sqrt(
            2 * (1 - np.sum(np.sqrt(hist_left * hist_right)))
        )  # Jeffries-Matusita distance
    else:
        sep = np.trapz(np.cumsum(hist_left), tpr)
    if max_sensitivity:
        threshold = bins[max(1, np.argmax(tpr > 0))]
    else:
        threshold = bins[np.argmin(1 - np.cumsum(hist_left) + tpr)]
    return sep, threshold


@torch.no_grad()
def compute_within_between_paf_costs(
    model: PoseModel,
    ground_truth: dict[str, np.ndarray],
    preprocessor: data.Preprocessor,
    device: str,
) -> tuple[defaultdict, defaultdict]:
    predictor = model.heads.bodypart.predictor
    images = [img_path for img_path in ground_truth]

    within = defaultdict(list)
    between = defaultdict(list)
    for image_path in tqdm(images):
        image, _ = preprocessor(image_path, {})
        outputs = model(image.to(device))
        preds = model.get_predictions(outputs)["bodypart"]["preds"][0]
        gt_pose_with_vis = ground_truth[image_path].transpose((1, 0, 2))

        # mask non-visible keypoints
        gt_pose = gt_pose_with_vis[..., :2].copy()
        gt_pose[gt_pose_with_vis[..., 2] <= 0] = np.nan

        if np.isnan(gt_pose).all():
            continue

        coords_pred = preds["coordinates"][0]
        costs_pred = preds["costs"]

        # Get animal IDs and corresponding indices in the arrays of detections
        lookup = dict()
        for i, (coord_pred, coord_gt) in enumerate(zip(coords_pred, gt_pose)):
            inds = np.flatnonzero(np.all(~np.isnan(coord_pred), axis=1))
            inds_gt = np.flatnonzero(np.all(~np.isnan(coord_gt), axis=1))
            if inds.size and inds_gt.size:
                neighbors = find_closest_neighbors(
                    coord_gt[inds_gt], coord_pred[inds], k=3
                )
                found = neighbors != -1
                lookup[i] = dict(zip(inds_gt[found], inds[neighbors[found]]))

        for k, v in costs_pred.items():
            paf = v["m1"]
            mask_within = np.zeros(paf.shape, dtype=bool)
            s, t = predictor.graph[k]
            if s not in lookup or t not in lookup:
                continue
            lu_s = lookup[s]
            lu_t = lookup[t]
            common_id = set(lu_s).intersection(lu_t)
            for id_ in common_id:
                mask_within[lu_s[id_], lu_t[id_]] = True
            within_vals = paf[mask_within]
            between_vals = paf[~mask_within]
            within[k].extend(within_vals)
            between[k].extend(between_vals)

    return within, between


def get_n_best_paf_graphs(
    model: PoseModel,
    ground_truth: dict[str, np.ndarray],
    preprocessor: data.Preprocessor,
    device: str,
    full_graph: Graph,
    root_edges: list[int] | None = None,
    n_graphs: int = 10,
    metric: str = "auc",
) -> tuple[list[list[int]], dict[int, float]]:
    return_preds = model.heads.bodypart.predictor.return_preds
    model.heads.bodypart.predictor.return_preds = True

    within_train, between_train = compute_within_between_paf_costs(
        model, ground_truth, preprocessor, device
    )
    existing_edges = list(set(k for k, v in within_train.items() if v))

    scores, _ = zip(
        *[
            _calc_separability(between_train[n], within_train[n], metric=metric)
            for n in existing_edges
        ]
    )

    # Find minimal skeleton
    G = nx.Graph()
    for edge, score in zip(existing_edges, scores):
        if np.isfinite(score):
            G.add_edge(*full_graph[edge], weight=score)

    order = np.asarray(existing_edges)[np.argsort(scores)[::-1]]
    if root_edges is None:
        root_edges = []
        for edge in nx.maximum_spanning_edges(G, data=False):
            root_edges.append(full_graph.index(sorted(edge)))

    n_edges = len(existing_edges) - len(root_edges)
    lengths = np.linspace(0, n_edges, min(n_graphs, n_edges + 1), dtype=int)[1:]
    order = order[np.isin(order, root_edges, invert=True)]
    best_edges = [root_edges]
    for length in lengths:
        best_edges.append(root_edges + list(order[:length]))

    model.heads.bodypart.predictor.return_preds = return_preds
    return best_edges, dict(zip(existing_edges, scores))


--- File: deeplabcut/pose_estimation_pytorch/apis/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import logging
import random
from pathlib import Path
from typing import Callable

import albumentations as A
import numpy as np
import pandas as pd

from deeplabcut.core.config import read_config_as_dict
from deeplabcut.core.engine import Engine
from deeplabcut.pose_estimation_pytorch.data.dataset import PoseDatasetParameters
from deeplabcut.pose_estimation_pytorch.data.dlcloader import (
    build_dlc_dataframe_columns,
)
from deeplabcut.pose_estimation_pytorch.data.postprocessor import (
    build_bottom_up_postprocessor,
    build_detector_postprocessor,
    build_top_down_postprocessor,
)
from deeplabcut.pose_estimation_pytorch.data.preprocessor import (
    build_bottom_up_preprocessor,
    build_top_down_preprocessor,
)
from deeplabcut.pose_estimation_pytorch.data.transforms import build_transforms
from deeplabcut.pose_estimation_pytorch.models import DETECTORS, PoseModel
from deeplabcut.pose_estimation_pytorch.runners import (
    build_inference_runner,
    DetectorInferenceRunner,
    DynamicCropper,
    InferenceRunner,
    PoseInferenceRunner,
)
from deeplabcut.pose_estimation_pytorch.runners.snapshots import (
    Snapshot,
    TorchSnapshotManager,
)
from deeplabcut.pose_estimation_pytorch.task import Task
from deeplabcut.pose_estimation_pytorch.utils import resolve_device
from deeplabcut.utils import auxfun_videos, auxiliaryfunctions


def parse_snapshot_index_for_analysis(
    cfg: dict,
    model_cfg: dict,
    snapshot_index: int | str | None,
    detector_snapshot_index: int | str | None,
) -> tuple[int, int | None]:
    """Gets the index of the snapshots to use for data analysis (e.g. video analysis)

    Args:
        cfg: The project configuration.
        model_cfg: The model configuration.
        snapshot_index: The index of the snapshot to use, if one was given by the user.
        detector_snapshot_index: The index of the detector snapshot to use, if one
            was given by the user.

    Returns:
        snapshot_index: the snapshot index to use for analysis
        detector_snapshot_index: the detector index to use for analysis, or None if no
            detector should be used
    """
    if snapshot_index is None:
        snapshot_index = cfg["snapshotindex"]
    if snapshot_index == "all":
        logging.warning(
            "snapshotindex is set to 'all' (in the config.yaml file or as given to "
            "`analyze_...`). Running data analysis with all snapshots is very "
            "costly! Use the function 'evaluate_network' to choose the best the "
            "snapshot. For now, changing snapshot index to -1. To evaluate another "
            "snapshot, you can change the value in the config file or call "
            "`analyze_videos` or `analyze_images` with your desired snapshot index."
        )
        snapshot_index = -1

    pose_task = Task(model_cfg["method"])
    if pose_task == Task.TOP_DOWN:
        if detector_snapshot_index is None:
            detector_snapshot_index = cfg.get("detector_snapshotindex", -1)

        if detector_snapshot_index == "all":
            logging.warning(
                f"detector_snapshotindex is set to '{detector_snapshot_index}' (in the "
                "config.yaml file or as given to `analyze_...`). Running data analysis "
                "with all snapshots is very costly! Use 'evaluate_network' to choose "
                "the best detector snapshot. For now, changing the detector snapshot "
                "index to -1. To evaluate another detector snapshot, you can change "
                "the value in the config file or call `analyze_videos` or "
                "`analyze_images` with your desired detector snapshot index."
            )
            detector_snapshot_index = -1

    else:
        detector_snapshot_index = None

    return snapshot_index, detector_snapshot_index


def return_train_network_path(
    config: str, shuffle: int = 1, trainingsetindex: int = 0, modelprefix: str = ""
) -> tuple[Path, Path, Path]:
    """
    Args:
        config: Full path of the config.yaml file as a string.
        shuffle: The shuffle index to select for training
        trainingsetindex: Which TrainingsetFraction to use (note that TrainingFraction
            is a list in config.yaml)
        modelprefix: the modelprefix for the model

    Returns:
        the path to the training pytorch pose configuration file
        the path to the test pytorch pose configuration file
        the path to the folder containing the snapshots
    """
    cfg = auxiliaryfunctions.read_config(config)
    project_path = Path(cfg["project_path"])
    train_frac = cfg["TrainingFraction"][trainingsetindex]
    model_folder = auxiliaryfunctions.get_model_folder(
        train_frac, shuffle, cfg, engine=Engine.PYTORCH, modelprefix=modelprefix
    )
    return (
        project_path / model_folder / "train" / "pytorch_config.yaml",
        project_path / model_folder / "test" / "pose_cfg.yaml",
        project_path / model_folder / "train",
    )


def get_model_snapshots(
    index: int | str,
    model_folder: Path,
    task: Task,
    snapshot_filter: list[str] | None = None,
) -> list[Snapshot]:
    """
    Args:
        index: Passing an index returns the snapshot with that index (where snapshots
            based on their number of training epochs, and the last snapshot is the
            "best" model based on validation metrics if one exists). Passing "best"
            returns the best snapshot from the training run. Passing "all" returns all
            snapshots.
        model_folder: The path to the folder containing the snapshots
        task: The task for which to return the snapshot
        snapshot_filter: List of snapshot names to return (e.g. ["snapshot-50",
            "snapshot-75"]). If defined, `index` will be ignored.

    Returns:
        If index=="all", returns all snapshots. Otherwise, returns a list containing a
        single snapshot, with the desired index.

    Raises:
        ValueError: If the index given is not valid
        ValueError: If index=="best" but there is no saved best model
    """
    snapshot_manager = TorchSnapshotManager(
        model_folder=model_folder, snapshot_prefix=task.snapshot_prefix
    )
    if snapshot_filter is not None:
        all_snapshots = snapshot_manager.snapshots()
        snapshots = [s for s in all_snapshots if s.path.stem in snapshot_filter]
        if len(snapshots) != len(snapshot_filter):
            print(f"Warning: could not find all `snapshots_to_evaluate`.")
            print(f"  Requested snapshots: {snapshot_filter}")
            print(f"  Found snapshots: {[s.path.stem for s in all_snapshots]}")
            print(f"  Snapshots returned: {[s.path.stem for s in snapshots]}")
        return snapshots

    if isinstance(index, str) and index.lower() == "best":
        best_snapshot = snapshot_manager.best()
        if best_snapshot is None:
            raise ValueError(f"No best snapshot found in {model_folder}")
        snapshots = [best_snapshot]
    elif isinstance(index, str) and index.lower() == "all":
        snapshots = snapshot_manager.snapshots()
    elif isinstance(index, int):
        all_snapshots = snapshot_manager.snapshots()
        if (
            len(all_snapshots) == 0
            or len(all_snapshots) <= index
            or (index < 0 and len(all_snapshots) < -index)
        ):
            names = [s.path.name for s in all_snapshots]
            raise ValueError(
                f"Found {len(all_snapshots)} snapshots in {model_folder} (with names "
                f"{names}) with prefix {snapshot_manager.snapshot_prefix}. Could "
                f"not return snapshot with index {index}."
            )

        snapshots = [all_snapshots[index]]
    else:
        raise ValueError(f"Invalid snapshotindex: {index}")

    return snapshots


def get_scorer_uid(snapshot: Snapshot, detector_snapshot: Snapshot | None) -> str:
    """
    Args:
        snapshot: the snapshot for which to get the scorer UID
        detector_snapshot: if a top-down model is used with a detector, the detector
            snapshot for which to get the scorer UID

    Returns:
        the uid to use for the scorer
    """
    snapshot_id = f"snapshot_{snapshot.uid()}"
    if detector_snapshot is not None:
        detect_id = detector_snapshot.uid()
        snapshot_id = f"detector_{detect_id}_{snapshot_id}"
    return snapshot_id


def get_scorer_name(
    cfg: dict,
    shuffle: int,
    train_fraction: float,
    snapshot_index: int | None = None,
    detector_index: int | None = None,
    snapshot_uid: str | None = None,
    modelprefix: str = "",
) -> str:
    """Get the scorer name for a particular PyTorch DeepLabCut shuffle

    Args:
        cfg: The project configuration.
        shuffle: The index of the shuffle for which to get the scorer
        train_fraction: The training fraction for the shuffle.
        snapshot_index: The index of the snapshot used. If None, the value is loaded
            from the project's config.yaml file.
        detector_index: For top-down models, the index of the detector used. If None,
            the value is loaded from the project's config.yaml file.
        snapshot_uid: If the snapshot_uid is not None, this value will be used instead
            of loading the snapshot and detector with given indices and calling
            utils.get_scorer_uid.
        modelprefix: The model prefix, if one was used.

    Returns:
        the scorer name
    """
    model_dir = Path(cfg["project_path"]) / auxiliaryfunctions.get_model_folder(
        train_fraction,
        shuffle,
        cfg,
        engine=Engine.PYTORCH,
        modelprefix=modelprefix,
    )
    train_dir = model_dir / "train"
    model_cfg = read_config_as_dict(str(train_dir / Engine.PYTORCH.pose_cfg_name))
    net_type = model_cfg["net_type"]
    pose_task = Task(model_cfg["method"])

    if snapshot_uid is None:
        if snapshot_index is None:
            snapshot_index = auxiliaryfunctions.get_snapshot_index_for_scorer(
                "snapshotindex", cfg["snapshotindex"]
            )
        if detector_index is None:
            detector_index = auxiliaryfunctions.get_snapshot_index_for_scorer(
                "detector_snapshotindex", cfg["detector_snapshotindex"]
            )

        snapshot = get_model_snapshots(snapshot_index, train_dir, pose_task)[0]
        detector_snapshot = None
        if detector_index is not None and pose_task == Task.TOP_DOWN:
            detector_snapshot = get_model_snapshots(
                detector_index, train_dir, Task.DETECT
            )[0]

        snapshot_uid = get_scorer_uid(snapshot, detector_snapshot)

    task, date = cfg["Task"], cfg["date"]
    name = "".join([p.capitalize() for p in net_type.split("_")])
    return f"DLC_{name}_{task}{date}shuffle{shuffle}_{snapshot_uid}"


def list_videos_in_folder(
    data_path: str | list[str],
    video_type: str | None,
    shuffle: bool = False,
) -> list[Path]:
    """
    Args:
        data_path: Path or list of paths to folders containing videos
        video_type: The type of video to filter for
        shuffle: If the paths point to directories, whether to shuffle the order of
            videos in the directory.

    Returns:
        The paths of videos to analyze.
    """
    if not isinstance(data_path, list):
        data_path = [data_path]
    video_paths = [Path(p) for p in data_path]

    videos = []
    for path in video_paths:
        if path.is_dir():
            if not video_type:
                video_suffixes = ["." + ext for ext in auxfun_videos.SUPPORTED_VIDEOS]
            else:
                video_suffixes = [video_type]

            suffixes = [s if s.startswith(".") else "." + s for s in video_suffixes]
            videos_in_dir = [file for file in path.iterdir() if file.suffix in suffixes]
            if shuffle:
                random.shuffle(videos_in_dir)
            videos += videos_in_dir
        else:
            assert (
                path.exists()
            ), f"Could not find the video: {path}. Check access rights."
            videos.append(path)

    return videos


def ensure_multianimal_df_format(df_predictions: pd.DataFrame) -> pd.DataFrame:
    """
    Convert dataframe to 'multianimal' format (with an "individuals" columns index)

    Args:
        df_predictions: the dataframe to convert

    Returns:
        the dataframe in MA format
    """
    df_predictions_ma = df_predictions.copy()
    try:
        df_predictions_ma.columns.get_level_values("individuals").unique().tolist()
    except KeyError:
        new_cols = pd.MultiIndex.from_tuples(
            [(col[0], "animal", col[1], col[2]) for col in df_predictions_ma.columns],
            names=["scorer", "individuals", "bodyparts", "coords"],
        )
        df_predictions_ma.columns = new_cols
    return df_predictions_ma


def _image_names_to_df_index(
    image_names: list[str],
    image_name_to_index: Callable[[str], tuple[str, ...]] | None = None,
) -> pd.MultiIndex | list[str]:
    """
    Creates index for predictions dataframe.
    This method is used in build_predictions_dataframe, but also in build_bboxes_dict_for_dataframe.
    It is important that these two methods return objects with the same index / keys.

    Args:
        image_names: list of image names
        image_name_to_index, optional: a transform to apply on each image_name
    """

    if image_name_to_index is not None:
        return pd.MultiIndex.from_tuples(
            [image_name_to_index(image_name) for image_name in image_names]
        )
    else:
        return image_names


def build_predictions_dataframe(
    scorer: str,
    predictions: dict[str, dict[str, np.ndarray]],
    parameters: PoseDatasetParameters,
    image_name_to_index: Callable[[str], tuple[str, ...]] | None = None,
) -> pd.DataFrame:
    """
    Builds a pandas DataFrame from pose prediction data. The resulting DataFrame
    includes properly formatted indices and column names for compatibility with
    DeepLabCut workflows.

    Args:
        scorer: The name of the scorer used to generate the predictions.
        predictions: A dictionary where each key is an image name and its value is
            another dictionary. The inner dictionary contains prediction data for
            "bodyparts" and optionally "unique_bodyparts". The "bodyparts" and
            "unique_bodyparts" data arrays are expected to be 3-dimensional, containing
            pose predictions in format (num_predicted_individuals, num_bodyparts, 3).
        parameters: Dataset-specific parameters required for constructing DataFrame
            columns.
        image_name_to_index: A callable function that takes an image name and returns
            a tuple representing the DataFrame index. If None, indices will be
            generated without transformation.

    Returns:
        A pandas DataFrame containing the processed prediction data for all provided
        images. The DataFrame index corresponds to the image names or their
        transformed values (if `image_name_to_index` is provided). The DataFrame
        columns are constructed using the provided scorer and parameters.
    """
    image_names = []
    prediction_data = []
    for image_name, image_predictions in predictions.items():
        image_data = image_predictions["bodyparts"][..., :3].reshape(-1)
        if "unique_bodyparts" in image_predictions:
            image_data = np.concatenate(
                [image_data, image_predictions["unique_bodyparts"][..., :3].reshape(-1)]
            )
        image_names.append(image_name)
        prediction_data.append(image_data)

    index = _image_names_to_df_index(image_names, image_name_to_index)

    return pd.DataFrame(
        prediction_data,
        index=index,
        columns=build_dlc_dataframe_columns(
            scorer=scorer,
            parameters=parameters,
            with_likelihood=True,
        ),
    )


def build_bboxes_dict_for_dataframe(
    predictions: dict[str, dict[str, np.ndarray]],
    image_name_to_index: Callable[[str], tuple[str, ...]] | None = None,
) -> dict:
    """
    Creates a dictionary with bounding boxes from predictions.

    The keys of the dictionary are the same as the index of the dataframe created by
    build_predictions_dataframe. Therefore, the structures returned by
    build_predictions_dataframe and by build_bboxes_dict_for_dataframe can be accessed
    with the same keys.

    Args:
        predictions: Dictionary containing the evaluation results
        image_name_to_index: a transform to apply on each image_name

    Returns:
        Dictionary with sames keys as in the dataframe returned by
        build_predictions_dataframe, and respective bounding boxes and scores, if any.
    """

    image_names = []
    bboxes_data = []
    for image_name, image_predictions in predictions.items():
        image_names.append(image_name)
        if "bboxes" in image_predictions and "bbox_scores" in image_predictions:
            bboxes_data.append(
                (image_predictions["bboxes"], image_predictions["bbox_scores"])
            )

    index = _image_names_to_df_index(image_names, image_name_to_index)

    return dict(zip(index, bboxes_data))


def get_inference_runners(
    model_config: dict,
    snapshot_path: str | Path,
    max_individuals: int | None = None,
    num_bodyparts: int | None = None,
    num_unique_bodyparts: int | None = None,
    batch_size: int = 1,
    device: str | None = None,
    with_identity: bool = False,
    transform: A.BaseCompose | None = None,
    detector_batch_size: int = 1,
    detector_path: str | Path | None = None,
    detector_transform: A.BaseCompose | None = None,
    dynamic: DynamicCropper | None = None,
) -> tuple[InferenceRunner, InferenceRunner | None]:
    """Builds the runners for pose estimation

    Args:
        model_config: the pytorch configuration file
        snapshot_path: the path of the snapshot from which to load the weights
        max_individuals: the maximum number of individuals per image (if None, uses the
            individuals defined in the model_config metadata)
        num_bodyparts: the number of bodyparts predicted by the model (if None, uses the
            bodyparts defined in the model_config metadata)
        num_unique_bodyparts: the number of unique_bodyparts predicted by the model (if
            None, uses the unique bodyparts defined in the model_config metadata)
        batch_size: the batch size to use for the pose model.
        with_identity: whether the pose model has an identity head
        device: if defined, overwrites the device selection from the model config
        transform: the transform for pose estimation. if None, uses the transform
            defined in the config.
        detector_batch_size: the batch size to use for the detector
        detector_path: the path to the detector snapshot from which to load weights,
            for top-down models (if a detector runner is needed)
        detector_transform: the transform for object detection. if None, uses the
            transform defined in the config.
        dynamic: The DynamicCropper used for video inference, or None if dynamic
            cropping should not be used. Only for bottom-up pose estimation models.
            Should only be used when creating inference runners for video pose
            estimation with batch size 1.

    Returns:
        a runner for pose estimation
        a runner for detection, if detector_path is not None
    """
    if max_individuals is None:
        max_individuals = len(model_config["metadata"]["individuals"])
    if num_bodyparts is None:
        num_bodyparts = len(model_config["metadata"]["bodyparts"])
    if num_unique_bodyparts is None:
        num_unique_bodyparts = len(model_config["metadata"]["unique_bodyparts"])

    pose_task = Task(model_config["method"])
    if device is None:
        device = resolve_device(model_config)

    if transform is None:
        transform = build_transforms(model_config["data"]["inference"])

    detector_runner = None
    if pose_task == Task.BOTTOM_UP:
        pose_preprocessor = build_bottom_up_preprocessor(
            color_mode=model_config["data"]["colormode"],
            transform=transform,
        )
        pose_postprocessor = build_bottom_up_postprocessor(
            max_individuals=max_individuals,
            num_bodyparts=num_bodyparts,
            num_unique_bodyparts=num_unique_bodyparts,
            with_identity=with_identity,
        )
    else:
        # FIXME: Cannot run detectors on MPS
        detector_device = device
        if device == "mps":
            detector_device = "cpu"

        crop_cfg = model_config["data"]["inference"].get("top_down_crop", {})
        width, height = crop_cfg.get("width", 256), crop_cfg.get("height", 256)
        margin = crop_cfg.get("margin", 0)

        pose_preprocessor = build_top_down_preprocessor(
            color_mode=model_config["data"]["colormode"],
            transform=transform,
            top_down_crop_size=(width, height),
            top_down_crop_margin=margin,
        )
        pose_postprocessor = build_top_down_postprocessor(
            max_individuals=max_individuals,
            num_bodyparts=num_bodyparts,
            num_unique_bodyparts=num_unique_bodyparts,
        )

        if detector_path is not None:
            if detector_transform is None:
                detector_transform = build_transforms(
                    model_config["detector"]["data"]["inference"]
                )

            detector_config = model_config["detector"]["model"]
            if "pretrained" in detector_config:
                detector_config["pretrained"] = False

            detector_runner = build_inference_runner(
                task=Task.DETECT,
                model=DETECTORS.build(detector_config),
                device=detector_device,
                snapshot_path=detector_path,
                batch_size=detector_batch_size,
                preprocessor=build_bottom_up_preprocessor(
                    color_mode=model_config["detector"]["data"]["colormode"],
                    transform=detector_transform,
                ),
                postprocessor=build_detector_postprocessor(
                    max_individuals=max_individuals,
                ),
                load_weights_only=model_config["detector"]["runner"].get(
                    "load_weights_only",
                    None,
                ),
            )

    pose_runner = build_inference_runner(
        task=pose_task,
        model=PoseModel.build(model_config["model"]),
        device=device,
        snapshot_path=snapshot_path,
        batch_size=batch_size,
        preprocessor=pose_preprocessor,
        postprocessor=pose_postprocessor,
        dynamic=dynamic,
        load_weights_only=model_config["runner"].get("load_weights_only", None),
    )
    return pose_runner, detector_runner


def get_detector_inference_runner(
    model_config: dict,
    snapshot_path: str | Path,
    batch_size: int = 1,
    device: str | None = None,
    max_individuals: int | None = None,
    transform: A.BaseCompose | None = None,
) -> DetectorInferenceRunner:
    """Builds an inference runner for object detection.

    Args:
        model_config: the pytorch configuration file
        snapshot_path: the path of the snapshot from which to load the weights
        max_individuals: the maximum number of individuals per image
        batch_size: the batch size to use for the pose model.
        device: if defined, overwrites the device selection from the model config
        transform: the transform for pose estimation. if None, uses the transform
            defined in the config.

    Returns:
        an inference runner for object detection
    """
    if device is None:
        device = resolve_device(model_config)
    elif device == "mps":  # FIXME(niels): Cannot run detectors on MPS
        device = "cpu"

    if max_individuals is None:
        max_individuals = len(model_config["metadata"]["individuals"])

    det_cfg = model_config["detector"]
    if transform is None:
        transform = build_transforms(det_cfg["data"]["inference"])

    if "pretrained" in det_cfg["model"]:
        det_cfg["model"]["pretrained"] = False

    preprocessor = build_bottom_up_preprocessor(det_cfg["data"]["colormode"], transform)
    postprocessor = build_detector_postprocessor(max_individuals=max_individuals)
    runner = build_inference_runner(
        task=Task.DETECT,
        model=DETECTORS.build(det_cfg["model"]),
        device=device,
        snapshot_path=snapshot_path,
        batch_size=batch_size,
        preprocessor=preprocessor,
        postprocessor=postprocessor,
        load_weights_only=det_cfg["runner"].get("load_weights_only", None),
    )

    if not isinstance(runner, DetectorInferenceRunner):
        raise RuntimeError(f"Failed to build DetectorInferenceRunner: {model_config}")

    return runner


def get_pose_inference_runner(
    model_config: dict,
    snapshot_path: str | Path,
    batch_size: int = 1,
    device: str | None = None,
    max_individuals: int | None = None,
    transform: A.BaseCompose | None = None,
    dynamic: DynamicCropper | None = None,
) -> PoseInferenceRunner:
    """Builds an inference runner for pose estimation.

    Args:
        model_config: the pytorch configuration file
        snapshot_path: the path of the snapshot from which to load the weights
        max_individuals: the maximum number of individuals per image
        batch_size: the batch size to use for the pose model.
        device: if defined, overwrites the device selection from the model config
        transform: the transform for pose estimation. if None, uses the transform
            defined in the config.
        dynamic: The DynamicCropper used for video inference, or None if dynamic
            cropping should not be used. Only for bottom-up pose estimation models.
            Should only be used when creating inference runners for video pose
            estimation with batch size 1.

    Returns:
        an inference runner for pose estimation
    """
    pose_task = Task(model_config["method"])
    metadata = model_config["metadata"]
    num_bodyparts = len(metadata["bodyparts"])
    num_unique = len(metadata["unique_bodyparts"])
    with_identity = bool(metadata["with_identity"])
    if max_individuals is None:
        max_individuals = len(metadata["individuals"])

    if device is None:
        device = resolve_device(model_config)

    if transform is None:
        transform = build_transforms(model_config["data"]["inference"])

    if pose_task == Task.BOTTOM_UP:
        pose_preprocessor = build_bottom_up_preprocessor(
            color_mode=model_config["data"]["colormode"],
            transform=transform,
        )
        pose_postprocessor = build_bottom_up_postprocessor(
            max_individuals=max_individuals,
            num_bodyparts=num_bodyparts,
            num_unique_bodyparts=num_unique,
            with_identity=with_identity,
        )
    else:
        crop_cfg = model_config["data"]["inference"].get("top_down_crop", {})
        width, height = crop_cfg.get("width", 256), crop_cfg.get("height", 256)
        margin = crop_cfg.get("margin", 0)

        pose_preprocessor = build_top_down_preprocessor(
            color_mode=model_config["data"]["colormode"],
            transform=transform,
            top_down_crop_size=(width, height),
            top_down_crop_margin=margin,
        )
        pose_postprocessor = build_top_down_postprocessor(
            max_individuals=max_individuals,
            num_bodyparts=num_bodyparts,
            num_unique_bodyparts=num_unique,
        )

    runner = build_inference_runner(
        task=pose_task,
        model=PoseModel.build(model_config["model"]),
        device=device,
        snapshot_path=snapshot_path,
        batch_size=batch_size,
        preprocessor=pose_preprocessor,
        postprocessor=pose_postprocessor,
        dynamic=dynamic,
        load_weights_only=model_config["runner"].get("load_weights_only", None),
    )
    if not isinstance(runner, PoseInferenceRunner):
        raise RuntimeError(f"Failed to build PoseInferenceRunner for {model_config}")

    return runner


--- File: deeplabcut/pose_estimation_pytorch/apis/training.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import argparse
import copy
import logging
from pathlib import Path

import albumentations as A
from torch.utils.data import DataLoader

import deeplabcut.core.config as config_utils
import deeplabcut.pose_estimation_pytorch.utils as utils
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.pose_estimation_pytorch.data import (
    build_transforms,
    COCOLoader,
    DLCLoader,
    Loader,
)
from deeplabcut.pose_estimation_pytorch.data.collate import COLLATE_FUNCTIONS
from deeplabcut.pose_estimation_pytorch.models import DETECTORS, PoseModel
from deeplabcut.pose_estimation_pytorch.modelzoo.memory_replay import (
    prepare_memory_replay,
)
from deeplabcut.pose_estimation_pytorch.runners import build_training_runner
from deeplabcut.pose_estimation_pytorch.runners.logger import (
    destroy_file_logging,
    LOGGER,
    setup_file_logging,
)
from deeplabcut.pose_estimation_pytorch.task import Task


def train(
    loader: Loader,
    run_config: dict,
    task: Task,
    device: str | None = "cpu",
    gpus: list[int] | None = None,
    logger_config: dict | None = None,
    snapshot_path: str | Path | None = None,
    transform: A.BaseCompose | None = None,
    inference_transform: A.BaseCompose | None = None,
    max_snapshots_to_keep: int | None = None,
    load_head_weights: bool = True,
) -> None:
    """Builds a model from a configuration and fits it to a dataset

    Args:
        loader: the loader containing the data to train on/validate with
        run_config: the model and run configuration
        task: the task to train the model for
        device: the torch device to train on (such as "cpu", "cuda", "mps")
        gpus: the list of GPU indices to use for multi-GPU training
        logger_config: the configuration of a logger to use
        snapshot_path: if continuing to train from a snapshot, the path containing the
            weights to load
        transform: if defined, overwrites the transform defined in the model config
        inference_transform: if defined, overwrites the inference transform defined in
            the model config
        max_snapshots_to_keep: the maximum number of snapshots to store for each model
        load_head_weights: When `snapshot_path` is not None and a pose model is being
            trained, whether to load the head weights from the saved snapshot.
    """
    weight_init = None
    pretrained = True

    if weight_init_cfg := run_config["train_settings"].get("weight_init"):
        weight_init = WeightInitialization.from_dict(weight_init_cfg)
        pretrained = False

    if task == Task.DETECT:
        model = DETECTORS.build(
            run_config["model"],
            weight_init=weight_init,
            pretrained=pretrained,
        )

    else:
        model = PoseModel.build(
            run_config["model"],
            weight_init=weight_init,
            pretrained_backbone=pretrained,
        )

    if max_snapshots_to_keep is not None:
        run_config["runner"]["snapshots"]["max_snapshots"] = max_snapshots_to_keep

    logger = None
    if logger_config is not None:
        logger = LOGGER.build(dict(**logger_config, model=model))
        logger.log_config(run_config)

    if device is None:
        device = utils.resolve_device(run_config)
    elif device == "auto":
        run_config["device"] = device
        device = utils.resolve_device(run_config)

    if gpus is None:
        gpus = run_config["runner"].get("gpus")

    if device == "mps" and task == Task.DETECT:
        device = "cpu"  # FIXME: Cannot train detectors on MPS

    if snapshot_path is None:
        snapshot_path = run_config.get("resume_training_from")

    model.to(device)  # Move model before giving its parameters to the optimizer
    runner = build_training_runner(
        runner_config=run_config["runner"],
        model_folder=loader.model_folder,
        task=task,
        model=model,
        device=device,
        gpus=gpus,
        snapshot_path=snapshot_path,
        load_head_weights=load_head_weights,
        logger=logger,
    )

    if transform is None:
        transform = build_transforms(run_config["data"]["train"])
    if inference_transform is None:
        inference_transform = build_transforms(run_config["data"]["inference"])

    logging.info("Data Transforms:")
    logging.info(f"  Training:   {transform}")
    logging.info(f"  Validation: {inference_transform}")

    train_dataset = loader.create_dataset(transform=transform, mode="train", task=task)
    valid_dataset = loader.create_dataset(
        transform=inference_transform, mode="test", task=task
    )

    collate_fn = None
    if collate_fn_cfg := run_config["data"]["train"].get("collate"):
        collate_fn = COLLATE_FUNCTIONS.build(collate_fn_cfg)
        logging.info(f"Using custom collate function: {collate_fn_cfg}")

    batch_size = run_config["train_settings"]["batch_size"]
    num_workers = run_config["train_settings"]["dataloader_workers"]
    pin_memory = run_config["train_settings"]["dataloader_pin_memory"]
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=num_workers,
        pin_memory=pin_memory,
    )
    valid_dataloader = DataLoader(valid_dataset, batch_size=1, shuffle=False)

    if (
        loader.model_cfg["model"].get("freeze_bn_stats", False)
        or loader.model_cfg["model"].get("backbone", {}).get("freeze_bn_stats", False)
        or batch_size == 1
    ):
        logging.info(
            "\nNote: According to your model configuration, you're training with batch "
            "size 1 and/or ``freeze_bn_stats=false``. This is not an optimal setting "
            "if you have powerful GPUs.\n"
            "This is good for small batch sizes (e.g., when training on a CPU), where "
            "you should keep ``freeze_bn_stats=true``.\n"
            "If you're using a GPU to train, you can obtain faster performance by "
            "setting a larger batch size (the biggest power of 2 where you don't get"
            "a CUDA out-of-memory error, such as 8, 16, 32 or 64 depending on the "
            "model, size of your images, and GPU memory) and ``freeze_bn_stats=false`` "
            "for the backbone of your model. \n"
            "This also allows you to increase the learning rate (empirically you can "
            "scale the learning rate by sqrt(batch_size) times).\n"
        )

    logging.info(
        f"Using {len(train_dataset)} images and {len(valid_dataset)} for testing"
    )
    if task == task.DETECT:
        logging.info("\nStarting object detector training...\n" + (50 * "-"))
    else:
        logging.info("\nStarting pose model training...\n" + (50 * "-"))

    runner.fit(
        train_dataloader,
        valid_dataloader,
        epochs=run_config["train_settings"]["epochs"],
        display_iters=run_config["train_settings"]["display_iters"],
    )


def train_network(
    config: str | Path,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    modelprefix: str = "",
    device: str | None = None,
    snapshot_path: str | Path | None = None,
    detector_path: str | Path | None = None,
    load_head_weights: bool = True,
    batch_size: int | None = None,
    epochs: int | None = None,
    save_epochs: int | None = None,
    detector_batch_size: int | None = None,
    detector_epochs: int | None = None,
    detector_save_epochs: int | None = None,
    display_iters: int | None = None,
    max_snapshots_to_keep: int | None = None,
    pose_threshold: float | None = 0.1,
    pytorch_cfg_updates: dict | None = None,
) -> None:
    """Trains a network for a project

    Args:
        config : path to the yaml config file of the project
        shuffle : index of the shuffle we want to train on
        trainingsetindex : training set index
        modelprefix: directory containing the deeplabcut configuration files to use
            to train the network (and where snapshots will be saved). By default, they
             are assumed to exist in the project folder.
        device: the torch device to train on (such as "cpu", "cuda", "mps")
        snapshot_path: if resuming training, the snapshot from which to resume
        detector_path: if resuming training of a top-down model, used to specify the
            detector snapshot from which to resume
        load_head_weights: if resuming training of a pose estimation model (either
            through the `snapshot_path` attribute or the `resume_training_from` key in
            the `pytorch_config.yaml` file), setting this to True also loads the weights
            for the model head (equivalent to the `keepdeconvweights` for  TensorFlow
            models). Note that if you change the number of bodyparts, you need to set
            this to false for re-training.
        batch_size: overrides the batch size to train with
        epochs: overrides the maximum number of epochs to train the model for
        save_epochs: overrides the number of epochs between each snapshot save
        detector_batch_size: Only for top-down models. Overrides the batch size with
            which to train the detector.
        detector_epochs: Only for top-down models. Overrides the maximum number of
            epochs to train the model for. Setting to 0 means the detector will not be
            trained.
        detector_save_epochs: Only for top-down models. Overrides the number of epochs
            between each snapshot of the detector is saved.
        display_iters: overrides the number of iterations between each log of the loss
            within an epoch
        max_snapshots_to_keep: the maximum number of snapshots to save for each model
        pose_threshold: Used for memory-replay. Pseudo-predictions with confidence lower
            than this threshold are discarded for memory-replay
        pytorch_cfg_updates: dict, optional, default = None.
            A dictionary of updates to the pytorch config. The keys are the dot-separated
            paths to the values to update in the config.
            For example, to update the gpus to run the training on, you can use:
            ```
            pytorch_cfg_updates={"runner.gpus": [0,1,2,3]}
            ```
            To see the full list - check the pytorch_cfg.yaml file in your project folder
    """
    loader = DLCLoader(
        config=config,
        shuffle=shuffle,
        trainset_index=trainingsetindex,
        modelprefix=modelprefix,
    )

    if weight_init_cfg := loader.model_cfg["train_settings"].get("weight_init"):
        weight_init = WeightInitialization.from_dict(weight_init_cfg)
        if weight_init.memory_replay:
            if weight_init.detector_snapshot_path is None:
                raise ValueError(
                    "When fine-tuning a SuperAnimal model with memory replay, a "
                    "detector must be given as well so animals can be detected in "
                    "images to obtain pseudo-labels. Please update your weight "
                    "initialization so that `detector_snapshot_path` is not None."
                )

            print("Preparing data for memory replay (this can take some time)")
            dataset_params = loader.get_dataset_parameters()
            prepare_memory_replay(
                config,
                loader,
                weight_init.dataset,
                weight_init.snapshot_path,
                weight_init.detector_snapshot_path,
                device,
                train_file="train.json",
                max_individuals=dataset_params.max_num_animals,
                pose_threshold=pose_threshold,
            )

            print("Loading memory replay data")
            loader = COCOLoader(
                project_root=loader.model_folder / "memory_replay",
                model_config_path=loader.model_config_path,
                train_json_filename="memory_replay_train.json",
            )

    if batch_size is not None:
        loader.model_cfg["train_settings"]["batch_size"] = batch_size
    if epochs is not None:
        loader.model_cfg["train_settings"]["epochs"] = epochs
    if save_epochs is not None:
        loader.model_cfg["runner"]["snapshots"]["save_epochs"] = save_epochs
    if display_iters is not None:
        loader.model_cfg["train_settings"]["display_iters"] = display_iters

    detector_cfg = loader.model_cfg.get("detector")
    if detector_cfg is not None:
        if detector_batch_size is not None:
            detector_cfg["train_settings"]["batch_size"] = detector_batch_size
        if detector_epochs is not None:
            detector_cfg["train_settings"]["epochs"] = detector_epochs
        if detector_save_epochs is not None:
            detector_cfg["runner"]["snapshots"]["save_epochs"] = detector_save_epochs
        if display_iters is not None:
            detector_cfg["train_settings"]["display_iters"] = display_iters

    if pytorch_cfg_updates is not None:
        loader.update_model_cfg(pytorch_cfg_updates)

    setup_file_logging(loader.model_folder / "train.txt")

    logging.info("Training with configuration:")
    config_utils.pretty_print(loader.model_cfg, print_fn=logging.info)

    # fix seed for reproducibility
    utils.fix_seeds(loader.model_cfg["train_settings"]["seed"])

    # get the pose task
    pose_task = Task(loader.model_cfg.get("method", "bu"))
    if (
        pose_task == Task.TOP_DOWN
        and loader.model_cfg["detector"]["train_settings"]["epochs"] > 0
    ):
        logger_config = None
        if loader.model_cfg.get("logger"):
            logger_config = copy.deepcopy(loader.model_cfg["logger"])
            logger_config["run_name"] += "-detector"

        detector_run_config = loader.model_cfg["detector"]
        detector_run_config["device"] = loader.model_cfg["device"]
        detector_run_config["train_settings"]["weight_init"] = loader.model_cfg[
            "train_settings"
        ].get("weight_init")
        train(
            loader=loader,
            run_config=detector_run_config,
            task=Task.DETECT,
            device=device,
            logger_config=logger_config,
            snapshot_path=detector_path,
            max_snapshots_to_keep=max_snapshots_to_keep,
        )

    if loader.model_cfg["train_settings"]["epochs"] > 0:
        train(
            loader=loader,
            run_config=loader.model_cfg,
            task=pose_task,
            device=device,
            logger_config=loader.model_cfg.get("logger"),
            snapshot_path=snapshot_path,
            max_snapshots_to_keep=max_snapshots_to_keep,
            load_head_weights=load_head_weights,
        )

    destroy_file_logging()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--config-path", type=str)
    parser.add_argument("--shuffle", type=int, default=1)
    parser.add_argument("--train-ind", type=int, default=0)
    parser.add_argument("--modelprefix", type=str, default="")
    args = parser.parse_args()
    train_network(
        config=args.config_path,
        shuffle=args.shuffle,
        trainingsetindex=args.train_ind,
        modelprefix=args.modelprefix,
    )


--- File: deeplabcut/pose_estimation_pytorch/apis/videos.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import copy
import logging
import pickle
import time
from pathlib import Path
from typing import Any

import albumentations as A
import numpy as np
import pandas as pd
from tqdm import tqdm

import deeplabcut.pose_estimation_pytorch.apis.utils as utils
import deeplabcut.pose_estimation_pytorch.runners.shelving as shelving
from deeplabcut.core.engine import Engine
from deeplabcut.pose_estimation_pytorch.apis.tracklets import (
    convert_detections2tracklets,
)
from deeplabcut.pose_estimation_pytorch.runners import InferenceRunner, DynamicCropper
from deeplabcut.pose_estimation_pytorch.task import Task
from deeplabcut.refine_training_dataset.stitch import stitch_tracklets
from deeplabcut.utils import auxiliaryfunctions, VideoReader


class VideoIterator(VideoReader):
    """A class to iterate over videos, with possible added context"""

    def __init__(
        self,
        video_path: str | Path,
        context: list[dict[str, Any]] | None = None,
        cropping: list[int] | None = None,
    ) -> None:
        super().__init__(str(video_path))
        self._context = context
        self._index = 0
        self._crop = cropping is not None
        if self._crop:
            self.set_bbox(*cropping)

    def set_crop(self, cropping: list[int] | None = None) -> None:
        """Sets the cropping parameters for the video."""
        self._crop = cropping is not None
        if self._crop:
            self.set_bbox(*cropping)
        else:
            self.set_bbox(0, 1, 0, 1, relative=True)

    def get_context(self) -> list[dict[str, Any]] | None:
        if self._context is None:
            return None

        return copy.deepcopy(self._context)

    def set_context(self, context: list[dict[str, Any]] | None) -> None:
        if context is None:
            self._context = None
            return

        self._context = copy.deepcopy(context)

    def __iter__(self):
        return self

    def __next__(self) -> np.ndarray | tuple[str, dict[str, Any]]:
        frame = self.read_frame(crop=self._crop)
        if frame is None:
            self._index = 0
            self.reset()
            raise StopIteration

        # Otherwise ValueError: At least one stride in the given numpy array is negative,
        # and tensors with negative strides are not currently supported. (You can probably
        # work around this by making a copy of your array  with array.copy().)
        frame = frame.copy()
        if self._context is None:
            self._index += 1
            return frame

        context = copy.deepcopy(self._context[self._index])
        self._index += 1
        return frame, context


def video_inference(
    video: str | Path | VideoIterator,
    pose_runner: InferenceRunner,
    detector_runner: InferenceRunner | None = None,
    cropping: list[int] | None = None,
    shelf_writer: shelving.ShelfWriter | None = None,
    robust_nframes: bool = False,
) -> list[dict[str, np.ndarray]]:
    """Runs inference on a video

    Args:
        video: The video to analyze
        pose_runner: The pose runner to run inference with
        detector_runner: When the pose model is a top-down model, a detector runner can
            be given to obtain bounding boxes for the video. If the pose model is a
            top-down model and no detector_runner is given, the bounding boxes must
            already be set in the VideoIterator (see examples).
        cropping: Optionally, video inference can be run on a cropped version of the
            video. To do so, pass a list containing 4 elements to specify which area
            of the video should be analyzed: ``[xmin, xmax, ymin, ymax]``.
        shelf_writer: By default, data are dumped in a pickle file at the end of the
            video analysis. Passing a shelf manager writes data to disk on-the-fly
            using a "shelf" (a pickle-based, persistent, database-like object by
            default, resulting in constant memory footprint). The returned list is
            then empty.
        robust_nframes: Evaluate a video's number of frames in a robust manner. This
            option is slower (as the whole video is read frame-by-frame), but does not
            rely on metadata, hence its robustness against file corruption.

    Returns:
        Predictions for each frame in the video. If a shelf_manager is given, this list
        will be empty and the predictions will exclusively be stored in the file written
        by the shelf.

    Examples:
        Bottom-up video analysis:
        >>> import deeplabcut.pose_estimation_pytorch as pep
        >>> from deeplabcut.core.config_utils import read_config_as_dict
        >>> model_cfg = read_config_as_dict("pytorch_config.yaml")
        >>> runner = pep.get_pose_inference_runner(model_cfg, "snapshot.pt")
        >>> video_predictions = pep.video_inference("video.mp4", runner)
        >>>

        Top-down video analysis:
        >>> import deeplabcut.pose_estimation_pytorch as pep
        >>> from deeplabcut.core.config_utils import read_config_as_dict
        >>> model_cfg = read_config_as_dict("pytorch_config.yaml")
        >>> runner = pep.get_pose_inference_runner(model_cfg, "snapshot.pt")
        >>> d_runner = pep.get_pose_inference_runner(model_cfg, "snapshot-detector.pt")
        >>> video_predictions = pep.video_inference("video.mp4", runner, d_runner)
        >>>

        Top-Down pose estimation with pre-computed bounding boxes:
        >>> import numpy as np
        >>> import deeplabcut.pose_estimation_pytorch as pep
        >>> from deeplabcut.core.config_utils import read_config_as_dict
        >>>
        >>> video_iterator = pep.VideoIterator("video.mp4")
        >>> video_iterator.set_context([
        >>>     { # frame 1 context
        >>>         "bboxes": np.array([[12, 17, 4, 5]]),  # format (x0, y0, w, h)
        >>>     },
        >>>     { # frame 1 context
        >>>         "bboxes": np.array([[12, 17, 4, 5], [18, 92, 54, 32]]),
        >>>     },
        >>>     ...
        >>> ])
        >>> model_cfg = read_config_as_dict("pytorch_config.yaml")
        >>> runner = pep.get_pose_inference_runner(model_cfg, "snapshot.pt")
        >>> video_predictions = pep.video_inference(video_iterator, runner)
        >>>
    """
    if not isinstance(video, VideoIterator):
        video = VideoIterator(str(video), cropping=cropping)
    elif cropping is not None:
        video.set_crop(cropping)

    n_frames = video.get_n_frames(robust=robust_nframes)
    vid_w, vid_h = video.dimensions
    print(f"Starting to analyze {video.video_path}")
    print(
        f"Video metadata: \n"
        f"  Overall # of frames:    {n_frames}\n"
        f"  Duration of video [s]:  {n_frames / max(1, video.fps):.2f}\n"
        f"  fps:                    {video.fps}\n"
        f"  resolution:             w={vid_w}, h={vid_h}\n"
    )

    if detector_runner is not None:
        print(f"Running detector with batch size {detector_runner.batch_size}")
        bbox_predictions = detector_runner.inference(images=tqdm(video))
        video.set_context(bbox_predictions)

    print(f"Running pose prediction with batch size {pose_runner.batch_size}")
    if shelf_writer is not None:
        shelf_writer.open()

    predictions = pose_runner.inference(images=tqdm(video), shelf_writer=shelf_writer)
    if shelf_writer is not None:
        shelf_writer.close()

    if shelf_writer is None and len(predictions) != n_frames:
        tip_url = "https://deeplabcut.github.io/DeepLabCut/docs/recipes/io.html"
        header = "#tips-on-video-re-encoding-and-preprocessing"
        logging.warning(
            f"The video metadata indicates that there {n_frames} in the video, but "
            f"only {len(predictions)} were able to be processed. This can happen if "
            "the video is corrupted. You can try to fix the issue by re-encoding your "
            f"video (tips on how to do that: {tip_url}{header})"
        )

    return predictions


def analyze_videos(
    config: str,
    videos: str | list[str],
    videotype: str | None = None,
    shuffle: int = 1,
    trainingsetindex: int = 0,
    save_as_csv: bool = False,
    in_random_order: bool = False,
    snapshot_index: int | str | None = None,
    detector_snapshot_index: int | str | None = None,
    device: str | None = None,
    destfolder: str | None = None,
    batch_size: int | None = None,
    detector_batch_size: int | None = None,
    dynamic: tuple[bool, float, int] = (False, 0.5, 10),
    modelprefix: str = "",
    use_shelve: bool = False,
    robust_nframes: bool = False,
    transform: A.Compose | None = None,
    auto_track: bool | None = True,
    n_tracks: int | None = None,
    animal_names: list[str] | None = None,
    calibrate: bool = False,
    identity_only: bool | None = False,
    overwrite: bool = False,
    cropping: list[int] | None = None,
    save_as_df: bool = False,
) -> str:
    """Makes prediction based on a trained network.

    The index of the trained network is specified by parameters in the config file
    (in particular the variable 'snapshot_index').

    Args:
        config: full path of the config.yaml file for the project
        videos: a str (or list of strings) containing the full paths to videos for
            analysis or a path to the directory, where all the videos with same
            extension are stored.
        videotype: checks for the extension of the video in case the input to the video
            is a directory. Only videos with this extension are analyzed. If left
            unspecified, keeps videos with extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv').
        shuffle: An integer specifying the shuffle index of the training dataset used for
            training the network.
        trainingsetindex: Integer specifying which TrainingsetFraction to use.
        save_as_csv: For multi-animal projects and when `auto_track=True`, passed
            along to the `stitch_tracklets` method to save tracks as CSV.
        in_random_order: Whether or not to analyze videos in a random order. This is
            only relevant when specifying a video directory in `videos`.
        device: the device to use for video analysis
        destfolder: specifies the destination folder for analysis data. If ``None``,
            the path of the video is used. Note that for subsequent analysis this
            folder also needs to be passed
        snapshot_index: index (starting at 0) of the snapshot to use to analyze the
            videos. To evaluate the last one, use -1. For example if we have
                - snapshot-0.pt
                - snapshot-50.pt
                - snapshot-100.pt
                - snapshot-best.pt
            and we want to evaluate snapshot-50.pt, snapshotindex should be 1. If None,
            the snapshot index is loaded from the project configuration.
        detector_snapshot_index: (only for top-down models) index of the detector
            snapshot to use, used in the same way as ``snapshot_index``
        dynamic: (state, detection threshold, margin) triplet. If the state is true,
            then dynamic cropping will be performed. That means that if an object is
            detected (i.e. any body part > detection threshold), then object boundaries
            are computed according to the smallest/largest x position and
            smallest/largest y position of all body parts. This  window is expanded by
            the margin and from then on only the posture within this crop is analyzed
            (until the object is lost, i.e. < detection threshold). The current position
            is utilized for updating the crop window for the next frame (this is why the
            margin is important and should be set large enough given the movement of the
            animal).
        modelprefix: directory containing the deeplabcut models to use when evaluating
            the network. By default, they are assumed to exist in the project folder.
        batch_size: the batch size to use for inference. Takes the value from the
            project config as a default.
        detector_batch_size: the batch size to use for detector inference. Takes the
            value from the project config as a default.
        transform: Optional custom transforms to apply to the video
        overwrite: Overwrite any existing videos
        use_shelve: By default, data are dumped in a pickle file at the end of the video
            analysis. Otherwise, data are written to disk on the fly using a "shelf";
            i.e., a pickle-based, persistent, database-like object by default, resulting
            in constant memory footprint.
        robust_nframes: Evaluate a video's number of frames in a robust manner. This
            option is slower (as the whole video is read frame-by-frame), but does not
            rely on metadata, hence its robustness against file corruption.
        auto_track: By default, tracking and stitching are automatically performed,
            producing the final h5 data file. This is equivalent to the behavior for
            single-animal projects.

            If ``False``, one must run ``convert_detections2tracklets`` and
            ``stitch_tracklets`` afterwards, in order to obtain the h5 file.
        n_tracks: Number of tracks to reconstruct. By default, taken as the number of
            individuals defined in the config.yaml. Another number can be passed if the
            number of animals in the video is different from the number of animals the
            model was trained on.
        animal_names: If you want the names given to individuals in the labeled data
            file, you can specify those names as a list here. If given and `n_tracks`
            is None, `n_tracks` will be set to `len(animal_names)`. If `n_tracks` is not
            None, then it must be equal to `len(animal_names)`. If it is not given, then
            `animal_names` will be loaded from the `individuals` in the project
            `config.yaml` file.
        identity_only: sub-call for auto_track. If ``True`` and animal identity was
            learned by the model, assembly and tracking rely exclusively on identity
            prediction.
        cropping: List of cropping coordinates as [x1, x2, y1, y2]. Note that the same
            cropping parameters will then be used for all videos. If different video
            crops are desired, run ``analyze_videos`` on individual videos with the
            corresponding cropping coordinates.
        save_as_df: Cannot be used when `use_shelve` is True. Saves the video
            predictions (before tracking results) to an H5 file containing a pandas
            DataFrame. If ``save_as_csv==True`` than the full predictions will also be
            saved in a CSV file.

    Returns:
        The scorer used to analyze the videos
    """
    # Create the output folder
    _validate_destfolder(destfolder)

    # Load the project configuration
    cfg = auxiliaryfunctions.read_config(config)
    project_path = Path(cfg["project_path"])
    train_fraction = cfg["TrainingFraction"][trainingsetindex]
    model_folder = project_path / auxiliaryfunctions.get_model_folder(
        train_fraction,
        shuffle,
        cfg,
        modelprefix=modelprefix,
        engine=Engine.PYTORCH,
    )
    train_folder = model_folder / "train"

    # Read the inference configuration, load the model
    model_cfg_path = train_folder / Engine.PYTORCH.pose_cfg_name
    model_cfg = auxiliaryfunctions.read_plainconfig(model_cfg_path)
    pose_task = Task(model_cfg["method"])

    pose_cfg_path = model_folder / "test" / "pose_cfg.yaml"
    pose_cfg = auxiliaryfunctions.read_plainconfig(pose_cfg_path)

    snapshot_index, detector_snapshot_index = utils.parse_snapshot_index_for_analysis(
        cfg, model_cfg, snapshot_index, detector_snapshot_index,
    )

    if cropping is None and cfg.get("cropping", False):
        cropping = cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"]

    # Get general project parameters
    multi_animal = cfg["multianimalproject"]
    bodyparts = model_cfg["metadata"]["bodyparts"]
    unique_bodyparts = model_cfg["metadata"]["unique_bodyparts"]
    individuals = model_cfg["metadata"]["individuals"]
    max_num_animals = len(individuals)

    if device is not None:
        model_cfg["device"] = device

    if batch_size is None:
        batch_size = cfg.get("batch_size", 1)

    if not multi_animal:
        save_as_df = True
        if use_shelve:
            print(
                "The ``use_shelve`` parameter cannot be used for single animal "
                "projects. Setting ``use_shelve=False``."
            )
            use_shelve = False

    dynamic = DynamicCropper.build(*dynamic)
    if pose_task != Task.BOTTOM_UP and dynamic is not None:
        print(
            "Turning off dynamic cropping. It should only be used for bottom-up "
            f"pose estimation models, but you are using a top-down model."
        )
        dynamic = None

    snapshot = utils.get_model_snapshots(snapshot_index, train_folder, pose_task)[0]
    print(f"Analyzing videos with {snapshot.path}")
    pose_runner = utils.get_pose_inference_runner(
        model_config=model_cfg,
        snapshot_path=snapshot.path,
        max_individuals=max_num_animals,
        batch_size=batch_size,
        transform=transform,
        dynamic=dynamic,
    )
    detector_runner = None

    detector_path, detector_snapshot = None, None
    if pose_task == Task.TOP_DOWN:
        if detector_snapshot_index is None:
            raise ValueError(
                "Cannot run videos analysis for top-down models without a detector "
                "snapshot! Please specify your desired detector_snapshotindex in your "
                "project's configuration file."
            )

        if detector_batch_size is None:
            detector_batch_size = cfg.get("detector_batch_size", 1)

        detector_snapshot = utils.get_model_snapshots(
            detector_snapshot_index, train_folder, Task.DETECT
        )[0]
        print(f"  -> Using detector {detector_snapshot.path}")
        detector_runner = utils.get_detector_inference_runner(
            model_config=model_cfg,
            snapshot_path=detector_snapshot.path,
            max_individuals=max_num_animals,
            batch_size=detector_batch_size,
        )

    dlc_scorer = utils.get_scorer_name(
        cfg,
        shuffle,
        train_fraction,
        snapshot_uid=utils.get_scorer_uid(snapshot, detector_snapshot),
        modelprefix=modelprefix,
    )

    # Reading video and init variables
    videos = utils.list_videos_in_folder(videos, videotype, shuffle=in_random_order)
    for video in videos:
        if destfolder is None:
            output_path = video.parent
        else:
            output_path = Path(destfolder)

        output_prefix = video.stem + dlc_scorer
        output_pkl = output_path / f"{output_prefix}_full.pickle"

        video_iterator = VideoIterator(video, cropping=cropping)

        shelf_writer = None
        if use_shelve:
            shelf_writer = shelving.ShelfWriter(
                pose_cfg=pose_cfg,
                filepath=output_pkl,
                num_frames=video_iterator.get_n_frames(robust=robust_nframes),
            )

        if not overwrite and output_pkl.exists():
            print(f"Video {video} already analyzed at {output_pkl}!")
        else:
            runtime = [time.time()]
            predictions = video_inference(
                video=video_iterator,
                pose_runner=pose_runner,
                detector_runner=detector_runner,
                shelf_writer=shelf_writer,
                robust_nframes=robust_nframes,
            )
            runtime.append(time.time())
            metadata = _generate_metadata(
                cfg=cfg,
                pytorch_config=model_cfg,
                dlc_scorer=dlc_scorer,
                train_fraction=train_fraction,
                batch_size=batch_size,
                cropping=cropping,
                runtime=(runtime[0], runtime[1]),
                video=video_iterator,
                robust_nframes=robust_nframes,
            )

            with open(output_path / f"{output_prefix}_meta.pickle", "wb") as f:
                pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)

            if use_shelve and save_as_df:
                print("Can't ``save_as_df`` as ``use_shelve=True``. Skipping.")

            if not use_shelve:
                output_data = _generate_output_data(pose_cfg, predictions)
                with open(output_pkl, "wb") as f:
                    pickle.dump(output_data, f, pickle.HIGHEST_PROTOCOL)

                if save_as_df:
                    create_df_from_prediction(
                        predictions=predictions,
                        multi_animal=multi_animal,
                        model_cfg=model_cfg,
                        dlc_scorer=dlc_scorer,
                        output_path=output_path,
                        output_prefix=output_prefix,
                        save_as_csv=save_as_csv,
                    )

            if multi_animal:
                _generate_assemblies_file(
                    full_data_path=output_pkl,
                    output_path=output_path / f"{output_prefix}_assemblies.pickle",
                    num_bodyparts=len(bodyparts),
                    num_unique_bodyparts=len(unique_bodyparts),
                )

                if auto_track:
                    convert_detections2tracklets(
                        config=config,
                        videos=str(video),
                        videotype=videotype,
                        shuffle=shuffle,
                        trainingsetindex=trainingsetindex,
                        overwrite=False,
                        identity_only=identity_only,
                        destfolder=str(output_path),
                    )
                    stitch_tracklets(
                        config,
                        [str(video)],
                        videotype,
                        shuffle,
                        trainingsetindex,
                        n_tracks=n_tracks,
                        animal_names=animal_names,
                        destfolder=str(output_path),
                        save_as_csv=save_as_csv,
                    )

    print(
        "The videos are analyzed. Now your research can truly start!\n"
        "You can create labeled videos with 'create_labeled_video'.\n"
        "If the tracking is not satisfactory for some videos, consider expanding the "
        "training set. You can use the function 'extract_outlier_frames' to extract a "
        "few representative outlier frames.\n"
    )

    return dlc_scorer


def create_df_from_prediction(
    predictions: list[dict[str, np.ndarray]],
    dlc_scorer: str,
    multi_animal: bool,
    model_cfg: dict,
    output_path: str | Path,
    output_prefix: str | Path,
    save_as_csv: bool = False,
) -> pd.DataFrame:
    pred_bodyparts = np.stack([p["bodyparts"][..., :3] for p in predictions])
    pred_unique_bodyparts = None
    if len(predictions) > 0 and "unique_bodyparts" in predictions[0]:
        pred_unique_bodyparts = np.stack([p["unique_bodyparts"] for p in predictions])

    output_h5 = Path(output_path) / f"{output_prefix}.h5"
    output_pkl = Path(output_path) / f"{output_prefix}_full.pickle"

    bodyparts = model_cfg["metadata"]["bodyparts"]
    unique_bodyparts = model_cfg["metadata"]["unique_bodyparts"]
    individuals = model_cfg["metadata"]["individuals"]
    n_individuals = len(individuals)

    print(f"Saving results in {output_h5} and {output_pkl}")
    coords = ["x", "y", "likelihood"]
    cols = [[dlc_scorer], bodyparts, coords]
    cols_names = ["scorer", "bodyparts", "coords"]

    if multi_animal:
        cols.insert(1, individuals)
        cols_names.insert(1, "individuals")

    results_df_index = pd.MultiIndex.from_product(cols, names=cols_names)
    pred_bodyparts = pred_bodyparts[:, :n_individuals]
    df = pd.DataFrame(
        pred_bodyparts.reshape((len(pred_bodyparts), -1)),
        columns=results_df_index,
        index=range(len(pred_bodyparts)),
    )
    if pred_unique_bodyparts is not None:
        unique_columns = [dlc_scorer], ["single"], unique_bodyparts, coords
        df_u = pd.DataFrame(
            pred_unique_bodyparts.reshape((len(pred_unique_bodyparts), -1)),
            columns=pd.MultiIndex.from_product(unique_columns, names=cols_names),
            index=range(len(pred_unique_bodyparts)),
        )
        df = df.join(df_u, how="outer")

    df.to_hdf(output_h5, key="df_with_missing", format="table", mode="w")
    if save_as_csv:
        df.to_csv(output_h5.with_suffix(".csv"))
    return df


def _generate_assemblies_file(
    full_data_path: Path,
    output_path: Path,
    num_bodyparts: int,
    num_unique_bodyparts: int,
) -> None:
    """Generates the assemblies file from predictions"""
    if full_data_path.exists():
        with open(full_data_path, "rb") as f:
            data = pickle.load(f)

    else:
        data = shelving.ShelfReader(full_data_path)
        data.open()

    num_frames = data["metadata"]["nframes"]
    str_width = data["metadata"].get("key_str_width")
    if str_width is None:
        keys = [k for k in data.keys() if k != "metadata"]
        str_width = len(keys[0]) - len("frame")

    assemblies = dict(single=dict())
    for frame_index in range(num_frames):
        frame_key = "frame" + str(frame_index).zfill(str_width)
        predictions = data[frame_key]

        keypoint_preds = predictions["coordinates"][0]
        keypoint_scores = predictions["confidence"]

        bpts = np.stack(keypoint_preds[:num_bodyparts])
        scores = np.stack(keypoint_scores[:num_bodyparts])
        preds = np.concatenate([bpts, scores], axis=-1)

        keypoint_id_scores = predictions.get("identity")
        if keypoint_id_scores is not None:
            keypoint_id_scores = np.stack(keypoint_id_scores[:num_bodyparts])
            keypoint_pred_ids = np.argmax(keypoint_id_scores, axis=2)
            keypoint_pred_ids = np.expand_dims(keypoint_pred_ids, axis=-1)
        else:
            num_bpts, num_preds = preds.shape[:2]
            keypoint_pred_ids = -np.ones((num_bpts, num_preds, 1))

        # reshape to (num_preds, num_bpts, 4)
        preds = np.concatenate([preds, keypoint_pred_ids], axis=-1)
        preds = preds.transpose((1, 0, 2))

        # remove all-missing predictions
        mask = ~np.all(preds < 0, axis=(1, 2))
        preds = preds[mask]

        assemblies[frame_index] = preds

        if num_unique_bodyparts > 0:
            unique_bpts = np.stack(keypoint_preds[num_bodyparts:])
            unique_scores = np.stack(keypoint_scores[num_bodyparts:])
            unique_preds = np.concatenate([unique_bpts, unique_scores], axis=-1)
            unique_preds = unique_preds.transpose((1, 0, 2))
            assemblies["single"][frame_index] = unique_preds[0]  # single prediction

    with open(output_path, "wb") as file:
        pickle.dump(assemblies, file, pickle.HIGHEST_PROTOCOL)

    if isinstance(data, shelving.ShelfReader):
        data.close()


def _validate_destfolder(destfolder: str | None) -> None:
    """Checks that the destfolder for video analysis is valid"""
    if destfolder is not None and destfolder != "":
        output_folder = Path(destfolder)
        if not output_folder.exists():
            print(f"Creating the output folder {output_folder}")
            output_folder.mkdir(parents=True)

        assert Path(
            output_folder
        ).is_dir(), f"Output folder must be a directory: you passed '{output_folder}'"


def _generate_metadata(
    cfg: dict,
    pytorch_config: dict,
    dlc_scorer: str,
    train_fraction: int,
    batch_size: int,
    cropping: list[int] | None,
    runtime: tuple[float, float],
    video: VideoIterator,
    robust_nframes: bool = False,
) -> dict:
    w, h = video.dimensions
    if cropping is None:
        cropping_parameters = [0, w, 0, h]
    else:
        if not len(cropping) == 4:
            raise ValueError(
                "The cropping parameters should be exactly 4 values: [x_min, x_max, "
                f"y_min, y_max]. Found {cropping}"
            )
        cropping_parameters = cropping

    metadata = {
        "start": runtime[0],
        "stop": runtime[1],
        "run_duration": runtime[1] - runtime[0],
        "Scorer": dlc_scorer,
        "pytorch-config": pytorch_config,
        "fps": video.fps,
        "batch_size": batch_size,
        "frame_dimensions": (w, h),
        "nframes": video.get_n_frames(robust=robust_nframes),
        "iteration (active-learning)": cfg["iteration"],
        "training set fraction": train_fraction,
        "cropping": cropping is not None,
        "cropping_parameters": cropping_parameters,
        "individuals": pytorch_config["metadata"]["individuals"],
        "bodyparts": pytorch_config["metadata"]["bodyparts"],
        "unique_bodyparts": pytorch_config["metadata"]["unique_bodyparts"],
    }
    return {"data": metadata}


def _generate_output_data(
    pose_config: dict,
    predictions: list[dict[str, np.ndarray]],
) -> dict:
    str_width = int(np.ceil(np.log10(len(predictions))))
    output = {
        "metadata": {
            "nms radius": pose_config.get("nmsradius"),
            "minimal confidence": pose_config.get("minconfidence"),
            "sigma": pose_config.get("sigma", 1),
            "PAFgraph": pose_config.get("partaffinityfield_graph"),
            "PAFinds": pose_config.get(
                "paf_best",
                np.arange(len(pose_config.get("partaffinityfield_graph", []))),
            ),
            "all_joints": [[i] for i in range(len(pose_config["all_joints"]))],
            "all_joints_names": [
                pose_config["all_joints_names"][i]
                for i in range(len(pose_config["all_joints"]))
            ],
            "nframes": len(predictions),
            "key_str_width": str_width,
        }
    }

    for frame_num, frame_predictions in enumerate(predictions):
        key = "frame" + str(frame_num).zfill(str_width)
        # shape (num_assemblies, num_bpts, 3)
        bodyparts = frame_predictions["bodyparts"]
        # shape (num_bpts, num_assemblies, 3)
        bodyparts = bodyparts.transpose((1, 0, 2))
        coordinates = [bpt[:, :2] for bpt in bodyparts]
        scores = [bpt[:, 2:3] for bpt in bodyparts]

        # full pickle has bodyparts and unique bodyparts in same array
        num_unique = 0
        if "unique_bodyparts" in frame_predictions:
            unique_bpts = frame_predictions["unique_bodyparts"].transpose((1, 0, 2))
            coordinates += [bpt[:, :2] for bpt in unique_bpts]
            scores += [bpt[:, 2:] for bpt in unique_bpts]
            num_unique = len(unique_bpts)

        output[key] = {
            "coordinates": (coordinates,),
            "confidence": scores,
            "costs": None,
        }

        if "bboxes" in frame_predictions:
            output[key]["bboxes"] = frame_predictions["bboxes"]
            output[key]["bbox_scores"] = frame_predictions["bbox_scores"]

        if "identity_scores" in frame_predictions:
            # Reshape id scores from (num_assemblies, num_bpts, num_individuals)
            # to the original DLC full pickle format: (num_bpts, num_assem, num_ind)
            id_scores = frame_predictions["identity_scores"]
            id_scores = id_scores.transpose((1, 0, 2))
            output[key]["identity"] = [bpt_id_scores for bpt_id_scores in id_scores]

            if num_unique > 0:
                # needed for create_video_with_all_detections to display unique bpts
                num_assem, num_ind = id_scores.shape[1:]
                output[key]["identity"] += [
                    -1 * np.ones((num_assem, num_ind)) for i in range(num_unique)
                ]

    return output


--- File: deeplabcut/pose_estimation_pytorch/config/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.config.make_pose_config import (
    make_basic_project_config,
    make_pytorch_pose_config,
    make_pytorch_test_config,
)
from deeplabcut.pose_estimation_pytorch.config.utils import (
    available_detectors,
    available_models,
    update_config,
    update_config_by_dotpath,
)
# For backwards compatibility
from deeplabcut.core.config import (
    read_config_as_dict,
    write_config,
    pretty_print,
)

--- File: deeplabcut/pose_estimation_pytorch/config/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Util functions to create pytorch pose configuration files"""
from __future__ import annotations

import copy
from pathlib import Path

from deeplabcut.core.config import read_config_as_dict
from deeplabcut.utils import auxiliaryfunctions


def replace_default_values(
    config: dict | list,
    num_bodyparts: int | None = None,
    num_individuals: int | None = None,
    backbone_output_channels: int | None = None,
    **kwargs,
) -> dict:
    """Replaces placeholder values in a model configuration with their actual values.

    This method allows to create template PyTorch configurations for models with values
    such as "num_bodyparts", which are replaced with the number of bodyparts for a
    project when making its Pytorch configuration.

    This code can also do some basic arithmetic. You can write "num_bodyparts x 2" (or
    any factor other than 2) for location refinement channels, and the number of
    channels will be twice the number of bodyparts. You can write
    "backbone_output_channels // 2" for the number of channels in a layer, and it will
    be half the number of channels output by the backbone. You can write
    "num_bodyparts + 1" (such as for DEKR heatmaps, where a "center" bodypart is added).

    The three base placeholder values that can be computed are "num_bodyparts",
    "num_individuals" and "backbone_output_channels". You can add more through the
    keyword arguments (such as "paf_graph": list[tuple[int, int]] or
    "paf_edges_to_keep": list[int] for DLCRNet models).

    Args:
        config: the configuration in which to replace default values
        num_bodyparts: the number of bodyparts
        num_individuals: the number of individuals
        backbone_output_channels: the number of backbone output channels
        kwargs: other placeholder values to fill in

    Returns:
        the configuration with placeholder values replaced

    Raises:
        ValueError: if there is a placeholder value who's "updated" value was not
            given to the method
    """

    def get_updated_value(variable: str) -> int | list[int]:
        var_parts = variable.strip().split(" ")
        var_name = var_parts[0]
        if updated_values[var_name] is None:
            raise ValueError(
                f"Found {variable} in the configuration file, but there is no default "
                f"value for this variable."
            )

        if len(var_parts) == 1:
            return updated_values[var_name]
        elif len(var_parts) == 3:
            operator, factor = var_parts[1], var_parts[2]
            if not factor.isdigit():
                raise ValueError(f"F must be an integer in variable: {variable}")

            factor = int(factor)
            if operator == "+":
                return updated_values[var_name] + factor
            elif operator == "x":
                return updated_values[var_name] * factor
            elif operator == "//":
                return updated_values[var_name] // factor
            else:
                raise ValueError(f"Unknown operator for variable: {variable}")

        raise ValueError(
            f"Found {variable} in the configuration file, but cannot parse it."
        )

    updated_values = {
        "num_bodyparts": num_bodyparts,
        "num_individuals": num_individuals,
        "backbone_output_channels": backbone_output_channels,
        **kwargs,
    }

    config = copy.deepcopy(config)
    if isinstance(config, dict):
        keys_to_update = list(config.keys())
    elif isinstance(config, list):
        keys_to_update = range(len(config))
    else:
        raise ValueError(f"Config to update must be dict or list, found {type(config)}")

    for k in keys_to_update:
        if isinstance(config[k], (list, dict)):
            config[k] = replace_default_values(
                config[k],
                num_bodyparts,
                num_individuals,
                backbone_output_channels,
                **kwargs,
            )
        elif (
            isinstance(config[k], str)
            and config[k].strip().split(" ")[0] in updated_values.keys()
        ):
            config[k] = get_updated_value(config[k])

    return config


def update_config(config: dict, updates: dict, copy_original: bool = True) -> dict:
    """Updates items in the configuration file

    The configuration dict should only be composed of primitive Python types
    (dict, list and values). This is the case when reading the file using
    `read_config_as_dict`.

    Args:
        config: the configuration dict to update
        updates: the updates to make to the configuration dict
        copy_original: whether to copy the original dict before updating it

    Returns:
        the updated dictionary
    """
    if copy_original:
        config = copy.deepcopy(config)

    for k, v in updates.items():
        if k in config and isinstance(config[k], dict) and isinstance(v, dict):
            if k in ("optimizer", "scheduler") and config["type"] != v["type"]:
                # if changing the optimizer or scheduler type, update all values
                config[k] = v
            else:
                config[k] = update_config(config[k], v, copy_original=False)
        else:
            config[k] = copy.deepcopy(v)
    return config


def update_config_by_dotpath(
    config: dict, updates: dict, copy_original: bool = True
) -> dict:
    """Updates items in the configuration file using dot notation for nested keys

    The configuration dict should only be composed of primitive Python types
    (dict, list and values). This is the case when reading the file using
    `read_config_as_dict`.

    Args:
        config: the configuration dict to update
        updates: single-level dict with dot notation keys indicating nested paths
            e.g. {"device": "cuda", "runner.gpus": [0,1]}
        copy_original: whether to copy the original dict before updating it

    Returns:
        the updated dictionary
    """
    if copy_original:
        config = copy.deepcopy(config)

    for key, value in updates.items():
        # Split key into parts by dots
        parts = key.split(".")

        # Handle non-nested case
        if len(parts) == 1:
            config[key] = copy.deepcopy(value)
            continue

        # Navigate to nested location
        current = config
        for part in parts[:-1]:
            if part not in current:
                current[part] = {}
            current = current[part]

        # Set the value at final location
        current[parts[-1]] = copy.deepcopy(value)

    return config


def get_config_folder_path() -> Path:
    """Returns: the Path to the folder containing the "configs" for DeepLabCut 3.0"""
    dlc_parent_path = Path(auxiliaryfunctions.get_deeplabcut_path())
    return dlc_parent_path / "pose_estimation_pytorch" / "config"


def load_base_config(config_folder_path: Path) -> dict:
    """Returns: the base configuration for all PyTorch DeepLabCut models"""
    base_dir = config_folder_path / "base"
    base_config = read_config_as_dict(base_dir / "base.yaml")
    return base_config


def load_backbones(configs_dir: Path) -> list[str]:
    """
    Args:
        configs_dir: the Path to the folder containing the "configs" for PyTorch
            DeepLabCut

    Returns:
        all backbones with default configurations that can be used
    """
    backbone_dir = configs_dir / "backbones"
    backbones = [p.stem for p in backbone_dir.iterdir() if p.suffix == ".yaml"]
    return backbones


def load_detectors(configs_dir: Path) -> list[str]:
    """
    Args:
        configs_dir: the Path to the folder containing the "configs" for PyTorch
            DeepLabCut

    Returns:
        all detectors that are available
    """
    detector_dir = configs_dir / "detectors"
    detectors = [p.stem for p in detector_dir.iterdir() if p.suffix == ".yaml"]
    return detectors


def available_models() -> list[str]:
    """Returns: the possible variants of models that can be used"""
    configs_folder_path = get_config_folder_path()
    backbones = load_backbones(configs_folder_path)
    models = set()
    for backbone in backbones:
        models.add(backbone)
        models.add("top_down_" + backbone)

    other_architectures = [
        p
        for p in configs_folder_path.iterdir()
        if p.is_dir() and not p.name in ("backbones", "base", "detectors")
    ]
    for folder in other_architectures:
        variants = [p.stem for p in folder.iterdir() if p.suffix == ".yaml"]
        for variant in variants:
            models.add(variant)

    return list(sorted(models))


def available_detectors() -> list[str]:
    """Returns: all the possible detectors that can be used"""
    return load_detectors(get_config_folder_path())


--- File: deeplabcut/pose_estimation_pytorch/config/make_pose_config.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Methods to create the configuration files for PyTorch DeepLabCut models"""
from __future__ import annotations

import copy
from pathlib import Path

from deeplabcut.core.config import read_config_as_dict, write_config
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.pose_estimation_pytorch.config.utils import (
    get_config_folder_path,
    load_backbones,
    load_base_config,
    replace_default_values,
    update_config,
)
from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal


def make_pytorch_pose_config(
    project_config: dict,
    pose_config_path: str | Path,
    net_type: str | None = None,
    top_down: bool = False,
    detector_type: str | None = None,
    weight_init: WeightInitialization | None = None,
    save: bool = False,
) -> dict:
    """Creates a PyTorch pose configuration file for a DeepLabCut project

    The base/ folder contains default configurations, such as data augmentations or
    heatmap heads (that can be used to predict pose or identity based on visual
    features). These files are used to create pose model configurations.

    All available backbone configurations are stored in the backbones/ folder.
        - any backbone can be a single animal model with a heatmap head added on top
        - any backbone can be a top-down model with a detector and a heatmap head
        - any backbone can be a bottom-up model with a detector and a heatmap + PAF head

    All other model architectures have their own folders, with different variants
    available. Top-down model architectures must specify `method: TD` in their
    configuration files, from which this method adds a backbone configuration.

    Placeholder values (such as `num_bodyparts` or `num_individuals`) are filled in
    based on the project config file.

    Args:
        project_config: the DeepLabCut project config
        pose_config_path: the path where the pytorch pose configuration will be saved
        net_type: the architecture of the desired pose estimation model
        top_down: when the net_type is a backbone, whether to create a top-down model
            by associating a detector to the pose model. Required for multi-animal
            projects when net_type is a backbone (as a backbone + heatmap head can only
            predict pose for single individuals).
        detector_type: for top-down pose models, the architecture of the desired object
            detection model
        weight_init: Specify how model weights should be initialized. If None, ImageNet
            pretrained weights from Timm will be loaded when training.
        save: Whether to save the model configuration file to the ``pose_config_path``.

    Returns:
        the PyTorch pose configuration file
    """
    multianimal_project = project_config.get("multianimalproject", False)
    individuals = project_config.get("individuals", ["single"])
    with_identity = project_config.get("identity")
    bodyparts = auxiliaryfunctions.get_bodyparts(project_config)
    unique_bpts = auxiliaryfunctions.get_unique_bodyparts(project_config)

    if net_type is None:
        net_type = project_config.get("default_net_type", "resnet_50")

    configs_dir = get_config_folder_path()
    pose_config = load_base_config(configs_dir)
    pose_config = add_metadata(project_config, pose_config, pose_config_path)
    pose_config["net_type"] = net_type

    backbones = load_backbones(configs_dir)
    if net_type in backbones:
        if not top_down and multianimal_project:
            model_cfg = create_backbone_with_paf_model(
                configs_dir=configs_dir,
                net_type=net_type,
                num_individuals=len(individuals),
                bodyparts=bodyparts,
                paf_parameters=_get_paf_parameters(project_config, bodyparts),
            )
        else:
            model_cfg = create_backbone_with_heatmap_model(
                configs_dir=configs_dir,
                net_type=net_type,
                multianimal_project=multianimal_project,
                bodyparts=bodyparts,
                top_down=top_down,
            )
    else:
        architecture = net_type.split("_")[0]
        default_value_kwargs = {}
        if architecture == "dlcrnet":
            default_value_kwargs.update(_get_paf_parameters(project_config, bodyparts))

        cfg_path = configs_dir / architecture / f"{net_type}.yaml"
        model_cfg = read_config_as_dict(cfg_path)
        model_cfg = replace_default_values(
            model_cfg,
            num_bodyparts=len(bodyparts),
            num_individuals=len(individuals),
            **default_value_kwargs,
        )

    is_top_down = model_cfg.get("method", "BU").upper() == "TD"
    if is_top_down:
        model_cfg = add_detector(
            configs_dir,
            model_cfg,
            len(individuals),
            detector_type=detector_type,
        )

    # add the default augmentations to the config
    aug_filename = "aug_top_down.yaml" if is_top_down else "aug_default.yaml"
    aug_cfg = {"data": read_config_as_dict(configs_dir / "base" / aug_filename)}
    pose_config = update_config(pose_config, aug_cfg)

    # add the model to the config
    pose_config = update_config(pose_config, model_cfg)

    # set the dataset from which to load weights
    if weight_init is not None:
        pose_config["train_settings"]["weight_init"] = weight_init.to_dict()

    # add a unique bodypart head if needed
    if len(unique_bpts) > 0:
        if is_top_down:
            raise ValueError(
                f"You selected a top-down model architecture ({net_type}), but you have"
                f" unique bodyparts, which is not yet implemented for top-down models."
                " Please select a bottom-up architecture such as `resnet_50` for single"
                " animal projects or `dlcrnet_50` for multi-animal projects."
            )

        pose_config = add_unique_bodypart_head(
            configs_dir,
            pose_config,
            num_unique_bodyparts=len(unique_bpts),
            backbone_output_channels=pose_config["model"]["backbone_output_channels"],
        )

    # add an identity head if needed
    if with_identity:
        if is_top_down:
            raise ValueError(
                f"You selected a top-down model architecture ({net_type}), but you have"
                f" set `identity: true`, which is not yet implemented for top-down"
                f" models. Please select a bottom-up architecture such as `dlcrnet_50`"
                f" to train with identity, or set `identity: false`."
            )

        pose_config = add_identity_head(
            configs_dir,
            pose_config,
            num_individuals=len(individuals),
            backbone_output_channels=pose_config["model"]["backbone_output_channels"],
        )

    # sort first-level keys to make it prettier
    pose_config = dict(sorted(pose_config.items()))

    if save:
        write_config(pose_config_path, pose_config, overwrite=True)

    return pose_config


def make_pytorch_test_config(
    model_config: dict,
    test_config_path: str | Path,
    save: bool = False,
) -> dict:
    """Creates the test configuration for a model

    Args:
        model_config: The PyTorch config for the model.
        test_config_path: The path of the test config
        save: Whether to save the test config to ``test_config_path``.

    Returns:
        The test configuration file.
    """
    bodyparts = model_config["metadata"]["bodyparts"]
    unique_bodyparts = model_config["metadata"]["unique_bodyparts"]
    all_joint_names = bodyparts + unique_bodyparts

    test_config = dict(
        dataset=model_config["metadata"]["project_path"],
        dataset_type="multi-animal-imgaug",  # required for downstream tracking
        num_joints=len(all_joint_names),
        all_joints=[[i] for i in range(len(all_joint_names))],
        all_joints_names=all_joint_names,
        net_type=model_config["net_type"],
        global_scale=1,
        scoremap_dir="test",
    )
    if save:
        write_config(test_config_path, test_config)

    return test_config


def make_basic_project_config(
    dataset_path: Path | str,
    bodyparts: list[str],
    max_individuals: int,
    multi_animal: bool = True,
) -> dict:
    """Creates a basic configuration dict that can be used to create model configs.

    This should be used to create the `project_config` given to
    `make_pytorch_pose_config` for non-DeepLabCut projects (e.g. when creating a
    configuration file for a model that will be trained on a COCO dataset).

    Args:
        dataset_path: The path to the dataset for which the config will be created.
        bodyparts: The bodyparts labeled for individuals in the dataset.
        max_individuals: The maximum number of individuals to detect in a single image.
        multi_animal: Whether multiple animals can be present in an image.

    Returns:
        The created project configuration dict that can be given to
        `make_pytorch_pose_config`.

    Examples:
        Creating a `pytorch_config` for a ResNet50 backbone with a part-affinity head (
        as multi_animal=True and top_down=False)

        >>> import deeplabcut.pose_estimation_pytorch as pep
        >>> project_config = pep.config.make_basic_project_config(
        >>>     dataset_path="/path/coco",
        >>>     bodyparts=["nose", "left_eye", "right_eye"],
        >>>     max_individuals=12,
        >>>     multi_animal=True,
        >>> )
        >>> model_config = pep.config.make_pytorch_pose_config(
        >>>     project_config=project_config,
        >>>     pose_config_path="/path/coco/models/resnet50/pytorch_config.yaml",
        >>>     net_type="resnet_50",
        >>>     top_down=False,
        >>>     save=True,
        >>> )

        Creating a `pytorch_config` for a ResNet50 backbone with a simple heatmap head
        (as the project is single-animal):

        >>> import deeplabcut.pose_estimation_pytorch as pep
        >>> project_config = pep.config.make_basic_project_config(
        >>>     dataset_path="/path/coco",
        >>>     bodyparts=["nose", "left_eye", "right_eye"],
        >>>     max_individuals=1,
        >>>     multi_animal=False,
        >>> )
        >>> model_config = pep.config.make_pytorch_pose_config(
        >>>     project_config=project_config,
        >>>     pose_config_path="/path/coco/models/resnet50/pytorch_config.yaml",
        >>>     net_type="resnet_50",
        >>>     top_down=False,
        >>>     save=True,
        >>> )
    """
    return dict(
        project_path=str(dataset_path),
        multianimalproject=multi_animal,
        bodyparts=bodyparts,
        multianimalbodyparts=bodyparts,
        uniquebodyparts=[],
        individuals=[f"individual{i:03d}" for i in range(max_individuals)],
    )


def add_metadata(
    project_config: dict, config: dict, pose_config_path: str | Path
) -> dict:
    """Adds metadata to a pytorch pose configuration

    Args:
        project_config: the project configuration
        config: the pytorch pose configuration
        pose_config_path: the path where the pytorch pose configuration will be saved

    Returns:
        the configuration with a `meta` key added
    """
    config = copy.deepcopy(config)
    config["metadata"] = {
        "project_path": project_config["project_path"],
        "pose_config_path": str(pose_config_path),
        "bodyparts": auxiliaryfunctions.get_bodyparts(project_config),
        "unique_bodyparts": auxiliaryfunctions.get_unique_bodyparts(project_config),
        "individuals": project_config.get("individuals", ["animal"]),
        "with_identity": project_config.get("identity", False),
    }
    return config


def create_backbone_with_heatmap_model(
    configs_dir: Path,
    net_type: str,
    multianimal_project: bool,
    bodyparts: list[str],
    top_down: bool,
) -> dict:
    """
    Creates a simple heatmap pose estimation model, composed of a backbone and a head
    predicting heatmaps and location refinement maps

    Args:
        configs_dir: path to the DeepLabCut "configs" directory
        net_type: the type of backbone to create the model with (e.g., resnet_50)
        multianimal_project: whether this model is created for a multi-animal project
        bodyparts: the bodyparts to detect
        top_down: whether the model will be associated to a detector to form a top-down
            pose estimation model

    Returns:
        the backbone + heatmap model configuration

    Raises:
        ValueError: if the model is being created for a multi-animal project but the
            head won't be associated with a detector (heatmaps can only predict
            bodyparts for a single individual).
    """
    if multianimal_project and not top_down:
        raise ValueError(
            "A pose model formed of a backbone and simple heatmap + location refinement"
            " head can only be used for single animal projects. As you're working with"
            " a multi-animal project, please select a multi-individual model instead of"
            f" {net_type} or use a detector to create a top-down model (create your"
            f" configuration with `make_pytorch_pose_config(..., top_down=True)`)."
        )

    # add the backbone to the config
    model_config = read_config_as_dict(configs_dir / "backbones" / f"{net_type}.yaml")
    backbone_output_channels = model_config["model"]["backbone_output_channels"]

    model_config["method"] = "bu"
    bodypart_head_name = "head_bodyparts.yaml"
    if top_down:
        model_config["method"] = "td"
        bodypart_head_name = "head_topdown.yaml"

    # add a bodypart head
    bodypart_head_config = read_config_as_dict(
        configs_dir / "base" / bodypart_head_name
    )
    model_config["model"]["heads"] = {
        "bodypart": replace_default_values(
            bodypart_head_config,
            num_bodyparts=len(bodyparts),
            backbone_output_channels=backbone_output_channels,
        )
    }
    return model_config


def create_backbone_with_paf_model(
    configs_dir: Path,
    net_type: str,
    num_individuals: int,
    bodyparts: list[str],
    paf_parameters: dict,
) -> dict:
    """
    Creates a pose estimation model, composed of a backbone and a head predicting
    heatmaps, location refinement maps and part affinity fields for multi-animal pose
    estimation.

    Args:
        configs_dir: path to the DeepLabCut "configs" directory
        net_type: the type of backbone to create the model with (e.g., resnet_50)
        num_individuals: the maximum number of individuals in a frame
        bodyparts: the bodyparts to detect
        paf_parameters: the parameters for the PAF

    Returns:
        the backbone + heatmap, location refinement, PAF model configuration
    """
    # add the backbone to the config
    model_config = read_config_as_dict(configs_dir / "backbones" / f"{net_type}.yaml")
    backbone_output_channels = model_config["model"]["backbone_output_channels"]

    # add a bodypart head
    bodypart_head_config = read_config_as_dict(
        configs_dir / "base" / f"head_bodyparts_with_paf.yaml"
    )
    model_config["model"]["heads"] = {
        "bodypart": replace_default_values(
            bodypart_head_config,
            num_bodyparts=len(bodyparts),
            num_individuals=num_individuals,
            backbone_output_channels=backbone_output_channels,
            **paf_parameters,
        )
    }
    return model_config


def add_detector(
    configs_dir: Path,
    config: dict,
    num_individuals: int,
    detector_type: str | None = None,
) -> dict:
    """Adds a detector to a model

    Args:
        configs_dir: path to the DeepLabCut "configs" directory
        config: model configuration to update
        num_individuals: the maximum number of individuals the model should detect
        detector_type: the type of detector to use (if None, uses ``ssdlite``)

    Returns:
        the model configuration with an added detector config
    """
    if detector_type is None:
        detector_type = "ssdlite"  # default detector

    detector_type = detector_type.lower()
    config = copy.deepcopy(config)
    detector_config = update_config(
        read_config_as_dict(configs_dir / "base" / "base_detector.yaml"),
        read_config_as_dict(configs_dir / "detectors" / f"{detector_type}.yaml"),
    )
    detector_config = replace_default_values(
        detector_config, num_individuals=num_individuals,
    )
    config["detector"] = dict(sorted(detector_config.items()))
    return config


def add_unique_bodypart_head(
    configs_dir: Path,
    config: dict,
    num_unique_bodyparts: int,
    backbone_output_channels: int,
) -> dict:
    """Adds a unique bodypart head to a model

    Args:
        configs_dir: path to the DeepLabCut "configs" directory
        config: model configuration to update
        num_unique_bodyparts: the number of unique bodyparts to detect
        backbone_output_channels: the number of channels output by the model backbone

    Returns:
        the configuration with an added unique bodypart head
    """
    config = copy.deepcopy(config)
    unique_head_config = replace_default_values(
        read_config_as_dict(configs_dir / "base" / "head_bodyparts.yaml"),
        num_bodyparts=num_unique_bodyparts,
        backbone_output_channels=backbone_output_channels,
    )
    unique_head_config["target_generator"]["label_keypoint_key"] = "keypoints_unique"
    config["model"]["heads"]["unique_bodypart"] = unique_head_config
    return config


def add_identity_head(
    configs_dir: Path,
    config: dict,
    num_individuals: int,
    backbone_output_channels: int,
) -> dict:
    """Adds an identity head to a model

    Args:
        configs_dir: path to the DeepLabCut "configs" directory
        config: model configuration to update
        num_individuals: the number of individuals to re-identify
        backbone_output_channels: the number of channels output by the model backbone

    Returns:
        the configuration with an added identity head
    """
    config = copy.deepcopy(config)
    id_head_config = read_config_as_dict(configs_dir / "base" / "head_identity.yaml")
    config["model"]["heads"]["identity"] = replace_default_values(
        id_head_config,
        num_individuals=num_individuals,
        backbone_output_channels=backbone_output_channels,
    )
    return config


def _get_paf_parameters(
    project_config: dict,
    bodyparts: list[str],
    num_limbs_threshold: int = 105,
    paf_graph_degree: int = 6,
) -> dict:
    """Gets values for PAF parameters from the project configuration"""
    paf_graph = [
        [i, j] for i in range(len(bodyparts)) for j in range(i + 1, len(bodyparts))
    ]
    num_limbs = len(paf_graph)
    # If the graph is unnecessarily large (with 15+ keypoints by default),
    # we randomly prune it to a size guaranteeing an average node degree of 6;
    # see Suppl. Fig S9c in Lauer et al., 2022.
    if num_limbs >= num_limbs_threshold:
        paf_graph = auxfun_multianimal.prune_paf_graph(
            paf_graph,
            average_degree=paf_graph_degree,
        )
        num_limbs = len(paf_graph)
    return {
        "paf_graph": paf_graph,
        "num_limbs": num_limbs,
        "paf_edges_to_keep": project_config.get("paf_best", list(range(num_limbs))),
    }


--- File: deeplabcut/pose_estimation_pytorch/config/rtmpose/rtmpose_s.yaml ---
data:
  inference:
    top_down_crop:
      width: 256
      height: 256
  train:
    random_bbox_transform:
      shift_factor: 0.16
      shift_prob: 0.3
      scale_factor: [0.75, 1.25]
      scale_prob: 1.0
      p: 1.0
    top_down_crop:
      width: 256
      height: 256
method: td  # Need to add a detector
model:
  backbone:
    type: CSPNeXt
    model_name: cspnext_s
    freeze_bn_stats: false
    freeze_bn_weights: false
    deepen_factor: 0.33
    widen_factor: 0.5
  backbone_output_channels: 512
  heads:
    bodypart:
      type: RTMCCHead
      weight_init: RTMPose
      target_generator:
        type: SimCCGenerator
        input_size: [256, 256]
        smoothing_type: gaussian
        sigma: [5.66, 5.66]
        simcc_split_ratio: 2.0
        label_smooth_weight: 0.0
        normalize: false
      criterion:
        x:
          type: KLDiscreteLoss
          use_target_weight: true
          beta: 10.0
          label_softmax: true
        y:
          type: KLDiscreteLoss
          use_target_weight: true
          beta: 10.0
          label_softmax: true
      predictor:
        type: SimCCPredictor
        simcc_split_ratio: 2.0
      input_size: [256, 256]
      in_channels: 512
      out_channels: "num_bodyparts"
      in_featuremap_size: [8, 8]  # input_size / backbone stride
      simcc_split_ratio: 2.0
      final_layer_kernel_size: 7
      gau_cfg:
        hidden_dims: 256
        s: 128
        expansion_factor: 2
        dropout_rate: 0
        drop_path: 0.0
        act_fn: "SiLU"
        use_rel_bias: false
        pos_enc: false
runner:
  optimizer:
    type: AdamW
    params:
      lr: 1e-3
  scheduler:
    type: SequentialLR
    params:
      schedulers:
      - type: LinearLR
        params:
          start_factor: 0.001
          end_factor: 1.0
          total_iters: 5
      - type: CosineAnnealingLR
        params:
          T_max: 200     # max_epochs // 2
          eta_min: 5e-5  # ~base_lr / 20
      - type: LRListScheduler
        params:
          milestones:
            - 0
          lr_list:
            - - 5e-5
      milestones:
      - 200              # max_epochs // 2
      - 400
train_settings:
  batch_size: 32
  dataloader_workers: 4
  dataloader_pin_memory: false
  epochs: 400


--- File: deeplabcut/pose_estimation_pytorch/config/rtmpose/rtmpose_x.yaml ---
data:
  inference:
    top_down_crop:
      width: 384
      height: 384
  train:
    random_bbox_transform:
      shift_factor: 0.16
      shift_prob: 0.3
      scale_factor: [ 0.75, 1.25 ]
      scale_prob: 1.0
      p: 1.0
    top_down_crop:
      width: 384
      height: 384
method: td  # Need to add a detector
model:
  backbone:
    type: CSPNeXt
    model_name: cspnext_x
    freeze_bn_stats: false
    freeze_bn_weights: false
    deepen_factor: 1.33
    widen_factor: 1.25
  backbone_output_channels: 1280
  heads:
    bodypart:
      type: RTMCCHead
      weight_init: RTMPose
      target_generator:
        type: SimCCGenerator
        input_size: [384, 384]
        smoothing_type: gaussian
        sigma: [6.93, 6.93]
        simcc_split_ratio: 2.0
        label_smooth_weight: 0.0
        normalize: false
      criterion:
        x:
          type: KLDiscreteLoss
          use_target_weight: true
          beta: 10.0
          label_softmax: true
        y:
          type: KLDiscreteLoss
          use_target_weight: true
          beta: 10.0
          label_softmax: true
      predictor:
        type: SimCCPredictor
        simcc_split_ratio: 2.0
      input_size: [384, 384]
      in_channels: 1280
      out_channels: "num_bodyparts"
      in_featuremap_size: [12, 12]  # input_size / backbone stride
      simcc_split_ratio: 2.0
      final_layer_kernel_size: 7
      gau_cfg:
        hidden_dims: 256
        s: 128
        expansion_factor: 2
        dropout_rate: 0
        drop_path: 0.0
        act_fn: "SiLU"
        use_rel_bias: false
        pos_enc: false
runner:
  optimizer:
    type: AdamW
    params:
      lr: 1e-3
  scheduler:
    type: SequentialLR
    params:
      schedulers:
      - type: LinearLR
        params:
          start_factor: 0.001
          end_factor: 1.0
          total_iters: 5
      - type: CosineAnnealingLR
        params:
          T_max: 200     # max_epochs // 2
          eta_min: 5e-5  # ~base_lr / 20
      - type: LRListScheduler
        params:
          milestones:
            - 0
          lr_list:
            - - 5e-5
      milestones:
      - 200              # max_epochs // 2
      - 400
train_settings:
  batch_size: 32
  dataloader_workers: 4
  dataloader_pin_memory: false
  epochs: 400


--- File: deeplabcut/pose_estimation_pytorch/config/rtmpose/rtmpose_m.yaml ---
data:
  inference:
    top_down_crop:
      width: 256
      height: 256
  train:
    random_bbox_transform:
      shift_factor: 0.16
      shift_prob: 0.3
      scale_factor: [0.75, 1.25]
      scale_prob: 1.0
      p: 1.0
    top_down_crop:
      width: 256
      height: 256
method: td  # Need to add a detector
model:
  backbone:
    type: CSPNeXt
    model_name: cspnext_m
    freeze_bn_stats: false
    freeze_bn_weights: false
    deepen_factor: 0.67
    widen_factor: 0.75
  backbone_output_channels: 768
  heads:
    bodypart:
      type: RTMCCHead
      weight_init: RTMPose
      target_generator:
        type: SimCCGenerator
        input_size: [256, 256]
        smoothing_type: gaussian
        sigma: [5.66, 5.66]
        simcc_split_ratio: 2.0
        label_smooth_weight: 0.0
        normalize: false
      criterion:
        x:
          type: KLDiscreteLoss
          use_target_weight: true
          beta: 10.0
          label_softmax: true
        y:
          type: KLDiscreteLoss
          use_target_weight: true
          beta: 10.0
          label_softmax: true
      predictor:
        type: SimCCPredictor
        simcc_split_ratio: 2.0
      input_size: [256, 256]
      in_channels: 768
      out_channels: "num_bodyparts"
      in_featuremap_size: [8, 8]  # input_size / backbone stride
      simcc_split_ratio: 2.0
      final_layer_kernel_size: 7
      gau_cfg:
        hidden_dims: 256
        s: 128
        expansion_factor: 2
        dropout_rate: 0
        drop_path: 0.0
        act_fn: "SiLU"
        use_rel_bias: false
        pos_enc: false
runner:
  optimizer:
    type: AdamW
    params:
      lr: 1e-3
  scheduler:
    type: SequentialLR
    params:
      schedulers:
      - type: LinearLR
        params:
          start_factor: 0.001
          end_factor: 1.0
          total_iters: 5
      - type: CosineAnnealingLR
        params:
          T_max: 200     # max_epochs // 2
          eta_min: 5e-5  # ~base_lr / 20
      - type: LRListScheduler
        params:
          milestones:
            - 0
          lr_list:
            - - 5e-5
      milestones:
      - 200              # max_epochs // 2
      - 400
train_settings:
  batch_size: 32
  dataloader_workers: 4
  dataloader_pin_memory: false
  epochs: 400


--- File: deeplabcut/pose_estimation_pytorch/config/dekr/dekr_w18.yaml ---
data:
  inference:
    auto_padding: # Required for HRNet backbones
      pad_width_divisor: 32
      pad_height_divisor: 32
model:
  backbone:
    type: HRNet
    model_name: hrnet_w18
    freeze_bn_stats: false
    freeze_bn_weights: false
    interpolate_branches: true
    increased_channel_count: false
  backbone_output_channels: 270
  heads:
    bodypart:
      type: DEKRHead
      weight_init: dekr
      target_generator:
        type: DEKRGenerator
        num_joints: "num_bodyparts"
        pos_dist_thresh: 17
        bg_weight: 0.1
      criterion:
        heatmap:
          type: DEKRHeatmapLoss
          weight: 1
        offset:
          type: DEKROffsetLoss
          weight: 0.03
      predictor:
        type: DEKRPredictor
        apply_sigmoid: false
        use_heatmap: false
        clip_scores: true
        num_animals: "num_individuals"
        keypoint_score_type: combined
        max_absorb_distance: 75
        nms_threshold: 0.05
        apply_pose_nms: true
      heatmap_config:
        channels:
        - 270
        - 18
        - "num_bodyparts + 1"  # num_bodyparts + center keypoint
        num_blocks: 1
        dilation_rate: 1
        final_conv_kernel: 1
      offset_config:
        channels:
        - 270
        - "num_bodyparts x 15"  # num_bodyparts * num_offset_per_kpt
        - "num_bodyparts"
        num_offset_per_kpt: 15
        num_blocks: 2
        dilation_rate: 1
        final_conv_kernel: 1
runner:
  optimizer:
    type: AdamW
    params:
      lr: 0.0005
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-4 ], [ 1e-5 ] ]
      milestones: [ 90, 120 ]
with_center_keypoints: true

--- File: deeplabcut/pose_estimation_pytorch/config/dekr/dekr_w48.yaml ---
data:
  inference:
    auto_padding: # Required for HRNet backbones
      pad_width_divisor: 32
      pad_height_divisor: 32
model:
  backbone:
    type: HRNet
    model_name: hrnet_w48
    freeze_bn_stats: false
    freeze_bn_weights: false
    interpolate_branches: true
    increased_channel_count: false
  backbone_output_channels: 720
  heads:
    bodypart:
      type: DEKRHead
      weight_init: dekr
      target_generator:
        type: DEKRGenerator
        num_joints: "num_bodyparts"
        pos_dist_thresh: 17
        bg_weight: 0.1
      criterion:
        heatmap:
          type: DEKRHeatmapLoss
          weight: 1
        offset:
          type: DEKROffsetLoss
          weight: 0.03
      predictor:
        type: DEKRPredictor
        apply_sigmoid: false
        use_heatmap: false
        clip_scores: true
        num_animals: "num_individuals"
        keypoint_score_type: combined
        max_absorb_distance: 75
        nms_threshold: 0.05
        apply_pose_nms: true
      heatmap_config:
        channels:
        - 720
        - 48
        - "num_bodyparts + 1"  # num_bodyparts + center keypoint
        num_blocks: 1
        dilation_rate: 1
        final_conv_kernel: 1
      offset_config:
        channels:
        - 720
        - "num_bodyparts x 15"  # num_bodyparts * num_offset_per_kpt
        - "num_bodyparts"
        num_offset_per_kpt: 15
        num_blocks: 2
        dilation_rate: 1
        final_conv_kernel: 1
runner:
  optimizer:
    type: AdamW
    params:
      lr: 0.0005
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-4 ], [ 1e-5 ] ]
      milestones: [ 90, 120 ]
with_center_keypoints: true

--- File: deeplabcut/pose_estimation_pytorch/config/dekr/dekr_w32.yaml ---
data:
  inference:
    auto_padding: # Required for HRNet backbones
      pad_width_divisor: 32
      pad_height_divisor: 32
model:
  backbone:
    type: HRNet
    model_name: hrnet_w32
    freeze_bn_stats: false
    freeze_bn_weights: false
    interpolate_branches: true
    increased_channel_count: false
  backbone_output_channels: 480
  heads:
    bodypart:
      type: DEKRHead
      weight_init: dekr
      target_generator:
        type: DEKRGenerator
        num_joints: "num_bodyparts"
        pos_dist_thresh: 17
        bg_weight: 0.1
      criterion:
        heatmap:
          type: DEKRHeatmapLoss
          weight: 1
        offset:
          type: DEKROffsetLoss
          weight: 0.03
      predictor:
        type: DEKRPredictor
        apply_sigmoid: false
        use_heatmap: false
        clip_scores: true
        num_animals: "num_individuals"
        keypoint_score_type: combined
        max_absorb_distance: 75
        nms_threshold: 0.05
        apply_pose_nms: true
      heatmap_config:
        channels:
        - 480
        - 32
        - "num_bodyparts + 1"  # num_bodyparts + center keypoint
        num_blocks: 1
        dilation_rate: 1
        final_conv_kernel: 1
      offset_config:
        channels:
        - 480
        - "num_bodyparts x 15"  # num_bodyparts * num_offset_per_kpt
        - "num_bodyparts"
        num_offset_per_kpt: 15
        num_blocks: 2
        dilation_rate: 1
        final_conv_kernel: 1
runner:
  optimizer:
    type: AdamW
    params:
      lr: 0.0005
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-4 ], [ 1e-5 ] ]
      milestones: [ 90, 120 ]
with_center_keypoints: true

--- File: deeplabcut/pose_estimation_pytorch/config/backbones/resnet_101.yaml ---
model:
  backbone:
    type: ResNet
    model_name: resnet101
    output_stride: 16
    freeze_bn_stats: false
    freeze_bn_weights: false
  backbone_output_channels: 2048
runner:
  optimizer:
    type: AdamW
    params:
      lr: 0.001
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-4 ], [ 1e-5 ] ]
      milestones: [ 90, 120 ]


--- File: deeplabcut/pose_estimation_pytorch/config/backbones/cspnext_s.yaml ---
model:
  backbone:
    type: CSPNeXt
    model_name: cspnext_s
    freeze_bn_stats: false
    freeze_bn_weights: false
    deepen_factor: 0.33
    widen_factor: 0.5
  backbone_output_channels: 512
runner:
  optimizer:
    type: AdamW
    params:
      lr: 0.001
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-4 ], [ 1e-5 ] ]
      milestones: [ 90, 120 ]


--- File: deeplabcut/pose_estimation_pytorch/config/backbones/cspnext_m.yaml ---
model:
  backbone:
    type: CSPNeXt
    model_name: cspnext_m
    freeze_bn_stats: false
    freeze_bn_weights: false
    deepen_factor: 0.67
    widen_factor: 0.75
  backbone_output_channels: 768
runner:
  optimizer:
    type: AdamW
    params:
      lr: 0.001
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-4 ], [ 1e-5 ] ]
      milestones: [ 90, 120 ]


--- File: deeplabcut/pose_estimation_pytorch/config/backbones/cspnext_x.yaml ---
model:
  backbone:
    type: CSPNeXt
    model_name: cspnext_x
    freeze_bn_stats: false
    freeze_bn_weights: false
    deepen_factor: 1.33
    widen_factor: 1.25
  backbone_output_channels: 1280
runner:
  optimizer:
    type: AdamW
    params:
      lr: 0.001
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-4 ], [ 1e-5 ] ]
      milestones: [ 90, 120 ]


--- File: deeplabcut/pose_estimation_pytorch/config/backbones/hrnet_w18.yaml ---
data:
  inference:
    auto_padding: # Required for HRNet backbones
      pad_width_divisor: 32
      pad_height_divisor: 32
  train:
    auto_padding: # Required for HRNet backbones
      pad_width_divisor: 32
      pad_height_divisor: 32
model:
  backbone:
    type: HRNet
    model_name: hrnet_w18
    freeze_bn_stats: true
    freeze_bn_weights: false
    interpolate_branches: false
    increased_channel_count: false  # changes backbone_output_channels to 128 when true
  backbone_output_channels: 18


--- File: deeplabcut/pose_estimation_pytorch/config/backbones/hrnet_w32.yaml ---
data:
  inference:
    auto_padding: # Required for HRNet backbones
      pad_width_divisor: 32
      pad_height_divisor: 32
  train:
    auto_padding: # Required for HRNet backbones
      pad_width_divisor: 32
      pad_height_divisor: 32
model:
  backbone:
    type: HRNet
    model_name: hrnet_w32
    freeze_bn_stats: true
    freeze_bn_weights: false
    interpolate_branches: false
    increased_channel_count: false  # changes backbone_output_channels to 128 when true
  backbone_output_channels: 32


--- File: deeplabcut/pose_estimation_pytorch/config/backbones/resnet_50.yaml ---
model:
  backbone:
    type: ResNet
    model_name: resnet50_gn
    output_stride: 16
    freeze_bn_stats: false
    freeze_bn_weights: false
  backbone_output_channels: 2048
runner:
  optimizer:
    type: AdamW
    params:
      lr: 0.001
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-4 ], [ 1e-5 ] ]
      milestones: [ 90, 120 ]

--- File: deeplabcut/pose_estimation_pytorch/config/backbones/hrnet_w48.yaml ---
data:
  inference:
    auto_padding: # Required for HRNet backbones
      pad_width_divisor: 32
      pad_height_divisor: 32
  train:
    auto_padding: # Required for HRNet backbones
      pad_width_divisor: 32
      pad_height_divisor: 32
model:
  backbone:
    type: HRNet
    model_name: hrnet_w48
    freeze_bn_stats: true
    freeze_bn_weights: false
    interpolate_branches: false
    increased_channel_count: false  # changes backbone_output_channels to 128 when true
  backbone_output_channels: 48


--- File: deeplabcut/pose_estimation_pytorch/config/detectors/ssdlite.yaml ---
model:
  type: SSDLite
  freeze_bn_stats: true
  freeze_bn_weights: false
train_settings:
  batch_size: 16


--- File: deeplabcut/pose_estimation_pytorch/config/detectors/fasterrcnn_mobilenet_v3_large_fpn.yaml ---
model:
  type: FasterRCNN
  freeze_bn_stats: true
  freeze_bn_weights: false
  variant: fasterrcnn_mobilenet_v3_large_fpn


--- File: deeplabcut/pose_estimation_pytorch/config/detectors/fasterrcnn_resnet50_fpn_v2.yaml ---
model:
  type: FasterRCNN
  freeze_bn_stats: true
  freeze_bn_weights: false
  variant: fasterrcnn_resnet50_fpn_v2


--- File: deeplabcut/pose_estimation_pytorch/config/animaltokenpose/animaltokenpose_base.yaml ---
# TODO: This default configuration file needs to be reviewed so it matches the original
 #  base TokenPose configuration, as defined in
 #  https://github.com/leeyegy/TokenPose/blob/main/experiments/coco/tokenpose/tokenpose_b_256_192_patch43_dim192_depth12_heads8.yaml
method: td  # Need to add a detector
model:
  backbone:
    type: HRNet
    model_name: hrnet_w32
    freeze_bn_stats: true
    freeze_bn_weights: false
    interpolate_branches: false
    increased_channel_count: false  # changes backbone_output_channels to 128 when true
  backbone_output_channels: 32
  neck:
    type: Transformer
    feature_size:
      - 64
      - 64
    patch_size:
      - 4
      - 4
    num_keypoints: "num_bodyparts"
    channels: 32
    dim: 192
    heads: 8
    depth: 6
  heads:
    bodypart:
      type: TransformerHead
      target_generator:
        type: HeatmapPlateauGenerator
        num_heatmaps: "num_bodyparts"
        pos_dist_thresh: 17
        heatmap_mode: KEYPOINT
        generate_locref: false
      criterion:
        type: WeightedBCECriterion
      predictor:
        type: HeatmapPredictor
        location_refinement: false
      dim: 192
      hidden_heatmap_dim: 384
      heatmap_dim: 4096
      apply_multi: true
      heatmap_size:
        - 64
        - 64
      apply_init: true
      head_stride: 1


--- File: deeplabcut/pose_estimation_pytorch/config/dlcrnet/dlcrnet_stride32_ms5.yaml ---
model:
  backbone:
    type: DLCRNet
    model_name: resnet50
    pretrained: true
    output_stride: 32
  backbone_output_channels: 2304
  pose_model:
    stride: 8
  heads:
    bodypart:
      type: DLCRNetHead
      predictor:
        type: PartAffinityFieldPredictor
        num_animals: "num_individuals"
        num_multibodyparts: "num_bodyparts"
        num_uniquebodyparts: 0
        nms_radius: 5
        sigma: 1.0
        locref_stdev: 7.2801
        min_affinity: 0.05
        graph: "paf_graph"
        edges_to_keep: "paf_edges_to_keep"
      target_generator:
        type: SequentialGenerator
        generators:
        - type: HeatmapPlateauGenerator
          num_heatmaps: "num_bodyparts"
          pos_dist_thresh: 17
          heatmap_mode: KEYPOINT
          generate_locref: true
          locref_std: 7.2801
        - type: PartAffinityFieldGenerator
          graph: "paf_graph"
          width: 20
      criterion:
        heatmap:
          type: WeightedBCECriterion
          weight: 1.0
        locref:
          type: WeightedHuberCriterion
          weight: 0.05
        paf:
          type: WeightedHuberCriterion
          weight: 0.1
      heatmap_config:
        channels:
        - 2304
        - 1152
        - "num_bodyparts"
        kernel_size:
        - 3
        - 3
        strides:
        - 2
        - 2
      locref_config:
        channels:
        - 2304
        - 1152
        - "num_bodyparts x 2"
        kernel_size:
        - 3
        - 3
        strides:
        - 2
        - 2
      paf_config:
        channels:
        - 2304
        - 1152
        - "num_limbs x 2"  # num_limbs = len(graph)
        kernel_size:
        - 3
        - 3
        strides:
        - 2
        - 2
      num_stages: 5
runner:
  eval_interval: 25  # slow evaluation with poor Part-Affinity fields


--- File: deeplabcut/pose_estimation_pytorch/config/dlcrnet/dlcrnet_stride16_ms5.yaml ---
model:
  backbone:
    type: DLCRNet
    model_name: resnet50
    pretrained: true
    output_stride: 16
  backbone_output_channels: 2304
  pose_model:
    stride: 8
  heads:
    bodypart:
      type: DLCRNetHead
      predictor:
        type: PartAffinityFieldPredictor
        num_animals: "num_individuals"
        num_multibodyparts: "num_bodyparts"
        num_uniquebodyparts: 0
        nms_radius: 5
        sigma: 1.0
        locref_stdev: 7.2801
        min_affinity: 0.05
        graph: "paf_graph"
        edges_to_keep: "paf_edges_to_keep"
      target_generator:
        type: SequentialGenerator
        generators:
        - type: HeatmapPlateauGenerator
          num_heatmaps: "num_bodyparts"
          pos_dist_thresh: 17
          heatmap_mode: KEYPOINT
          generate_locref: true
          locref_std: 7.2801
        - type: PartAffinityFieldGenerator
          graph: "paf_graph"
          width: 20
      criterion:
        heatmap:
          type: WeightedBCECriterion
          weight: 1.0
        locref:
          type: WeightedHuberCriterion
          weight: 0.05
        paf:
          type: WeightedHuberCriterion
          weight: 0.1
      heatmap_config:
        channels:
        - 2304
        - "num_bodyparts"
        kernel_size:
        - 3
        strides:
        - 2
      locref_config:
        channels:
        - 2304
        - "num_bodyparts x 2"
        kernel_size:
        - 3
        strides:
        - 2
      paf_config:
        channels:
        - 2304
        - "num_limbs x 2"  # num_limbs = len(graph)
        kernel_size:
        - 3
        strides:
        - 2
      num_stages: 5
runner:
  eval_interval: 25  # slow evaluation with poor Part-Affinity fields


--- File: deeplabcut/pose_estimation_pytorch/config/base/base_detector.yaml ---
data:
  colormode: RGB
  inference:
    normalize_images: true
  train:
    affine:
      p: 0.5
      rotation: 30
      scaling: [ 1.0, 1.0 ]
      translation: 40
    collate:
      type: ResizeFromDataSizeCollate
      min_scale: 0.4
      max_scale: 1.0
      min_short_side: 128
      max_short_side: 1152
      multiple_of: 32
      to_square: false
    hflip: true
    normalize_images: true
device: auto
runner:
  type: DetectorTrainingRunner
  key_metric: "test.mAP@50:95"
  key_metric_asc: true
  eval_interval: 10
  optimizer:
    type: AdamW
    params:
      lr: 1e-4
  scheduler:
    type: LRListScheduler
    params:
      milestones: [ 160 ]
      lr_list: [ [ 1e-5 ] ]
  snapshots:
    max_snapshots: 5
    save_epochs: 25
    save_optimizer_state: false
train_settings:
  batch_size: 1
  dataloader_workers: 0
  dataloader_pin_memory: false
  display_iters: 500
  epochs: 250


--- File: deeplabcut/pose_estimation_pytorch/config/base/head_bodyparts.yaml ---
type: HeatmapHead
weight_init: normal
predictor:
  type: HeatmapPredictor
  apply_sigmoid: false
  clip_scores: true
  location_refinement: true
  locref_std: 7.2801
target_generator:
  type: HeatmapGaussianGenerator
  num_heatmaps: "num_bodyparts"
  pos_dist_thresh: 17
  heatmap_mode: KEYPOINT
  gradient_masking: false
  generate_locref: true
  locref_std: 7.2801
criterion:
  heatmap:
    type: WeightedMSECriterion
    weight: 1.0
  locref:
    type: WeightedHuberCriterion
    weight: 0.05
heatmap_config:
  channels:
  - "backbone_output_channels"
  - "num_bodyparts"
  kernel_size:
  - 3
  strides:
  - 2
locref_config:
  channels:
  - "backbone_output_channels"
  - "num_bodyparts x 2"
  kernel_size:
  - 3
  strides:
  - 2


--- File: deeplabcut/pose_estimation_pytorch/config/base/aug_top_down.yaml ---
bbox_margin: 20
colormode: RGB
inference:
  normalize_images: true
  top_down_crop:
    width: 256
    height: 256
train:
  affine:
    p: 0.5
    rotation: 30
    scaling: [1.0, 1.0]
    translation: 0
  collate: null
  covering: false
  gaussian_noise: 12.75
  hist_eq: false
  motion_blur: false
  normalize_images: true
  top_down_crop:
    width: 256
    height: 256


--- File: deeplabcut/pose_estimation_pytorch/config/base/head_identity.yaml ---
type: HeatmapHead
predictor:
  type: IdentityPredictor
  apply_sigmoid: true
target_generator:
  type: HeatmapPlateauGenerator
  num_heatmaps: "num_individuals"
  pos_dist_thresh: 17
  heatmap_mode: INDIVIDUAL
  gradient_masking: false
  generate_locref: false
criterion:
  heatmap:
    type: WeightedBCECriterion
    weight: 1.0
heatmap_config:
  channels:
  - "backbone_output_channels"
  - "num_individuals"
  kernel_size:
  - 3
  strides:
  - 2


--- File: deeplabcut/pose_estimation_pytorch/config/base/aug_default.yaml ---
bbox_margin: 20
colormode: RGB
inference:
  normalize_images: true
train:
  affine:
    p: 0.5
    rotation: 30
    scaling: [0.5, 1.25]
    translation: 0
  covering: false
  crop_sampling:
    width: 448
    height: 448
    max_shift: 0.1
    method: hybrid
  gaussian_noise: 12.75
  hist_eq: false
  motion_blur: false
  normalize_images: true


--- File: deeplabcut/pose_estimation_pytorch/config/base/head_topdown.yaml ---
type: HeatmapHead
weight_init: normal
predictor:
  type: HeatmapPredictor
  apply_sigmoid: false
  clip_scores: true
  location_refinement: true
  locref_std: 7.2801
target_generator:
  type: HeatmapGaussianGenerator
  num_heatmaps: "num_bodyparts"
  pos_dist_thresh: 17
  heatmap_mode: KEYPOINT
  gradient_masking: true
  background_weight: 0.0
  generate_locref: true
  locref_std: 7.2801
criterion:
  heatmap:
    type: WeightedMSECriterion
    weight: 1.0
  locref:
    type: WeightedHuberCriterion
    weight: 0.05
heatmap_config:
  channels:
  - "backbone_output_channels"
  kernel_size: []
  strides: []
  final_conv:
    out_channels: "num_bodyparts"
    kernel_size: 1
locref_config:
  channels:
    - "backbone_output_channels"
  kernel_size: []
  strides: []
  final_conv:
    out_channels: "num_bodyparts x 2"
    kernel_size: 1


--- File: deeplabcut/pose_estimation_pytorch/config/base/head_bodyparts_with_paf.yaml ---
type: DLCRNetHead
predictor:
  type: PartAffinityFieldPredictor
  num_animals: "num_individuals"
  num_multibodyparts: "num_bodyparts"
  num_uniquebodyparts: 0
  nms_radius: 5
  sigma: 1.0
  locref_stdev: 7.2801
  min_affinity: 0.05
  graph: "paf_graph"
  edges_to_keep: "paf_edges_to_keep"
  apply_sigmoid: true
  clip_scores: false
target_generator:
  type: SequentialGenerator
  generators:
  - type: HeatmapPlateauGenerator
    num_heatmaps: "num_bodyparts"
    pos_dist_thresh: 17
    heatmap_mode: KEYPOINT
    gradient_masking: false
    generate_locref: true
    locref_std: 7.2801
  - type: PartAffinityFieldGenerator
    graph: "paf_graph"
    width: 20
criterion:
  heatmap:
    type: WeightedBCECriterion
    weight: 1.0
  locref:
    type: WeightedHuberCriterion
    weight: 0.05
  paf:
    type: WeightedHuberCriterion
    weight: 0.1
heatmap_config:
  channels:
  - "backbone_output_channels"
  - "num_bodyparts"
  kernel_size:
  - 3
  strides:
  - 2
locref_config:
  channels:
  - "backbone_output_channels"
  - "num_bodyparts x 2"
  kernel_size:
  - 3
  strides:
  - 2
paf_config:
  channels:
  - "backbone_output_channels"
  - "num_limbs x 2"  # num_limbs = len(graph)
  kernel_size:
  - 3
  strides:
  - 2
num_stages: 5


--- File: deeplabcut/pose_estimation_pytorch/config/base/base.yaml ---
device: auto
method: bu
runner:
  type: PoseTrainingRunner
  gpus: null
  key_metric: "test.mAP"
  key_metric_asc: true
  eval_interval: 10
  optimizer:
    type: AdamW
    params:
      lr: 0.0001
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-5 ], [ 1e-6 ] ]
      milestones: [ 160, 190 ]
  snapshots:
    max_snapshots: 5
    save_epochs: 25
    save_optimizer_state: false
train_settings:
  batch_size: 8
  dataloader_workers: 0
  dataloader_pin_memory: false
  display_iters: 500
  epochs: 200
  seed: 42


--- File: deeplabcut/pose_estimation_pytorch/models/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.models.backbones.base import BACKBONES
from deeplabcut.pose_estimation_pytorch.models.criterions import (
    CRITERIONS,
    LOSS_AGGREGATORS,
)
from deeplabcut.pose_estimation_pytorch.models.detectors import DETECTORS
from deeplabcut.pose_estimation_pytorch.models.heads.base import HEADS
from deeplabcut.pose_estimation_pytorch.models.model import PoseModel
from deeplabcut.pose_estimation_pytorch.models.necks.base import NECKS
from deeplabcut.pose_estimation_pytorch.models.predictors import PREDICTORS
from deeplabcut.pose_estimation_pytorch.models.target_generators import (
    TARGET_GENERATORS,
)


--- File: deeplabcut/pose_estimation_pytorch/models/model.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import copy
import logging

import torch
import torch.nn as nn

from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.pose_estimation_pytorch.models.backbones import BACKBONES, BaseBackbone
from deeplabcut.pose_estimation_pytorch.models.criterions import (
    CRITERIONS,
    LOSS_AGGREGATORS,
)
from deeplabcut.pose_estimation_pytorch.models.heads import BaseHead, HEADS
from deeplabcut.pose_estimation_pytorch.models.necks import BaseNeck, NECKS
from deeplabcut.pose_estimation_pytorch.models.predictors import PREDICTORS
from deeplabcut.pose_estimation_pytorch.models.target_generators import (
    TARGET_GENERATORS,
)


class PoseModel(nn.Module):
    """A pose estimation model

    A pose estimation model is composed of a backbone, optionally a neck, and an
    arbitrary number of heads. Outputs are computed as follows:
    """

    def __init__(
        self,
        cfg: dict,
        backbone: BaseBackbone,
        heads: dict[str, BaseHead],
        neck: BaseNeck | None = None,
    ) -> None:
        """
        Args:
            cfg: configuration dictionary for the model.
            backbone: backbone network architecture.
            heads: the heads for the model
            neck: neck network architecture (default is None). Defaults to None.
        """
        super().__init__()
        self.cfg = cfg
        self.backbone = backbone
        self.heads = nn.ModuleDict(heads)
        self.neck = neck
        self.output_features = False

        self._strides = {
            name: _model_stride(self.backbone.stride, head.stride)
            for name, head in heads.items()
        }

    def forward(self, x: torch.Tensor) -> dict[str, dict[str, torch.Tensor]]:
        """
        Forward pass of the PoseModel.

        Args:
            x: input images

        Returns:
            Outputs of head groups
        """
        if x.dim() == 3:
            x = x[None, :]
        features = self.backbone(x)
        if self.neck:
            features = self.neck(features)

        outputs = {}
        if self.output_features:
            outputs["backbone"] = dict(features=features)

        for head_name, head in self.heads.items():
            outputs[head_name] = head(features)
        return outputs

    def get_loss(
        self,
        outputs: dict[str, dict[str, torch.Tensor]],
        targets: dict[str, dict[str, torch.Tensor]],
    ) -> dict[str, torch.Tensor]:
        total_losses = []
        losses: dict[str, torch.Tensor] = {}
        for name, head in self.heads.items():
            head_losses = head.get_loss(outputs[name], targets[name])
            total_losses.append(head_losses["total_loss"])
            for k, v in head_losses.items():
                losses[f"{name}_{k}"] = v

        # TODO: Different aggregation for multi-head loss?
        losses["total_loss"] = torch.mean(torch.stack(total_losses))
        return losses

    def get_target(
        self,
        outputs: dict[str, dict[str, torch.Tensor]],
        labels: dict,
    ) -> dict[str, dict]:
        """Summary:
        Get targets for model training.

        Args:
            outputs: output of each head group
            labels: dictionary of labels

        Returns:
            targets: dict of the targets for each model head group
        """
        return {
            name: head.target_generator(self._strides[name], outputs[name], labels)
            for name, head in self.heads.items()
        }

    def get_predictions(self, outputs: dict[str, dict[str, torch.Tensor]]) -> dict:
        """Abstract method for the forward pass of the Predictor.

        Args:
            outputs: outputs of the model heads

        Returns:
            A dictionary containing the predictions of each head group
        """
        predictions = {
            name: head.predictor(self._strides[name], outputs[name])
            for name, head in self.heads.items()
        }
        if self.output_features:
            predictions["backbone"] = outputs["backbone"]

        return predictions

    def get_stride(self, head: str) -> int:
        """
        Args:
            head: The head for which to get the total stride.

        Returns:
            The total stride for the outputs of the head.

        Raises:
            ValueError: If there is no such head.
        """
        return self._strides[head]

    @staticmethod
    def build(
        cfg: dict,
        weight_init: None | WeightInitialization = None,
        pretrained_backbone: bool = False,
    ) -> "PoseModel":
        """
        Args:
            cfg: The configuration of the model to build.
            weight_init: How model weights should be initialized. If None, ImageNet
                pre-trained backbone weights are loaded from Timm.
            pretrained_backbone: Whether to load an ImageNet-pretrained weights for
                the backbone. This should only be set to True when building a model
                which will be trained on a transfer learning task.

        Returns:
            the built pose model
        """
        cfg["backbone"]["pretrained"] = pretrained_backbone
        backbone = BACKBONES.build(dict(cfg["backbone"]))

        neck = None
        if cfg.get("neck"):
            neck = NECKS.build(dict(cfg["neck"]))

        heads = {}
        for name, head_cfg in cfg["heads"].items():
            head_cfg = copy.deepcopy(head_cfg)
            if "type" in head_cfg["criterion"]:
                head_cfg["criterion"] = CRITERIONS.build(head_cfg["criterion"])
            else:
                weights = {}
                criterions = {}
                for loss_name, criterion_cfg in head_cfg["criterion"].items():
                    weights[loss_name] = criterion_cfg.get("weight", 1.0)
                    criterion_cfg = {
                        k: v for k, v in criterion_cfg.items() if k != "weight"
                    }
                    criterions[loss_name] = CRITERIONS.build(criterion_cfg)

                aggregator_cfg = {"type": "WeightedLossAggregator", "weights": weights}
                head_cfg["aggregator"] = LOSS_AGGREGATORS.build(aggregator_cfg)
                head_cfg["criterion"] = criterions

            head_cfg["target_generator"] = TARGET_GENERATORS.build(
                head_cfg["target_generator"]
            )
            head_cfg["predictor"] = PREDICTORS.build(head_cfg["predictor"])
            heads[name] = HEADS.build(head_cfg)

        model = PoseModel(cfg=cfg, backbone=backbone, neck=neck, heads=heads)

        if weight_init is not None:
            logging.info(f"Loading pretrained model weights: {weight_init}")
            logging.info(f"The pose model is loading from {weight_init.snapshot_path}")
            snapshot = torch.load(weight_init.snapshot_path, map_location="cpu")
            state_dict = snapshot["model"]

            # load backbone state dict
            model.backbone.load_state_dict(filter_state_dict(state_dict, "backbone"))

            # if there's a neck, load state dict
            if model.neck is not None:
                model.neck.load_state_dict(filter_state_dict(state_dict, "neck"))

            # load head state dicts
            if weight_init.with_decoder:
                all_head_state_dicts = filter_state_dict(state_dict, "heads")
                conversion_tensor = torch.from_numpy(weight_init.conversion_array)
                for name, head in model.heads.items():
                    head_state_dict = filter_state_dict(all_head_state_dicts, name)

                    # requires WeightConversionMixin
                    if not weight_init.memory_replay:
                        head_state_dict = head.convert_weights(
                            state_dict=head_state_dict,
                            module_prefix="",
                            conversion=conversion_tensor,
                        )

                    head.load_state_dict(head_state_dict)

        return model


def filter_state_dict(state_dict: dict, module: str) -> dict[str, torch.Tensor]:
    """
    Filters keys in the state dict for a module to only keep a given prefix. Removes
    the module from the keys (e.g. for module="backbone", "backbone.stage1.weight" will
    be converted to "stage1.weight" so the state dict can be loaded into the backbone
    directly).

    Args:
        state_dict: the state dict
        module: the module to keep, e.g. "backbone"

    Returns:
        the filtered state dict, with the module removed from the keys

    Examples:
        state_dict = {"backbone.conv.weight": t1, "head.conv.weight": t2}
        filtered = filter_state_dict(state_dict, "backbone")
        # filtered = {"conv.weight": t1}
        model.backbone.load_state_dict(filtered)
    """
    return {
        ".".join(k.split(".")[1:]): v  # remove 'backbone.' from the keys
        for k, v in state_dict.items()
        if k.startswith(module)
    }


def _model_stride(backbone_stride: int | float, head_stride: int | float) -> float:
    """Computes the model stride from a backbone and a head"""
    if head_stride > 0:
        return backbone_stride / head_stride

    return backbone_stride * -head_stride


--- File: deeplabcut/pose_estimation_pytorch/models/weight_init.py ---
"""Ways to initialize weights for PyTorch modules"""

from __future__ import annotations

from abc import ABC, abstractmethod

import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry


def _build_weight_init(cfg: str | dict, **kwargs) -> BaseWeightInitializer:
    """Builds a BaseWeightInitializer using its config or the name of the initializer

    Args:
        cfg: Either the name of the initializer (e.g. 'normal') or the config
        **kwargs: Other parameters given by the Registry.

    Returns:
        the built BaseWeightInitializer
    """
    if isinstance(cfg, str):
        cfg = {"type": cfg.title().replace("_", "")}
    return build_from_cfg(cfg, **kwargs)


WEIGHT_INIT = Registry("weight_init", build_func=_build_weight_init)


class BaseWeightInitializer(ABC):
    """Class to used to initialize model weights"""

    @abstractmethod
    def init_weights(self, model: nn.Module) -> None:
        """Initializes weights for a model.

        Args:
            model: The model for which to initialize weights
        """


@WEIGHT_INIT.register_module
class Normal(BaseWeightInitializer):
    """Class to used to initialize model weights using a normal distribution

    Weights are initialized with a normal distribution, and biases are initialized to 0.

    Attributes:
        std: the standard deviation to use to initialize weights
    """

    def __init__(self, std: float = 0.001):
        self.std = std

    def init_weights(self, model: nn.Module) -> None:
        for name, module in model.named_parameters():
            if "bias" in name:
                nn.init.constant_(module, 0)
            else:
                nn.init.normal_(module, std=self.std)


@WEIGHT_INIT.register_module
class Dekr(BaseWeightInitializer):
    """Class to used to initialize model weights in the same way as DEKR

    Attributes:
        std: the standard deviation to use to initialize weights
    """

    def __init__(self, std: float = 0.001):
        self.std = std

    def init_weights(self, model: nn.Module) -> None:
        for name, module in model.named_parameters():
            if "bias" in name:
                nn.init.constant_(module, 0)
            else:
                nn.init.normal_(module, std=self.std)

            if hasattr(module, "transform_matrix_conv"):
                nn.init.constant_(module.transform_matrix_conv.weight, 0)
                if hasattr(module, "bias"):
                    nn.init.constant_(module.transform_matrix_conv.bias, 0)
            if hasattr(module, "translation_conv"):
                nn.init.constant_(module.translation_conv.weight, 0)
                if hasattr(module, "bias"):
                    nn.init.constant_(module.translation_conv.bias, 0)


@WEIGHT_INIT.register_module
class Rtmpose(BaseWeightInitializer):
    """Class to used to initialize head weights in the same way as RTMPose"""

    def init_weights(self, model: nn.Module) -> None:
        for module in model.modules():
            if isinstance(module, nn.Conv2d):
                nn.init.normal_(module.weight, std=0.001)
                nn.init.constant_(module.bias, 0)
            elif isinstance(module, nn.BatchNorm2d):
                nn.init.constant_(module.weight, 1)
                nn.init.constant_(module.bias, 1)
            elif isinstance(module, nn.Linear):
                nn.init.normal_(module.weight, std=0.01)
                nn.init.constant_(module.bias, 0)


--- File: deeplabcut/pose_estimation_pytorch/models/necks/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.models.necks.base import BaseNeck, NECKS
from deeplabcut.pose_estimation_pytorch.models.necks.transformer import Transformer


--- File: deeplabcut/pose_estimation_pytorch/models/necks/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import math

import torch


def make_sine_position_embedding(
    h: int, w: int, d_model: int, temperature: int = 10000, scale: float = 2 * math.pi
) -> torch.Tensor:
    """Generate sine position embeddings for a given height, width, and model dimension.

    Args:
        h: Height of the embedding.
        w: Width of the embedding.
        d_model: Dimension of the model.
        temperature: Temperature parameter for position embedding calculation.
                     Defaults to 10000.
        scale: Scaling factor for position embedding. Defaults to 2 * math.pi.

    Returns:
        Sine position embeddings with shape (batch_size, d_model, h * w).

    Example:
        >>> h, w, d_model = 10, 20, 512
        >>> pos_emb = make_sine_position_embedding(h, w, d_model)
        >>> print(pos_emb.shape)  # Output: torch.Size([1, 512, 200])
    """
    area = torch.ones(1, h, w)
    y_embed = area.cumsum(1, dtype=torch.float32)
    x_embed = area.cumsum(2, dtype=torch.float32)
    one_direction_feats = d_model // 2
    eps = 1e-6
    y_embed = y_embed / (y_embed[:, -1:, :] + eps) * scale
    x_embed = x_embed / (x_embed[:, :, -1:] + eps) * scale

    dim_t = torch.arange(one_direction_feats, dtype=torch.float32)
    dim_t = temperature ** (2 * (dim_t // 2) / one_direction_feats)

    pos_x = x_embed[:, :, :, None] / dim_t
    pos_y = y_embed[:, :, :, None] / dim_t
    pos_x = torch.stack(
        (pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4
    ).flatten(3)
    pos_y = torch.stack(
        (pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4
    ).flatten(3)
    pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)
    pos = pos.flatten(2).permute(0, 2, 1)

    return pos


--- File: deeplabcut/pose_estimation_pytorch/models/necks/transformer.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from typing import Tuple

import torch
from einops import rearrange, repeat
from timm.layers import trunc_normal_

from deeplabcut.pose_estimation_pytorch.models.necks.base import BaseNeck, NECKS
from deeplabcut.pose_estimation_pytorch.models.necks.layers import TransformerLayer
from deeplabcut.pose_estimation_pytorch.models.necks.utils import (
    make_sine_position_embedding,
)

MIN_NUM_PATCHES = 16
BN_MOMENTUM = 0.1


@NECKS.register_module
class Transformer(BaseNeck):
    """Transformer Neck for pose estimation.
       title={TokenPose: Learning Keypoint Tokens for Human Pose Estimation},
       author={Yanjie Li and Shoukui Zhang and Zhicheng Wang and Sen Yang and Wankou Yang and Shu-Tao Xia and Erjin Zhou},
       booktitle={IEEE/CVF International Conference on Computer Vision (ICCV)},
       year={2021}

    Args:
        feature_size: Size of the input feature map (height, width).
        patch_size: Size of each patch used in the transformer.
        num_keypoints: Number of keypoints in the pose estimation task.
        dim: Dimension of the transformer.
        depth: Number of transformer layers.
        heads: Number of self-attention heads in the transformer.
        mlp_dim: Dimension of the MLP used in the transformer.
                                 Defaults to 3.
        apply_init: Whether to apply weight initialization.
                                     Defaults to False.
        heatmap_size: Size of the heatmap. Defaults to [64, 64].
        channels: Number of channels in each patch. Defaults to 32.
        dropout: Dropout rate for embeddings. Defaults to 0.0.
        emb_dropout: Dropout rate for transformer layers.
                                       Defaults to 0.0.
        pos_embedding_type: Type of positional embedding.
                            Either 'sine-full', 'sine', or 'learnable'.
                            Defaults to "sine-full".

    Examples:
        # Creating a Transformer neck with sine positional embedding
        transformer = Transformer(
            feature_size=(128, 128),
            patch_size=(16, 16),
            num_keypoints=17,
            dim=256,
            depth=6,
            heads=8,
            pos_embedding_type="sine"
        )

        # Creating a Transformer neck with learnable positional embedding
        transformer = Transformer(
            feature_size=(256, 256),
            patch_size=(32, 32),
            num_keypoints=17,
            dim=512,
            depth=12,
            heads=16,
            pos_embedding_type="learnable"
        )
    """

    def __init__(
        self,
        *,
        feature_size: Tuple[int, int],
        patch_size: Tuple[int, int],
        num_keypoints: int,
        dim: int,
        depth: int,
        heads: int,
        mlp_dim: int = 3,
        apply_init: bool = False,
        heatmap_size: Tuple[int, int] = (64, 64),
        channels: int = 32,
        dropout: float = 0.0,
        emb_dropout: float = 0.0,
        pos_embedding_type: str = "sine-full"
    ):
        super().__init__()

        num_patches = (feature_size[0] // (patch_size[0])) * (
            feature_size[1] // (patch_size[1])
        )
        patch_dim = channels * patch_size[0] * patch_size[1]

        self.inplanes = 64
        self.patch_size = patch_size
        self.heatmap_size = heatmap_size
        self.num_keypoints = num_keypoints
        self.num_patches = num_patches
        self.pos_embedding_type = pos_embedding_type
        self.all_attn = self.pos_embedding_type == "sine-full"

        self.keypoint_token = torch.nn.Parameter(
            torch.zeros(1, self.num_keypoints, dim)
        )
        h, w = (
            feature_size[0] // (self.patch_size[0]),
            feature_size[1] // (self.patch_size[1]),
        )

        self._make_position_embedding(w, h, dim, pos_embedding_type)

        self.patch_to_embedding = torch.nn.Linear(patch_dim, dim)
        self.dropout = torch.nn.Dropout(emb_dropout)

        self.transformer1 = TransformerLayer(
            dim,
            depth,
            heads,
            mlp_dim,
            dropout,
            num_keypoints=num_keypoints,
            scale_with_head=True,
        )
        self.transformer2 = TransformerLayer(
            dim,
            depth,
            heads,
            mlp_dim,
            dropout,
            num_keypoints=num_keypoints,
            all_attn=self.all_attn,
            scale_with_head=True,
        )
        self.transformer3 = TransformerLayer(
            dim,
            depth,
            heads,
            mlp_dim,
            dropout,
            num_keypoints=num_keypoints,
            all_attn=self.all_attn,
            scale_with_head=True,
        )

        self.to_keypoint_token = torch.nn.Identity()

        if apply_init:
            self.apply(self._init_weights)

    def _make_position_embedding(
        self, w: int, h: int, d_model: int, pe_type="learnable"
    ):
        """Create position embeddings for the transformer.

        Args:
            w: Width of the input feature map.
            h: Height of the input feature map.
            d_model: Dimension of the transformer encoder.
            pe_type: Type of position embeddings.
                     Either "learnable" or "sine". Defaults to "learnable".
        """
        with torch.no_grad():
            self.pe_h = h
            self.pe_w = w
            length = h * w
        if pe_type != "learnable":
            self.pos_embedding = torch.nn.Parameter(
                make_sine_position_embedding(h, w, d_model), requires_grad=False
            )
        else:
            self.pos_embedding = torch.nn.Parameter(
                torch.zeros(1, self.num_patches + self.num_keypoints, d_model)
            )

    def _make_layer(
        self, block: torch.nn.Module, planes: int, blocks: int, stride: int = 1
    ) -> torch.nn.Sequential:
        """Create a layer of the transformer encoder.

        Args:
            block: The basic building block of the layer.
            planes: Number of planes in the layer.
            blocks: Number of blocks in the layer.
            stride: Stride value. Defaults to 1.

        Returns:
            The layer of the transformer encoder.
        """
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = torch.nn.Sequential(
                torch.nn.Conv2d(
                    self.inplanes,
                    planes * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                torch.nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return torch.nn.Sequential(*layers)

    def _init_weights(self, m: torch.nn.Module):
        """Initialize the weights of the model.

        Args:
            m: A module of the model.
        """
        print("Initialization...")
        if isinstance(m, torch.nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, torch.nn.Linear) and m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
        elif isinstance(m, torch.nn.LayerNorm):
            torch.nn.init.constant_(m.bias, 0)
            torch.nn.init.constant_(m.weight, 1.0)

    def forward(self, feature: torch.Tensor, mask=None) -> torch.Tensor:
        """Forward pass through the Transformer neck.

        Args:
            feature: Input feature map.
            mask: Mask to apply to the transformer.
                  Defaults to None.

        Returns:
            Output tensor from the transformer neck.

        Examples:
            # Assuming feature is a torch.Tensor of shape (batch_size, channels, height, width)
            output = transformer(feature)
        """
        p = self.patch_size

        x = rearrange(
            feature, "b c (h p1) (w p2) -> b (h w) (p1 p2 c)", p1=p[0], p2=p[1]
        )
        x = self.patch_to_embedding(x)

        b, n, _ = x.shape

        keypoint_tokens = repeat(self.keypoint_token, "() n d -> b n d", b=b)
        if self.pos_embedding_type in ["sine", "sine-full"]:
            x += self.pos_embedding[:, :n]
            x = torch.cat((keypoint_tokens, x), dim=1)
        else:
            x = torch.cat((keypoint_tokens, x), dim=1)
            x += self.pos_embedding[:, : (n + self.num_keypoints)]
        x = self.dropout(x)

        x1 = self.transformer1(x, mask, self.pos_embedding)
        x2 = self.transformer2(x1, mask, self.pos_embedding)
        x3 = self.transformer3(x2, mask, self.pos_embedding)

        x1_out = self.to_keypoint_token(x1[:, 0 : self.num_keypoints])
        x2_out = self.to_keypoint_token(x2[:, 0 : self.num_keypoints])
        x3_out = self.to_keypoint_token(x3[:, 0 : self.num_keypoints])

        x = torch.cat((x1_out, x2_out, x3_out), dim=2)
        return x


--- File: deeplabcut/pose_estimation_pytorch/models/necks/layers.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import torch
import torch.nn.functional as F
from einops import rearrange, repeat


class Residual(torch.nn.Module):
    """Residual block module.

    This module implements a residual block for the transformer layers.

    Attributes:
        fn: The function to apply in the residual block.
    """

    def __init__(self, fn: torch.nn.Module):
        """Initialize the Residual block.

        Args:
            fn: The function to apply in the residual block.
        """
        super().__init__()
        self.fn = fn

    def forward(self, x: torch.Tensor, **kwargs):
        """Forward pass through the Residual block.

        Args:
            x: Input tensor.
            **kwargs: Additional keyword arguments for the function.

        Returns:
            Output tensor.
        """
        return self.fn(x, **kwargs) + x


class PreNorm(torch.nn.Module):
    """PreNorm block module.

    This module implements pre-normalization for the transformer layers.

    Attributes:
        dim: Dimension of the input tensor.
        fn: The function to apply after normalization.
        fusion_factor: Fusion factor for layer normalization.
                       Defaults to 1.
    """

    def __init__(self, dim: int, fn: torch.nn.Module, fusion_factor: int = 1):
        """Initialize the PreNorm block.

        Args:
            dim: Dimension of the input tensor.
            fn: The function to apply after normalization.
            fusion_factor: Fusion factor for layer normalization.
                           Defaults to 1.
        """
        super().__init__()
        self.norm = torch.nn.LayerNorm(dim * fusion_factor)
        self.fn = fn

    def forward(self, x, **kwargs):
        """Forward pass through the PreNorm block.

        Args:
            x: Input tensor.
            **kwargs: Additional keyword arguments for the function.

        Returns:
            Output tensor.
        """
        return self.fn(self.norm(x), **kwargs)


class FeedForward(torch.nn.Module):
    """FeedForward block module.

    This module implements the feedforward layer in the transformer layers.

    Attributes:
        dim: Dimension of the input tensor.
        hidden_dim: Dimension of the hidden layer.
        dropout: Dropout rate. Defaults to 0.0.
    """

    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.0):
        """Initialize the FeedForward block.

        Args:
            dim: Dimension of the input tensor.
            hidden_dim: Dimension of the hidden layer.
            dropout: Dropout rate. Defaults to 0.0.
        """
        super().__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(dim, hidden_dim),
            torch.nn.GELU(),
            torch.nn.Dropout(dropout),
            torch.nn.Linear(hidden_dim, dim),
            torch.nn.Dropout(dropout),
        )

    def forward(self, x: torch.Tensor):
        """Forward pass through the FeedForward block.

        Args:
            x: Input tensor.

        Returns:
            Output tensor.
        """
        return self.net(x)


class Attention(torch.nn.Module):
    """Attention block module.

    This module implements the attention mechanism in the transformer layers.

    Attributes:
        dim: Dimension of the input tensor.
        heads: Number of attention heads. Defaults to 8.
        dropout: Dropout rate. Defaults to 0.0.
        num_keypoints: Number of keypoints. Defaults to None.
        scale_with_head: Scale attention with the number of heads.
                         Defaults to False.
    """

    def __init__(
        self,
        dim: int,
        heads: int = 8,
        dropout: float = 0.0,
        num_keypoints: int = None,
        scale_with_head: bool = False,
    ):
        """Initialize the Attention block.

        Args:
            dim: Dimension of the input tensor.
            heads: Number of attention heads. Defaults to 8.
            dropout: Dropout rate. Defaults to 0.0.
            num_keypoints: Number of keypoints. Defaults to None.
            scale_with_head: Scale attention with the number of heads.
                             Defaults to False.
        """
        super().__init__()
        self.heads = heads
        self.scale = (dim // heads) ** -0.5 if scale_with_head else dim ** -0.5

        self.to_qkv = torch.nn.Linear(dim, dim * 3, bias=False)
        self.to_out = torch.nn.Sequential(
            torch.nn.Linear(dim, dim), torch.nn.Dropout(dropout)
        )
        self.num_keypoints = num_keypoints

    def forward(self, x: torch.Tensor, mask: torch.Tensor = None):
        """Forward pass through the Attention block.

        Args:
            x: Input tensor.
            mask: Attention mask. Defaults to None.

        Returns:
            Output tensor.
        """
        b, n, _, h = *x.shape, self.heads
        qkv = self.to_qkv(x).chunk(3, dim=-1)
        q, k, v = map(lambda t: rearrange(t, "b n (h d) -> b h n d", h=h), qkv)

        dots = torch.einsum("bhid,bhjd->bhij", q, k) * self.scale
        mask_value = -torch.finfo(dots.dtype).max

        if mask is not None:
            mask = F.pad(mask.flatten(1), (1, 0), value=True)
            assert mask.shape[-1] == dots.shape[-1], "mask has incorrect dimensions"
            mask = mask[:, None, :] * mask[:, :, None]
            dots.masked_fill_(~mask, mask_value)
            del mask

        attn = dots.softmax(dim=-1)

        out = torch.einsum("bhij,bhjd->bhid", attn, v)

        out = rearrange(out, "b h n d -> b n (h d)")
        out = self.to_out(out)
        return out


class TransformerLayer(torch.nn.Module):
    """TransformerLayer block module.

    This module implements the Transformer layer in the transformer model.

    Attributes:
        dim: Dimension of the input tensor.
        depth: Depth of the transformer layer.
        heads: Number of attention heads.
        mlp_dim: Dimension of the MLP layer.
        dropout: Dropout rate.
        num_keypoints: Number of keypoints. Defaults to None.
        all_attn: Apply attention to all keypoints.
                  Defaults to False.
        scale_with_head: Scale attention with the number of heads.
                         Defaults to False.
    """

    def __init__(
        self,
        dim: int,
        depth: int,
        heads: int,
        mlp_dim: int,
        dropout: float,
        num_keypoints: int = None,
        all_attn: bool = False,
        scale_with_head: bool = False,
    ):
        """Initialize the TransformerLayer block.

        Args:
            dim: Dimension of the input tensor.
            depth: Depth of the transformer layer.
            heads: Number of attention heads.
            mlp_dim: Dimension of the MLP layer.
            dropout: Dropout rate.
            num_keypoints: Number of keypoints. Defaults to None.
            all_attn: Apply attention to all keypoints. Defaults to False.
            scale_with_head: Scale attention with the number of heads. Defaults to False.
        """
        super().__init__()
        self.layers = torch.nn.ModuleList([])
        self.all_attn = all_attn
        self.num_keypoints = num_keypoints
        for _ in range(depth):
            self.layers.append(
                torch.nn.ModuleList(
                    [
                        Residual(
                            PreNorm(
                                dim,
                                Attention(
                                    dim,
                                    heads=heads,
                                    dropout=dropout,
                                    num_keypoints=num_keypoints,
                                    scale_with_head=scale_with_head,
                                ),
                            )
                        ),
                        Residual(
                            PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))
                        ),
                    ]
                )
            )

    def forward(
        self, x: torch.Tensor, mask: torch.Tensor = None, pos: torch.Tensor = None
    ):
        """Forward pass through the TransformerLayer block.

        Args:
            x: Input tensor.
            mask: Attention mask. Defaults to None.
            pos: Positional encoding. Defaults to None.

        Returns:
            Output tensor.
        """
        for idx, (attn, ff) in enumerate(self.layers):
            if idx > 0 and self.all_attn:
                x[:, self.num_keypoints :] += pos
            x = attn(x, mask=mask)
            x = ff(x)
        return x


--- File: deeplabcut/pose_estimation_pytorch/models/necks/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from abc import ABC, abstractmethod

import torch

from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry

NECKS = Registry("necks", build_func=build_from_cfg)


class BaseNeck(ABC, torch.nn.Module):
    """Base Neck class for pose estimation"""

    def __init__(self):
        super().__init__()

    @abstractmethod
    def forward(self, x: torch.Tensor):
        """Abstract method for the forward pass through the Neck.

        Args:
            x: Input tensor.

        Returns:
            Output tensor.
        """
        pass

    def _init_weights(self, pretrained: str):
        """Initialize the Neck with pretrained weights.

        Args:
            pretrained: Path to the pretrained weights.

        Returns:
            None
        """
        if pretrained:
            self.model.load_state_dict(torch.load(pretrained))


--- File: deeplabcut/pose_estimation_pytorch/models/predictors/dekr_predictor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from __future__ import annotations

import torch
import torch.nn.functional as F

from deeplabcut.pose_estimation_pytorch.models.predictors import (
    BasePredictor,
    PREDICTORS,
)


@PREDICTORS.register_module
class DEKRPredictor(BasePredictor):
    """DEKR Predictor class for multi-animal pose estimation.

    This class regresses keypoints and assembles them (if multianimal project)
    from the output of DEKR (Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression).
    Based on:
        Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression
        Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, Jingdong Wang
        CVPR
        2021
    Code based on:
        https://github.com/HRNet/DEKR

    Args:
        num_animals (int): Number of animals in the project.
        detection_threshold (float, optional): Threshold for detection. Defaults to 0.01.
        apply_sigmoid (bool, optional): Apply sigmoid to heatmaps. Defaults to True.
        use_heatmap (bool, optional): Use heatmap to refine keypoint predictions. Defaults to True.
        keypoint_score_type (str): Type of score to compute for keypoints. "heatmap" applies the heatmap
            score to each keypoint. "center" applies the score of the center of each individual to
            all of its keypoints. "combined" multiplies the score of the heatmap and individual
            center for each keypoint.

    Attributes:
        num_animals (int): Number of animals in the project.
        detection_threshold (float): Threshold for detection.
        apply_sigmoid (bool): Apply sigmoid to heatmaps.
        use_heatmap (bool): Use heatmap.
        keypoint_score_type (str): Type of score to compute for keypoints. "heatmap" applies the heatmap
            score to each keypoint. "center" applies the score of the center of each individual to
            all of its keypoints. "combined" multiplies the score of the heatmap and individual
            center for each keypoint.

    Example:
        # Create a DEKRPredictor instance with 2 animals.
        predictor = DEKRPredictor(num_animals=2)

        # Make a forward pass with outputs and scale factors.
        outputs = (heatmaps, offsets)  # tuple of heatmaps and offsets
        scale_factors = (0.5, 0.5)  # tuple of scale factors for the poses
        poses_with_scores = predictor.forward(outputs, scale_factors)
    """

    default_init = {"apply_sigmoid": True, "detection_threshold": 0.01}

    def __init__(
        self,
        num_animals: int,
        detection_threshold: float = 0.01,
        apply_sigmoid: bool = True,
        clip_scores: bool = False,
        use_heatmap: bool = True,
        keypoint_score_type: str = "combined",
        max_absorb_distance: int = 75,
        nms_threshold: float = 0.05,
        apply_pose_nms: bool = True,
    ):
        """
        Args:
            num_animals: Number of animals in the project.
            detection_threshold: Threshold for detection
            apply_sigmoid: Apply sigmoid to heatmaps
            clip_scores: If a sigmoid is not applied, this can be used to clip scores
                for predicted keypoints to values in [0, 1].
            use_heatmap: Use heatmap to refine the keypoint predictions.
            keypoint_score_type: Type of score to compute for keypoints. "heatmap"
                applies the heatmap score to each keypoint. "center" applies the score
                of the center of each individual to all of its keypoints. "combined"
                multiplies the score of the heatmap and individual for each keypoint.
            nms_threshold: Threshold for NMS of pose.
            apply_pose_nms: Whether to apply pose NMS
        """
        super().__init__()
        self.num_animals = num_animals
        self.detection_threshold = detection_threshold
        self.apply_sigmoid = apply_sigmoid
        self.clip_scores = clip_scores
        self.use_heatmap = use_heatmap
        self.keypoint_score_type = keypoint_score_type
        if self.keypoint_score_type not in ("heatmap", "center", "combined"):
            raise ValueError(f"Unknown keypoint score type: {self.keypoint_score_type}")

        self.max_absorb_distance = max_absorb_distance
        self.nms_threshold = nms_threshold
        self.apply_pose_nms = apply_pose_nms

    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor]
    ) -> dict[str, torch.Tensor]:
        """Forward pass of DEKRPredictor.

        Args:
            stride: the stride of the model
            outputs: outputs of the model heads (heatmap, locref)

        Returns:
            A dictionary containing a "poses" key with the output tensor as value, and
            optionally a "unique_bodyparts" with the unique bodyparts tensor as value.

        Example:
            # Assuming you have 'outputs' (heatmaps and offsets) and 'scale_factors' for poses
            poses_with_scores = predictor.forward(outputs, scale_factors)
        """
        heatmaps, offsets = outputs["heatmap"], outputs["offset"]
        scale_factors = stride, stride

        if self.apply_sigmoid:
            heatmaps = F.sigmoid(heatmaps)

        posemap = self.offset_to_pose(offsets)

        batch_size, num_joints_with_center, h, w = heatmaps.shape
        num_joints = num_joints_with_center - 1

        center_heatmaps = heatmaps[:, -1]
        pose_ind, ctr_scores = self.get_top_values(center_heatmaps)

        posemap = posemap.permute(0, 2, 3, 1).view(batch_size, h * w, -1, 2)
        poses = torch.zeros(batch_size, pose_ind.shape[1], num_joints, 2).to(
            ctr_scores.device
        )
        for i in range(batch_size):
            pose = posemap[i, pose_ind[i]]
            poses[i] = pose

        if self.use_heatmap:
            poses = self._update_pose_with_heatmaps(poses, heatmaps[:, :-1])

        if self.keypoint_score_type == "center":
            score = (
                ctr_scores.unsqueeze(-1)
                .expand(batch_size, -1, num_joints)
                .unsqueeze(-1)
            )
        elif self.keypoint_score_type == "heatmap":
            score = self.get_heat_value(poses, heatmaps).unsqueeze(-1)
        elif self.keypoint_score_type == "combined":
            center_score = (
                ctr_scores.unsqueeze(-1)
                .expand(batch_size, -1, num_joints)
                .unsqueeze(-1)
            )
            htmp_score = self.get_heat_value(poses, heatmaps).unsqueeze(-1)
            score = center_score * htmp_score
        else:
            raise ValueError(f"Unknown keypoint score type: {self.keypoint_score_type}")

        poses[:, :, :, 0] = (
            poses[:, :, :, 0] * scale_factors[1] + 0.5 * scale_factors[1]
        )
        poses[:, :, :, 1] = (
            poses[:, :, :, 1] * scale_factors[0] + 0.5 * scale_factors[0]
        )

        if self.clip_scores:
            score = torch.clip(score, min=0, max=1)

        poses_w_scores = torch.cat([poses, score], dim=3)
        if self.apply_pose_nms:
            poses_w_scores = self.pose_nms(poses_w_scores)

        return {"poses": poses_w_scores}

    def get_locations(
        self, height: int, width: int, device: torch.device
    ) -> torch.Tensor:
        """Get locations for offsets.

        Args:
            height: Height of the offsets.
            width: Width of the offsets.
            device: Device to use.

        Returns:
            Offset locations.

        Example:
            # Assuming you have 'height', 'width', and 'device'
            locations = predictor.get_locations(height, width, device)
        """
        shifts_x = torch.arange(0, width, step=1, dtype=torch.float32).to(device)
        shifts_y = torch.arange(0, height, step=1, dtype=torch.float32).to(device)
        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x, indexing="ij")
        shift_x = shift_x.reshape(-1)
        shift_y = shift_y.reshape(-1)
        locations = torch.stack((shift_x, shift_y), dim=1)
        return locations

    def get_reg_poses(self, offsets: torch.Tensor, num_joints: int) -> torch.Tensor:
        """Get the regression poses from offsets.

        Args:
            offsets: Offsets tensor.
            num_joint: Number of joints.

        Returns:
            Regression poses.

        Example:
            # Assuming you have 'offsets' tensor and 'num_joints'
            regression_poses = predictor.get_reg_poses(offsets, num_joints)
        """
        batch_size, _, h, w = offsets.shape
        offsets = offsets.permute(0, 2, 3, 1).reshape(batch_size, h * w, num_joints, 2)
        locations = self.get_locations(h, w, offsets.device)
        locations = locations[None, :, None, :].expand(batch_size, -1, num_joints, -1)
        poses = locations - offsets

        return poses

    def offset_to_pose(self, offsets: torch.Tensor) -> torch.Tensor:
        """Convert offsets to poses.

        Args:
            offsets: Offsets tensor.

        Returns:
            Poses from offsets.

        Example:
            # Assuming you have 'offsets' tensor
            poses = predictor.offset_to_pose(offsets)
        """
        batch_size, num_offset, h, w = offsets.shape
        num_joints = int(num_offset / 2)
        reg_poses = self.get_reg_poses(offsets, num_joints)

        reg_poses = (
            reg_poses.contiguous()
            .view(batch_size, h * w, 2 * num_joints)
            .permute(0, 2, 1)
        )
        reg_poses = reg_poses.contiguous().view(batch_size, -1, h, w).contiguous()

        return reg_poses

    def max_pool(self, heatmap: torch.Tensor) -> torch.Tensor:
        """Apply max pooling to the heatmap.

        Args:
            heatmap: Heatmap tensor.

        Returns:
            Max pooled heatmap.

        Example:
            # Assuming you have 'heatmap' tensor
            max_pooled_heatmap = predictor.max_pool(heatmap)
        """
        pool1 = torch.nn.MaxPool2d(3, 1, 1)
        pool2 = torch.nn.MaxPool2d(5, 1, 2)
        pool3 = torch.nn.MaxPool2d(7, 1, 3)
        map_size = (heatmap.shape[1] + heatmap.shape[2]) / 2.0
        maxm = pool2(
            heatmap
        )  # Here I think pool 2 is a good match for default 17 pos_dist_tresh

        return maxm

    def get_top_values(
        self, heatmap: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Get top values from the heatmap.

        Args:
            heatmap: Heatmap tensor.

        Returns:
            Position indices and scores.

        Example:
            # Assuming you have 'heatmap' tensor
            positions, scores = predictor.get_top_values(heatmap)
        """
        maximum = self.max_pool(heatmap)
        maximum = torch.eq(maximum, heatmap)
        heatmap *= maximum

        batchsize, ny, nx = heatmap.shape
        heatmap_flat = heatmap.reshape(batchsize, nx * ny)

        scores, pos_ind = torch.topk(heatmap_flat, self.num_animals, dim=1)

        return pos_ind, scores

    def _update_pose_with_heatmaps(
        self, _poses: torch.Tensor, kpt_heatmaps: torch.Tensor
    ):
        """If a heatmap center is close enough from the regressed point, the final
        prediction is the center of this heatmap

        Args:
            poses: poses tensor, shape (batch_size, num_animals, num_keypoints, 2)
            kpt_heatmaps: heatmaps (does not contain the center heatmap), shape (batch_size, num_keypoints, h, w)
        """
        poses = _poses.clone()
        maxm = self.max_pool(kpt_heatmaps)
        maxm = torch.eq(maxm, kpt_heatmaps).float()
        kpt_heatmaps *= maxm
        batch_size, num_keypoints, h, w = kpt_heatmaps.shape
        kpt_heatmaps = kpt_heatmaps.view(batch_size, num_keypoints, -1)
        val_k, ind = kpt_heatmaps.topk(self.num_animals, dim=2)

        x = ind % w
        y = (ind / w).long()
        heats_ind = torch.stack((x, y), dim=3)

        for b in range(batch_size):
            for i in range(num_keypoints):
                heat_ind = heats_ind[b, i].float()
                pose_ind = poses[b, :, i]
                pose_heat_diff = pose_ind[:, None, :] - heat_ind
                pose_heat_diff.pow_(2)
                pose_heat_diff = pose_heat_diff.sum(2)
                pose_heat_diff.sqrt_()
                keep_ind = torch.argmin(pose_heat_diff, dim=1)

                for p in range(keep_ind.shape[0]):
                    if pose_heat_diff[p, keep_ind[p]] < self.max_absorb_distance:
                        poses[b, p, i] = heat_ind[keep_ind[p]]

        return poses

    def get_heat_value(
        self, pose_coords: torch.Tensor, heatmaps: torch.Tensor
    ) -> torch.Tensor:
        """Get heat values for pose coordinates and heatmaps.

        Args:
            pose_coords: Pose coordinates tensor (batch_size, num_animals, num_joints, 2)
            heatmaps: Heatmaps tensor (batch_size, 1+num_joints, h, w).

        Returns:
            Heat values.

        Example:
            # Assuming you have 'pose_coords' and 'heatmaps' tensors
            heat_values = predictor.get_heat_value(pose_coords, heatmaps)
        """
        h, w = heatmaps.shape[2:]
        heatmaps_nocenter = heatmaps[:, :-1].flatten(
            2, 3
        )  # (batch_size, num_joints, h*w)

        # Predicted poses based on the offset can be outside the image
        x = torch.clamp(torch.floor(pose_coords[:, :, :, 0]), 0, w - 1).long()
        y = torch.clamp(torch.floor(pose_coords[:, :, :, 1]), 0, h - 1).long()
        keypoint_poses = (y * w + x).mT  # (batch, num_joints, num_individuals)
        scores = torch.gather(heatmaps_nocenter, 2, keypoint_poses)
        return scores.mT  # (batch, num_individuals, num_joints)

    def pose_nms(self, poses: torch.Tensor) -> torch.Tensor:
        """Non-Maximum Suppression (NMS) for regressed poses.

        Args:
            poses: Pose proposals of shape (batch_size, num_people, num_joints, 3).
                The poses for each element in the batch should be sorted by score (the
                highest score prediction should be first).

        Returns:
            Pose proposals after non-maximum suppression.
        """
        batch_size, num_people, num_joints, _ = poses.shape
        if num_people == 0:
            return poses

        xy = poses[:, :, :, :2]
        w = xy[..., 0].max(dim=-1)[0] - xy[..., 0].min(dim=-1)[0]
        h = xy[..., 1].max(dim=-1)[0] - xy[..., 1].min(dim=-1)[0]
        area = torch.clamp((w * w) + (h * h), min=1)
        area = area.repeat(1, 1, num_people * num_joints)
        area = area.reshape(batch_size, num_people, num_people, num_joints)

        # TODO(niels): Optimize code
        # swap (batch, num_people) dims to be able to broadcast the diff
        xy_ = xy.transpose(1, 0)

        # compute the difference between keypoints
        pose_diff = xy_[:, None] - xy_
        pose_diff.pow_(2)

        # put batch first again
        pose_diff = pose_diff.transpose(2, 0)

        # Compute error between people pairs
        pose_dist = pose_diff.sum(dim=-1)
        pose_dist.sqrt_()

        pose_thresh = self.nms_threshold * torch.sqrt(area)
        pose_dist = (pose_dist < pose_thresh).sum(dim=-1)
        nms_pose = pose_dist > self.nms_threshold  # shape (b, num_people, num_people)

        for batch_idx in range(batch_size):
            # TODO(niels): Optimize code
            kept = torch.zeros(num_people, dtype=torch.bool)
            batch_order = []
            kept_indices = set()
            ignored_indices = set()
            for person_idx in range(num_people):
                if person_idx in ignored_indices:
                    continue

                kept_indices.add(person_idx)
                batch_order.append(person_idx)
                kept[person_idx] = True

                suppressed = (
                    nms_pose[batch_idx, person_idx]
                    .nonzero()
                    .detach()
                    .reshape(-1)
                    .tolist()
                )
                for to_suppress in suppressed:
                    if to_suppress not in kept_indices:
                        ignored_indices.add(to_suppress)

            for idx in ignored_indices:
                batch_order.append(idx)

            # Mask out suppressed predictions
            poses[batch_idx, ~kept] = -1

            # Re-order predictions so the non-suppressed ones are up top
            poses[batch_idx] = poses[batch_idx, batch_order]

        return poses


--- File: deeplabcut/pose_estimation_pytorch/models/predictors/paf_predictor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import numpy as np
import torch
import torch.nn.functional as F
from numpy.typing import NDArray

from deeplabcut.pose_estimation_pytorch.models.predictors.base import (
    BasePredictor,
    PREDICTORS,
)
from deeplabcut.core import inferenceutils

Graph = list[tuple[int, int]]


@PREDICTORS.register_module
class PartAffinityFieldPredictor(BasePredictor):
    """Predictor class for multiple animal pose estimation with part affinity fields.

    Args:
        num_animals: Number of animals in the project.
        num_multibodyparts: Number of animal's body parts (ignoring unique body parts).
        num_uniquebodyparts: Number of unique body parts.  # FIXME - should not be needed here if we separate the unique bodypart head
        graph: Part affinity field graph edges.
        edges_to_keep: List of indices in `graph` of the edges to keep.
        locref_stdev: Standard deviation for location refinement.
        nms_radius: Radius of the Gaussian kernel.
        sigma: Width of the 2D Gaussian distribution.
        min_affinity: Minimal edge affinity to add a body part to an Assembly.

    Returns:
        Regressed keypoints from heatmaps, locref_maps and part affinity fields, as in Tensorflow maDLC.
    """

    default_init = {
        "locref_stdev": 7.2801,
        "nms_radius": 5,
        "sigma": 1,
        "min_affinity": 0.05,
    }

    def __init__(
        self,
        num_animals: int,
        num_multibodyparts: int,
        num_uniquebodyparts: int,
        graph: Graph,
        edges_to_keep: list[int],
        locref_stdev: float,
        nms_radius: int,
        sigma: float,
        min_affinity: float,
        add_discarded: bool = False,
        apply_sigmoid: bool = True,
        clip_scores: bool = False,
        force_fusion: bool = False,
        return_preds: bool = False,
    ):
        """Initialize the PartAffinityFieldPredictor class.

        Args:
            num_animals: Number of animals in the project.
            num_multibodyparts: Number of animal's body parts (ignoring unique body parts).
            num_uniquebodyparts: Number of unique body parts.
            graph: Part affinity field graph edges.
            edges_to_keep: List of indices in `graph` of the edges to keep.
            locref_stdev: Standard deviation for location refinement.
            nms_radius: Radius of the Gaussian kernel.
            sigma: Width of the 2D Gaussian distribution.
            min_affinity: Minimal edge affinity to add a body part to an Assembly.
            return_preds: Whether to return predictions alongside the animals' poses

        Returns:
            None
        """
        super().__init__()
        self.num_animals = num_animals
        self.num_multibodyparts = num_multibodyparts
        self.num_uniquebodyparts = num_uniquebodyparts
        self.graph = graph
        self.edges_to_keep = edges_to_keep
        self.locref_stdev = locref_stdev
        self.nms_radius = nms_radius
        self.return_preds = return_preds
        self.sigma = sigma
        self.apply_sigmoid = apply_sigmoid
        self.clip_scores = clip_scores
        self.sigmoid = torch.nn.Sigmoid()
        self.assembler = inferenceutils.Assembler.empty(
            num_animals,
            n_multibodyparts=num_multibodyparts,
            n_uniquebodyparts=num_uniquebodyparts,
            graph=graph,
            paf_inds=edges_to_keep,
            min_affinity=min_affinity,
            add_discarded=add_discarded,
            force_fusion=force_fusion,
        )

    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor]
    ) -> dict[str, torch.Tensor]:
        """Forward pass of PartAffinityFieldPredictor. Gets predictions from model output.

        Args:
            stride: the stride of the model
            outputs: Output tensors from previous layers.
                output = heatmaps, locref, pafs
                heatmaps: torch.Tensor([batch_size, num_joints, height, width])
                locref: torch.Tensor([batch_size, num_joints, height, width])

        Returns:
            A dictionary containing a "poses" key with the output tensor as value.

        Example:
            >>> predictor = PartAffinityFieldPredictor(num_animals=3, location_refinement=True, locref_stdev=7.2801)
            >>> output = (torch.rand(32, 17, 64, 64), torch.rand(32, 34, 64, 64), torch.rand(32, 136, 64, 64))
            >>> stride = 8
            >>> poses = predictor.forward(stride, output)
        """
        heatmaps = outputs["heatmap"]
        locrefs = outputs["locref"]
        pafs = outputs["paf"]
        scale_factors = stride, stride
        batch_size, n_channels, height, width = heatmaps.shape

        if self.apply_sigmoid:
            heatmaps = self.sigmoid(heatmaps)

        # Filter predicted heatmaps with a 2D Gaussian kernel as in:
        # https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf
        kernel = self.make_2d_gaussian_kernel(
            sigma=self.sigma, size=self.nms_radius * 2 + 1
        )[None, None]
        kernel = kernel.repeat(n_channels, 1, 1, 1).to(heatmaps.device)
        heatmaps = F.conv2d(
            heatmaps, kernel, stride=1, padding="same", groups=n_channels
        )

        peaks = self.find_local_peak_indices_maxpool_nms(
            heatmaps, self.nms_radius, threshold=0.01
        )
        if ~torch.any(peaks):
            poses = -torch.ones(
                (batch_size, self.num_animals, self.num_multibodyparts, 5)
            )
            results = dict(poses=poses)
            if self.return_preds:
                results["preds"] = [dict(coordinates=[[]], costs=[])],

            return results

        locrefs = locrefs.reshape(batch_size, n_channels, 2, height, width)
        locrefs = locrefs * self.locref_stdev
        pafs = pafs.reshape(batch_size, -1, 2, height, width)

        graph = [self.graph[ind] for ind in self.edges_to_keep]
        preds = self.compute_peaks_and_costs(
            heatmaps,
            locrefs,
            pafs,
            peaks,
            graph,
            self.edges_to_keep,
            scale_factors,
            n_id_channels=0,  # FIXME Handle identity training
        )
        poses = -torch.ones((batch_size, self.num_animals, self.num_multibodyparts, 5))
        poses_unique = -torch.ones((batch_size, 1, self.num_uniquebodyparts, 4))
        for i, data_dict in enumerate(preds):
            assemblies, unique = self.assembler._assemble(data_dict, ind_frame=0)
            if assemblies is not None:
                for j, assembly in enumerate(assemblies):
                    poses[i, j, :, :4] = torch.from_numpy(assembly.data)
                    poses[i, j, :, 4] = assembly.affinity
            if unique is not None:
                poses_unique[i, 0, :, :4] = torch.from_numpy(unique)

        if self.clip_scores:
            poses[..., 2] = torch.clip(poses[..., 2], min=0, max=1)

        out = {"poses": poses}
        if self.return_preds:
            out["preds"] = preds
        return out

    @staticmethod
    def find_local_peak_indices_maxpool_nms(
        input_: torch.Tensor, radius: int, threshold: float
    ) -> torch.Tensor:
        pooled = F.max_pool2d(input_, kernel_size=radius, stride=1, padding=radius // 2)
        maxima = input_ * torch.eq(input_, pooled).float()
        peak_indices = torch.nonzero(maxima >= threshold, as_tuple=False)
        return peak_indices.int()

    @staticmethod
    def make_2d_gaussian_kernel(sigma: float, size: int) -> torch.Tensor:
        k = torch.arange(-size // 2 + 1, size // 2 + 1, dtype=torch.float32) ** 2
        k = F.softmax(-k / (2 * (sigma ** 2)), dim=0)
        return torch.einsum("i,j->ij", k, k)

    @staticmethod
    def calc_peak_locations(
        locrefs: torch.Tensor,
        peak_inds_in_batch: torch.Tensor,
        strides: tuple[float, float],
    ) -> torch.Tensor:
        s, b, r, c = peak_inds_in_batch.T
        stride_y, stride_x = strides
        strides = torch.Tensor((stride_x, stride_y)).to(locrefs.device)
        off = locrefs[s, b, :, r, c]
        loc = strides * peak_inds_in_batch[:, [3, 2]] + strides // 2 + off
        return loc

    @staticmethod
    def compute_edge_costs(
        pafs: NDArray,
        peak_inds_in_batch: NDArray,
        graph: Graph,
        paf_inds: list[int],
        n_bodyparts: int,
        n_points: int = 10,
        n_decimals: int = 3,
    ) -> list[dict[int, NDArray]]:
        # Clip peak locations to PAFs dimensions
        h, w = pafs.shape[-2:]
        peak_inds_in_batch[:, 2] = np.clip(peak_inds_in_batch[:, 2], 0, h - 1)
        peak_inds_in_batch[:, 3] = np.clip(peak_inds_in_batch[:, 3], 0, w - 1)

        n_samples = pafs.shape[0]
        sample_inds = []
        edge_inds = []
        all_edges = []
        all_peaks = []
        for i in range(n_samples):
            samples_i = peak_inds_in_batch[:, 0] == i
            peak_inds = peak_inds_in_batch[samples_i, 1:]
            if not np.any(peak_inds):
                continue
            peaks = peak_inds[:, 1:]
            bpt_inds = peak_inds[:, 0]
            idx = np.arange(peaks.shape[0])
            idx_per_bpt = {j: idx[bpt_inds == j].tolist() for j in range(n_bodyparts)}
            edges = []
            for k, (s, t) in zip(paf_inds, graph):
                inds_s = idx_per_bpt[s]
                inds_t = idx_per_bpt[t]
                if not (inds_s and inds_t):
                    continue
                candidate_edges = ((i, j) for i in inds_s for j in inds_t)
                edges.extend(candidate_edges)
                edge_inds.extend([k] * len(inds_s) * len(inds_t))
            if not edges:
                continue
            sample_inds.extend([i] * len(edges))
            all_edges.extend(edges)
            all_peaks.append(peaks[np.asarray(edges)])
        if not all_peaks:
            return [dict() for _ in range(n_samples)]

        sample_inds = np.asarray(sample_inds, dtype=np.int32)
        edge_inds = np.asarray(edge_inds, dtype=np.int32)
        all_edges = np.asarray(all_edges, dtype=np.int32)
        all_peaks = np.concatenate(all_peaks)
        vecs_s = all_peaks[:, 0]
        vecs_t = all_peaks[:, 1]
        vecs = vecs_t - vecs_s
        lengths = np.linalg.norm(vecs, axis=1).astype(np.float32)
        lengths += np.spacing(1, dtype=np.float32)
        xy = np.linspace(vecs_s, vecs_t, n_points, axis=1, dtype=np.int32)
        y = pafs[
            sample_inds.reshape((-1, 1)),
            edge_inds.reshape((-1, 1)),
            :,
            xy[..., 0],
            xy[..., 1],
        ]
        integ = np.trapz(y, xy[..., ::-1], axis=1)
        affinities = np.linalg.norm(integ, axis=1).astype(np.float32)
        affinities /= lengths
        np.round(affinities, decimals=n_decimals, out=affinities)
        np.round(lengths, decimals=n_decimals, out=lengths)

        # Form cost matrices
        all_costs = []
        for i in range(n_samples):
            samples_i_mask = sample_inds == i
            costs = dict()
            for k in paf_inds:
                edges_k_mask = edge_inds == k
                idx = np.flatnonzero(samples_i_mask & edges_k_mask)
                s, t = all_edges[idx].T
                n_sources = np.unique(s).size
                n_targets = np.unique(t).size
                costs[k] = dict()
                costs[k]["m1"] = affinities[idx].reshape((n_sources, n_targets))
                costs[k]["distance"] = lengths[idx].reshape((n_sources, n_targets))
            all_costs.append(costs)

        return all_costs

    @staticmethod
    def _linspace(start: torch.Tensor, stop: torch.Tensor, num: int) -> torch.Tensor:
        # Taken from https://github.com/pytorch/pytorch/issues/61292#issue-937937159
        steps = torch.linspace(0, 1, num, dtype=torch.float32, device=start.device)
        steps = steps.reshape([-1, *([1] * start.ndim)])
        out = start[None] + steps * (stop - start)[None]
        return out.swapaxes(0, 1)

    def compute_peaks_and_costs(
        self,
        heatmaps: torch.Tensor,
        locrefs: torch.Tensor,
        pafs: torch.Tensor,
        peak_inds_in_batch: torch.Tensor,
        graph: Graph,
        paf_inds: list[int],
        strides: tuple[float, float],
        n_id_channels: int,
        n_points: int = 10,
        n_decimals: int = 3,
    ) -> list[dict[str, NDArray]]:
        n_samples, n_channels = heatmaps.shape[:2]
        n_bodyparts = n_channels - n_id_channels
        pos = self.calc_peak_locations(locrefs, peak_inds_in_batch, strides)
        pos = np.round(pos.detach().cpu().numpy(), decimals=n_decimals)
        heatmaps = heatmaps.detach().cpu().numpy()
        pafs = pafs.detach().cpu().numpy()
        peak_inds_in_batch = peak_inds_in_batch.detach().cpu().numpy()
        costs = self.compute_edge_costs(
            pafs, peak_inds_in_batch, graph, paf_inds, n_bodyparts, n_points, n_decimals
        )
        s, b, r, c = peak_inds_in_batch.T
        prob = np.round(heatmaps[s, b, r, c], n_decimals).reshape((-1, 1))
        if n_id_channels:
            ids = np.round(heatmaps[s, -n_id_channels:, r, c], n_decimals)

        peaks_and_costs = []
        for i in range(n_samples):
            xy = []
            p = []
            id_ = []
            samples_i_mask = peak_inds_in_batch[:, 0] == i
            for j in range(n_bodyparts):
                bpts_j_mask = peak_inds_in_batch[:, 1] == j
                idx = np.flatnonzero(samples_i_mask & bpts_j_mask)
                xy.append(pos[idx])
                p.append(prob[idx])
                if n_id_channels:
                    id_.append(ids[idx])
            dict_ = {"coordinates": (xy,), "confidence": p}
            if costs is not None:
                dict_["costs"] = costs[i]
            if n_id_channels:
                dict_["identity"] = id_
            peaks_and_costs.append(dict_)

        return peaks_and_costs

    def set_paf_edges_to_keep(self, edge_indices: list[int]) -> None:
        """Sets the PAF edge indices to use to assemble individuals

        Args:
            edge_indices: The indices of edges in the graph to keep.
        """
        self.edges_to_keep = edge_indices
        self.assembler.paf_inds = edge_indices


--- File: deeplabcut/pose_estimation_pytorch/models/predictors/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.models.predictors.base import (
    PREDICTORS,
    BasePredictor,
)
from deeplabcut.pose_estimation_pytorch.models.predictors.dekr_predictor import (
    DEKRPredictor,
)
from deeplabcut.pose_estimation_pytorch.models.predictors.identity_predictor import (
    IdentityPredictor,
)
from deeplabcut.pose_estimation_pytorch.models.predictors.paf_predictor import (
    PartAffinityFieldPredictor,
)
from deeplabcut.pose_estimation_pytorch.models.predictors.sim_cc import (
    SimCCPredictor,
)
from deeplabcut.pose_estimation_pytorch.models.predictors.single_predictor import (
    HeatmapPredictor,
)


--- File: deeplabcut/pose_estimation_pytorch/models/predictors/single_predictor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from typing import Tuple

import torch

from deeplabcut.pose_estimation_pytorch.models.predictors.base import (
    BasePredictor,
    PREDICTORS,
)


@PREDICTORS.register_module
class HeatmapPredictor(BasePredictor):
    """Predictor class for pose estimation from heatmaps (and optionally locrefs).

    Args:
        location_refinement: Enable location refinement.
        locref_std: Standard deviation for location refinement.
        apply_sigmoid: Apply sigmoid to heatmaps. Defaults to True.

    Returns:
        Regressed keypoints from heatmaps and locref_maps of baseline DLC model (ResNet + Deconv).
    """

    def __init__(
        self,
        apply_sigmoid: bool = True,
        clip_scores: bool = False,
        location_refinement: bool = True,
        locref_std: float = 7.2801,
    ):
        """
        Args:
            apply_sigmoid: Apply sigmoid to heatmaps. Defaults to True.
            clip_scores: If a sigmoid is not applied, this can be used to clip scores
                for predicted keypoints to values in [0, 1].
            location_refinement : Enable location refinement.
            locref_std: Standard deviation for location refinement.
        """
        super().__init__()
        self.apply_sigmoid = apply_sigmoid
        self.clip_scores = clip_scores
        self.sigmoid = torch.nn.Sigmoid()
        self.location_refinement = location_refinement
        self.locref_std = locref_std

    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor]
    ) -> dict[str, torch.Tensor]:
        """Forward pass of SinglePredictor. Gets predictions from model output.

        Args:
            stride: the stride of the model
            outputs: output of the model heads (heatmap, locref)

        Returns:
            A dictionary containing a "poses" key with the output tensor as value.

        Example:
            >>> predictor = HeatmapPredictor(location_refinement=True, locref_std=7.2801)
            >>> stride = 8
            >>> output = {"heatmap": torch.rand(32, 17, 64, 64), "locref": torch.rand(32, 17, 64, 64)}
            >>> poses = predictor.forward(stride, output)
        """
        heatmaps = outputs["heatmap"]
        scale_factors = stride, stride

        if self.apply_sigmoid:
            heatmaps = self.sigmoid(heatmaps)

        heatmaps = heatmaps.permute(0, 2, 3, 1)
        batch_size, height, width, num_joints = heatmaps.shape

        locrefs = None
        if self.location_refinement:
            locrefs = outputs["locref"]
            locrefs = locrefs.permute(0, 2, 3, 1).reshape(
                batch_size, height, width, num_joints, 2
            )
            locrefs = locrefs * self.locref_std

        poses = self.get_pose_prediction(heatmaps, locrefs, scale_factors)

        if self.clip_scores:
            poses[..., 2] = torch.clip(poses[..., 2], min=0, max=1)

        return {"poses": poses}

    def get_top_values(
        self, heatmap: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Get the top values from the heatmap.

        Args:
            heatmap: Heatmap tensor.

        Returns:
            Y and X indices of the top values.

        Example:
            >>> predictor = HeatmapPredictor(location_refinement=True, locref_std=7.2801)
            >>> heatmap = torch.rand(32, 17, 64, 64)
            >>> Y, X = predictor.get_top_values(heatmap)
        """
        batchsize, ny, nx, num_joints = heatmap.shape
        heatmap_flat = heatmap.reshape(batchsize, nx * ny, num_joints)
        heatmap_top = torch.argmax(heatmap_flat, dim=1)
        y, x = heatmap_top // nx, heatmap_top % nx
        return y, x

    def get_pose_prediction(
        self, heatmap: torch.Tensor, locref: torch.Tensor | None, scale_factors
    ) -> torch.Tensor:
        """Gets the pose prediction given the heatmaps and locref.

        Args:
            heatmap: Heatmap tensor with the following format (batch_size, height, width, num_joints)
            locref: Locref tensor with the following format (batch_size, height, width, num_joints, 2)
            scale_factors: Scale factors for the poses.

        Returns:
            Pose predictions of the format: (batch_size, num_people = 1, num_joints, 3)

        Example:
            >>> predictor = HeatmapPredictor(location_refinement=True, locref_std=7.2801)
            >>> heatmap = torch.rand(32, 17, 64, 64)
            >>> locref = torch.rand(32, 17, 64, 64, 2)
            >>> scale_factors = (0.5, 0.5)
            >>> poses = predictor.get_pose_prediction(heatmap, locref, scale_factors)
        """
        y, x = self.get_top_values(heatmap)

        batch_size, num_joints = x.shape

        dz = torch.zeros((batch_size, 1, num_joints, 3), device=heatmap.device)
        for b in range(batch_size):
            for j in range(num_joints):
                dz[b, 0, j, 2] = heatmap[b, y[b, j], x[b, j], j]
                if locref is not None:
                    dz[b, 0, j, :2] = locref[b, y[b, j], x[b, j], j, :]

        x, y = torch.unsqueeze(x, 1), torch.unsqueeze(y, 1)

        x = x * scale_factors[1] + 0.5 * scale_factors[1] + dz[:, :, :, 0]
        y = y * scale_factors[0] + 0.5 * scale_factors[0] + dz[:, :, :, 1]

        pose = torch.zeros((batch_size, 1, num_joints, 3), device=heatmap.device)
        pose[:, :, :, 0] = x
        pose[:, :, :, 1] = y
        pose[:, :, :, 2] = dz[:, :, :, 2]
        return pose


--- File: deeplabcut/pose_estimation_pytorch/models/predictors/identity_predictor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Predictor to generate identity maps from head outputs"""
import torch
import torch.nn as nn
import torchvision.transforms.functional as F

from deeplabcut.pose_estimation_pytorch.models.predictors.base import (
    BasePredictor,
    PREDICTORS,
)


@PREDICTORS.register_module
class IdentityPredictor(BasePredictor):
    """Predictor to generate identity maps from head outputs

    Attributes:
        apply_sigmoid: Apply sigmoid to heatmaps. Defaults to True.
    """

    def __init__(self, apply_sigmoid: bool = True):
        """
        Args:
            apply_sigmoid: Apply sigmoid to heatmaps. Defaults to True.
        """
        super().__init__()
        self.apply_sigmoid = apply_sigmoid
        self.sigmoid = nn.Sigmoid()

    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor]
    ) -> dict[str, torch.Tensor]:
        """
        Swaps the dimensions so the heatmap are (batch_size, h, w, num_individuals),
        optionally applies a sigmoid to the heatmaps, and rescales it to be the size
        of the original image (so that the identity scores of keypoints can be computed)

        Args:
            stride: the stride of the model
            outputs: output of the model identity head, of shape (b, num_idv, w', h')

        Returns:
            A dictionary containing a "heatmap" key with the identity heatmap tensor as
            value.
        """
        heatmaps = outputs["heatmap"]
        h_out, w_out = heatmaps.shape[2:]
        h_in, w_in = int(h_out * stride), int(w_out * stride)
        heatmaps = F.resize(
            heatmaps,
            size=[h_in, w_in],
            interpolation=F.InterpolationMode.BILINEAR,
            antialias=True,
        )
        if self.apply_sigmoid:
            heatmaps = self.sigmoid(heatmaps)

        # permute to have shape (batch_size, h, w, num_individuals)
        heatmaps = heatmaps.permute((0, 2, 3, 1))
        return {"heatmap": heatmaps}


--- File: deeplabcut/pose_estimation_pytorch/models/predictors/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from abc import ABC, abstractmethod

import torch
from torch import nn

from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry

PREDICTORS = Registry("predictors", build_func=build_from_cfg)


class BasePredictor(ABC, nn.Module):
    """The base Predictor class.

    This class is an abstract base class (ABC) for defining predictors used in the DeepLabCut Toolbox.
    All predictor classes should inherit from this base class and implement the forward method.
    Regresses keypoint coordinates from a models output maps

    Attributes:
        num_animals: Number of animals in the project. Should be set in subclasses.

    Example:
        # Create a subclass that inherits from BasePredictor and implements the forward method.
        class MyPredictor(BasePredictor):
            def __init__(self, num_animals):
                super().__init__()
                self.num_animals = num_animals

            def forward(self, outputs):
                # Implement the forward pass of your custom predictor here.
                pass
    """

    def __init__(self):
        super().__init__()
        self.num_animals = None

    @abstractmethod
    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor]
    ) -> dict[str, torch.Tensor]:
        """Abstract method for the forward pass of the Predictor.

        Args:
            stride: the stride of the model
            outputs: outputs of the model heads

        Returns:
            A dictionary containing a "poses" key with the output tensor as value, and
            optionally a "unique_bodyparts" with the unique bodyparts tensor as value.

        Raises:
            NotImplementedError: This method must be implemented in subclasses.
        """
        pass


--- File: deeplabcut/pose_estimation_pytorch/models/predictors/sim_cc.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""SimCC predictor for the RTMPose model

Based on the official ``mmpose`` SimCC codec and RTMCC head implementation. For more
information, see <https://github.com/open-mmlab/mmpose>.
"""
from __future__ import annotations

import numpy as np
import torch

from deeplabcut.pose_estimation_pytorch.models.predictors.base import (
    BasePredictor,
    PREDICTORS,
)


@PREDICTORS.register_module
class SimCCPredictor(BasePredictor):
    """Class used to make pose predictions from RTMPose head outputs

    The RTMPose model uses coordinate classification for pose estimation. For more
    information, see "SimCC: a Simple Coordinate Classification Perspective for Human
    Pose Estimation" (<https://arxiv.org/pdf/2107.03332>) and "RTMPose: Real-Time
    Multi-Person Pose Estimation based on MMPose" (<https://arxiv.org/pdf/2303.07399>).

    Args:
        simcc_split_ratio: The split ratio of pixels, as described in SimCC.
    """

    def __init__(self, simcc_split_ratio: float = 2.0) -> None:
        super().__init__()
        self.simcc_split_ratio = simcc_split_ratio

    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor]
    ) -> dict[str, torch.Tensor]:
        simcc_x = outputs["x"].detach().cpu().numpy()
        simcc_y = outputs["y"].detach().cpu().numpy()
        keypoints, scores = get_simcc_maximum(simcc_x, simcc_y)

        if keypoints.ndim == 2:
            keypoints = keypoints[None, :]
            scores = scores[None, :]

        keypoints /= self.simcc_split_ratio
        scores = scores.reshape((*scores.shape, -1))
        keypoints_with_score = np.concatenate([keypoints, scores], axis=-1)
        keypoints_with_score = torch.tensor(keypoints_with_score).unsqueeze(1)
        return dict(poses=keypoints_with_score)


def get_simcc_maximum(
    simcc_x: np.ndarray,
    simcc_y: np.ndarray,
    apply_softmax: bool = False,
) -> tuple[np.ndarray, np.ndarray]:
    """Get maximum response location and value from SimCC representations.

    Note:
        instance number: N
        num_keypoints: K
        heatmap height: H
        heatmap width: W

    Args:
        simcc_x (np.ndarray): x-axis SimCC in shape (K, Wx) or (N, K, Wx)
        simcc_y (np.ndarray): y-axis SimCC in shape (K, Wy) or (N, K, Wy)
        apply_softmax (bool): whether to apply softmax on the heatmap.
            Defaults to False.

    Returns:
        tuple:
        - locs (np.ndarray): locations of maximum heatmap responses in shape
            (K, 2) or (N, K, 2)
        - vals (np.ndarray): values of maximum heatmap responses in shape
            (K,) or (N, K)
    """

    assert isinstance(simcc_x, np.ndarray), "simcc_x should be numpy.ndarray"
    assert isinstance(simcc_y, np.ndarray), "simcc_y should be numpy.ndarray"
    assert simcc_x.ndim == 2 or simcc_x.ndim == 3, f"Invalid shape {simcc_x.shape}"
    assert simcc_y.ndim == 2 or simcc_y.ndim == 3, f"Invalid shape {simcc_y.shape}"
    assert simcc_x.ndim == simcc_y.ndim, f"{simcc_x.shape} != {simcc_y.shape}"

    if simcc_x.ndim == 3:
        N, K, Wx = simcc_x.shape
        simcc_x = simcc_x.reshape(N * K, -1)
        simcc_y = simcc_y.reshape(N * K, -1)
    else:
        N = None

    if apply_softmax:
        simcc_x = simcc_x - np.max(simcc_x, axis=1, keepdims=True)
        simcc_y = simcc_y - np.max(simcc_y, axis=1, keepdims=True)
        ex, ey = np.exp(simcc_x), np.exp(simcc_y)
        simcc_x = ex / np.sum(ex, axis=1, keepdims=True)
        simcc_y = ey / np.sum(ey, axis=1, keepdims=True)

    x_locs = np.argmax(simcc_x, axis=1)
    y_locs = np.argmax(simcc_y, axis=1)
    locs = np.stack((x_locs, y_locs), axis=-1).astype(np.float32)
    max_val_x = np.amax(simcc_x, axis=1)
    max_val_y = np.amax(simcc_y, axis=1)

    mask = max_val_x > max_val_y
    max_val_x[mask] = max_val_y[mask]
    vals = max_val_x
    locs[vals <= 0.0] = -1

    if N:
        locs = locs.reshape(N, K, 2)
        vals = vals.reshape(N, K)

    return locs, vals


--- File: deeplabcut/pose_estimation_pytorch/models/heads/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.models.heads.base import HEADS, BaseHead
from deeplabcut.pose_estimation_pytorch.models.heads.dekr import DEKRHead
from deeplabcut.pose_estimation_pytorch.models.heads.dlcrnet import DLCRNetHead
from deeplabcut.pose_estimation_pytorch.models.heads.rtmcc_head import RTMCCHead
from deeplabcut.pose_estimation_pytorch.models.heads.simple_head import HeatmapHead
from deeplabcut.pose_estimation_pytorch.models.heads.transformer import TransformerHead


--- File: deeplabcut/pose_estimation_pytorch/models/heads/simple_head.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import torch
import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.models.criterions import (
    BaseCriterion,
    BaseLossAggregator,
)
from deeplabcut.pose_estimation_pytorch.models.heads.base import (
    BaseHead,
    HEADS,
    WeightConversionMixin,
)
from deeplabcut.pose_estimation_pytorch.models.predictors import BasePredictor
from deeplabcut.pose_estimation_pytorch.models.target_generators import BaseGenerator
from deeplabcut.pose_estimation_pytorch.models.weight_init import BaseWeightInitializer


@HEADS.register_module
class HeatmapHead(WeightConversionMixin, BaseHead):
    """Deconvolutional head to predict maps from the extracted features.

    This class implements a simple deconvolutional head to predict maps from the
    extracted features.

    Args:
        predictor: The predictor used to transform heatmaps into keypoints.
        target_generator: The module to generate target heatmaps from keypoints.
        criterion: The loss criterion(s) for the head.
        aggregator: The loss aggregator to use, if multiple criterions are used.
        heatmap_config: The configuration for the heatmap outputs of the head.
        locref_config: The configuration for the location refinement outputs (None if
            no location refinement should be used).
        weight_init: The way to initialize weights for the head. If None, default
            PyTorch initialization is used. Otherwise, a BaseWeightInitializer can be
            given (or a configuration for a BaseWeightInitializer). To initialize
            the weights with a normal distribution, you could pass
            ``weight_init="normal"`` (which initializes weights using a Normal
            distribution 0.001 and biases with 0), or you could pass ``weight_init={
            type="normal", std=0.01}`` to change the standard deviation used. All
            BaseWeightInitializers are defined in deeplabcut/pose_estimation_pytorch/
            models/weight_init.py.
    """

    def __init__(
        self,
        predictor: BasePredictor,
        target_generator: BaseGenerator,
        criterion: dict[str, BaseCriterion] | BaseCriterion,
        aggregator: BaseLossAggregator | None,
        heatmap_config: dict,
        locref_config: dict | None = None,
        weight_init: str | dict | BaseWeightInitializer | None = None,
    ) -> None:
        heatmap_head = DeconvModule(**heatmap_config)
        locref_head = None
        if locref_config is not None:
            locref_head = DeconvModule(**locref_config)

            # check that the heatmap and locref modules have the same stride
            if heatmap_head.stride != locref_head.stride:
                raise ValueError(
                    f"Invalid model config: Your heatmap and locref need to have the "
                    f"same stride (found {heatmap_head.stride}, "
                    f"{locref_head.stride}). Please check your config (found "
                    f"heatmap_config={heatmap_config}, locref_config={locref_config}"
                )

        super().__init__(
            heatmap_head.stride,
            predictor,
            target_generator,
            criterion,
            aggregator,
            weight_init,
        )
        self.heatmap_head = heatmap_head
        self.locref_head = locref_head
        self._init_weights()

    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:
        outputs = {"heatmap": self.heatmap_head(x)}
        if self.locref_head is not None:
            outputs["locref"] = self.locref_head(x)
        return outputs

    @staticmethod
    def convert_weights(
        state_dict: dict[str, torch.Tensor],
        module_prefix: str,
        conversion: torch.Tensor,
    ) -> dict[str, torch.Tensor]:
        """Converts pre-trained weights to be fine-tuned on another dataset

        Args:
            state_dict: the state dict for the pre-trained model
            module_prefix: the prefix for weights in this head (e.g., 'heads.bodypart.')
            conversion: the mapping of old indices to new indices
        """
        state_dict = DeconvModule.convert_weights(
            state_dict,
            f"{module_prefix}heatmap_head.",
            conversion,
        )

        locref_conversion = torch.stack(
            [2 * conversion, 2 * conversion + 1],
            dim=1,
        ).reshape(-1)
        state_dict = DeconvModule.convert_weights(
            state_dict,
            f"{module_prefix}locref_head.",
            locref_conversion,
        )
        return state_dict


class DeconvModule(nn.Module):
    """
    Deconvolutional module to predict maps from the extracted features.
    """

    def __init__(
        self,
        channels: list[int],
        kernel_size: list[int],
        strides: list[int],
        final_conv: dict | None = None,
    ) -> None:
        """
        Args:
            channels: List containing the number of input and output channels for each
                deconvolutional layer.
            kernel_size: List containing the kernel size for each deconvolutional layer.
            strides: List containing the stride for each deconvolutional layer.
            final_conv: Configuration for a conv layer after the deconvolutional layers,
                if one should be added. Must have keys "out_channels" and "kernel_size".
        """
        super().__init__()
        if not (len(channels) == len(kernel_size) + 1 == len(strides) + 1):
            raise ValueError(
                "Incorrect DeconvModule configuration: there should be one more number"
                f" of channels than kernel_sizes and strides, found {len(channels)} "
                f"channels, {len(kernel_size)} kernels and {len(strides)} strides."
            )

        in_channels = channels[0]
        head_stride = 1
        self.deconv_layers = nn.Identity()
        if len(kernel_size) > 0:
            self.deconv_layers = nn.Sequential(
                *self._make_layers(in_channels, channels[1:], kernel_size, strides)
            )
            for s in strides:
                head_stride *= s

        self.stride = head_stride
        self.final_conv = nn.Identity()
        if final_conv:
            self.final_conv = nn.Conv2d(
                in_channels=channels[-1],
                out_channels=final_conv["out_channels"],
                kernel_size=final_conv["kernel_size"],
                stride=1,
            )

    @staticmethod
    def _make_layers(
        in_channels: int,
        out_channels: list[int],
        kernel_sizes: list[int],
        strides: list[int],
    ) -> list[nn.Module]:
        """
        Helper function to create the deconvolutional layers.

        Args:
            in_channels: number of input channels to the module
            out_channels: number of output channels of each layer
            kernel_sizes: size of the deconvolutional kernel
            strides: stride for the convolution operation

        Returns:
            the deconvolutional layers
        """
        layers = []
        for out_channels, k, s in zip(out_channels, kernel_sizes, strides):
            layers.append(
                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=k, stride=s)
            )
            layers.append(nn.ReLU())
            in_channels = out_channels
        return layers[:-1]

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass of the HeatmapHead

        Args:
            x: input tensor

        Returns:
            out: output tensor
        """
        x = self.deconv_layers(x)
        x = self.final_conv(x)
        return x

    @staticmethod
    def convert_weights(
        state_dict: dict[str, torch.Tensor],
        module_prefix: str,
        conversion: torch.Tensor,
    ) -> dict[str, torch.Tensor]:
        """Converts pre-trained weights to be fine-tuned on another dataset

        Args:
            state_dict: the state dict for the pre-trained model
            module_prefix: the prefix for weights in this head (e.g., 'heads.bodypart')
            conversion: the mapping of old indices to new indices
        """
        if f"{module_prefix}final_conv.weight" in state_dict:
            # has final convolution
            weight_key = f"{module_prefix}final_conv.weight"
            bias_key = f"{module_prefix}final_conv.bias"
            state_dict[weight_key] = state_dict[weight_key][conversion]
            state_dict[bias_key] = state_dict[bias_key][conversion]
            return state_dict

        # get the last deconv layer of the net
        next_index = 0
        while f"{module_prefix}deconv_layers.{next_index}.weight" in state_dict:
            next_index += 1
        last_index = next_index - 1

        # if there are deconv layers for this module prefix (there might not be,
        # e.g., when there are no location refinement layers in a heatmap head)
        if last_index >= 0:
            weight_key = f"{module_prefix}deconv_layers.{last_index}.weight"
            bias_key = f"{module_prefix}deconv_layers.{last_index}.bias"

            # for ConvTranspose2d, the weight shape is (in_channels, out_channels, ...)
            # while it's (out_channels, in_channels, ...) for Conv2d
            state_dict[weight_key] = state_dict[weight_key][:, conversion]
            state_dict[bias_key] = state_dict[bias_key][conversion]

        return state_dict


--- File: deeplabcut/pose_estimation_pytorch/models/heads/rtmcc_head.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Modified SimCC head for the RTMPose model

Based on the official ``mmpose`` RTMCC head implementation. For more information, see
<https://github.com/open-mmlab/mmpose>.
"""
from __future__ import annotations

import torch
import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.models.criterions import (
    BaseCriterion,
    BaseLossAggregator,
)
from deeplabcut.pose_estimation_pytorch.models.heads.base import (
    BaseHead,
    HEADS,
)
from deeplabcut.pose_estimation_pytorch.models.modules import (
    GatedAttentionUnit,
    ScaleNorm,
)
from deeplabcut.pose_estimation_pytorch.models.predictors import BasePredictor
from deeplabcut.pose_estimation_pytorch.models.target_generators import BaseGenerator
from deeplabcut.pose_estimation_pytorch.models.weight_init import BaseWeightInitializer


@HEADS.register_module
class RTMCCHead(BaseHead):
    """RTMPose Coordinate Classification head

    The RTMCC head is itself adapted from the SimCC head. For more information, see
    "SimCC: a Simple Coordinate Classification Perspective for Human Pose Estimation"
    (<https://arxiv.org/pdf/2107.03332>) and "RTMPose: Real-Time Multi-Person Pose
    Estimation based on MMPose" (<https://arxiv.org/pdf/2303.07399>).

    Args:
        input_size: The size of images given to the pose estimation model.
        in_channels: The number of input channels for the head.
        out_channels: Number of channels output by the head (number of bodyparts).
        in_featuremap_size: The size of the input feature map for the head. This is
            equal to the input_size divided by the backbone stride.
        simcc_split_ratio: The split ratio of pixels, as described in SimCC.
        final_layer_kernel_size: Kernel size of the final convolutional layer.
        gau_cfg: Configuration for the GatedAttentionUnit.
        predictor: The predictor for the head. Should usually be a `SimCCPredictor`.
        target_generator: The target generator for the head. Should usually be a
            `SimCCGenerator`.
        criterion: The loss criterions for the RTMCC outputs. There should be a
            criterion for "x" and a criterion for "y".
        aggregator: The loss aggregator to combine the losses.
        weight_init: The weight initializer to use for the head.
    """

    def __init__(
        self,
        input_size: tuple[int, int],
        in_channels: int,
        out_channels: int,
        in_featuremap_size: tuple[int, int],
        simcc_split_ratio: float,
        final_layer_kernel_size: int,
        gau_cfg: dict,
        predictor: BasePredictor,
        target_generator: BaseGenerator,
        criterion: dict[str, BaseCriterion],
        aggregator: BaseLossAggregator,
        weight_init: str | dict | BaseWeightInitializer | None = None,
    ) -> None:
        super().__init__(
            1,
            predictor,
            target_generator,
            criterion,
            aggregator,
            weight_init,
        )

        self.input_size = input_size
        self.in_channels = in_channels
        self.out_channels = out_channels

        self.in_featuremap_size = in_featuremap_size
        self.simcc_split_ratio = simcc_split_ratio

        flatten_dims = self.in_featuremap_size[0] * self.in_featuremap_size[1]
        out_w = int(self.input_size[0] * self.simcc_split_ratio)
        out_h = int(self.input_size[1] * self.simcc_split_ratio)

        self.gau = GatedAttentionUnit(
            num_token=self.out_channels,
            in_token_dims=gau_cfg["hidden_dims"],
            out_token_dims=gau_cfg["hidden_dims"],
            expansion_factor=gau_cfg["expansion_factor"],
            s=gau_cfg["s"],
            eps=1e-5,
            dropout_rate=gau_cfg["dropout_rate"],
            drop_path=gau_cfg["drop_path"],
            attn_type="self-attn",
            act_fn=gau_cfg["act_fn"],
            use_rel_bias=gau_cfg["use_rel_bias"],
            pos_enc=gau_cfg["pos_enc"],
        )

        self.final_layer = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=final_layer_kernel_size,
            stride=1,
            padding=final_layer_kernel_size // 2,
        )
        self.mlp = nn.Sequential(
            ScaleNorm(flatten_dims),
            nn.Linear(flatten_dims, gau_cfg["hidden_dims"], bias=False),
        )

        self.cls_x = nn.Linear(gau_cfg["hidden_dims"], out_w, bias=False)
        self.cls_y = nn.Linear(gau_cfg["hidden_dims"], out_h, bias=False)

    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:
        feats = self.final_layer(x)  # -> B, K, H, W
        feats = torch.flatten(feats, start_dim=2)  # -> B, K, hidden=HxW
        feats = self.mlp(feats)  # -> B, K, hidden
        feats = self.gau(feats)
        x, y = self.cls_x(feats), self.cls_y(feats)
        return dict(x=x, y=y)

    @staticmethod
    def update_input_size(model_cfg: dict, input_size: tuple[int, int]) -> None:
        """Updates an RTMPose model configuration file for a new image input size

        Args:
            model_cfg: The model configuration to update in-place.
            input_size: The updated input (width, height).
        """
        _sigmas = {192: 4.9, 256: 5.66, 288: 6, 384: 6.93}

        def _sigma(size: int) -> float:
            sigma = _sigmas.get(size)
            if sigma is None:
                return 2.87 + 0.01 * size

            return sigma

        w, h = input_size
        model_cfg["data"]["inference"]["top_down_crop"] = dict(width=w, height=h)
        model_cfg["data"]["train"]["top_down_crop"] = dict(width=w, height=h)
        head_cfg = model_cfg["model"]["heads"]["bodypart"]
        head_cfg["input_size"] = input_size
        head_cfg["in_featuremap_size"] = h // 32, w // 32
        head_cfg["target_generator"]["input_size"] = input_size
        head_cfg["target_generator"]["sigma"] = (_sigma(w), _sigma(h))


--- File: deeplabcut/pose_estimation_pytorch/models/heads/transformer.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import torch
from einops import rearrange
from timm.layers import trunc_normal_
from torch import nn as nn

from deeplabcut.pose_estimation_pytorch.models.criterions import BaseCriterion
from deeplabcut.pose_estimation_pytorch.models.heads import BaseHead, HEADS
from deeplabcut.pose_estimation_pytorch.models.predictors import BasePredictor
from deeplabcut.pose_estimation_pytorch.models.target_generators import BaseGenerator


@HEADS.register_module
class TransformerHead(BaseHead):
    """
    Transformer Head module to predict heatmaps using a transformer-based approach
    """

    def __init__(
        self,
        predictor: BasePredictor,
        target_generator: BaseGenerator,
        criterion: BaseCriterion,
        dim: int,
        hidden_heatmap_dim: int,
        heatmap_dim: int,
        apply_multi: bool,
        heatmap_size: tuple[int, int],
        apply_init: bool,
        head_stride: int,
    ):
        """
        Args:
            dim: Dimension of the input features.
            hidden_heatmap_dim: Dimension of the hidden features in the MLP head.
            heatmap_dim: Dimension of the output heatmaps.
            apply_multi: If True, apply a multi-layer perceptron (MLP) with LayerNorm
                to generate heatmaps. If False, directly apply a single linear
                layer for heatmap prediction.
            heatmap_size: Tuple (height, width) representing the size of the output
                heatmaps.
            apply_init: If True, apply weight initialization to the module's layers.
            head_stride: The stride for the head (or neck + head pair), where positive
                values indicate an increase in resolution while negative values a
                decrease. Assuming that H and W are divisible by head_stride, this is
                the value such that if a backbone outputs an encoding of shape
                (C, H, W), the head will output heatmaps of shape:
                    (C, H * head_stride, W * head_stride)    if head_stride > 0
                    (C, -H/head_stride, -W/head_stride)      if head_stride < 0
        """
        super().__init__(head_stride, predictor, target_generator, criterion)
        self.mlp_head = (
            nn.Sequential(
                nn.LayerNorm(dim * 3),
                nn.Linear(dim * 3, hidden_heatmap_dim),
                nn.LayerNorm(hidden_heatmap_dim),
                nn.Linear(hidden_heatmap_dim, heatmap_dim),
            )
            if (dim * 3 <= hidden_heatmap_dim * 0.5 and apply_multi)
            else nn.Sequential(nn.LayerNorm(dim * 3), nn.Linear(dim * 3, heatmap_dim))
        )
        self.heatmap_size = heatmap_size
        # trunc_normal_(self.keypoint_token, std=.02)
        if apply_init:
            self.apply(self._init_weights)

    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:
        x = self.mlp_head(x)
        x = rearrange(
            x,
            "b c (p1 p2) -> b c p1 p2",
            p1=self.heatmap_size[0],
            p2=self.heatmap_size[1],
        )
        return {"heatmap": x}

    def _init_weights(self, m: nn.Module) -> None:
        """
        Custom weight initialization for linear and layer normalization layers.

        Args:
            m: module to initialize
        """
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)


--- File: deeplabcut/pose_estimation_pytorch/models/heads/dlcrnet.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import torch
import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.models.criterions import (
    BaseCriterion,
    BaseLossAggregator,
)
from deeplabcut.pose_estimation_pytorch.models.heads.base import HEADS
from deeplabcut.pose_estimation_pytorch.models.heads.simple_head import (
    DeconvModule,
    HeatmapHead,
)
from deeplabcut.pose_estimation_pytorch.models.predictors import BasePredictor
from deeplabcut.pose_estimation_pytorch.models.target_generators import BaseGenerator
from deeplabcut.pose_estimation_pytorch.models.weight_init import BaseWeightInitializer


@HEADS.register_module
class DLCRNetHead(HeatmapHead):
    """A head for DLCRNet models using Part-Affinity Fields to predict individuals"""

    def __init__(
        self,
        predictor: BasePredictor,
        target_generator: BaseGenerator,
        criterion: dict[str, BaseCriterion],
        aggregator: BaseLossAggregator,
        heatmap_config: dict,
        locref_config: dict,
        paf_config: dict,
        num_stages: int = 5,
        features_dim: int = 128,
        weight_init: str | dict | BaseWeightInitializer | None = None,
    ) -> None:
        self.num_stages = num_stages
        # FIXME Cleaner __init__ to avoid initializing unused layers
        in_channels = heatmap_config["channels"][0]
        num_keypoints = heatmap_config["channels"][-1]
        num_limbs = paf_config["channels"][-1]  # Already has the 2x multiplier
        in_refined_channels = features_dim + num_keypoints + num_limbs
        if num_stages > 0:
            heatmap_config["channels"][0] = paf_config["channels"][0] = (
                in_refined_channels
            )
            locref_config["channels"][0] = locref_config["channels"][-1]

        super().__init__(
            predictor,
            target_generator,
            criterion,
            aggregator,
            heatmap_config,
            locref_config,
            weight_init,
        )
        if num_stages > 0:
            self.stride *= 2  # extra deconv layer where it's multi-stage

        self.paf_head = DeconvModule(**paf_config)

        self.convt1 = self._make_layer_same_padding(
            in_channels=in_channels, out_channels=num_keypoints
        )
        self.convt2 = self._make_layer_same_padding(
            in_channels=in_channels, out_channels=locref_config["channels"][-1]
        )
        self.convt3 = self._make_layer_same_padding(
            in_channels=in_channels, out_channels=num_limbs
        )
        self.convt4 = self._make_layer_same_padding(
            in_channels=in_channels, out_channels=features_dim
        )
        self.hm_ref_layers = nn.ModuleList()
        self.paf_ref_layers = nn.ModuleList()
        for _ in range(num_stages):
            self.hm_ref_layers.append(
                self._make_refinement_layer(
                    in_channels=in_refined_channels, out_channels=num_keypoints
                )
            )
            self.paf_ref_layers.append(
                self._make_refinement_layer(
                    in_channels=in_refined_channels, out_channels=num_limbs
                )
            )
        self._init_weights()

    def _make_layer_same_padding(
        self, in_channels: int, out_channels: int
    ) -> nn.ConvTranspose2d:
        # FIXME There is no consensual solution to emulate TF behavior in pytorch
        # see https://github.com/pytorch/pytorch/issues/3867
        return nn.ConvTranspose2d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=3,
            stride=2,
            padding=1,
            output_padding=1,
        )

    def _make_refinement_layer(self, in_channels: int, out_channels: int) -> nn.Conv2d:
        """Summary:
        Helper function to create a refinement layer.

        Args:
            in_channels: number of input channels
            out_channels: number of output channels

        Returns:
            refinement_layer: the refinement layer.
        """
        return nn.Conv2d(
            in_channels, out_channels, kernel_size=3, stride=1, padding="same"
        )

    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:
        if self.num_stages > 0:
            stage1_hm_out = self.convt1(x)
            stage1_paf_out = self.convt3(x)
            features = self.convt4(x)
            stage2_in = torch.cat((stage1_hm_out, stage1_paf_out, features), dim=1)
            stage_in = stage2_in
            stage_paf_out = stage1_paf_out
            stage_hm_out = stage1_hm_out
            for i, (hm_ref_layer, paf_ref_layer) in enumerate(
                zip(self.hm_ref_layers, self.paf_ref_layers)
            ):
                pre_stage_hm_out = stage_hm_out
                stage_hm_out = hm_ref_layer(stage_in)
                stage_paf_out = paf_ref_layer(stage_in)
                if i > 0:
                    stage_hm_out += pre_stage_hm_out
                stage_in = torch.cat((stage_hm_out, stage_paf_out, features), dim=1)
            return {
                "heatmap": self.heatmap_head(stage_in),
                "locref": self.locref_head(self.convt2(x)),
                "paf": self.paf_head(stage_in),
            }
        return {
            "heatmap": self.heatmap_head(x),
            "locref": self.locref_head(x),
            "paf": self.paf_head(x),
        }


--- File: deeplabcut/pose_estimation_pytorch/models/heads/dekr.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import torch
import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.models.criterions import (
    BaseCriterion,
    BaseLossAggregator,
)
from deeplabcut.pose_estimation_pytorch.models.heads.base import BaseHead, HEADS
from deeplabcut.pose_estimation_pytorch.models.modules.conv_block import (
    AdaptBlock,
    BaseBlock,
    BasicBlock,
)
from deeplabcut.pose_estimation_pytorch.models.predictors import BasePredictor
from deeplabcut.pose_estimation_pytorch.models.target_generators import BaseGenerator
from deeplabcut.pose_estimation_pytorch.models.weight_init import BaseWeightInitializer


@HEADS.register_module
class DEKRHead(BaseHead):
    """
    DEKR head based on:
        Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression
        Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, Jingdong Wang, CVPR 2021
    Code based on:
        https://github.com/HRNet/DEKR
    """

    def __init__(
        self,
        predictor: BasePredictor,
        target_generator: BaseGenerator,
        criterion: dict[str, BaseCriterion],
        aggregator: BaseLossAggregator,
        heatmap_config: dict,
        offset_config: dict,
        weight_init: str | dict | BaseWeightInitializer | None = "dekr",
        stride: int | float = 1,  # head stride - should always be 1 for DEKR
    ) -> None:
        super().__init__(
            stride, predictor, target_generator, criterion, aggregator, weight_init
        )
        self.heatmap_head = DEKRHeatmap(**heatmap_config)
        self.offset_head = DEKROffset(**offset_config)
        self._init_weights()

    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:
        return {"heatmap": self.heatmap_head(x), "offset": self.offset_head(x)}


class DEKRHeatmap(nn.Module):
    """
    DEKR head to compute the heatmaps corresponding to keypoints based on:
        Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression
        Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, Jingdong Wang, CVPR 2021
    Code based on:
        https://github.com/HRNet/DEKR
    """

    def __init__(
        self,
        channels: tuple[int],
        num_blocks: int,
        dilation_rate: int,
        final_conv_kernel: int,
        block: type(BaseBlock) = BasicBlock,
    ) -> None:
        """Summary:
        Constructor of the HeatmapDEKRHead.
        Loads the data.

        Args:
            channels: tuple containing the number of channels for the head.
            num_blocks: number of blocks in the head
            dilation_rate: dilation rate for the head
            final_conv_kernel: kernel size for the final convolution
            block: type of block to use in the head. Defaults to BasicBlock.

        Returns:
            None

        Examples:
            channels = (64,128,17)
            num_blocks = 3
            dilation_rate = 2
            final_conv_kernel = 3
            block = BasicBlock
        """
        super().__init__()
        self.bn_momentum = 0.1
        self.inp_channels = channels[0]
        self.num_joints_with_center = channels[
            2
        ]  # Should account for the center being a joint
        self.final_conv_kernel = final_conv_kernel

        self.transition_heatmap = self._make_transition_for_head(
            self.inp_channels, channels[1]
        )
        self.head_heatmap = self._make_heatmap_head(
            block, num_blocks, channels[1], dilation_rate
        )

    def _make_transition_for_head(
        self, in_channels: int, out_channels: int
    ) -> nn.Sequential:
        """Summary:
        Construct the transition layer for the head.

        Args:
            in_channels: number of input channels
            out_channels: number of output channels

        Returns:
            Transition layer consisting of Conv2d, BatchNorm2d, and ReLU
        """
        transition_layer = [
            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(True),
        ]
        return nn.Sequential(*transition_layer)

    def _make_heatmap_head(
        self,
        block: type(BaseBlock),
        num_blocks: int,
        num_channels: int,
        dilation_rate: int,
    ) -> nn.ModuleList:
        """Summary:
        Construct the heatmap head

        Args:
            block: type of block to use in the head.
            num_blocks: number of blocks in the head.
            num_channels: number of input channels for the head.
            dilation_rate: dilation rate for the head.

        Returns:
            List of modules representing the heatmap head layers.
        """
        heatmap_head_layers = []

        feature_conv = self._make_layer(
            block, num_channels, num_channels, num_blocks, dilation=dilation_rate
        )
        heatmap_head_layers.append(feature_conv)

        heatmap_conv = nn.Conv2d(
            in_channels=num_channels,
            out_channels=self.num_joints_with_center,
            kernel_size=self.final_conv_kernel,
            stride=1,
            padding=1 if self.final_conv_kernel == 3 else 0,
        )
        heatmap_head_layers.append(heatmap_conv)

        return nn.ModuleList(heatmap_head_layers)

    def _make_layer(
        self,
        block: type(BaseBlock),
        in_channels: int,
        out_channels: int,
        num_blocks: int,
        stride: int = 1,
        dilation: int = 1,
    ) -> nn.Sequential:
        """Summary:
        Construct a layer in the head.

        Args:
            block: type of block to use in the head.
            in_channels: number of input channels for the layer.
            out_channels: number of output channels for the layer.
            num_blocks: number of blocks in the layer.
            stride: stride for the convolutional layer. Defaults to 1.
            dilation: dilation rate for the convolutional layer. Defaults to 1.

        Returns:
            Sequential layer containing the specified num_blocks.
        """
        downsample = None
        if stride != 1 or in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    in_channels,
                    out_channels * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                nn.BatchNorm2d(
                    out_channels * block.expansion, momentum=self.bn_momentum
                ),
            )

        layers = [
            block(in_channels, out_channels, stride, downsample, dilation=dilation)
        ]
        in_channels = out_channels * block.expansion
        for _ in range(1, num_blocks):
            layers.append(block(in_channels, out_channels, dilation=dilation))

        return nn.Sequential(*layers)

    def forward(self, x):
        heatmap = self.head_heatmap[1](self.head_heatmap[0](self.transition_heatmap(x)))

        return heatmap


class DEKROffset(nn.Module):
    """
    DEKR module to compute the offset from the center corresponding to each keypoints:
        Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression
        Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, Jingdong Wang, CVPR 2021
    Code based on:
    https://github.com/HRNet/DEKR
    """

    def __init__(
        self,
        channels: tuple[int, ...],
        num_offset_per_kpt: int,
        num_blocks: int,
        dilation_rate: int,
        final_conv_kernel: int,
        block: type(BaseBlock) = AdaptBlock,
    ) -> None:
        """Args:
        channels: tuple containing the number of input, offset, and output channels.
        num_offset_per_kpt: number of offset values per keypoint.
        num_blocks: number of blocks in the head.
        dilation_rate: dilation rate for convolutional layers.
        final_conv_kernel: kernel size for the final convolution.
        block: type of block to use in the head. Defaults to AdaptBlock.
        """
        super().__init__()
        self.inp_channels = channels[0]
        self.num_joints = channels[2]
        self.num_joints_with_center = self.num_joints + 1

        self.bn_momentum = 0.1
        self.offset_perkpt = num_offset_per_kpt
        self.num_joints_without_center = self.num_joints
        self.offset_channels = self.offset_perkpt * self.num_joints_without_center
        assert self.offset_channels == channels[1]

        self.num_blocks = num_blocks
        self.dilation_rate = dilation_rate
        self.final_conv_kernel = final_conv_kernel

        self.transition_offset = self._make_transition_for_head(
            self.inp_channels, self.offset_channels
        )
        (
            self.offset_feature_layers,
            self.offset_final_layer,
        ) = self._make_separete_regression_head(
            block,
            num_blocks=num_blocks,
            num_channels_per_kpt=self.offset_perkpt,
            dilation_rate=self.dilation_rate,
        )

    def _make_layer(
        self,
        block: type(BaseBlock),
        in_channels: int,
        out_channels: int,
        num_blocks: int,
        stride: int = 1,
        dilation: int = 1,
    ) -> nn.Sequential:
        """Summary:
        Create a sequential layer with the specified block and number of num_blocks.

        Args:
            block: block type to use in the layer.
            in_channels: number of input channels.
            out_channels: number of output channels.
            num_blocks: number of blocks to be stacked in the layer.
            stride: stride for the first block. Defaults to 1.
            dilation: dilation rate for the blocks. Defaults to 1.

        Returns:
            A sequential layer containing stacked num_blocks.

        Examples:
            input:
                block=BasicBlock
                in_channels=64
                out_channels=128
                num_blocks=3
                stride=1
                dilation=1
        """
        downsample = None
        if stride != 1 or in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(
                    in_channels,
                    out_channels * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                nn.BatchNorm2d(
                    out_channels * block.expansion, momentum=self.bn_momentum
                ),
            )

        layers = []
        layers.append(
            block(in_channels, out_channels, stride, downsample, dilation=dilation)
        )
        in_channels = out_channels * block.expansion
        for _ in range(1, num_blocks):
            layers.append(block(in_channels, out_channels, dilation=dilation))

        return nn.Sequential(*layers)

    def _make_transition_for_head(
        self, in_channels: int, out_channels: int
    ) -> nn.Sequential:
        """Summary:
        Create a transition layer for the head.

        Args:
            in_channels: number of input channels
            out_channels: number of output channels

        Returns:
            Sequential layer containing the transition operations.
        """
        transition_layer = [
            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(True),
        ]
        return nn.Sequential(*transition_layer)

    def _make_separete_regression_head(
        self,
        block: type(BaseBlock),
        num_blocks: int,
        num_channels_per_kpt: int,
        dilation_rate: int,
    ) -> tuple:
        """Summary:

        Args:
            block: type of block to use in the head
            num_blocks: number of blocks in the regression head
            num_channels_per_kpt: number of channels per keypoint
            dilation_rate: dilation rate for the regression head

        Returns:
            A tuple containing two ModuleList objects.
            The first ModuleList contains the feature convolution layers for each keypoint,
            and the second ModuleList contains the final offset convolution layers.
        """
        offset_feature_layers = []
        offset_final_layer = []

        for _ in range(self.num_joints):
            feature_conv = self._make_layer(
                block,
                num_channels_per_kpt,
                num_channels_per_kpt,
                num_blocks,
                dilation=dilation_rate,
            )
            offset_feature_layers.append(feature_conv)

            offset_conv = nn.Conv2d(
                in_channels=num_channels_per_kpt,
                out_channels=2,
                kernel_size=self.final_conv_kernel,
                stride=1,
                padding=1 if self.final_conv_kernel == 3 else 0,
            )
            offset_final_layer.append(offset_conv)

        return nn.ModuleList(offset_feature_layers), nn.ModuleList(offset_final_layer)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Summary:
        Perform forward pass through the OffsetDEKRHead.

        Args:
            x: input tensor to the head.

        Returns:
            offset: Computed offsets from the center corresponding to each keypoint.
            The tensor will have the shape (N, num_joints * 2, H, W), where N is the batch size,
            num_joints is the number of keypoints, and H and W are the height and width of the output tensor.
        """
        final_offset = []
        offset_feature = self.transition_offset(x)

        for j in range(self.num_joints):
            final_offset.append(
                self.offset_final_layer[j](
                    self.offset_feature_layers[j](
                        offset_feature[
                            :, j * self.offset_perkpt : (j + 1) * self.offset_perkpt
                        ]
                    )
                )
            )

        offset = torch.cat(final_offset, dim=1)

        return offset


--- File: deeplabcut/pose_estimation_pytorch/models/heads/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from abc import ABC, abstractmethod

import torch
import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.models.criterions import (
    BaseCriterion,
    BaseLossAggregator,
)
from deeplabcut.pose_estimation_pytorch.models.predictors import BasePredictor
from deeplabcut.pose_estimation_pytorch.models.target_generators import BaseGenerator
from deeplabcut.pose_estimation_pytorch.models.weight_init import (
    BaseWeightInitializer,
    WEIGHT_INIT,
)
from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry

HEADS = Registry("heads", build_func=build_from_cfg)


class BaseHead(ABC, nn.Module):
    """A head for pose estimation models

    Attributes:
        stride: The stride for the head (or neck + head pair), where positive values
            indicate an increase in resolution while negative values a decrease.
            Assuming that H and W are divisible by `stride`, this is the value such
            that if a backbone outputs an encoding of shape (C, H, W), the head will
            output heatmaps of shape:
                (C, H * stride, W * stride)       if stride > 0
                (C, -H/stride, -W/stride)         if stride < 0
        predictor: an object to generate predictions from the head outputs
        target_generator: a target generator which must output a target for each
            output key of this module (i.e. if forward returns a "heatmap" tensor and
            an "offset" tensor, then targets must be generated for both)
        criterion: either a single criterion (e.g. if this head only outputs heatmaps)
            or a dictionary mapping the outputs of this head to the criterion to use
            (e.g. a "heatmap" criterion and an "offset" criterion for DEKR).
        aggregator: if the criterion is a dictionary, cannot be none. used to combine
            the individual losses from this head into one "total_loss"
    """

    def __init__(
        self,
        stride: int | float,
        predictor: BasePredictor,
        target_generator: BaseGenerator,
        criterion: dict[str, BaseCriterion] | BaseCriterion,
        aggregator: BaseLossAggregator | None = None,
        weight_init: str | dict | BaseWeightInitializer | None = None,
    ) -> None:
        super().__init__()
        if stride == 0:
            raise ValueError(f"Stride must not be 0. Found {stride}.")

        self.stride = stride
        self.predictor = predictor
        self.target_generator = target_generator
        self.criterion = criterion
        self.aggregator = aggregator

        self.weight_init: BaseWeightInitializer | None = None
        if isinstance(weight_init, BaseWeightInitializer):
            self.weight_init = weight_init
        elif isinstance(weight_init, (str, dict)):
            self.weight_init = WEIGHT_INIT.build(weight_init)
        elif weight_init is not None:
            raise ValueError(
                f"Could not parse ``weight_init`` parameter: {weight_init}."
            )

        if isinstance(criterion, dict):
            if aggregator is None:
                raise ValueError(
                    f"When multiple criterions are defined, a loss aggregator must "
                    "also be given"
                )
        else:
            if aggregator is not None:
                raise ValueError(
                    f"Cannot use a loss aggregator with a single criterion"
                )

    @abstractmethod
    def forward(self, x: torch.Tensor) -> dict[str, torch.Tensor]:
        """
        Given the feature maps for an image ()

        Args:
            x: the feature maps, of shape (b, c, h, w)

        Returns:
            the head outputs (e.g. "heatmap", "locref")
        """
        pass

    def get_loss(
        self,
        outputs: dict[str, torch.Tensor],
        targets: dict[str, dict[str, torch.Tensor]],
    ) -> dict[str, torch.Tensor]:
        """
        Computes the loss for this head

        Args:
            outputs: the outputs of this head
            targets: the targets for this head

        Returns:
            A dictionary containing minimally "total_loss" key mapping to the total
            loss for this head (from which backwards() should be called). Can contain
            other keys containing losses that can be logged for informational purposes.
        """
        if self.aggregator is None:
            assert len(outputs) == len(targets) == 1
            key = [k for k in outputs.keys()][0]
            return {"total_loss": self.criterion(outputs[key], **targets[key])}

        losses = {
            name: criterion(outputs[name], **targets[name])
            for name, criterion in self.criterion.items()
        }
        losses["total_loss"] = self.aggregator(losses)
        return losses

    def _init_weights(self) -> None:
        """Should be called once all modules for the class are created"""
        if self.weight_init is not None:
            self.weight_init.init_weights(self)


class WeightConversionMixin(ABC):
    """A mixin for heads that can re-order and/or filter the output channels.

    This mixin is useful to convert SuperAnimal model weights such that they can be used
    in downstream projects (either existing or new), where only a subset of keypoints
    are available (and where they might be re-ordered).
    """

    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

    @staticmethod
    @abstractmethod
    def convert_weights(
        state_dict: dict[str, torch.Tensor],
        module_prefix: str,
        conversion: torch.Tensor,
    ) -> dict[str, torch.Tensor]:
        """Converts pre-trained weights to be fine-tuned on another dataset

        Args:
            state_dict: the state dict for the pre-trained model
            module_prefix: the prefix for weights in this head (e.g., 'heads.bodypart.')
            conversion: the mapping of old indices to new indices

        Examples:
            A SuperAnimal model was trained on the keypoints ["ear_left", "ear_right",
            "eye_left", "eye_right", "nose"]. A down-stream project has the bodyparts
            labeled ["nose", "eye_left", "eye_right"]. The SuperAnimal weights can be
            converted (to be used with the downstream project) with the following code:

                ``
                state_dict = torch.load(
                    snapshot_path, map_location=torch.device('cpu')
                )["model"]
                state_dict = HeadClass.convert_weights(
                    state_dict,
                    "heads.bodypart",
                    [4, 2, 3]
                )
                ``
        """
        pass


--- File: deeplabcut/pose_estimation_pytorch/models/backbones/hrnet.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import timm
import torch
import torch.nn as nn
import torch.nn.functional as F

from deeplabcut.pose_estimation_pytorch.models.backbones.base import (
    BACKBONES,
    BaseBackbone,
)


@BACKBONES.register_module
class HRNet(BaseBackbone):
    """HRNet backbone.

    This version returns high-resolution feature maps of size 1/4 * original_image_size.
    This is obtained using bilinear interpolation and concatenation of all the outputs
    of the HRNet stages.

    The model outputs 4 branches, with strides 4, 8, 16 and 32.

    Args:
        stride: The stride of the HRNet. Should always be 4, except for custom models.
        model_name: Any HRNet variant available through timm (e.g., 'hrnet_w32',
            'hrnet_w48'). See timm for more options.
        pretrained: If True, loads the backbone with ImageNet pretrained weights from
            timm.
        interpolate_branches: Needed for DEKR. Instead of returning features from the
            high-resolution branch, interpolates all other branches to the same shape
            and concatenates them.
        increased_channel_count: As described by timm, it "allows grabbing increased
            channel count features using part of the classification head" (otherwise,
            the default features are returned).
        kwargs: BaseBackbone kwargs

    Attributes:
        model: the HRNet model
    """

    def __init__(
        self,
        stride: int = 4,
        model_name: str = "hrnet_w32",
        pretrained: bool = False,
        interpolate_branches: bool = False,
        increased_channel_count: bool = False,
        **kwargs,
    ) -> None:
        super().__init__(stride=stride, **kwargs)
        self.model = _load_hrnet(model_name, pretrained, increased_channel_count)
        self.interpolate_branches = interpolate_branches

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the HRNet backbone.

        Args:
            x: Input tensor of shape (batch_size, channels, height, width).

        Returns:
            the feature map

        Example:
            >>> import torch
            >>> from deeplabcut.pose_estimation_pytorch.models.backbones import HRNet
            >>> backbone = HRNet(model_name='hrnet_w32', pretrained=False)
            >>> x = torch.randn(1, 3, 256, 256)
            >>> y = backbone(x)
        """
        y_list = self.model(x)
        if not self.interpolate_branches:
            return y_list[0]

        x0_h, x0_w = y_list[0].size(2), y_list[0].size(3)
        x = torch.cat(
            [
                y_list[0],
                F.interpolate(y_list[1], size=(x0_h, x0_w), mode="bilinear"),
                F.interpolate(y_list[2], size=(x0_h, x0_w), mode="bilinear"),
                F.interpolate(y_list[3], size=(x0_h, x0_w), mode="bilinear"),
            ],
            1,
        )
        return x


def _load_hrnet(
    model_name: str,
    pretrained: bool,
    increased_channel_count: bool,
) -> nn.Module:
    """Loads a TIMM HRNet model.

    Args:
        model_name: Any HRNet variant available through timm (e.g., 'hrnet_w32',
            'hrnet_w48'). See timm for more options.
        pretrained: If True, loads the backbone with ImageNet pretrained weights from
            timm.
        increased_channel_count: As described by timm, it "allows grabbing increased
            channel count features using part of the classification head" (otherwise,
            the default features are returned).

    Returns:
        the HRNet model
    """
    # First stem conv is used for stride 2 features, so only return branches 1-4
    return timm.create_model(
        model_name,
        pretrained=pretrained,
        features_only=True,
        feature_location="incre" if increased_channel_count else "",
        out_indices=(1, 2, 3, 4),
    )


--- File: deeplabcut/pose_estimation_pytorch/models/backbones/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.models.backbones.base import (
    BACKBONES,
    BaseBackbone,
)
from deeplabcut.pose_estimation_pytorch.models.backbones.cspnext import CSPNeXt
from deeplabcut.pose_estimation_pytorch.models.backbones.hrnet import HRNet
from deeplabcut.pose_estimation_pytorch.models.backbones.resnet import ResNet, DLCRNet


--- File: deeplabcut/pose_estimation_pytorch/models/backbones/cspnext.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Implementation of the CSPNeXt Backbone

Based on the ``mmdetection`` CSPNeXt implementation. For more information, see:
<https://github.com/open-mmlab/mmdetection/blob/main/mmdet/models/backbones/cspnext.py>

For more details about this architecture, see `RTMDet: An Empirical Study of Designing
Real-Time Object Detectors`: https://arxiv.org/abs/1711.05101.
"""
from dataclasses import dataclass

import torch
import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.models.backbones.base import (
    BACKBONES,
    BaseBackbone,
    HuggingFaceWeightsMixin,
)
from deeplabcut.pose_estimation_pytorch.models.modules.csp import (
    CSPConvModule,
    CSPLayer,
    SPPBottleneck,
)


@dataclass(frozen=True)
class CSPNeXtLayerConfig:
    """Configuration for a CSPNeXt layer"""
    in_channels: int
    out_channels: int
    num_blocks: int
    add_identity: bool
    use_spp: bool


@BACKBONES.register_module
class CSPNeXt(HuggingFaceWeightsMixin, BaseBackbone):
    """CSPNeXt Backbone

    Args:
        model_name: The model variant to build. If ``pretrained==True``, must be one of
            the variants for which weights are available on HuggingFace (in the
            `DeepLabCut/DeepLabCut-Backbones` hub, e.g. `cspnext_m`).
        pretrained: Whether to load pretrained weights for the model.
        arch: The model architecture to build. Must be one of the keys of the
            ``CSPNeXt.ARCH`` attribute (e.g. `P5`, `P6`, ...).
        expand_ratio: Ratio used to adjust the number of channels of the hidden layer.
        deepen_factor: Number of blocks in each CSP layer is multiplied by this value.
        widen_factor: Number of channels in each layer is multiplied by this value.
        out_indices: The branch indices to output. If a tuple of integers, the outputs
            are returned as a list of tensors. If a single integer, a tensor is returned
            containing the configured index.
        channel_attention: Add channel attention to all stages
        norm_layer: The type of normalization layer to use.
        activation_fn: The type of activation function to use.
        **kwargs: BaseBackbone kwargs.
    """

    ARCH: dict[str, list[CSPNeXtLayerConfig]] = {
        "P5": [
            CSPNeXtLayerConfig(64, 128, 3, True, False),
            CSPNeXtLayerConfig(128, 256, 6, True, False),
            CSPNeXtLayerConfig(256, 512, 6, True, False),
            CSPNeXtLayerConfig(512, 1024, 3, False, True),
        ],
        "P6": [
            CSPNeXtLayerConfig(64, 128, 3, True, False),
            CSPNeXtLayerConfig(128, 256, 6, True, False),
            CSPNeXtLayerConfig(256, 512, 6, True, False),
            CSPNeXtLayerConfig(512, 768, 3, True, False),
            CSPNeXtLayerConfig(768, 1024, 3, False, True),
        ]
    }

    def __init__(
        self,
        model_name: str = "cspnext_m",
        pretrained: bool = False,
        arch: str = "P5",
        expand_ratio: float = 0.5,
        deepen_factor: float = 0.67,
        widen_factor: float = 0.75,
        out_indices: int | tuple[int, ...] = -1,
        channel_attention: bool = True,
        norm_layer: str = "SyncBN",
        activation_fn: str = "SiLU",
        **kwargs,
    ) -> None:
        super().__init__(stride=32, **kwargs)
        if arch not in self.ARCH:
            raise ValueError(
                f"Unknown `CSPNeXT` architecture: {arch}. Must be one of "
                f"{self.ARCH.keys()}"
            )

        self.model_name = model_name
        self.layer_configs = self.ARCH[arch]
        self.stem_out_channels = self.layer_configs[0].in_channels
        self.spp_kernel_sizes = (5, 9, 13)

        # stem has stride 2
        self.stem = nn.Sequential(
            CSPConvModule(
                in_channels=3,
                out_channels=int(self.stem_out_channels * widen_factor // 2),
                kernel_size=3,
                padding=1,
                stride=2,
                norm_layer=norm_layer,
                activation_fn=activation_fn,
            ),
            CSPConvModule(
                in_channels=int(self.stem_out_channels * widen_factor // 2),
                out_channels=int(self.stem_out_channels * widen_factor // 2),
                kernel_size=3,
                padding=1,
                stride=1,
                norm_layer=norm_layer,
                activation_fn=activation_fn,
            ),
            CSPConvModule(
                in_channels=int(self.stem_out_channels * widen_factor // 2),
                out_channels=int(self.stem_out_channels * widen_factor),
                kernel_size=3,
                padding=1,
                stride=1,
                norm_layer=norm_layer,
                activation_fn=activation_fn,
            )
        )
        self.layers = ["stem"]

        for i, layer_cfg in enumerate(self.layer_configs):
            layer_cfg: CSPNeXtLayerConfig
            in_channels = int(layer_cfg.in_channels * widen_factor)
            out_channels = int(layer_cfg.out_channels * widen_factor)
            num_blocks = max(round(layer_cfg.num_blocks * deepen_factor), 1)
            stage = []
            conv_layer = CSPConvModule(
                in_channels,
                out_channels,
                3,
                stride=2,
                padding=1,
                norm_layer=norm_layer,
                activation_fn=activation_fn,
            )
            stage.append(conv_layer)
            if layer_cfg.use_spp:
                spp = SPPBottleneck(
                    out_channels,
                    out_channels,
                    kernel_sizes=self.spp_kernel_sizes,
                    norm_layer=norm_layer,
                    activation_fn=activation_fn,
                )
                stage.append(spp)

            csp_layer = CSPLayer(
                out_channels,
                out_channels,
                num_blocks=num_blocks,
                add_identity=layer_cfg.add_identity,
                expand_ratio=expand_ratio,
                channel_attention=channel_attention,
                norm_layer=norm_layer,
                activation_fn=activation_fn,
            )
            stage.append(csp_layer)
            self.add_module(f'stage{i + 1}', nn.Sequential(*stage))
            self.layers.append(f'stage{i + 1}')

        self.single_output = isinstance(out_indices, int)
        if self.single_output:
            if out_indices == -1:
                out_indices = len(self.layers) - 1
            out_indices = (out_indices,)
        self.out_indices = out_indices

        if pretrained:
            weights_filename = f"{model_name}.pt"
            weights_path = self.download_weights(weights_filename, force=False)
            snapshot = torch.load(weights_path, map_location="cpu", weights_only=True)
            self.load_state_dict(snapshot["state_dict"])

    def forward(self, x: torch.Tensor) -> torch.Tensor | tuple[torch.Tensor]:
        outs = []
        for i, layer_name in enumerate(self.layers):
            layer = getattr(self, layer_name)
            x = layer(x)
            if i in self.out_indices:
                outs.append(x)

        if self.single_output:
            return outs[-1]

        return tuple(outs)


--- File: deeplabcut/pose_estimation_pytorch/models/backbones/resnet.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import timm
import torch
import torch.nn as nn
from torchvision.transforms.functional import resize

from deeplabcut.pose_estimation_pytorch.models.backbones.base import (
    BACKBONES,
    BaseBackbone,
)


@BACKBONES.register_module
class ResNet(BaseBackbone):
    """ResNet backbone.

    This class represents a typical ResNet backbone for pose estimation.

    Attributes:
        model: the ResNet model
    """

    def __init__(
        self,
        model_name: str = "resnet50",
        output_stride: int = 32,
        pretrained: bool = False,
        drop_path_rate: float = 0.0,
        drop_block_rate: float = 0.0,
        **kwargs,
    ) -> None:
        """Initialize the ResNet backbone.

        Args:
            model_name: Name of the ResNet model to use, e.g., 'resnet50', 'resnet101'
            output_stride: Output stride of the network, 32, 16, or 8.
            pretrained: If True, initializes with ImageNet pretrained weights.
            drop_path_rate: Stochastic depth drop-path rate
            drop_block_rate: Drop block rate
            kwargs: BaseBackbone kwargs
        """
        super().__init__(stride=output_stride, **kwargs)
        self.model = timm.create_model(
            model_name,
            output_stride=output_stride,
            pretrained=pretrained,
            drop_path_rate=drop_path_rate,
            drop_block_rate=drop_block_rate,
        )
        self.model.fc = nn.Identity()  # remove the FC layer

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the ResNet backbone.

        Args:
            x: Input tensor.

        Returns:
            torch.Tensor: Output tensor.
        Example:
            >>> import torch
            >>> from deeplabcut.pose_estimation_pytorch.models.backbones import ResNet
            >>> backbone = ResNet(model_name='resnet50', pretrained=False)
            >>> x = torch.randn(1, 3, 256, 256)
            >>> y = backbone(x)

            Expected Output Shape:
                If input size is (batch_size, 3, shape_x, shape_y), the output shape
                will be (batch_size, 3, shape_x//16, shape_y//16)
        """
        return self.model.forward_features(x)


@BACKBONES.register_module
class DLCRNet(ResNet):
    def __init__(
        self,
        model_name: str = "resnet50",
        output_stride: int = 32,
        pretrained: bool = True,
        **kwargs,
    ) -> None:
        super().__init__(model_name, output_stride, pretrained, **kwargs)
        self.interm_features = {}
        self.model.layer1[2].register_forward_hook(self._get_features("bank1"))
        self.model.layer2[2].register_forward_hook(self._get_features("bank2"))
        self.conv_block1 = self._make_conv_block(
            in_channels=512, out_channels=512, kernel_size=3, stride=2
        )
        self.conv_block2 = self._make_conv_block(
            in_channels=512, out_channels=128, kernel_size=1, stride=1
        )
        self.conv_block3 = self._make_conv_block(
            in_channels=256, out_channels=256, kernel_size=3, stride=2
        )
        self.conv_block4 = self._make_conv_block(
            in_channels=256, out_channels=256, kernel_size=3, stride=2
        )
        self.conv_block5 = self._make_conv_block(
            in_channels=256, out_channels=128, kernel_size=1, stride=1
        )

    def _make_conv_block(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int,
        stride: int,
        momentum: float = 0.001,  # (1 - decay)
    ) -> torch.nn.Sequential:
        return nn.Sequential(
            nn.Conv2d(
                in_channels, out_channels, kernel_size=kernel_size, stride=stride
            ),
            nn.BatchNorm2d(out_channels, momentum=momentum),
            nn.ReLU(),
        )

    def _get_features(self, name):
        def hook(model, input, output):
            self.interm_features[name] = output.detach()

        return hook

    def forward(self, x):
        out = super().forward(x)

        # Fuse intermediate features
        bank_2_s8 = self.interm_features["bank2"]
        bank_1_s4 = self.interm_features["bank1"]
        bank_2_s16 = self.conv_block1(bank_2_s8)
        bank_2_s16 = self.conv_block2(bank_2_s16)
        bank_1_s8 = self.conv_block3(bank_1_s4)
        bank_1_s16 = self.conv_block4(bank_1_s8)
        bank_1_s16 = self.conv_block5(bank_1_s16)
        # Resizing here is required to guarantee all shapes match, as
        # Conv2D(..., padding='same') is invalid for strided convolutions.
        h, w = out.shape[-2:]
        bank_1_s16 = resize(bank_1_s16, [h, w], antialias=True)
        bank_2_s16 = resize(bank_2_s16, [h, w], antialias=True)

        return torch.cat((bank_1_s16, bank_2_s16, out), dim=1)


--- File: deeplabcut/pose_estimation_pytorch/models/backbones/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import logging
import shutil
from abc import ABC, abstractmethod
from pathlib import Path

import torch
import torch.nn as nn
from huggingface_hub import hf_hub_download

from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry

BACKBONES = Registry("backbones", build_func=build_from_cfg)


class BaseBackbone(ABC, nn.Module):
    """Base Backbone class for pose estimation.

    Attributes:
        stride: the stride for the backbone
        freeze_bn_weights: freeze weights of batch norm layers during training
        freeze_bn_stats: freeze stats of batch norm layers during training
    """

    def __init__(
        self,
        stride: int | float,
        freeze_bn_weights: bool = True,
        freeze_bn_stats: bool = True,
    ):
        super().__init__()
        self.stride = stride
        self.freeze_bn_weights = freeze_bn_weights
        self.freeze_bn_stats = freeze_bn_stats

    @abstractmethod
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Abstract method for the forward pass through the backbone.

        Args:
            x: Input tensor of shape (batch_size, channels, height, width).

        Returns:
            a feature map for the input, of shape (batch_size, c', h', w')
        """
        pass

    def freeze_batch_norm_layers(self) -> None:
        """Freezes batch norm layers

        Running mean + var are always given to F.batch_norm, except when the layer is
        in `train` mode and track_running_stats is False, see
            https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html
        So to 'freeze' the running stats, the only way is to set the layer to "eval"
        mode.
        """
        for module in self.modules():
            if isinstance(module, nn.BatchNorm2d):
                if self.freeze_bn_weights:
                    module.weight.requires_grad = False
                    module.bias.requires_grad = False
                if self.freeze_bn_stats:
                    module.eval()

    def train(self, mode: bool = True) -> None:
        """Sets the module in training or evaluation mode.

        Args:
            mode: whether to set training mode (True) or evaluation mode (False)
        """
        super().train(mode)
        if self.freeze_bn_weights or self.freeze_bn_stats:
            self.freeze_batch_norm_layers()


class HuggingFaceWeightsMixin:
    """Mixin for backbones where the pretrained weights are stored on HuggingFace"""

    def __init__(
        self,
        backbone_weight_folder: str | Path | None = None,
        repo_id: str = "DeepLabCut/DeepLabCut-Backbones",
        *args,
        **kwargs,
    ) -> None:
        super().__init__(*args, **kwargs)
        if backbone_weight_folder is None:
            backbone_weight_folder = Path(__file__).parent / "pretrained_weights"
        else:
            backbone_weight_folder = Path(backbone_weight_folder).resolve()

        self.backbone_weight_folder = backbone_weight_folder
        self.repo_id = repo_id

    def download_weights(self, filename: str, force: bool = False) -> Path:
        """Downloads the backbone weights from the HuggingFace repo

        Args:
            filename: The name of the model file to download in the repo.
            force: Whether to re-download the file if it already exists locally.

        Returns:
            The path to the model snapshot.
        """
        model_path = self.backbone_weight_folder / filename
        if model_path.exists():
            if not force:
                return model_path
            model_path.unlink()

        logging.info(f"Downloading the pre-trained backbone to {model_path}")
        self.backbone_weight_folder.mkdir(exist_ok=True, parents=False)
        output_path = Path(
            hf_hub_download(
                self.repo_id, filename, cache_dir=self.backbone_weight_folder
            )
        )

        # resolve gets the actual path if the output path is a symlink
        output_path = output_path.resolve()
        # move to the target path
        output_path.rename(model_path)

        # delete downloaded artifacts
        uid, rid = self.repo_id.split("/")
        artifact_dir = self.backbone_weight_folder / f"models--{uid}--{rid}"
        if artifact_dir.exists():
            shutil.rmtree(artifact_dir)

        return model_path


--- File: deeplabcut/pose_estimation_pytorch/models/target_generators/heatmap_targets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from abc import abstractmethod
from enum import Enum

import numpy as np
import torch

from deeplabcut.pose_estimation_pytorch.models.target_generators.base import (
    BaseGenerator,
    TARGET_GENERATORS,
)


class HeatmapGenerator(BaseGenerator):
    """Abstract class to generate target heatmap targets (with/without locref)

    Can generate target heatmaps either for pose estimation (one keypoint), or for
    individual identification.

    This class is abstract, and heatmap targets should be generated through its
    subclasses (such as HeatmapPlateauGenerator)
    """

    class Mode(Enum):
        """
        KEYPOINT generates one heatmap per type of keypoint (for pose estimation heads)
        INDIVIDUAL generates one heatmap per individual (for identification heads)
        """

        INDIVIDUAL = "INDIVIDUAL"
        KEYPOINT = "KEYPOINT"

        @classmethod
        def _missing_(cls, value):
            if isinstance(value, str):
                value = value.upper()
                for member in cls:
                    if member.value == value:
                        return member
            return None

    def __init__(
        self,
        num_heatmaps: int,
        pos_dist_thresh: int,
        heatmap_mode: str | Mode = Mode.KEYPOINT,
        gradient_masking: bool = False,
        background_weight: float = 0.1,
        generate_locref: bool = True,
        locref_std: float = 7.2801,
        **kwargs,
    ):
        """
        Args:
            num_heatmaps: the number of heatmaps to generate
            pos_dist_thresh: 3*std of the gaussian. We think of dist_thresh as a radius
                and std is a 'diameter'.
            mode: the mode to generate heatmaps for
            gradient_masking: Whether to mask the gradient when a bodypart is undefined
                (has visibility ``0`` in the dataset). WARNING: Do not set this option
                for bottom-up models, as a keypoint missing for one animal means the
                gradients for all animals will be set to 0 for that image.
                Gradients for inputs that have the visibility flag ``-1`` will always be
                masked, as this flag indicates that the keypoint is not defined for the
                image.
            background_weight: If ``gradient_masking == True`, the weight to apply to
                the loss for background pixels.
            learned_id_target: whether to generate the heatmap for keypoints
                or for learned IDs
            generate_locref: whether to generate location refinement maps
            locref_std: the STD for the location refinement maps, if defined

        Examples:
            input:
                locref_std = 7.2801, default value in pytorch config
                num_joints = 6
                po_dist_thresh = 17, default value in pytorch config
        """
        super().__init__(**kwargs)
        self.num_heatmaps = num_heatmaps
        self.dist_thresh = float(pos_dist_thresh)
        self.dist_thresh_sq = self.dist_thresh**2
        self.std = 2 * self.dist_thresh / 3

        if isinstance(heatmap_mode, str):
            heatmap_mode = HeatmapGenerator.Mode(heatmap_mode)
        self.heatmap_mode = heatmap_mode

        self.gradient_masking = gradient_masking
        self.background_weight = background_weight

        self.generate_locref = generate_locref
        self.locref_scale = 1.0 / locref_std

    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor], labels: dict
    ) -> dict[str, dict[str, torch.Tensor]]:
        """
        Given the annotations and predictions of your keypoints, this function returns the targets,
        a dictionary containing the heatmaps, locref_maps and locref_masks.

        Args:
            stride: the stride of the model
            outputs: output of each model head
            labels: the labels for the inputs (each tensor should have shape (b, ...))

        Returns:
            The targets for the heatmap and locref heads:
                {
                    "heatmap": {
                        "target": heatmaps,
                        "weights":  heatmap_weights,
                    },
                    "locref": {  # optional
                        "target": locref_map,
                        "weights": locref_weights,
                    }
                }

        Examples:
            input:
                annotations = {
                    "keypoints": torch.randint(
                        1, min(image_size), (batch_size, num_animals, num_joints, 2)
                    )
                }
                image_size = (256, 256)
                model_stride = 4
            output:
                targets = {
                    "heatmap": {
                        "target": array of shape (batch_size, 64, 64, num_joints),
                        "weights": array of shape (batch_size, 64, 64, num_joints),
                    },
                    "locref": {
                        "target": array of shape (batch_size, 64, 64, num_joints),
                        "weights": array of shape (batch_size, 64, 64, num_joints),
                    }
                }
        """
        stride_y, stride_x = stride, stride
        batch_size, _, height, width = outputs["heatmap"].shape
        coords = labels[self.label_keypoint_key].cpu().numpy()
        if len(coords.shape) == 3:  # for single animal: add individual dimension
            coords = coords.reshape((batch_size, 1, *coords.shape[1:]))

        if self.heatmap_mode == HeatmapGenerator.Mode.KEYPOINT:
            # transpose the individuals and keypoints to iterate over bodyparts
            coords = coords.transpose((0, 2, 1, 3))
        if self.heatmap_mode == HeatmapGenerator.Mode.INDIVIDUAL:
            # re-order the individuals to always have the same order
            # TODO: Optimize
            sorted_coords = -np.ones_like(coords)
            for i, batch_individuals in enumerate(labels["individual_ids"]):
                for j, individual_id in enumerate(batch_individuals):
                    if individual_id >= 0:
                        sorted_coords[i, individual_id] = coords[i, j]
            coords = sorted_coords

        map_size = batch_size, height, width
        heatmap = np.zeros((*map_size, self.num_heatmaps), dtype=np.float32)
        weights = np.ones(
            (batch_size, self.num_heatmaps, height, width),
            dtype=np.float32,
        )

        locref_map, locref_mask = None, None
        if self.generate_locref:
            locref_map = np.zeros((*map_size, self.num_heatmaps * 2), dtype=np.float32)
            locref_mask = np.zeros_like(locref_map, dtype=int)

        grid = np.mgrid[:height, :width].transpose((1, 2, 0))
        grid[:, :, 0] = grid[:, :, 0] * stride_y + stride_y / 2
        grid[:, :, 1] = grid[:, :, 1] * stride_x + stride_x / 2

        # heatmap (batch_size, height, width, num_kpts)
        # coords (batch_size, num_kpts, num_individuals, 3)
        for b in range(batch_size):
            for heatmap_idx, group_keypoints in enumerate(coords[b]):
                for keypoint in group_keypoints:
                    if self.gradient_masking and keypoint[-1] == 0:
                        # apply background weight if keypoints are missing
                        weights[b, heatmap_idx] = self.background_weight
                    elif keypoint[-1] == -1:
                        # always mask weights when the keypoint is undefined
                        weights[b, heatmap_idx] = 0.0
                    elif keypoint[-1] > 0:
                        # keypoint visible
                        self.update(
                            heatmap=heatmap[b, :, :, heatmap_idx],
                            grid=grid,
                            keypoint=keypoint[..., :2],
                            locref_map=self.get_locref(locref_map, b, heatmap_idx),
                            locref_mask=self.get_locref(locref_mask, b, heatmap_idx),
                        )

        hm_device = outputs["heatmap"].device
        heatmap = heatmap.transpose((0, 3, 1, 2))
        target = {
            "heatmap": {
                "target": torch.tensor(heatmap, device=hm_device),
                "weights": torch.tensor(weights, device=hm_device),
            }
        }

        if self.generate_locref:
            locref_map = locref_map.transpose((0, 3, 1, 2))
            locref_mask = locref_mask.transpose((0, 3, 1, 2))
            target["locref"] = {
                "target": torch.tensor(locref_map, device=outputs["locref"].device),
                "weights": torch.tensor(locref_mask, device=outputs["locref"].device),
            }

        return target

    def get_locref(
        self,
        locref_map_or_mask: np.ndarray | None,
        batch_idx: int,
        heatmap_idx: int,
    ) -> np.ndarray | None:
        """
        Args:
            locref_map_or_mask: the locref array to return (either the map or mask), of
                shape (batch_size, height, width, num_heatmaps)
            batch_idx: the index of the batch
            heatmap_idx: the index of the heatmap for which we want the location
                refinement maps or masks

        Returns:
            the location refinement maps/masks of shape (height, width, 2)
        """
        if not self.generate_locref:
            return None

        start_idx = 2 * heatmap_idx
        end_idx = start_idx + 2
        return locref_map_or_mask[batch_idx, :, :, start_idx:end_idx]

    @abstractmethod
    def update(
        self,
        heatmap: np.ndarray,
        grid: np.mgrid,
        keypoint: np.ndarray,
        locref_map: np.ndarray | None,
        locref_mask: np.ndarray | None,
    ) -> None:
        """
        Updates the heatmap and locref targets in-place following an update rule (e.g.,
        Gaussian or Plateau).

        Args:
            heatmap: the heatmap to update of shape (height, width)
            grid: the grid for ???
            keypoint: the keypoint with which to update the maps
            locref_map: the location refinement maps of shape (height, width, 2), if
                self.generate_locref = True
            locref_mask: the location refinement masks of shape (height, width, 2), if
                self.generate_locref = True
        """
        raise NotImplementedError


@TARGET_GENERATORS.register_module
class HeatmapGaussianGenerator(HeatmapGenerator):
    """Generates gaussian heatmaps (and locref) targets from keypoints"""

    def update(
        self,
        heatmap: np.ndarray,
        grid: np.mgrid,
        keypoint: np.ndarray,
        locref_map: np.ndarray | None,
        locref_mask: np.ndarray | None,
    ) -> None:
        """Updates the heatmap (and locref if defined) with gaussian values"""
        # revert keypoints to follow image convention: from x,y to y,x
        keypoint = keypoint.copy()[::-1]

        dist = np.linalg.norm(grid - keypoint, axis=2) ** 2
        heatmap_j = np.exp(-dist / (2 * self.std**2))
        heatmap[:, :] = np.maximum(heatmap, heatmap_j)

        if locref_map is not None:
            dx = keypoint[1] - grid.copy()[:, :, 1]
            dy = keypoint[0] - grid.copy()[:, :, 0]
            locref_map[:, :, 0] = dx * self.locref_scale
            locref_map[:, :, 1] = dy * self.locref_scale

        if locref_mask is not None:
            locref_mask[dist <= self.dist_thresh_sq] = 1


@TARGET_GENERATORS.register_module
class HeatmapPlateauGenerator(HeatmapGenerator):
    """Generates plateau heatmaps (and locref) targets from keypoints"""

    def update(
        self,
        heatmap: np.ndarray,
        grid: np.mgrid,
        keypoint: np.ndarray,
        locref_map: np.ndarray | None,
        locref_mask: np.ndarray | None,
    ) -> None:
        """Updates the heatmap (and locref if defined) with plateau values"""
        # revert keypoints to follow image convention: from x,y to y,x
        keypoint = keypoint.copy()[::-1]
        dist = np.sum((grid - keypoint) ** 2, axis=2)
        mask = dist <= self.dist_thresh_sq
        heatmap[mask] = 1

        if locref_map is not None:
            dx = keypoint[1] - grid.copy()[:, :, 1]
            dy = keypoint[0] - grid.copy()[:, :, 0]
            locref_map[mask, 0] = (dx * self.locref_scale)[mask]
            locref_map[mask, 1] = (dy * self.locref_scale)[mask]

        if locref_mask is not None:
            locref_mask[mask] = 1


--- File: deeplabcut/pose_estimation_pytorch/models/target_generators/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.models.target_generators.base import (
    TARGET_GENERATORS,
    BaseGenerator,
    SequentialGenerator,
)
from deeplabcut.pose_estimation_pytorch.models.target_generators.dekr_targets import (
    DEKRGenerator,
)
from deeplabcut.pose_estimation_pytorch.models.target_generators.heatmap_targets import (
    HeatmapGaussianGenerator,
    HeatmapPlateauGenerator,
)
from deeplabcut.pose_estimation_pytorch.models.target_generators.pafs_targets import (
    PartAffinityFieldGenerator,
)
from deeplabcut.pose_estimation_pytorch.models.target_generators.sim_cc import (
    SimCCGenerator,
)


--- File: deeplabcut/pose_estimation_pytorch/models/target_generators/pafs_targets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from math import sqrt

import numpy as np
import torch

from deeplabcut.pose_estimation_pytorch.models.target_generators.base import (
    BaseGenerator,
    TARGET_GENERATORS,
)


@TARGET_GENERATORS.register_module
class PartAffinityFieldGenerator(BaseGenerator):
    """
    Generate part affinity field targets from ground truth keypoints in order
    to train baseline multi-animal deeplabcut model (ResNet + Deconv)
    """

    def __init__(self, graph: list[list[int, int]], width: float):
        """
        Args:
            graph: list of pairs of keypoint indices forming
                the graph edges
            width: width of the vector field in pixels

        Examples:
            input:
                graph = [(0, 1), (0, 2), (1, 2)]
                width = 20.0, default value in pytorch config
        """
        super().__init__()
        self.graph = graph
        self.width = width
        self.num_limbs = len(graph)

    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor], labels: dict
    ) -> dict[str, dict[str, torch.Tensor]]:
        stride_y, stride_x = stride, stride
        batch_size, _, height, width = outputs["heatmap"].shape
        coords = labels[self.label_keypoint_key].cpu().numpy()

        paf_map = np.zeros(
            (batch_size, height, width, self.num_limbs * 2), dtype=np.float32
        )
        grid = np.mgrid[:height, :width].transpose((1, 2, 0))
        grid[:, :, 0] = grid[:, :, 0] * stride_y + stride_y / 2
        grid[:, :, 1] = grid[:, :, 1] * stride_x + stride_x / 2
        y, x = np.rollaxis(grid, 2)

        for b in range(batch_size):
            for _, kpts_animal in enumerate(coords[b]):
                visible = set(np.flatnonzero(kpts_animal[..., -1] > 0))
                kpts_animal = kpts_animal[..., :2]
                for l, (bp1, bp2) in enumerate(self.graph):
                    if not (bp1 in visible and bp2 in visible):
                        continue

                    j1_x, j1_y = kpts_animal[bp1]
                    j2_x, j2_y = kpts_animal[bp2]
                    vec_x = j2_x - j1_x
                    vec_y = j2_y - j1_y
                    dist = sqrt(vec_x ** 2 + vec_y ** 2)
                    if dist > 0:
                        vec_x_norm = vec_x / dist
                        vec_y_norm = vec_y / dist
                        vec = [
                            vec_x_norm * j1_x + vec_y_norm * j1_y,
                            vec_x_norm * j2_x + vec_y_norm * j2_y,
                        ]
                        vec_ortho = j1_y * vec_x_norm - j1_x * vec_y_norm

                        distance_along = vec_x_norm * x + vec_y_norm * y
                        distance_across = (
                            ((y * vec_x_norm - x * vec_y_norm) - vec_ortho)
                            * 1.0
                            / self.width
                        )

                        mask1 = (distance_along >= min(vec)) & (
                            distance_along <= max(vec)
                        )
                        distance_across_abs = np.abs(distance_across)
                        mask2 = distance_across_abs <= 1
                        mask = mask1 & mask2
                        temp = 1 - distance_across_abs[mask]
                        paf_map[b, mask, l * 2 + 0] = vec_x_norm * temp
                        paf_map[b, mask, l * 2 + 1] = vec_y_norm * temp

        paf_map = paf_map.transpose((0, 3, 1, 2))
        return {
            "paf": {
                "target": torch.tensor(
                    paf_map, device=outputs["paf"].device
                )
            }
        }


--- File: deeplabcut/pose_estimation_pytorch/models/target_generators/dekr_targets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import numpy as np
import torch

from deeplabcut.pose_estimation_pytorch.models.target_generators.base import (
    BaseGenerator,
    TARGET_GENERATORS,
)


@TARGET_GENERATORS.register_module
class DEKRGenerator(BaseGenerator):
    """
    Generate ground truth target for DEKR model training based on:
        Bottom-Up Human Pose Estimation Via Disentangled Keypoint Regression
        Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, Jingdong Wang, CVPR 2021
    Code based on:
        https://github.com/HRNet/DEKR
    """

    def __init__(
        self, num_joints: int, pos_dist_thresh: int, bg_weight: float = 0.1, **kwargs
    ):
        """
        Args:
            num_joints: number of keypoints
            pos_dist_thresh: 3*std of the gaussian
            bg_weight:background weight. Defaults to 0.1.
        """
        super().__init__(**kwargs)

        self.num_joints = num_joints
        self.num_heatmaps = self.num_joints + 1
        self.pos_dist_thresh = pos_dist_thresh
        self.bg_weight = bg_weight

    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor], labels: dict
    ) -> dict[str, dict[str, torch.Tensor]]:
        """
        Given the annotations and predictions of your keypoints, this function returns the targets,
        a dictionary containing the heatmaps, locref_maps and locref_masks.
        Args:
            stride: the stride of the model
            outputs: output of each model head
            labels: the labels for the inputs (each tensor should have shape (b, ...))

        Returns:
            The targets for the DEKR heatmap and offset heads:
                {
                    "heatmap": {
                        "target": heatmaps,
                        "weights":  heatmap_weights,
                    },
                    "offset": {
                        "target": offset_map,
                        "weights": offset_weights,
                    }
                }

        Examples:
            input:
                labels = {"keypoints":torch.randint(1,min(image_size),(batch_size, num_animals, num_joints, 2))}
                prediction = [torch.rand((batch_size, num_joints, image_size[0], image_size[1]))]
                image_size = (256, 256)
            output:
                targets = {
                    "heatmap": {"target": heatmaps, "weights":  heatmap_weights},
                    "offset": {"target": offset_map, "weights": offset_masks}
                }
        """
        stride_y, stride_x = stride, stride
        batch_size, _, output_h, output_w = outputs["heatmap"].shape
        coords = labels[self.label_keypoint_key].cpu().numpy()
        area = labels["area"].cpu().numpy()

        assert (
            self.num_joints + 1 == coords.shape[2]
        ), f"the number of joints should be {coords.shape}"

        # TODO make it possible to differentiate between center sigma and other sigmas
        scale = max(1 / stride_x, 1 / stride_y)
        sgm, ct_sgm = (self.pos_dist_thresh / 2) * scale, self.pos_dist_thresh * scale
        radius = self.pos_dist_thresh * scale

        heatmap_shape = batch_size, self.num_heatmaps, output_h, output_w
        heatmaps = np.zeros(heatmap_shape, dtype=np.float32)
        heatmap_weights = 2 * np.ones(heatmap_shape, dtype=np.float32)

        offset_shape = batch_size, self.num_joints * 2, output_h, output_w
        offset_map = np.zeros(offset_shape, dtype=np.float32)
        weight_map = np.zeros(offset_shape, dtype=np.float32)

        area_map = np.zeros((batch_size, output_h, output_w), dtype=np.float32)
        for b in range(batch_size):
            for person_id, p in enumerate(coords[b]):
                idx_center = len(p) - 1
                ct_x = int(p[-1, 0])
                ct_y = int(p[-1, 1])

                ct_x_sm = (ct_x - stride_x / 2) / stride_x
                ct_y_sm = (ct_y - stride_y / 2) / stride_y
                for idx, pt in enumerate(p):
                    if pt[-1] == -1:
                        # full gradient masking
                        heatmap_weights[b, idx] = 0.0
                        continue
                    elif pt[-1] <= 0:
                        continue

                    if idx == idx_center:
                        sigma = ct_sgm
                    else:
                        sigma = sgm

                    x, y = pt[0], pt[1]
                    x_sm, y_sm = (
                        (x - stride_x / 2) / stride_x,
                        (y - stride_y / 2) / stride_y,
                    )

                    if x_sm < 0 or y_sm < 0 or x_sm >= output_w or y_sm >= output_h:
                        continue

                    # HEATMAP COMPUTATION
                    ul = (
                        int(np.floor(x_sm - 3 * sigma - 1)),
                        int(np.floor(y_sm - 3 * sigma - 1)),
                    )
                    br = (
                        int(np.ceil(x_sm + 3 * sigma + 2)),
                        int(np.ceil(y_sm + 3 * sigma + 2)),
                    )

                    cc, dd = max(0, ul[0]), min(br[0], output_w)
                    aa, bb = max(0, ul[1]), min(br[1], output_h)

                    joint_rg = np.zeros((bb - aa, dd - cc))
                    for sy in range(aa, bb):
                        for sx in range(cc, dd):
                            joint_rg[sy - aa, sx - cc] = dekr_heatmap_val(
                                sigma, sx, sy, x_sm, y_sm
                            )

                    heatmaps[b, idx, aa:bb, cc:dd] = np.maximum(
                        heatmaps[b, idx, aa:bb, cc:dd], joint_rg
                    )
                    heatmap_weights[b, idx, aa:bb, cc:dd] = 1.0

                    # OFFSET COMPUTATION
                    if idx != idx_center:
                        start_x = max(int(ct_x_sm - radius), 0)
                        start_y = max(int(ct_y_sm - radius), 0)
                        end_x = min(int(ct_x_sm + radius), output_w)
                        end_y = min(int(ct_y_sm + radius), output_h)

                        for pos_x in range(start_x, end_x):
                            for pos_y in range(start_y, end_y):
                                offset_x = pos_x - x_sm
                                offset_y = pos_y - y_sm
                                if (
                                    offset_map[b, idx * 2, pos_y, pos_x] != 0
                                    or offset_map[b, idx * 2 + 1, pos_y, pos_x] != 0
                                ):
                                    if area_map[b, pos_y, pos_x] < area[b, person_id]:
                                        continue
                                offset_map[b, idx * 2, pos_y, pos_x] = offset_x
                                offset_map[b, idx * 2 + 1, pos_y, pos_x] = offset_y
                                # TODO find a decent constant make weights vary giving animal area
                                weight_map[b, idx * 2, pos_y, pos_x] = 1.0 / np.sqrt(
                                    area[b, person_id]
                                )
                                weight_map[
                                    b, idx * 2 + 1, pos_y, pos_x
                                ] = 1.0 / np.sqrt(area[b, person_id])
                                area_map[b, pos_y, pos_x] = area[b, person_id]

        heatmap_weights[heatmap_weights == 2] = self.bg_weight
        return {
            "heatmap": {
                "target": torch.tensor(heatmaps, device=outputs["heatmap"].device),
                "weights": torch.tensor(
                    heatmap_weights, device=outputs["heatmap"].device
                ),
            },
            "offset": {
                "target": torch.tensor(offset_map, device=outputs["offset"].device),
                "weights": torch.tensor(weight_map, device=outputs["offset"].device),
            },
        }


def dekr_heatmap_val(sigma: float, x: float, y: float, x0: float, y0: float) -> float:
    """
    Calculates the corresponding heat value of point (x,y) given the heat distribution centered
    at (x0,y0) and spread value of sigma.

    Args:
        sigma: controls the spread or width of the heat distribution
        x: x coord of a point on the image grid
        y: y coord of a point on the image grid
        x0: x center coordinate of the heat distribution
        y0: y center coordinate of the heat distribution

    Returns:
        g: calculated heat value represents the intensity of the heat at a given position
    """
    return np.exp(-((x - x0) ** 2 + (y - y0) ** 2) / (2 * sigma ** 2))


--- File: deeplabcut/pose_estimation_pytorch/models/target_generators/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from abc import ABC, abstractmethod

import torch
import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry

TARGET_GENERATORS = Registry("target_generators", build_func=build_from_cfg)


class BaseGenerator(ABC, nn.Module):  # TODO: Should this really be a module?
    """Generates target maps from ground truth annotations to train models

    The outputs of the target generator are used to compute losses for model heads. If
    the head outputs "heatmap" and "offset" tensors, then the corresponding generator
    must output target "heatmap" and "offset" tensors. The targets themselves are
    dictionaries, and passed as keyword-arguments to the criterions. This allows to
    pass masks to the criterions.

    Generally, this means that for each head output (such as "heatmap"), a dict will be
    generated with a "target" key (for the target heatmap) and optionally a "weights"
    key (see the WeightedCriterion classes).
    """

    def __init__(self, label_keypoint_key: str = "keypoints"):
        super().__init__()
        self.label_keypoint_key = label_keypoint_key

    @abstractmethod
    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor], labels: dict
    ) -> dict[str, dict[str, torch.Tensor]]:
        """Generates targets

        Args:
            stride: the stride of the model
            outputs: output of a model head
            labels: the labels for the inputs (each tensor should have shape (b, ...))

        Returns:
            a dictionary mapping the heads to the inputs of the criterion
                {
                    "heatmap": {
                        "target": heatmaps,
                        "weights":  heatmap_weights,
                    },
                    "locref": {
                        "target": locref_map,
                        "weights": locref_weights,
                    }
                }
        """


@TARGET_GENERATORS.register_module
class SequentialGenerator(BaseGenerator):
    def __init__(self, generators: list[dict], label_keypoint_key: str = "keypoints"):
        super().__init__(label_keypoint_key)
        self._generators = [TARGET_GENERATORS.build(dict_) for dict_ in generators]

    @property
    def generators(self):
        return self._generators

    def forward(
        self, stride: int, outputs: dict[str, torch.Tensor], labels: dict
    ) -> dict[str, dict[str, torch.Tensor]]:
        dict_ = {}
        for gen in self.generators:
            dict_.update(gen(stride, outputs, labels))
        return dict_

    def __repr__(self):
        generators_repr = ", ".join(repr(gen) for gen in self._generators)
        return (
            f"<{self.__class__.__name__}(generators=[{generators_repr}], "
            f"label_keypoint_key='{self.label_keypoint_key}')>"
        )


--- File: deeplabcut/pose_estimation_pytorch/models/target_generators/sim_cc.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Modified SimCC target generator for the RTMPose model

Based on the official ``mmpose`` SimCC codec and RTMCC head implementation. For more
information, see <https://github.com/open-mmlab/mmpose>.
"""
from __future__ import annotations

from itertools import product

import numpy as np
import torch

from deeplabcut.pose_estimation_pytorch.models.target_generators.base import (
    BaseGenerator,
    TARGET_GENERATORS,
)


@TARGET_GENERATORS.register_module
class SimCCGenerator(BaseGenerator):
    """Class used generate targets from RTMPose head outputs

    The RTMPose model uses coordinate classification for pose estimation. For more
    information, see "SimCC: a Simple Coordinate Classification Perspective for Human
    Pose Estimation" (<https://arxiv.org/pdf/2107.03332>) and "RTMPose: Real-Time
    Multi-Person Pose Estimation based on MMPose" (<https://arxiv.org/pdf/2303.07399>).

    Args:
        input_size: The size of images given to the pose estimation model.
        smoothing_type: Smoothing strategy ("gaussian" or "standard")
        sigma: The sigma value in the Gaussian SimCC label. If a single value, used for
            both x and y. If two values, the sigmas for (x, y).
        simcc_split_ratio: The split ratio of pixels, as described in SimCC.
        label_smooth_weight: Label Smoothing weight.
        normalize: Normalize the heatmaps before returning.
        **kwargs,
    """

    def __init__(
        self,
        input_size: tuple[int, int],
        smoothing_type: str = "gaussian",
        sigma: float | int | tuple[float, ...] = 6.0,
        simcc_split_ratio: float = 2.0,
        label_smooth_weight: float = 0.0,
        normalize: bool = True,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        self.input_size = input_size
        self.smoothing_type = smoothing_type
        self.simcc_split_ratio = simcc_split_ratio
        self.label_smooth_weight = label_smooth_weight
        self.normalize = normalize

        if isinstance(sigma, (float, int)):
            self.sigma = np.array([sigma, sigma])
        else:
            self.sigma = np.array(sigma)

        if self.smoothing_type not in {"gaussian", "standard"}:
            raise ValueError(
                f"{self.__class__.__name__} got invalid `smoothing_type` value"
                f"{self.smoothing_type}. Should be one of "
                '{"gaussian", "standard"}'
            )

        if self.smoothing_type == "gaussian" and self.label_smooth_weight > 0:
            raise ValueError(
                "Attribute `label_smooth_weight` is only " "used for `standard` mode."
            )

        if self.label_smooth_weight < 0.0 or self.label_smooth_weight > 1.0:
            raise ValueError("`label_smooth_weight` should be in range [0, 1]")

        if self.smoothing_type == "gaussian":
            self.generator = self._generate_gaussian
        elif self.smoothing_type == "standard":
            self.generator = self._generate_standard
        else:
            raise ValueError(
                f"{self.__class__.__name__} got invalid `smoothing_type` value"
                f"{self.smoothing_type}. Should be one of "
                '{"gaussian", "standard"}'
            )

    def forward(
        self, stride: float, outputs: dict[str, torch.Tensor], labels: dict
    ) -> dict[str, dict[str, torch.Tensor]]:
        device = outputs["x"].device
        keypoints = labels[self.label_keypoint_key].cpu().numpy()
        batch_size = len(keypoints)

        if len(keypoints.shape) == 3:  # for single animal: add individual dimension
            keypoints = keypoints.reshape((batch_size, 1, *keypoints.shape[1:]))

        xs, ys, ws = [], [], []
        for batch_keypoints in keypoints:
            keypoints = batch_keypoints[:, :, :2]
            keypoints_visible = batch_keypoints[:, :, 2]
            x_labels, y_labels, weights = self.generator(keypoints, keypoints_visible)
            xs.append(x_labels)
            ys.append(y_labels)
            ws.append(weights)

        x_labels = np.stack(xs)
        y_labels = np.stack(ys)
        weights = np.stack(ws)
        return dict(
            x=dict(
                target=torch.tensor(x_labels, device=device),
                weights=torch.tensor(weights, device=device),
            ),
            y=dict(
                target=torch.tensor(y_labels, device=device),
                weights=torch.tensor(weights, device=device),
            ),
        )

    def _generate_standard(
        self, keypoints: np.ndarray, keypoints_visible: np.ndarray | None = None
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Encoding keypoints into SimCC labels with Standard Label Smoothing.

        Labels will be one-hot vectors if self.label_smooth_weight==0.0
        """
        N, K, _ = keypoints.shape
        w, h = self.input_size
        W = np.around(w * self.simcc_split_ratio).astype(int)
        H = np.around(h * self.simcc_split_ratio).astype(int)

        keypoints_split, keypoint_weights = self._map_coordinates(
            keypoints, keypoints_visible
        )

        target_x = np.zeros((N, K, W), dtype=np.float32)
        target_y = np.zeros((N, K, H), dtype=np.float32)

        for n, k in product(range(N), range(K)):
            # skip unlabeled keypoints
            if keypoints_visible[n, k] < 0.5:
                continue

            # get center coordinates
            mu_x, mu_y = keypoints_split[n, k].astype(np.int64)

            # detect abnormal coords and assign the weight 0
            if mu_x >= W or mu_y >= H or mu_x < 0 or mu_y < 0:
                keypoint_weights[n, k] = 0
                continue

            if self.label_smooth_weight > 0:
                target_x[n, k] = self.label_smooth_weight / (W - 1)
                target_y[n, k] = self.label_smooth_weight / (H - 1)

            target_x[n, k, mu_x] = 1.0 - self.label_smooth_weight
            target_y[n, k, mu_y] = 1.0 - self.label_smooth_weight

        return target_x, target_y, keypoint_weights

    def _map_coordinates(
        self, keypoints: np.ndarray, keypoints_visible: np.ndarray | None = None
    ) -> tuple[np.ndarray, np.ndarray]:
        """Mapping keypoint coordinates into SimCC space"""
        keypoints_split = keypoints.copy()
        # set non-visible keypoints to 0; deals with NaNs
        keypoints_split[keypoints_visible <= 0] = 0
        keypoints_split = np.around(keypoints_split * self.simcc_split_ratio)
        keypoints_split = keypoints_split.astype(np.int64)
        keypoint_weights = (keypoints_visible > 0).astype(keypoints_split.dtype)
        return keypoints_split, keypoint_weights

    def _generate_gaussian(
        self, keypoints: np.ndarray, keypoints_visible: np.ndarray | None = None
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Encoding keypoints into SimCC labels with Gaussian Label Smoothing"""
        N, K, _ = keypoints.shape
        w, h = self.input_size
        W = np.around(w * self.simcc_split_ratio).astype(int)
        H = np.around(h * self.simcc_split_ratio).astype(int)

        keypoints_split, keypoint_weights = self._map_coordinates(
            keypoints, keypoints_visible
        )

        target_x = np.zeros((N, K, W), dtype=np.float32)
        target_y = np.zeros((N, K, H), dtype=np.float32)

        # 3-sigma rule
        radius = self.sigma * 3

        # xy grid
        x = np.arange(0, W, 1, dtype=np.float32)
        y = np.arange(0, H, 1, dtype=np.float32)

        for n, k in product(range(N), range(K)):
            # skip unlabeled keypoints
            if keypoints_visible[n, k] < 0.5:
                continue

            mu = keypoints_split[n, k]

            # check that the gaussian has in-bounds part
            left, top = mu - radius
            right, bottom = mu + radius + 1

            if left >= W or top >= H or right < 0 or bottom < 0:
                keypoint_weights[n, k] = 0
                continue

            mu_x, mu_y = mu

            target_x[n, k] = np.exp(-((x - mu_x) ** 2) / (2 * self.sigma[0] ** 2))
            target_y[n, k] = np.exp(-((y - mu_y) ** 2) / (2 * self.sigma[1] ** 2))

        if self.normalize:
            norm_value = self.sigma * np.sqrt(np.pi * 2)
            target_x /= norm_value[0]
            target_y /= norm_value[1]

        return target_x, target_y, keypoint_weights


--- File: deeplabcut/pose_estimation_pytorch/models/detectors/torchvision.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Module to adapt torchvision detectors for DeepLabCut"""
from __future__ import annotations

import torch
import torchvision.models.detection as detection

from deeplabcut.pose_estimation_pytorch.models.detectors.base import (
    BaseDetector,
)


class TorchvisionDetectorAdaptor(BaseDetector):
    """An adaptor for torchvision detectors

    This class is an adaptor for torchvision detectors to DeepLabCut detectors. Some of
    the models (from fastest to most powerful) available are:
      - ssdlite320_mobilenet_v3_large
      - fasterrcnn_mobilenet_v3_large_fpn
      - fasterrcnn_resnet50_fpn_v2

    This class should not be used out-of-the-box. Subclasses (such as FasterRCNN or
    SSDLite) should be used instead.

    The torchvision implementation does not allow to get both predictions and losses
    with a single forward pass. Therefore, during evaluation only bounding box metrics
    (mAP, mAR) are available for the test set. See validation loss issue:
    - https://discuss.pytorch.org/t/compute-validation-loss-for-faster-rcnn/62333/12
    - https://stackoverflow.com/a/65347721

    Args:
        model: The torchvision model to use (see all options at
            https://pytorch.org/vision/stable/models.html#object-detection).
        weights: The weights to load for the model. If None, no pre-trained weights are
            loaded.
        num_classes: Number of classes that the model should output. If None, the number
            of classes the model is pre-trained on is used.
        freeze_bn_stats: Whether to freeze stats for BatchNorm layers.
        freeze_bn_weights: Whether to freeze weights for BatchNorm layers.
        box_score_thresh: during inference, only return proposals with a classification
            score greater than box_score_thresh
    """

    def __init__(
        self,
        model: str,
        weights: str | None = None,
        num_classes: int | None = 2,
        freeze_bn_stats: bool = False,
        freeze_bn_weights: bool = False,
        box_score_thresh: float = 0.01,
        model_kwargs: dict | None = None,
    ) -> None:
        super().__init__(
            freeze_bn_stats=freeze_bn_stats,
            freeze_bn_weights=freeze_bn_weights,
            pretrained=weights is not None,
        )

        # Load the model
        model_fn = getattr(detection, model)
        if model_kwargs is None:
            model_kwargs = {}

        self.model = model_fn(
            weights=weights,
            box_score_thresh=box_score_thresh,
            num_classes=num_classes,
            **model_kwargs,
        )

        # See source:  https://stackoverflow.com/a/65347721
        self.model.eager_outputs = lambda losses, detections: (losses, detections)

    def forward(
        self, x: torch.Tensor, targets: list[dict[str, torch.Tensor]] | None = None
    ) -> tuple[dict[str, torch.Tensor], list[dict[str, torch.Tensor]]]:
        """
        Forward pass of the torchvision detector

        Args:
            x: images to be processed, of shape (b, c, h, w)
            targets: ground-truth boxes present in the images

        Returns:
            losses: {'loss_name': loss_value}
            detections: for each of the b images, {"boxes": bounding_boxes}
        """
        return self.model(x, targets)

    def get_target(self, labels: dict) -> list[dict[str, torch.Tensor]]:
        """
        Returns target in a format a torchvision detector can handle

        Args:
            labels: dict of annotations, must contain the keys:
                area: tensor containing area information for each annotation
                labels: tensor containing class labels for each annotation
                is_crowd: tensor indicating if each annotation is a crowd (1) or not (0)
                image_id: tensor containing image ids for each annotation
                boxes: tensor containing bounding box information for each annotation

        Returns:
            res: list of dictionaries, each representing target information for a single
                annotation. Each dictionary contains the following keys:
                    'area'
                    'labels'
                    'is_crowd'
                    'boxes'

        Examples:
            input:
                annotations = {
                    "area": torch.Tensor([100, 200]),
                    "labels": torch.Tensor([1, 2]),
                    "is_crowd": torch.Tensor([0, 1]),
                    "boxes": torch.Tensor([[10, 20, 30, 40], [50, 60, 70, 80]])
                }
            output:
                res =  [
                    {
                        'area': tensor([100.]),
                        'labels': tensor([1]),
                        'image_id': tensor([1]),
                        'is_crowd': tensor([0]),
                        'boxes': tensor([[10., 20., 40., 60.]])
                    },
                    {
                        'area': tensor([200.]),
                        'labels': tensor([2]),
                        'image_id': tensor([1]),
                        'is_crowd': tensor([1]),
                        'boxes': tensor([[50., 60., 70., 80.]])
                    }
                ]
        """
        res = []
        for i, box_ann in enumerate(labels["boxes"]):
            mask = (box_ann[:, 2] > 0.0) & (box_ann[:, 3] > 0.0)
            box_ann = box_ann[mask]
            # bbox format conversion (x, y, w, h) -> (x1, y1, x2, y2)
            box_ann[:, 2] += box_ann[:, 0]
            box_ann[:, 3] += box_ann[:, 1]
            res.append(
                {
                    "area": labels["area"][i][mask],
                    "labels": labels["labels"][i][mask].long(),
                    "is_crowd": labels["is_crowd"][i][mask].long(),
                    "boxes": box_ann,
                }
            )

        return res


--- File: deeplabcut/pose_estimation_pytorch/models/detectors/ssd.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import torchvision.models.detection as detection

from deeplabcut.pose_estimation_pytorch.models.detectors.base import DETECTORS
from deeplabcut.pose_estimation_pytorch.models.detectors.torchvision import (
    TorchvisionDetectorAdaptor,
)


@DETECTORS.register_module
class SSDLite(TorchvisionDetectorAdaptor):
    """An SSD object detection model"""

    def __init__(
        self,
        freeze_bn_stats: bool = False,
        freeze_bn_weights: bool = False,
        pretrained: bool = False,
        pretrained_from_imagenet: bool = False,
        box_score_thresh: float = 0.01,
    ) -> None:
        model_kwargs = dict(weights_backbone=None)
        if pretrained_from_imagenet:
            model_kwargs["weights_backbone"] = "IMAGENET1K_V2"

        super().__init__(
            model="ssdlite320_mobilenet_v3_large",
            weights=None,
            num_classes=2,
            freeze_bn_stats=freeze_bn_stats,
            freeze_bn_weights=freeze_bn_weights,
            box_score_thresh=box_score_thresh,
            model_kwargs=model_kwargs,
        )

        if pretrained and not pretrained_from_imagenet:
            weights = detection.SSDLite320_MobileNet_V3_Large_Weights.verify("COCO_V1")
            state_dict = weights.get_state_dict(progress=False, check_hash=True)
            for k, v in state_dict.items():
                key_parts = k.split(".")
                if (
                    len(key_parts) == 6
                    and key_parts[0] == "head"
                    and key_parts[1] == "classification_head"
                    and key_parts[2] == "module_list"
                    and key_parts[4] == "1"
                    and key_parts[5] in ("weight", "bias")
                ):
                    # number of COCO classes: 90 + background (91)
                    # number of DLC classes: 1 + background (2)
                    # -> only keep weights for the background + first class

                    # future improvement: find best-suited class for the project
                    #   and use those weights, instead of naively taking the first
                    all_classes_size = v.shape[0]
                    two_classes_size = 2 * (all_classes_size // 91)
                    state_dict[k] = v[:two_classes_size]

            self.model.load_state_dict(state_dict)


--- File: deeplabcut/pose_estimation_pytorch/models/detectors/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.models.detectors.base import (
    DETECTORS,
    BaseDetector,
)
from deeplabcut.pose_estimation_pytorch.models.detectors.fasterRCNN import FasterRCNN
from deeplabcut.pose_estimation_pytorch.models.detectors.ssd import SSDLite


--- File: deeplabcut/pose_estimation_pytorch/models/detectors/fasterRCNN.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import torchvision.models.detection as detection

from deeplabcut.pose_estimation_pytorch.models.detectors.base import DETECTORS
from deeplabcut.pose_estimation_pytorch.models.detectors.torchvision import (
    TorchvisionDetectorAdaptor,
)


@DETECTORS.register_module
class FasterRCNN(TorchvisionDetectorAdaptor):
    """A FasterRCNN detector

    Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
        Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. "Faster r-cnn: Towards
        real-time object detection with region proposal networks." Advances in neural
        information processing systems 28 (2015).

    This class is a wrapper of the torchvision implementation of a FasterRCNN (source:
    https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py).

    Some of the available FasterRCNN variants (from fastest to most powerful):
      - fasterrcnn_mobilenet_v3_large_fpn
      - fasterrcnn_resnet50_fpn
      - fasterrcnn_resnet50_fpn_v2

    Args:
        variant: The FasterRCNN variant to use (see all options at
            https://pytorch.org/vision/stable/models.html#object-detection).
        pretrained: Whether to load model weights pretrained on COCO
        box_score_thresh: during inference, only return proposals with a classification
            score greater than box_score_thresh
    """

    def __init__(
        self,
        freeze_bn_stats: bool = False,
        freeze_bn_weights: bool = False,
        variant: str = "fasterrcnn_mobilenet_v3_large_fpn",
        pretrained: bool = False,
        box_score_thresh: float = 0.01,
    ) -> None:
        if not variant.lower().startswith("fasterrcnn"):
            raise ValueError(
                "The version must start with `fasterrcnn`. See available models at "
                "https://pytorch.org/vision/stable/models.html#object-detection"
            )

        super().__init__(
            model=variant,
            weights=("COCO_V1" if pretrained else None),
            num_classes=None,
            freeze_bn_stats=freeze_bn_stats,
            freeze_bn_weights=freeze_bn_weights,
            box_score_thresh=box_score_thresh,
        )

        # Modify the base predictor to output the correct number of classes
        num_classes = 2
        in_features = self.model.roi_heads.box_predictor.cls_score.in_features
        self.model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(
            in_features, num_classes
        )


--- File: deeplabcut/pose_estimation_pytorch/models/detectors/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import logging
from abc import ABC, abstractmethod

import torch
import torch.nn as nn

from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry


def _build_detector(
    cfg: dict,
    weight_init: WeightInitialization | None = None,
    pretrained: bool = False,
    **kwargs,
) -> BaseDetector:
    """Builds a detector using its configuration file

    Args:
        cfg: The detector configuration.
        weight_init: The weight initialization to use.
        pretrained: Whether COCO pretrained weights should be loaded for the detector
        **kwargs: Other parameters given by the Registry.

    Returns:
        the built detector
    """
    cfg["pretrained"] = pretrained
    detector: BaseDetector = build_from_cfg(cfg, **kwargs)

    if weight_init is not None and weight_init.detector_snapshot_path is not None:
        logging.info(
            f"Loading detector checkpoint from {weight_init.detector_snapshot_path}"
        )
        snapshot = torch.load(weight_init.detector_snapshot_path, map_location="cpu")
        detector.load_state_dict(snapshot["model"])

    return detector


DETECTORS = Registry("detectors", build_func=_build_detector)


class BaseDetector(ABC, nn.Module):
    """
    Definition of the class BaseDetector object.
    This is an abstract class defining the common structure and inference for detectors.
    """

    def __init__(
        self,
        freeze_bn_stats: bool = False,
        freeze_bn_weights: bool = False,
        pretrained: bool = False,
    ) -> None:
        super().__init__()
        self.freeze_bn_stats = freeze_bn_stats
        self.freeze_bn_weights = freeze_bn_weights
        self._pretrained = pretrained

    @abstractmethod
    def forward(
        self, x: torch.Tensor, targets: list[dict[str, torch.Tensor]] | None = None
    ) -> tuple[dict[str, torch.Tensor], list[dict[str, torch.Tensor]]]:
        """
        Forward pass of the detector

        Args:
            x: images to be processed
            targets: ground-truth boxes present in each images

        Returns:
            losses: {'loss_name': loss_value}
            detections: for each of the b images, {"boxes": bounding_boxes}
        """
        pass

    @abstractmethod
    def get_target(self, labels: dict) -> list[dict]:
        """
        Get the target for training the detector

        Args:
            labels: annotations containing keypoints, bounding boxes, etc.

        Returns:
            list of dictionaries, each representing target information for a single annotation.
        """
        pass

    def freeze_batch_norm_layers(self) -> None:
        """Freezes batch norm layers

        Running mean + var are always given to F.batch_norm, except when the layer is
        in `train` mode and track_running_stats is False, see
            https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html
        So to 'freeze' the running stats, the only way is to set the layer to "eval"
        mode.
        """
        for module in self.modules():
            if isinstance(module, nn.modules.batchnorm._BatchNorm):
                if self.freeze_bn_weights:
                    module.weight.requires_grad = False
                    module.bias.requires_grad = False
                if self.freeze_bn_stats:
                    module.eval()

    def train(self, mode: bool = True) -> None:
        """Sets the module in training or evaluation mode.

        Args:
            mode: whether to set training mode (True) or evaluation mode (False)
        """
        super().train(mode)
        if self.freeze_bn_weights or self.freeze_bn_stats:
            self.freeze_batch_norm_layers()


--- File: deeplabcut/pose_estimation_pytorch/models/modules/norm.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Normalization layers"""
from __future__ import annotations

import torch
import torch.nn as nn


class ScaleNorm(nn.Module):
    """Implementation of ScaleNorm

    ScaleNorm was introduced in "Transformers without Tears: Improving the Normalization
    of Self-Attention".

    Code based on the `mmpose` implementation. See https://github.com/open-mmlab/mmpose
    for more details.

    Args:
        dim: The dimension of the scale vector.
        eps: The minimum value in clamp.
    """

    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.scale = dim ** -0.5
        self.eps = eps
        self.g = nn.Parameter(torch.ones(1))

    def forward(self, x):
        norm = torch.linalg.norm(x, dim=-1, keepdim=True)
        norm = norm * self.scale
        return x / norm.clamp(min=self.eps) * self.g


--- File: deeplabcut/pose_estimation_pytorch/models/modules/gated_attention_unit.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Gated Attention Unit

Based on the building blocks used for the ``mmdetection`` CSPNeXt implementation. For
more information, see <https://github.com/open-mmlab/mmdetection>.
"""
from __future__ import annotations

import math

import torch
import torch.nn as nn
import torch.nn.functional as F
import timm.layers as timm_layers

from deeplabcut.pose_estimation_pytorch.models.modules.norm import ScaleNorm


def rope(x, dim):
    """Applies Rotary Position Embedding to input tensor."""
    shape = x.shape
    if isinstance(dim, int):
        dim = [dim]

    spatial_shape = [shape[i] for i in dim]
    total_len = 1
    for i in spatial_shape:
        total_len *= i

    position = torch.reshape(
        torch.arange(total_len, dtype=torch.int, device=x.device), spatial_shape
    )

    for i in range(dim[-1] + 1, len(shape) - 1, 1):
        position = torch.unsqueeze(position, dim=-1)

    half_size = shape[-1] // 2
    freq_seq = -torch.arange(half_size, dtype=torch.int, device=x.device) / float(
        half_size
    )
    inv_freq = 10000**-freq_seq

    sinusoid = position[..., None] * inv_freq[None, None, :]

    sin = torch.sin(sinusoid)
    cos = torch.cos(sinusoid)
    x1, x2 = torch.chunk(x, 2, dim=-1)

    return torch.cat([x1 * cos - x2 * sin, x2 * cos + x1 * sin], dim=-1)


class Scale(nn.Module):
    """Scale vector by element multiplications.

    Args:
        dim: The dimension of the scale vector.
        init_value: The initial value of the scale vector.
        trainable: Whether the scale vector is trainable.
    """

    def __init__(self, dim, init_value=1.0, trainable=True):
        super().__init__()
        self.scale = nn.Parameter(init_value * torch.ones(dim), requires_grad=trainable)

    def forward(self, x):
        return x * self.scale


class GatedAttentionUnit(nn.Module):
    """Gated Attention Unit (GAU) in RTMBlock"""

    def __init__(
        self,
        num_token,
        in_token_dims,
        out_token_dims,
        expansion_factor=2,
        s=128,
        eps=1e-5,
        dropout_rate=0.0,
        drop_path=0.0,
        attn_type="self-attn",
        act_fn="SiLU",
        bias=False,
        use_rel_bias=True,
        pos_enc=False,
    ):
        super(GatedAttentionUnit, self).__init__()
        self.s = s
        self.num_token = num_token
        self.use_rel_bias = use_rel_bias
        self.attn_type = attn_type
        self.pos_enc = pos_enc

        if drop_path > 0.0:
            self.drop_path = timm_layers.DropPath(drop_path)
        else:
            self.drop_path = nn.Identity()

        self.e = int(in_token_dims * expansion_factor)
        if use_rel_bias:
            if attn_type == "self-attn":
                self.w = nn.Parameter(
                    torch.rand([2 * num_token - 1], dtype=torch.float)
                )
            else:
                self.a = nn.Parameter(torch.rand([1, s], dtype=torch.float))
                self.b = nn.Parameter(torch.rand([1, s], dtype=torch.float))
        self.o = nn.Linear(self.e, out_token_dims, bias=bias)

        if attn_type == "self-attn":
            self.uv = nn.Linear(in_token_dims, 2 * self.e + self.s, bias=bias)
            self.gamma = nn.Parameter(torch.rand((2, self.s)))
            self.beta = nn.Parameter(torch.rand((2, self.s)))
        else:
            self.uv = nn.Linear(in_token_dims, self.e + self.s, bias=bias)
            self.k_fc = nn.Linear(in_token_dims, self.s, bias=bias)
            self.v_fc = nn.Linear(in_token_dims, self.e, bias=bias)
            nn.init.xavier_uniform_(self.k_fc.weight)
            nn.init.xavier_uniform_(self.v_fc.weight)

        self.ln = ScaleNorm(in_token_dims, eps=eps)

        nn.init.xavier_uniform_(self.uv.weight)

        if act_fn == "SiLU" or act_fn == nn.SiLU:
            self.act_fn = nn.SiLU(True)
        elif act_fn == "ReLU" or act_fn == nn.ReLU:
            self.act_fn = nn.ReLU(True)
        else:
            raise NotImplementedError

        if in_token_dims == out_token_dims:
            self.shortcut = True
            self.res_scale = Scale(in_token_dims)
        else:
            self.shortcut = False

        self.sqrt_s = math.sqrt(s)

        self.dropout_rate = dropout_rate

        if dropout_rate > 0.0:
            self.dropout = nn.Dropout(dropout_rate)

    def rel_pos_bias(self, seq_len, k_len=None):
        """Add relative position bias."""

        if self.attn_type == "self-attn":
            t = F.pad(self.w[: 2 * seq_len - 1], [0, seq_len]).repeat(seq_len)
            t = t[..., :-seq_len].reshape(-1, seq_len, 3 * seq_len - 2)
            r = (2 * seq_len - 1) // 2
            t = t[..., r:-r]
        else:
            a = rope(self.a.repeat(seq_len, 1), dim=0)
            b = rope(self.b.repeat(k_len, 1), dim=0)
            t = torch.bmm(a, b.permute(0, 2, 1))
        return t

    def _forward(self, inputs):
        """GAU Forward function."""

        if self.attn_type == "self-attn":
            x = inputs
        else:
            x, k, v = inputs

        x = self.ln(x)

        # [B, K, in_token_dims] -> [B, K, e + e + s]
        uv = self.uv(x)
        uv = self.act_fn(uv)

        if self.attn_type == "self-attn":
            # [B, K, e + e + s] -> [B, K, e], [B, K, e], [B, K, s]
            u, v, base = torch.split(uv, [self.e, self.e, self.s], dim=2)
            # [B, K, 1, s] * [1, 1, 2, s] + [2, s] -> [B, K, 2, s]
            base = base.unsqueeze(2) * self.gamma[None, None, :] + self.beta

            if self.pos_enc:
                base = rope(base, dim=1)
            # [B, K, 2, s] -> [B, K, s], [B, K, s]
            q, k = torch.unbind(base, dim=2)

        else:
            # [B, K, e + s] -> [B, K, e], [B, K, s]
            u, q = torch.split(uv, [self.e, self.s], dim=2)

            k = self.k_fc(k)  # -> [B, K, s]
            v = self.v_fc(v)  # -> [B, K, e]

            if self.pos_enc:
                q = rope(q, 1)
                k = rope(k, 1)

        # [B, K, s].permute() -> [B, s, K]
        # [B, K, s] x [B, s, K] -> [B, K, K]
        qk = torch.bmm(q, k.permute(0, 2, 1))

        if self.use_rel_bias:
            if self.attn_type == "self-attn":
                bias = self.rel_pos_bias(q.size(1))
            else:
                bias = self.rel_pos_bias(q.size(1), k.size(1))
            qk += bias[:, : q.size(1), : k.size(1)]
        # [B, K, K]
        kernel = torch.square(F.relu(qk / self.sqrt_s))

        if self.dropout_rate > 0.0:
            kernel = self.dropout(kernel)
        # [B, K, K] x [B, K, e] -> [B, K, e]
        x = u * torch.bmm(kernel, v)

        # [B, K, e] -> [B, K, out_token_dims]
        x = self.o(x)

        return x

    def forward(self, x):
        if self.shortcut:
            if self.attn_type == "cross-attn":
                res_shortcut = x[0]
            else:
                res_shortcut = x
            main_branch = self.drop_path(self._forward(x))
            return self.res_scale(res_shortcut) + main_branch
        else:
            return self.drop_path(self._forward(x))


--- File: deeplabcut/pose_estimation_pytorch/models/modules/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.models.modules.conv_block import (
    AdaptBlock,
    BasicBlock,
    Bottleneck,
)
from deeplabcut.pose_estimation_pytorch.models.modules.conv_module import (
    HighResolutionModule,
)
from deeplabcut.pose_estimation_pytorch.models.modules.gated_attention_unit import (
    GatedAttentionUnit,
)
from deeplabcut.pose_estimation_pytorch.models.modules.norm import (
    ScaleNorm,
)


--- File: deeplabcut/pose_estimation_pytorch/models/modules/csp.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Implementation of modules needed for the CSPNeXt Backbone. Used in CSP-style models.

Based on the building blocks used for the ``mmdetection`` CSPNeXt implementation. For
more information, see <https://github.com/open-mmlab/mmdetection>.
"""
import torch
import torch.nn as nn


def build_activation(activation_fn: str, *args, **kwargs) -> nn.Module:
    if activation_fn == "SiLU":
        return nn.SiLU(*args, **kwargs)
    elif activation_fn == "ReLU":
        return nn.ReLU(*args, **kwargs)

    raise NotImplementedError(
        f"Unknown `CSPNeXT` activation: {activation_fn}. Must be one of 'SiLU', 'ReLU'"
    )


def build_norm(norm: str, *args, **kwargs) -> nn.Module:
    if norm == "SyncBN":
        return nn.SyncBatchNorm(*args, **kwargs)
    elif norm == "BN":
        return nn.BatchNorm2d(*args, **kwargs)

    raise NotImplementedError(
        f"Unknown `CSPNeXT` norm_layer: {norm}. Must be one of 'SyncBN', 'BN'"
    )


class SPPBottleneck(nn.Module):
    """Spatial pyramid pooling layer used in YOLOv3-SPP and (among others) CSPNeXt

    Args:
        in_channels: input channels to the bottleneck
        out_channels: output channels of the bottleneck
        kernel_sizes: kernel sizes for the pooling layers
        norm_layer: norm layer for the bottleneck
        activation_fn: activation function for the bottleneck
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_sizes: tuple[int, ...] = (5, 9, 13),
        norm_layer: str | None = "SyncBN",
        activation_fn: str | None = "SiLU",
    ):
        super().__init__()
        mid_channels = in_channels // 2
        self.conv1 = CSPConvModule(
            in_channels,
            mid_channels,
            kernel_size=1,
            stride=1,
            norm_layer=norm_layer,
            activation_fn=activation_fn,
        )

        self.poolings = nn.ModuleList(
            [
                nn.MaxPool2d(kernel_size=ks, stride=1, padding=ks // 2)
                for ks in kernel_sizes
            ]
        )
        conv2_channels = mid_channels * (len(kernel_sizes) + 1)
        self.conv2 = CSPConvModule(
            conv2_channels,
            out_channels,
            kernel_size=1,
            norm_layer=norm_layer,
            activation_fn=activation_fn,
        )

    def forward(self, x):
        x = self.conv1(x)
        with torch.amp.autocast("cuda", enabled=False):
            x = torch.cat([x] + [pooling(x) for pooling in self.poolings], dim=1)
        x = self.conv2(x)
        return x


class ChannelAttention(nn.Module):
    """Channel attention Module.

    Args:
        channels: Number of input/output channels of the layer.
    """

    def __init__(self, channels: int) -> None:
        super().__init__()
        self.global_avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Conv2d(channels, channels, 1, 1, 0, bias=True)
        self.act = nn.Hardsigmoid(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        with torch.amp.autocast("cuda", enabled=False):
            out = self.global_avgpool(x)
        out = self.fc(out)
        out = self.act(out)
        return x * out


class CSPConvModule(nn.Module):
    """Configurable convolution module used for CSPNeXT.

    Applies sequentially
      - a convolution
      - (optional) a norm layer
      - (optional) an activation function

    Args:
        in_channels: Input channels of the convolution.
        out_channels: Output channels of the convolution.
        kernel_size: Convolution kernel size.
        stride: Convolution stride.
        padding: Convolution padding.
        dilation: Convolution dilation.
        groups: Number of blocked connections from input to output channels.
        norm_layer: Norm layer to apply, if any.
        activation_fn: Activation function to apply, if any.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int | tuple[int, int],
        stride: int | tuple[int, int] = 1,
        padding: int | tuple[int, int] = 0,
        dilation: int | tuple[int, int] = 1,
        groups: int = 1,
        norm_layer: str | None = None,
        activation_fn: str | None = "ReLU",
    ):
        super().__init__()

        self.with_activation = activation_fn is not None
        self.with_bias = norm_layer is None
        self.with_norm = norm_layer is not None

        self.conv = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
            bias=self.with_bias,
        )
        self.activate = None
        self.norm = None

        if self.with_norm:
            self.norm = build_norm(norm_layer, out_channels)

        if self.with_activation:
            # Careful when adding activation functions: some should not be in-place
            self.activate = build_activation(activation_fn, inplace=True)

        self._init_weights()

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv(x)
        if self.with_norm:
            x = self.norm(x)
        if self.with_activation:
            x = self.activate(x)
        return x

    def _init_weights(self) -> None:
        """Same init as in <mmcv> convolutions"""
        nn.init.kaiming_normal_(self.conv.weight, a=0, nonlinearity="relu")
        if self.with_bias:
            nn.init.constant_(self.conv.bias, 0)

        if self.with_norm:
            nn.init.constant_(self.norm.weight, 1)
            nn.init.constant_(self.norm.bias, 0)


class DepthwiseSeparableConv(nn.Module):
    """Depth-wise separable convolution module used for CSPNeXT.

    Applies sequentially
      - a depth-wise conv
      - a point-wise conv

    Args:
        in_channels: Input channels of the convolution.
        out_channels: Output channels of the convolution.
        kernel_size: Convolution kernel size.
        stride: Convolution stride.
        padding: Convolution padding.
        dilation: Convolution dilation.
        norm_layer: Norm layer to apply, if any.
        activation_fn: Activation function to apply, if any.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int | tuple[int, int],
        stride: int | tuple[int, int] = 1,
        padding: int | tuple[int, int] = 0,
        dilation: int | tuple[int, int] = 1,
        norm_layer: str | None = None,
        activation_fn: str | None = "ReLU",
    ):
        super().__init__()

        # depthwise convolution
        self.depthwise_conv = CSPConvModule(
            in_channels,
            in_channels,
            kernel_size,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=in_channels,
            norm_layer=norm_layer,
            activation_fn=activation_fn,
        )

        self.pointwise_conv = CSPConvModule(
            in_channels,
            out_channels,
            kernel_size=1,
            norm_layer=norm_layer,
            activation_fn=activation_fn,
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.depthwise_conv(x)
        x = self.pointwise_conv(x)
        return x


class CSPNeXtBlock(nn.Module):
    """Basic bottleneck block used in CSPNeXt.

    Args:
        in_channels: input channels for the block
        out_channels: output channels for the block
        expansion: expansion factor for the hidden channels
        add_identity: add a skip-connection to the block
        kernel_size: kernel size for the DepthwiseSeparableConv
        norm_layer: Norm layer to apply, if any.
        activation_fn: Activation function to apply, if any.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        expansion: float = 0.5,
        add_identity: bool = True,
        kernel_size: int = 5,
        norm_layer: str | None = None,
        activation_fn: str | None = "ReLU",
    ) -> None:
        super().__init__()
        hidden_channels = int(out_channels * expansion)
        self.conv1 = CSPConvModule(
            in_channels,
            hidden_channels,
            3,
            stride=1,
            padding=1,
            norm_layer=norm_layer,
            activation_fn=activation_fn,
        )
        self.conv2 = DepthwiseSeparableConv(
            hidden_channels,
            out_channels,
            kernel_size,
            stride=1,
            padding=kernel_size // 2,
            norm_layer=norm_layer,
            activation_fn=activation_fn,
        )
        self.add_identity = add_identity and in_channels == out_channels

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward function."""
        identity = x
        out = self.conv1(x)
        out = self.conv2(out)

        if self.add_identity:
            return out + identity
        else:
            return out


class CSPLayer(nn.Module):
    """Cross Stage Partial Layer.

    Args:
        in_channels: input channels for the layer
        out_channels: output channels for the block
        expand_ratio: expansion factor for the mid-channels
        num_blocks: the number of blocks to use
        add_identity: add a skip-connection to the blocks
        channel_attention: whether to apply channel attention
        norm_layer: Norm layer to apply, if any.
        activation_fn: Activation function to apply, if any.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        expand_ratio: float = 0.5,
        num_blocks: int = 1,
        add_identity: bool = True,
        channel_attention: bool = False,
        norm_layer: str | None = None,
        activation_fn: str | None = "ReLU",
    ) -> None:
        super().__init__()
        mid_channels = int(out_channels * expand_ratio)
        self.channel_attention = channel_attention
        self.main_conv = CSPConvModule(
            in_channels,
            mid_channels,
            1,
            norm_layer=norm_layer,
            activation_fn=activation_fn,
        )
        self.short_conv = CSPConvModule(
            in_channels,
            mid_channels,
            1,
            norm_layer=norm_layer,
            activation_fn=activation_fn,
        )
        self.final_conv = CSPConvModule(
            2 * mid_channels,
            out_channels,
            1,
            norm_layer=norm_layer,
            activation_fn=activation_fn,
        )

        self.blocks = nn.Sequential(
            *[
                CSPNeXtBlock(
                    mid_channels,
                    mid_channels,
                    1.0,
                    add_identity,
                    norm_layer=norm_layer,
                    activation_fn=activation_fn,
                )
                for _ in range(num_blocks)
            ]
        )
        if channel_attention:
            self.attention = ChannelAttention(2 * mid_channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward function."""
        x_short = self.short_conv(x)

        x_main = self.main_conv(x)
        x_main = self.blocks(x_main)

        x_final = torch.cat((x_main, x_short), dim=1)

        if self.channel_attention:
            x_final = self.attention(x_final)
        return self.final_conv(x_final)


--- File: deeplabcut/pose_estimation_pytorch/models/modules/conv_block.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""The code is based on DEKR: https://github.com/HRNet/DEKR/tree/main"""
from __future__ import annotations

from abc import ABC, abstractmethod

import torch
import torch.nn as nn
import torchvision.ops as ops

from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry

BLOCKS = Registry("blocks", build_func=build_from_cfg)


class BaseBlock(ABC, nn.Module):
    """Abstract Base class for defining custom blocks.

    This class defines an abstract base class for creating custom blocks used in the HigherHRNet for Human Pose Estimation.

    Attributes:
        bn_momentum: Batch normalization momentum.

    Methods:
        forward(x): Abstract method for defining the forward pass of the block.
    """

    def __init__(self):
        super().__init__()
        self.bn_momentum = 0.1

    @abstractmethod
    def forward(self, x: torch.Tensor):
        """Abstract method for defining the forward pass of the block.

        Args:
            x: Input tensor.

        Returns:
            Output tensor.
        """
        pass

    def _init_weights(self, pretrained: str | None):
        """Method for initializing block weights from pretrained models.

        Args:
            pretrained: Path to pretrained model weights.
        """
        if pretrained:
            self.load_state_dict(torch.load(pretrained))


@BLOCKS.register_module
class BasicBlock(BaseBlock):
    """Basic Residual Block.

    This class defines a basic residual block used in HigherHRNet.

    Attributes:
        expansion: The expansion factor used in the block.

    Args:
        in_channels: Number of input channels.
        out_channels: Number of output channels.
        stride: Stride value for the convolutional layers. Default is 1.
        downsample: Downsample layer to be used in the residual connection. Default is None.
        dilation: Dilation rate for the convolutional layers. Default is 1.
    """

    expansion: int = 1

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        stride: int = 1,
        downsample: nn.Module | None = None,
        dilation: int = 1,
    ):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=3,
            stride=stride,
            padding=dilation,
            bias=False,
            dilation=dilation,
        )
        self.bn1 = nn.BatchNorm2d(out_channels, momentum=self.bn_momentum)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(
            in_channels,
            out_channels,
            kernel_size=3,
            stride=stride,
            padding=dilation,
            bias=False,
            dilation=dilation,
        )
        self.bn2 = nn.BatchNorm2d(out_channels, momentum=self.bn_momentum)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the BasicBlock.

        Args:
            x: Input tensor.

        Returns:
            Output tensor.
        """
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


@BLOCKS.register_module
class Bottleneck(BaseBlock):
    """Bottleneck Residual Block.

    This class defines a bottleneck residual block used in HigherHRNet.

    Attributes:
        expansion: The expansion factor used in the block.

    Args:
        in_channels: Number of input channels.
        out_channels: Number of output channels.
        stride: Stride value for the convolutional layers. Default is 1.
        downsample: Downsample layer to be used in the residual connection. Default is None.
        dilation: Dilation rate for the convolutional layers. Default is 1.
    """

    expansion: int = 4

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        stride: int = 1,
        downsample: nn.Module | None = None,
        dilation: int = 1,
    ):
        super(Bottleneck, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels, momentum=self.bn_momentum)
        self.conv2 = nn.Conv2d(
            out_channels,
            out_channels,
            kernel_size=3,
            stride=stride,
            padding=dilation,
            bias=False,
            dilation=dilation,
        )
        self.bn2 = nn.BatchNorm2d(out_channels, momentum=self.bn_momentum)
        self.conv3 = nn.Conv2d(
            out_channels, out_channels * self.expansion, kernel_size=1, bias=False
        )
        self.bn3 = nn.BatchNorm2d(
            out_channels * self.expansion, momentum=self.bn_momentum
        )
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the Bottleneck block.

        Args:
            x : Input tensor.

        Returns:
            Output tensor.
        """
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


@BLOCKS.register_module
class AdaptBlock(BaseBlock):
    """Adaptive Residual Block with Deformable Convolution.

    This class defines an adaptive residual block with deformable convolution used in HigherHRNet.

    Attributes:
        expansion: The expansion factor used in the block.

    Args:
        in_channels: Number of input channels.
        out_channels: Number of output channels.
        stride: Stride value for the convolutional layers. Default is 1.
        downsample: Downsample layer to be used in the residual connection. Default is None.
        dilation: Dilation rate for the convolutional layers. Default is 1.
        deformable_groups: Number of deformable groups in the deformable convolution. Default is 1.
    """

    expansion: int = 1

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        stride: int = 1,
        downsample: nn.Module | None = None,
        dilation: int = 1,
        deformable_groups: int = 1,
    ):
        super(AdaptBlock, self).__init__()
        regular_matrix = torch.tensor(
            [[-1, -1, -1, 0, 0, 0, 1, 1, 1], [-1, 0, 1, -1, 0, 1, -1, 0, 1]]
        )
        self.register_buffer("regular_matrix", regular_matrix.float())
        self.downsample = downsample
        self.transform_matrix_conv = nn.Conv2d(in_channels, 4, 3, 1, 1, bias=True)
        self.translation_conv = nn.Conv2d(in_channels, 2, 3, 1, 1, bias=True)
        self.adapt_conv = ops.DeformConv2d(
            in_channels,
            out_channels,
            kernel_size=3,
            stride=stride,
            padding=dilation,
            dilation=dilation,
            bias=False,
            groups=deformable_groups,
        )
        self.bn = nn.BatchNorm2d(out_channels, momentum=self.bn_momentum)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward pass through the AdaptBlock.

        Args:
            x: Input tensor.

        Returns:
            Output tensor.
        """
        residual = x

        N, _, H, W = x.shape
        transform_matrix = self.transform_matrix_conv(x)
        transform_matrix = transform_matrix.permute(0, 2, 3, 1).reshape(
            (N * H * W, 2, 2)
        )
        offset = torch.matmul(transform_matrix, self.regular_matrix)
        offset = offset - self.regular_matrix
        offset = offset.transpose(1, 2).reshape((N, H, W, 18)).permute(0, 3, 1, 2)

        translation = self.translation_conv(x)
        offset[:, 0::2, :, :] += translation[:, 0:1, :, :]
        offset[:, 1::2, :, :] += translation[:, 1:2, :, :]

        out = self.adapt_conv(x, offset)
        out = self.bn(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out


--- File: deeplabcut/pose_estimation_pytorch/models/modules/conv_module.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""The code is based on DEKR: https://github.com/HRNet/DEKR/tree/main"""
import logging
from typing import List

import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.models.modules import BasicBlock

BN_MOMENTUM = 0.1
logger = logging.getLogger(__name__)


class HighResolutionModule(nn.Module):
    """High-Resolution Module.

    This class implements the High-Resolution Module used in HigherHRNet for Human Pose Estimation.

    Args:
        num_branches: Number of branches in the module.
        block: The block type used in each branch of the module.
        num_blocks: List containing the number of blocks in each branch.
        num_inchannels: List containing the number of input channels for each branch.
        num_channels: List containing the number of output channels for each branch.
        fuse_method: The fusion method used in the module.
        multi_scale_output: Whether to output multi-scale features. Default is True.
    """

    def __init__(
        self,
        num_branches: int,
        block: BasicBlock,
        num_blocks: int,
        num_inchannels: int,
        num_channels: int,
        fuse_method: str,
        multi_scale_output: bool = True,
    ):
        super(HighResolutionModule, self).__init__()
        self._check_branches(
            num_branches, block, num_blocks, num_inchannels, num_channels
        )

        self.num_inchannels = num_inchannels
        self.fuse_method = fuse_method
        self.num_branches = num_branches

        self.multi_scale_output = multi_scale_output

        self.branches = self._make_branches(
            num_branches, block, num_blocks, num_channels
        )
        self.fuse_layers = self._make_fuse_layers()
        self.relu = nn.ReLU(True)

    def _check_branches(
        self,
        num_branches: int,
        block: BasicBlock,
        num_blocks: int,
        num_inchannels: int,
        num_channels: int,
    ):
        if num_branches != len(num_blocks):
            error_msg = "NUM_BRANCHES({}) <> NUM_BLOCKS({})".format(
                num_branches, len(num_blocks)
            )
            logger.error(error_msg)
            raise ValueError(error_msg)

        if num_branches != len(num_channels):
            error_msg = "NUM_BRANCHES({}) <> NUM_CHANNELS({})".format(
                num_branches, len(num_channels)
            )
            logger.error(error_msg)
            raise ValueError(error_msg)

        if num_branches != len(num_inchannels):
            error_msg = "NUM_BRANCHES({}) <> NUM_INCHANNELS({})".format(
                num_branches, len(num_inchannels)
            )
            logger.error(error_msg)
            raise ValueError(error_msg)

    def _make_one_branch(
        self,
        branch_index: int,
        block: BasicBlock,
        num_blocks: int,
        num_channels: int,
        stride: int = 1,
    ) -> nn.Sequential:
        downsample = None
        if (
            stride != 1
            or self.num_inchannels[branch_index]
            != num_channels[branch_index] * block.expansion
        ):
            downsample = nn.Sequential(
                nn.Conv2d(
                    self.num_inchannels[branch_index],
                    num_channels[branch_index] * block.expansion,
                    kernel_size=1,
                    stride=stride,
                    bias=False,
                ),
                nn.BatchNorm2d(
                    num_channels[branch_index] * block.expansion, momentum=BN_MOMENTUM
                ),
            )

        layers = []
        layers.append(
            block(
                self.num_inchannels[branch_index],
                num_channels[branch_index],
                stride,
                downsample,
            )
        )
        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion
        for i in range(1, num_blocks[branch_index]):
            layers.append(
                block(self.num_inchannels[branch_index], num_channels[branch_index])
            )

        return nn.Sequential(*layers)

    def _make_branches(
        self, num_branches: int, block: BasicBlock, num_blocks: int, num_channels: int
    ) -> nn.ModuleList:
        branches = []

        for i in range(num_branches):
            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))

        return nn.ModuleList(branches)

    def _make_fuse_layers(self) -> nn.ModuleList:
        if self.num_branches == 1:
            return None

        num_branches = self.num_branches
        num_inchannels = self.num_inchannels
        fuse_layers = []
        for i in range(num_branches if self.multi_scale_output else 1):
            fuse_layer = []
            for j in range(num_branches):
                if j > i:
                    fuse_layer.append(
                        nn.Sequential(
                            nn.Conv2d(
                                num_inchannels[j],
                                num_inchannels[i],
                                1,
                                1,
                                0,
                                bias=False,
                            ),
                            nn.BatchNorm2d(num_inchannels[i]),
                            nn.Upsample(scale_factor=2 ** (j - i), mode="nearest"),
                        )
                    )
                elif j == i:
                    fuse_layer.append(None)
                else:
                    conv3x3s = []
                    for k in range(i - j):
                        if k == i - j - 1:
                            num_outchannels_conv3x3 = num_inchannels[i]
                            conv3x3s.append(
                                nn.Sequential(
                                    nn.Conv2d(
                                        num_inchannels[j],
                                        num_outchannels_conv3x3,
                                        3,
                                        2,
                                        1,
                                        bias=False,
                                    ),
                                    nn.BatchNorm2d(num_outchannels_conv3x3),
                                )
                            )
                        else:
                            num_outchannels_conv3x3 = num_inchannels[j]
                            conv3x3s.append(
                                nn.Sequential(
                                    nn.Conv2d(
                                        num_inchannels[j],
                                        num_outchannels_conv3x3,
                                        3,
                                        2,
                                        1,
                                        bias=False,
                                    ),
                                    nn.BatchNorm2d(num_outchannels_conv3x3),
                                    nn.ReLU(True),
                                )
                            )
                    fuse_layer.append(nn.Sequential(*conv3x3s))
            fuse_layers.append(nn.ModuleList(fuse_layer))

        return nn.ModuleList(fuse_layers)

    def get_num_inchannels(self) -> int:
        return self.num_inchannels

    def forward(self, x) -> List:
        """Forward pass through the HighResolutionModule.

        Args:
            x: List of input tensors for each branch.

        Returns:
            List of output tensors after processing through the module.
        """
        if self.num_branches == 1:
            return [self.branches[0](x[0])]

        for i in range(self.num_branches):
            x[i] = self.branches[i](x[i])

        x_fuse = []

        for i in range(len(self.fuse_layers)):
            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])
            for j in range(1, self.num_branches):
                if i == j:
                    y = y + x[j]
                else:
                    y = y + self.fuse_layers[i][j](x[j])
            x_fuse.append(self.relu(y))

        return x_fuse


--- File: deeplabcut/pose_estimation_pytorch/models/criterions/kl_discrete.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""SimCC Discrete KL Divergence loss with Gaussian Label Smoothing.

Can be used for SimCC-type heads. Modified from the `mmpose` implementation. For more
details, see <https://github.com/open-mmlab/mmpose>.
"""
import torch
import torch.nn as nn
import torch.nn.functional as F

from deeplabcut.pose_estimation_pytorch.models.criterions.base import (
    BaseCriterion,
    CRITERIONS,
)


@CRITERIONS.register_module
class KLDiscreteLoss(BaseCriterion):
    """KLDiscrete loss

    Args:
        beta: Temperature for the softmax.
        label_softmax: Use softmax on the labels.
        label_beta: Temperature for the softmax on the labels.
        use_target_weight: Allows the use a weighted loss for different joints.
        mask: Indices of masked keypoints.
        mask_weight: Weight for masked keypoints.
    """

    def __init__(
        self,
        beta: float = 1.0,
        label_softmax: bool = False,
        label_beta: float = 10.0,
        use_target_weight: bool = True,
        mask: list[int] | None = None,
        mask_weight: float = 1.0,
    ):
        super().__init__()
        self.beta = beta
        self.label_softmax = label_softmax
        self.label_beta = label_beta
        self.use_target_weight = use_target_weight
        self.mask = mask
        self.mask_weight = mask_weight

        self.log_softmax = nn.LogSoftmax(dim=1)
        self.kl_loss = nn.KLDivLoss(reduction="none")

    def forward(
        self,
        output: torch.Tensor,
        target: torch.Tensor,
        weights: torch.Tensor | float = 1.0,
        **kwargs,
    ) -> torch.Tensor:
        n, k, _ = output.shape
        if self.use_target_weight and isinstance(weights, torch.Tensor):
            weight = weights.reshape(-1)
        else:
            weight = 1.0

        pred = output.reshape(-1, output.size(-1))
        target = target.reshape(-1, target.size(-1))
        loss = self.criterion(pred, target).mul(weight)
        if self.mask is not None:
            loss = loss.reshape(n, k)
            loss[:, self.mask] = loss[:, self.mask] * self.mask_weight

        return loss.sum() / k

    def criterion(self, dec_outs, labels):
        log_pt = self.log_softmax(dec_outs * self.beta)
        if self.label_softmax:
            labels = F.softmax(labels * self.label_beta, dim=1)
        loss = torch.mean(self.kl_loss(log_pt, labels), dim=1)
        return loss


--- File: deeplabcut/pose_estimation_pytorch/models/criterions/weighted.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from deeplabcut.pose_estimation_pytorch.models.criterions import utils
from deeplabcut.pose_estimation_pytorch.models.criterions.base import (
    BaseCriterion,
    CRITERIONS,
)


class WeightedCriterion(BaseCriterion):
    """Base class for weighted criterions"""

    def __init__(self, criterion: nn.Module):
        super().__init__()
        self.criterion = criterion

    def forward(
        self,
        output: torch.Tensor,
        target: torch.Tensor,
        weights: torch.Tensor | float = 1.0,
        **kwargs,
    ) -> torch.Tensor:
        """
        Args:
            output: predicted tensor
            target: target tensor
            weights: weights for each element in the loss calculation. If a float,
                weights all elements by that value. Defaults to 1.

        Returns:
            the weighted loss
        """
        # shape of loss: (batch_size, n_kpts, heatmap_size, heatmap_size)
        loss = self.criterion(output, target)
        n_elems = utils.count_nonzero_elems(loss, weights)
        if n_elems == 0:
            n_elems = 1

        return torch.sum(loss * weights) / n_elems


@CRITERIONS.register_module
class WeightedMSECriterion(WeightedCriterion):
    """
    Weighted Mean Squared Error (MSE) Loss.

    This loss computes the Mean Squared Error between the prediction and target tensors,
    but it also incorporates weights to adjust the contribution of each element in the loss
    calculation. The loss is computed element-wise, and elements with a weight of 0 (masked items)
    are excluded from the loss calculation.
    """

    def __init__(self) -> None:
        super().__init__(nn.MSELoss(reduction="none"))

    def forward(
        self,
        output: torch.Tensor,
        target: torch.Tensor,
        weights: torch.Tensor | float = 1.0,
        **kwargs,
    ) -> torch.Tensor:
        """
        Args:
            output: predicted tensor
            target: target tensor
            weights: weights for each element in the loss calculation. If a float,
                weights all elements by that value. Defaults to 1.

        Returns:
            the weighted loss
        """
        # shape of loss: (batch_size, n_kpts, h, w)
        loss = self.criterion(output, target)
        n_elems = utils.count_nonzero_elems(loss, weights)
        if n_elems == 0:
            n_elems = 1

        return torch.sum(loss * weights) / n_elems


@CRITERIONS.register_module
class WeightedHuberCriterion(WeightedCriterion):
    """
    Weighted Huber Loss.

    This loss computes the Huber loss between the prediction and target tensors,
    but it also incorporates weights to adjust the contribution of each element in the loss
    calculation. The loss is computed element-wise, and elements with a weight of 0 are
    excluded from the loss calculation.
    """

    def __init__(self) -> None:
        super().__init__(nn.HuberLoss(reduction="none"))


@CRITERIONS.register_module
class WeightedBCECriterion(WeightedCriterion):
    """
    Weighted Binary Cross Entropy (BCE) Loss.

    This loss computes the Binary Cross Entropy loss between the prediction and target tensors,
    but it also incorporates weights to adjust the contribution of each element in the loss
    calculation. The loss is computed element-wise, and elements with a weight of 0 are
    excluded from the loss calculation.
    """

    def __init__(self) -> None:
        super().__init__(nn.BCEWithLogitsLoss(reduction="none"))


--- File: deeplabcut/pose_estimation_pytorch/models/criterions/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.models.criterions.aggregators import (
    WeightedLossAggregator,
)
from deeplabcut.pose_estimation_pytorch.models.criterions.base import (
    CRITERIONS,
    LOSS_AGGREGATORS,
    BaseCriterion,
    BaseLossAggregator,
)
from deeplabcut.pose_estimation_pytorch.models.criterions.dekr import (
    DEKRHeatmapLoss,
    DEKROffsetLoss,
)
from deeplabcut.pose_estimation_pytorch.models.criterions.kl_discrete import (
    KLDiscreteLoss,
)
from deeplabcut.pose_estimation_pytorch.models.criterions.weighted import (
    WeightedBCECriterion,
    WeightedHuberCriterion,
    WeightedMSECriterion,
)


--- File: deeplabcut/pose_estimation_pytorch/models/criterions/aggregators.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import torch

from deeplabcut.pose_estimation_pytorch.models.criterions.base import (
    BaseLossAggregator,
    LOSS_AGGREGATORS,
)


@LOSS_AGGREGATORS.register_module
class WeightedLossAggregator(BaseLossAggregator):
    def __init__(self, weights: dict[str, float]) -> None:
        super().__init__()
        self.weights = weights

    def forward(self, losses: dict[str, torch.Tensor]) -> torch.Tensor:
        weighted_losses = [
            weight * losses[loss_name] for loss_name, weight in self.weights.items()
        ]
        return torch.mean(torch.stack(weighted_losses))


--- File: deeplabcut/pose_estimation_pytorch/models/criterions/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import torch


def count_nonzero_elems(
    losses: torch.Tensor, weights: float | torch.Tensor, per_batch: bool = False
):
    """
    Compute the number of elements in the loss function induced by `weights`.
    This is a torch implementation of https://github.com/tensorflow/tensorflow/blob/4dacf3f368eb7965e9b5c3bbdd5193986081c3b2/tensorflow/python/ops/losses/losses_impl.py#L89

    Args:
        losses (Tensor): Tensor of shape [batch_size, d1, ... dN].
        weights (Tensor): Tensor of shape [], [batch_size] or [batch_size, d1, ... dK], where K < N.
        per_batch (bool): Whether to return the number of elements per batch or as a sum total.

    Returns:
        Tensor: The number of present (non-zero) elements in the losses tensor.
    """
    if isinstance(weights, float):
        if weights != 0.0:
            return losses.numel()
        else:
            return torch.tensor(0)

    weights = torch.as_tensor(weights, dtype=torch.float32)

    # Check for non-zero weights and broadcast to match losses
    present = torch.where(
        weights == 0.0, torch.zeros_like(weights), torch.ones_like(weights)
    )
    present = present.expand_as(losses)

    # Reduce sum across the desired dimensions
    if per_batch:
        reduction_dims = tuple(range(1, present.dim()))
        return torch.sum(present, dim=reduction_dims, keepdim=True)
    else:
        return torch.sum(present)


--- File: deeplabcut/pose_estimation_pytorch/models/criterions/dekr.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Loss criterions for DEKR models"""
from __future__ import annotations

import torch

from deeplabcut.pose_estimation_pytorch.models.criterions.base import (
    BaseCriterion,
    CRITERIONS,
)


@CRITERIONS.register_module
class DEKRHeatmapLoss(BaseCriterion):
    """DEKR Heatmap loss"""

    def forward(
        self,
        output: torch.Tensor,
        target: torch.Tensor,
        weights: torch.Tensor | float = 1.0,
        **kwargs,
    ) -> torch.Tensor:
        """
        Args:
            output: the output from which to compute the loss
            target: the target for the loss
            weights: the weights for the loss

        Returns:
            the DEKR offset loss
        """
        assert output.size() == target.size()
        loss = ((output - target) ** 2) * weights
        return loss.mean(dim=3).mean(dim=2).mean(dim=1).mean(dim=0)


@CRITERIONS.register_module
class DEKROffsetLoss(BaseCriterion):
    """DEKR Offset loss"""

    def __init__(self, beta: float = 1 / 9):
        super().__init__()
        self.beta = beta

    def smooth_l1_loss(self, pred, gt):
        l1_loss = torch.abs(pred - gt)
        return torch.where(
            l1_loss < self.beta,
            0.5 * l1_loss ** 2 / self.beta,
            l1_loss - 0.5 * self.beta,
        )

    def forward(
        self,
        output: torch.Tensor,
        target: torch.Tensor,
        weights: torch.Tensor | float = 1.0,
        **kwargs,
    ) -> torch.Tensor:
        """
        Args:
            output: the output from which to compute the loss
            target: the target for the loss
            weights: the weights for the loss

        Returns:
            the DEKR offset loss
        """
        assert output.size() == target.size()
        num_pos = torch.nonzero(weights > 0).size()[0]
        loss = self.smooth_l1_loss(output, target) * weights
        if num_pos == 0:
            num_pos = 1.0
        loss = loss.sum() / num_pos
        return loss


--- File: deeplabcut/pose_estimation_pytorch/models/criterions/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from abc import ABC, abstractmethod

import torch
import torch.nn as nn

from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry

LOSS_AGGREGATORS = Registry("loss_aggregators", build_func=build_from_cfg)
CRITERIONS = Registry("criterions", build_func=build_from_cfg)


class BaseCriterion(ABC, nn.Module):
    def __init__(self) -> None:
        super().__init__()

    @abstractmethod
    def forward(
        self, output: torch.Tensor, target: torch.Tensor, **kwargs
    ) -> torch.Tensor:
        """
        Args:
            output: the output from which to compute the loss
            target: the target for the loss

        Returns:
            the different losses for the module, including one "total_loss" key which
            is the loss from which to start backpropagation
        """
        raise NotImplementedError


class BaseLossAggregator(ABC, nn.Module):
    @abstractmethod
    def forward(self, losses: dict[str, torch.Tensor]) -> torch.Tensor:
        """
        Args:
            losses: the losses to aggregate

        Returns:
            the aggregate loss
        """
        raise NotImplementedError


--- File: deeplabcut/pose_estimation_pytorch/modelzoo/config.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Methods to create the configuration files to fine-tune SuperAnimal models"""
from __future__ import annotations

import os
from pathlib import Path

from ruamel.yaml import YAML

import deeplabcut.pose_estimation_pytorch.config.utils as config_utils
import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.core.config import (
    read_config_as_dict,
    write_config,
)
from deeplabcut.core.engine import Engine
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import (
    get_super_animal_model_config_path,
    get_super_animal_project_config_path,
)
from deeplabcut.pose_estimation_pytorch.task import Task


def make_super_animal_finetune_config(
    weight_init: WeightInitialization,
    project_config: dict,
    pose_config_path: str | Path,
    model_name: str,
    detector_name: str | None,
    save: bool = False,
) -> dict:
    """
    Creates a PyTorch pose configuration file to finetune a SuperAnimal model on a
    downstream project.

    Args:
        weight_init: The weight initialization configuration.
        project_config: The project configuration.
        pose_config_path: The path where the pose configuration file will be saved
        model_name: The type of neural net to finetune.
        detector_name: The type of detector to use for the SuperAnimal model. If None is
            given, the model will be set to a Bottom-Up framework.
        save: Whether to save the model configuration file to the ``pose_config_path``.

    Returns:
        The generated pose configuration file.

    Raises:
        ValueError: If `weight_init.with_decoder = False`. This method only creates
            configs to fine-tune SuperAnimal models. Call `make_pytorch_pose_config`
            to create configuration files for transfer learning.
    """
    bodyparts = af.get_bodyparts(project_config)
    if weight_init.dataset is None:
        raise ValueError(
            "You must set the ``WeightInitialization.dataset`` when fine-tuning "
            "SuperAnimal models."
        )

    if not weight_init.with_decoder:
        raise ValueError(
            "Can only call ``make_super_animal_finetune_config`` when "
            f" `with_decoder=True`, but you had {weight_init}. Please set "
            "`with_decoder=True` to fine-tune a model or call "
            "`make_pytorch_pose_config` to create a transfer learning "
            "pose configuration file."
        )

    converted_bodyparts = bodyparts
    if weight_init.bodyparts is not None:
        assert len(weight_init.bodyparts) == len(weight_init.conversion_array)
        converted_bodyparts = weight_init.bodyparts
    elif len(bodyparts) != len(weight_init.conversion_array):
        raise ValueError(
            "You don't have the same number of bodyparts in your project config as "
            f"number of entries your conversion array ({bodyparts} vs "
            f"{weight_init.conversion_array}). If you're fine-tuning from "
            "SuperAnimal on a subset of your bodyparts, you must specify which "
            "ones in `WeightInitialization.bodyparts`. This should be done "
            "automatically when creating the `weight_init` with "
            "`WeightInitialization.build`."
        )

    # Load the exact pose configuration file for the model to fine-tune
    pose_config = create_config_from_modelzoo(
        super_animal=weight_init.dataset,
        model_name=model_name,
        detector_name=detector_name,
        converted_bodyparts=converted_bodyparts,
        weight_init=weight_init,
        project_config=project_config,
        pose_config_path=pose_config_path,
    )
    if save:
        write_config(pose_config_path, pose_config, overwrite=True)

    return pose_config


def create_config_from_modelzoo(
    super_animal: str,
    model_name: str,
    detector_name: str | None,
    converted_bodyparts: list[str],
    weight_init: WeightInitialization,
    project_config: dict,
    pose_config_path: str | Path,
) -> dict:
    """Creates a model configuration file to fine-tune a SuperAnimal model

    Args:
        super_animal: The SuperAnimal dataset on which the model was trained.
        model_name: The type of neural net to finetune.
        detector_name: The type of detector to use for the SuperAnimal model. If None is
            given, the model will be set to a Bottom-Up framework.
        converted_bodyparts: The project bodyparts that the model will learn.
        weight_init: The weight initialization to use.
        project_config: The project configuration.
        pose_config_path: The path where the pose configuration file will be saved.

    Returns:
        The generated pose configuration file.
    """
    # load the model configuration
    model_cfg = read_config_as_dict(
        get_super_animal_model_config_path(model_name)
    )
    if detector_name is None:
        model_cfg["method"] = Task.BOTTOM_UP.aliases[0].lower()
        # Use default bottom-up image augmentation if no detector is given (the collate
        # function might be needed).
        config_dir = config_utils.get_config_folder_path()
        aug = read_config_as_dict(config_dir / "base" / "aug_default.yaml")
        model_cfg["data"]["train"] = aug["train"]
    else:
        model_cfg["method"] = Task.TOP_DOWN.aliases[0].lower()
        model_cfg["detector"] = read_config_as_dict(
            get_super_animal_model_config_path(detector_name)
        )

    # use SuperAnimal bodyparts
    if weight_init.memory_replay:
        super_animal_project_config = read_config_as_dict(
            get_super_animal_project_config_path(super_animal)
        )
        converted_bodyparts = super_animal_project_config["bodyparts"]

    model_cfg["net_type"] = model_name
    model_cfg["metadata"] = {
        "project_path": project_config["project_path"],
        "pose_config_path": str(pose_config_path),
        "bodyparts": converted_bodyparts,
        "unique_bodyparts": [],
        "individuals": project_config.get("individuals", ["animal"]),
        "with_identity": False,
    }

    model_cfg["model"] = config_utils.replace_default_values(
        model_cfg["model"], num_bodyparts=len(converted_bodyparts)
    )
    model_cfg["train_settings"]["weight_init"] = weight_init.to_dict()

    # sort first-level keys to make it prettier
    return dict(sorted(model_cfg.items()))


def write_pytorch_config_for_memory_replay(config_path, shuffle, pytorch_config):
    cfg = af.read_config(config_path)
    trainIndex = 0
    dlc_proj_root = Path(config_path).parent
    model_folder = dlc_proj_root / af.get_model_folder(
        cfg["TrainingFraction"][trainIndex], shuffle, cfg, engine=Engine.PYTORCH
    )
    os.makedirs(model_folder / "train", exist_ok=True)
    out_path = model_folder / "train" / "pytorch_config.yaml"
    with open(str(out_path), "w") as f:
        yaml = YAML()
        yaml.dump(pytorch_config, f)


--- File: deeplabcut/pose_estimation_pytorch/modelzoo/train_from_coco.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""File to train a model on a COCO dataset"""

from __future__ import annotations

import copy
from pathlib import Path

from deeplabcut.pose_estimation_pytorch import COCOLoader, utils
from deeplabcut.pose_estimation_pytorch.apis.training import train
from deeplabcut.pose_estimation_pytorch.runners.logger import setup_file_logging
from deeplabcut.pose_estimation_pytorch.task import Task


def adaptation_train(
    project_root: str | Path,
    model_folder: str | Path,
    train_file: str,
    test_file: str,
    model_config_path: str | Path,
    device: str | None,
    epochs: int | None,
    save_epochs: int | None,
    detector_epochs: int | None,
    detector_save_epochs: int | None,
    snapshot_path: str | None,
    detector_path: str | None,
    batch_size: int = 8,
    detector_batch_size: int = 8,
    eval_interval: int | None = None,
):
    setup_file_logging(Path(model_folder) / "log.txt")
    loader = COCOLoader(
        project_root=project_root,
        model_config_path=model_config_path,
        train_json_filename=train_file,
        test_json_filename=test_file,
    )

    utils.fix_seeds(loader.model_cfg["train_settings"]["seed"])

    updates = {
        "detector.model.freeze_bn_stats": True,
        "detector.runner.snapshots.max_snapshots": 5,
        "detector.runner.snapshots.save_epochs": detector_save_epochs or 1,
        "detector.train_settings.batch_size": detector_batch_size,
        "detector.train_settings.epochs": detector_epochs or 4,
        "model.backbone.freeze_bn_stats": True,
        "runner.snapshots.max_snapshots": 5,
        "runner.snapshots.save_epochs": save_epochs or 1,
        "train_settings.batch_size": batch_size,
        "train_settings.epochs": epochs or 4,
    }

    if eval_interval is not None:
        updates["runner.eval_interval"] = eval_interval

    loader.update_model_cfg(updates)

    pose_task = Task(loader.model_cfg["method"])
    if pose_task == Task.TOP_DOWN:
        logger_config = None
        if loader.model_cfg.get("logger"):
            logger_config = copy.deepcopy(loader.model_cfg["logger"])
            logger_config["run_name"] += "-detector"

        if loader.model_cfg["detector"]["train_settings"]["epochs"] > 0:
            train(
                loader=loader,
                run_config=loader.model_cfg["detector"],
                task=Task.DETECT,
                device=device,
                logger_config=logger_config,
                snapshot_path=detector_path,
            )

    train(
        loader=loader,
        run_config=loader.model_cfg,
        task=pose_task,
        device=device,
        logger_config=loader.model_cfg.get("logger"),
        snapshot_path=snapshot_path,
    )


--- File: deeplabcut/pose_estimation_pytorch/modelzoo/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import (
    download_super_animal_snapshot,
    get_snapshot_folder_path,
    get_super_animal_model_config_path,
    get_super_animal_project_config_path,
    get_super_animal_snapshot_path,
    load_super_animal_config,
)


--- File: deeplabcut/pose_estimation_pytorch/modelzoo/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import inspect
import subprocess
import warnings
from pathlib import Path

import torch
from dlclibrary import download_huggingface_model

import deeplabcut.pose_estimation_pytorch.config.utils as config_utils
from deeplabcut.core.config import read_config_as_dict
from deeplabcut.pose_estimation_pytorch.config.make_pose_config import add_metadata
from deeplabcut.utils import auxiliaryfunctions


def get_model_configs_folder_path() -> Path:
    """Returns: the folder containing the SuperAnimal model configuration files"""
    return Path(auxiliaryfunctions.get_deeplabcut_path()) / "modelzoo" / "model_configs"


def get_project_configs_folder_path() -> Path:
    """Returns: the folder containing the SuperAnimal project configuration files"""
    return (
        Path(auxiliaryfunctions.get_deeplabcut_path()) / "modelzoo" / "project_configs"
    )


def get_snapshot_folder_path() -> Path:
    """Returns: the path to the folder containing the SuperAnimal model snapshots"""
    return Path(auxiliaryfunctions.get_deeplabcut_path()) / "modelzoo" / "checkpoints"


def get_super_animal_model_config_path(model_name: str) -> Path:
    """Gets the path to the configuration file for a SuperAnimal model.

    Args:
        model_name: The name of the model for which to get the path.

    Returns:
        The path to the config file for a SuperAnimal model.
    """
    return get_model_configs_folder_path() / f"{model_name}.yaml"


def get_super_animal_project_config_path(super_animal: str) -> Path:
    """Gets the path to a SuperAnimal project configuration file.

    Args:
        super_animal: The name of the SuperAnimal for which to get the config path.

    Returns:
        The path to the config file for a SuperAnimal project.
    """
    return get_project_configs_folder_path() / f"{super_animal}.yaml"


def get_super_animal_snapshot_path(
    dataset: str,
    model_name: str,
    download: bool = True,
) -> Path:
    """Gets the path to the snapshot containing SuperAnimal model weights.

    Args:
        dataset: The name of the SuperAnimal dataset.
        model_name: The name of the model.
        download: Whether to download the weights if they aren't already there.

    Returns:
        The path to the weights for a SuperAnimal model.
    """
    model_path = get_snapshot_folder_path() / f"{dataset}_{model_name}.pt"
    if download and not model_path.exists():
        download_super_animal_snapshot(dataset, model_name)

    return model_path


def load_super_animal_config(
    super_animal: str,
    model_name: str,
    detector_name: str | None = None,
    max_individuals: int = 30,
    device: str | None = None,
) -> dict:
    """Loads the model configuration file for a model, detector and SuperAnimal

    Args:
        super_animal: The name of the SuperAnimal for which to create the model config.
        model_name: The name of the model for which to create the model config.
        detector_name: The name of the detector for which to create the model config.
        max_individuals: The maximum number of detections to make in an image
        device: The device to use to train/run inference on the model

    Returns:
        The model configuration for a SuperAnimal-pretrained model.
    """
    project_cfg_path = get_super_animal_project_config_path(super_animal=super_animal)
    project_config = read_config_as_dict(project_cfg_path)

    model_cfg_path = get_super_animal_model_config_path(model_name=model_name)
    model_config = read_config_as_dict(model_cfg_path)
    model_config = add_metadata(project_config, model_config, model_cfg_path)
    model_config = update_config(model_config, max_individuals, device)

    if detector_name is None:
        model_config["method"] = "BU"
    else:
        detector_cfg_path = get_super_animal_model_config_path(model_name=detector_name)
        detector_cfg = read_config_as_dict(detector_cfg_path)
        model_config["method"] = "TD"
        model_config["detector"] = detector_cfg
    return model_config


def download_super_animal_snapshot(dataset: str, model_name: str) -> Path:
    """Downloads a SuperAnimal snapshot

    Args:
        dataset: The name of the SuperAnimal dataset for which to download a snapshot.
        model_name: The name of the model for which to download a snapshot.

    Returns:
        The path to the downloaded snapshot.

    Raises:
        RuntimeError if the model fails to download.
    """
    snapshot_dir = get_snapshot_folder_path()
    model_name = f"{dataset}_{model_name}"
    model_path = snapshot_dir / f"{model_name}.pt"

    download_huggingface_model(model_name, target_dir=str(snapshot_dir))
    if not model_path.exists():
        raise RuntimeError(f"Failed to download {model_name} to {model_path}")

    return snapshot_dir / f"{model_name}.pt"


def get_gpu_memory_map():
    """Get the current gpu usage."""
    result = subprocess.check_output(
        ["nvidia-smi", "--query-gpu=memory.free", "--format=csv,nounits,noheader"],
        encoding="utf-8",
    )
    gpu_memory = [int(x) for x in result.strip().split("\n")]
    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))

    return gpu_memory_map


def select_device():
    if torch.cuda.is_available():
        return torch.device(f"cuda:0")
    else:
        return torch.device("cpu")


def raise_warning_if_called_directly():
    current_frame = inspect.currentframe()
    caller_frame = inspect.getouterframes(current_frame, 2)
    caller_name = caller_frame[1].filename

    if not "pose_estimation_" in caller_name:
        warnings.warn(
            f"{caller_name} is intended for internal use only and should not be called directly.",
            UserWarning,
        )


def update_config(config: dict, max_individuals: int, device: str):
    """Loads the model configuration file for a model, detector and SuperAnimal

    Args:
        config: The default model configuration file.
        max_individuals: The maximum number of detections to make in an image
        device: The device to use to train/run inference on the model

    Returns:
        The model configuration for a SuperAnimal-pretrained model.
    """
    config = config_utils.replace_default_values(
        config,
        num_bodyparts=len(config["metadata"]["bodyparts"]),
        num_individuals=max_individuals,
        backbone_output_channels=config["model"]["backbone_output_channels"],
    )
    config["metadata"]["individuals"] = [f"animal{i}" for i in range(max_individuals)]

    config["device"] = device
    if "detector" in config:
        config["detector"]["device"] = device

    return config


--- File: deeplabcut/pose_estimation_pytorch/modelzoo/memory_replay.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import json
import os
from collections import defaultdict
from pathlib import Path

import numpy as np
from scipy.optimize import linear_sum_assignment
from scipy.spatial import distance

import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.modelzoo.generalized_data_converter.datasets import (
    COCOPoseDataset,
    MaDLCPoseDataset,
    SingleDLCPoseDataset,
)
from deeplabcut.pose_estimation_pytorch.apis.utils import get_inference_runners
from deeplabcut.pose_estimation_pytorch.data.dlcloader import DLCLoader
from deeplabcut.pose_estimation_pytorch.modelzoo import (
    get_super_animal_project_config_path,
)
from deeplabcut.utils.pseudo_label import calculate_iou


def get_pose_predictions(
    loader: DLCLoader,
    images: list[str],
    bboxes: dict[str, list],
    superanimal_name: str,
    model_snapshot_path: str | Path,
    detector_snapshot_path: str | Path,
    max_individuals: int,
    device: str | None = None,
) -> dict[str, dict]:
    """Gets predictions made by a SuperAnimal model on a DeepLabCut project

    Args:
        loader: The path to the root of the project.
        images: The images on which to run inference with the SuperAnimal model.
        bboxes: The ground truth bounding boxes for each image in the project.
        superanimal_name: The name of the SuperAnimal dataset being used.
        model_snapshot_path: The path to the SuperAnimal pose snapshot.
        detector_snapshot_path: The path to the SuperAnimal detector snapshot.
        max_individuals: The maximum number of individuals to detect per image.
        device: The CUDA device to use.

    Returns:
        The predictions made by the SuperAnimal model on each image in the images list.
    """
    model_name = detector_snapshot_path.stem + "-" + model_snapshot_path.stem
    predictions_folder = (
        loader.project_path / "memory_replay" / superanimal_name / model_name
    )
    predictions_folder.mkdir(exist_ok=True, parents=True)
    predictions_file = predictions_folder / "pseudo-labels.json"

    # COCO-format annotations file containing predictions made by the SuperAnimal model
    sa_predictions = {}
    if predictions_file.exists():
        with open(predictions_file, "r") as f:
            raw_sa_predictions = json.load(f)

        # parse predictions to convert lists to numpy arrays
        for image, predictions in raw_sa_predictions.items():
            sa_predictions[image] = {
                "bodyparts": np.array(predictions["bodyparts"]),
                "bboxes": np.array(predictions["bboxes"]),
                # "bbox_scores": np.array(predictions["bbox_scores"]),
            }

    # get images that need to be processed
    processed_images = set(sa_predictions.keys())
    images_to_process = [image for image in (set(images) - processed_images)]

    # if all images have been processed by the SuperAnimal model, return the predictions
    if len(images_to_process) == 0:
        return sa_predictions

    pose_runner, detector_runner = get_inference_runners(
        loader.model_cfg,
        snapshot_path=model_snapshot_path,
        max_individuals=max_individuals,
        num_bodyparts=len(loader.model_cfg["metadata"]["bodyparts"]),
        num_unique_bodyparts=len(loader.model_cfg["metadata"]["unique_bodyparts"]),
        device=device,
        detector_path=detector_snapshot_path,
    )

    # FIXME(niels, yeshaokai) - Use the detector to combine GT-keypoint created bounding
    #  boxes and predicted bounding boxes - keep the larger of the two
    # bbox_predictions = detector_runner.inference(images=images_to_process)
    pose_inputs = [
        (
            str(loader.project_path / Path(image)),
            {"bboxes": np.array(bboxes[image])}
        )
        for image in images_to_process
    ]
    predictions = pose_runner.inference(pose_inputs)

    for image, prediction in zip(images_to_process, predictions):
        sa_predictions[image] = prediction

    # save the updated SuperAnimal predictions
    json_sa_predictions = {
        image: {
            "bodyparts": predictions["bodyparts"].tolist(),
            "bboxes": predictions["bboxes"].tolist(),
            # "bbox_scores": predictions["bbox_scores"].tolist(),
        }
        for image, predictions in sa_predictions.items()
    }
    with open(predictions_file, "w") as f:
        json.dump(json_sa_predictions, f, indent=2)

    return sa_predictions


# this is reading from a coco project
def prepare_memory_replay_dataset(
    loader: DLCLoader,
    source_dataset_folder: str | Path,
    superanimal_name: str,
    model_snapshot_path: str,
    detector_snapshot_path: str,
    max_individuals: int = 1,
    train_file: str = "train.json",
    pose_threshold: float = 0.0,
    device: str | None = None,
):
    """
    Need to first run inference on the source project train file
    """
    project_root = loader.project_path.resolve()
    source_dataset_folder = Path(source_dataset_folder).resolve()

    # Contains the ground truth annotations for the DeepLabCut project
    # .../dlc-models-pytorch/.../...shuffle0/train/memory_replay/annotations/train.json
    with open(source_dataset_folder / "annotations" / train_file, "r") as f:
        project_gt = json.load(f)

    # parse the GT so that image paths are in the format (no matter the OS):
    # "labeled-data/{video_name}/{image_name}"
    for image in project_gt["images"]:
        image["file_name"] = "/".join(Path(image["file_name"]).parts[-3:])

    image_id_to_name = {}
    image_id_to_annotations = defaultdict(list)

    image_name_to_id = {}
    image_name_to_gt = defaultdict(list)
    image_name_to_bbox = defaultdict(list)

    for image in project_gt["images"]:
        image_name_to_id[image["file_name"]] = image["id"]
        image_id_to_name[image["id"]] = image["file_name"]

    for anno in project_gt["annotations"]:
        name = image_id_to_name[anno["image_id"]]
        image_name_to_gt[name].append(anno)
        image_name_to_bbox[name].append(anno["bbox"])

    image_ids = list(image_name_to_id.values())
    for annotation in project_gt["annotations"]:
        image_id = annotation["image_id"]
        if annotation["image_id"] in image_ids:
            image_id_to_annotations[image_id].append(annotation)

    image_name_to_prediction = get_pose_predictions(
        loader=loader,
        images=[image["file_name"] for image in project_gt["images"]],
        bboxes=image_name_to_bbox,
        superanimal_name=superanimal_name,
        model_snapshot_path=model_snapshot_path,
        detector_snapshot_path=detector_snapshot_path,
        max_individuals=max_individuals,
        device=device,
    )

    def xywh2xyxy(bbox):
        temp_bbox = np.copy(bbox)
        temp_bbox[2:] = temp_bbox[:2] + temp_bbox[2:]
        return temp_bbox

    def optimal_match(gts_list, preds_list):
        arranged_preds_list = []
        num_gts = len(gts_list)
        num_preds = len(preds_list)
        cost_matrix = np.zeros((num_gts, num_preds))

        for i in range(num_gts):
            for j in range(num_preds):
                cost_matrix[i, j] = distance.euclidean(
                    gts_list[i][..., :2].flatten(), preds_list[j][..., :2].flatten()
                )
        row_ind, col_ind = linear_sum_assignment(cost_matrix)

        return col_ind

    num_bodyparts = len(project_gt["categories"][0]["keypoints"])
    for image_name, gts in image_name_to_gt.items():
        bbox_gts = [np.array(gt["bbox"]) for gt in gts]
        bbox_gts = [xywh2xyxy(e) for e in bbox_gts]
        prediction = image_name_to_prediction[image_name]
        bbox_preds = [xywh2xyxy(pred) for pred in prediction["bboxes"]]
        optimal_pred_indices = optimal_match(bbox_gts, bbox_preds)

        for idx in range(len(bbox_gts)):
            if idx == len(optimal_pred_indices):
                break

            optimal_index = optimal_pred_indices[idx]
            matched_gt = np.array(gts[idx]["keypoints"])
            matched_pred = prediction["bodyparts"][optimal_index]
            bbox_gt = bbox_gts[idx]
            bbox_pred = bbox_preds[idx]

            # maybe check iou of two bbox
            iou = calculate_iou(bbox_gt, bbox_pred)
            if iou < 0.7:
                matched_gt = np.ones_like(matched_gt) * -1
                gts[idx]["keypoints"] = list(matched_gt.flatten())
            else:
                matched_gt = matched_gt.reshape(num_bodyparts, -1)
                matched_pred = matched_pred.reshape(num_bodyparts, -1)
                mask = matched_gt == -1
                matched_gt[mask] = matched_pred[mask]
                # after the mixing, we don't care about confidence anymore

                for kpt_idx in range(len(matched_gt)):
                    if 0 < matched_gt[kpt_idx][2] < pose_threshold:
                        matched_gt[kpt_idx][2] = -1
                    elif matched_gt[kpt_idx][2] > 0:
                        matched_gt[kpt_idx][2] = 2

                gts[idx]["keypoints"] = list(matched_gt.flatten())

    # memory replay path
    memory_replay_train_file_path = os.path.join(
        source_dataset_folder, "annotations", "memory_replay_train.json"
    )

    # parse the GT to put the image paths back into OS-specific format
    for image in project_gt["images"]:
        image_rel_path = image["file_name"].split("/")
        image["file_name"] = str(project_root.resolve() / Path(*image_rel_path))

    with open(memory_replay_train_file_path, "w") as f:
        json.dump(project_gt, f, indent=4)


def prepare_memory_replay(
    config: str | Path,
    loader: DLCLoader,
    superanimal_name: str,
    model_snapshot_path: str | Path,
    detector_snapshot_path: str | Path,
    device: str,
    max_individuals: int = 3,
    train_file: str = "train.json",
    pose_threshold: float = 0.1,
) -> None:
    """Prepares a shuffle to be trained with memory replay.

    To be trained using memory replay, predictions must be made on all images in the
    dataset using the SuperAnimal model. Predictions for bodyparts that aren't labeled
    in the DeepLabCut project are then used as pseudo-labels during training.

    This method will create a COCO-format dataset in the same folder as the
    ``pytorch_config.yaml`` (the model folder).

    Args:
        config: Path to the DeepLabCut project configuration file.
        loader: The loader used to load the training/test data on which a model will
            be fine-tuned with memory replay.
        superanimal_name: The name of the SuperAnimal model that is being fine-tuned.
        model_snapshot_path: Path to the SuperAnimal pose snapshot to fine-tune.
        detector_snapshot_path: Path to the SuperAnimal detector snapshot to fine-tune.
        device: Device to use to run inference using the SuperAnimal model.
        max_individuals: Maximum number of animals that can be present in a frame.
        train_file: Name of the file containing train annotations (e.g. `train.json`).
        pose_threshold: The minimum score for a prediction to be used as a pseudo-label.
    """
    cfg = af.read_config(config)
    super_animal_cfg = af.read_plainconfig(
        get_super_animal_project_config_path(super_animal=superanimal_name)
    )

    if "individuals" in cfg:
        temp_dataset = MaDLCPoseDataset(
            str(loader.project_path), "temp_dataset", shuffle=loader.shuffle
        )
    else:
        temp_dataset = SingleDLCPoseDataset(
            str(loader.project_path), "temp_dataset", shuffle=loader.shuffle
        )

    memory_replay_folder = loader.model_folder / "memory_replay"
    temp_dataset.materialize(
        memory_replay_folder,
        framework="coco",
        append_image_id=False,
        no_image_copy=True,  # use the images in the labeled-data folder
    )

    weight_init_cfg = loader.model_cfg["train_settings"].get("weight_init")
    if weight_init_cfg is None:
        raise ValueError(
            "You can only train models with memory replay when you are fine-tuning a "
            "SuperAnimal model. Please look at the documentation to see how to create "
            "a training dataset to fine-tune one of the SuperAnimal models."
        )

    weight_init = WeightInitialization.from_dict(weight_init_cfg)
    if not weight_init.with_decoder:
        raise ValueError(
            "You can only train models with memory replay when you are fine-tuning a "
            "SuperAnimal model. Please look at the documentation to see how to create "
            "a training dataset to fine-tune one of the SuperAnimal models. Ensure "
            "that a conversion table is specified for your project and that you select"
            "``with_decoder=True`` for your ``WeightInitialization``."
        )

    dataset = COCOPoseDataset(memory_replay_folder, "memory_replay_dataset")

    # here we project the original DLC projects to superanimal space and save them into
    # a coco project format
    bodyparts = af.get_bodyparts(cfg)
    sa_bodyparts = af.get_bodyparts(super_animal_cfg)
    conversion_table = {}
    for idx, bpt in enumerate(bodyparts):
        conversion_table[bpt] = sa_bodyparts[weight_init.conversion_array[idx]]

    dataset.project_with_conversion_table(
        table_path=None,
        table_dict=dict(
            master_keypoints=sa_bodyparts,
            conversion_table=conversion_table,
        ),
    )

    dataset.materialize(
        memory_replay_folder, framework="coco", deepcopy=False, no_image_copy=True,
    )

    # then in this function, we do pseudo label to match prediction and gts to create
    # memory-replay dataset that will be named memory_replay_train.json
    prepare_memory_replay_dataset(
        loader,
        memory_replay_folder,
        superanimal_name,
        model_snapshot_path,
        detector_snapshot_path,
        max_individuals=max_individuals,
        device=device,
        train_file=train_file,
        pose_threshold=pose_threshold,
    )


--- File: deeplabcut/pose_estimation_pytorch/modelzoo/inference.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import json
import os
from pathlib import Path
from typing import Optional, Union

import numpy as np

from deeplabcut.modelzoo.utils import get_super_animal_scorer, get_superanimal_colormaps
from deeplabcut.pose_estimation_pytorch.apis.videos import (
    create_df_from_prediction,
    video_inference,
    VideoIterator,
)
from deeplabcut.pose_estimation_pytorch.apis.utils import get_inference_runners
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import (
    raise_warning_if_called_directly,
)
from deeplabcut.utils.make_labeled_video import create_video


class NumpyEncoder(json.JSONEncoder):
    """Special json encoder for numpy types"""

    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()  # Convert ndarray to list
        return json.JSONEncoder.default(self, obj)


def construct_bodypart_names(max_individuals, bodyparts):
    multianimalbodyparts = []
    for i in range(max_individuals):
        for bodypart in bodyparts:
            multianimalbodyparts.append(f"{bodypart}_{i}")
    return multianimalbodyparts


def _video_inference_superanimal(
    video_paths: Union[str, list],
    superanimal_name: str,
    model_cfg: dict,
    model_snapshot_path: str | Path,
    detector_snapshot_path: str | Path | None,
    max_individuals: int,
    pcutoff: float,
    batch_size: int = 1,
    detector_batch_size: int = 1,
    cropping: list[int] | None = None,
    dest_folder: Optional[str] = None,
    output_suffix: str = "",
    plot_bboxes: bool = True,
    bboxes_pcutoff: float = 0.9,
) -> dict:
    """
    Perform inference on a video using a superanimal model from the model zoo specified by `superanimal_name`.
    During inference, the video is analyzed using the specified model and the results are saved in the specified
    destination folder. The predictions are saved in the form of a .h5 file. The video with the predictions is saved
    in the form of a .mp4 file.

    WARNING: This function is an internal utility function and should not be
    called directly. It is designed to be used by deeplabcut.modelzoo.api.video_inference.py

    Args:
        video_paths: Path to the video to be analyzed or list of paths to videos to be
            analyzed
        superanimal_name: Name of the SuperAnimal project (e.g. superanimal_quadruped)
        model_cfg: The name of the pose model architecture to use for inference.
        model_snapshot_path: The path to the pose model snapshot to use for inference.
        detector_snapshot_path: The path to the detector snapshot to use for inference.
        max_individuals: Maximum number of individuals in the video
        pcutoff: Cutoff for cutting off the predicted keypoints with probability lower
            than pcutoff
        batch_size: The batch size to use for video inference.
        cropping: List of cropping coordinates as [x1, x2, y1, y2]. Note that the same
            cropping parameters will then be used for all videos. If different video
            crops are desired, run ``video_inference_superanimal`` on individual videos
            with the corresponding cropping coordinates.
        detector_batch_size: The batch size to use for the detector for video inference.
        dest_folder: Destination folder for the results. If not specified, the
            results are saved in the same folder as the video. Defaults to None.
        output_suffix: The suffix to add to output file names (e.g. _before_adapt)

    Returns:
        results: Dictionary with the result pd.DataFrame for each video

    Raises:
        Warning: If the function is called directly.
    """
    raise_warning_if_called_directly()
    pose_runner, detector_runner = get_inference_runners(
        model_config=model_cfg,
        snapshot_path=model_snapshot_path,
        max_individuals=max_individuals,
        num_bodyparts=len(model_cfg["metadata"]["bodyparts"]),
        num_unique_bodyparts=0,
        batch_size=batch_size,
        detector_batch_size=detector_batch_size,
        detector_path=detector_snapshot_path,
    )
    results = {}

    if isinstance(video_paths, str):
        video_paths = [video_paths]

    if dest_folder is None:
        dest_folder = Path(video_paths[0]).parent

    if not os.path.exists(dest_folder):
        os.makedirs(dest_folder)

    for video_path in video_paths:
        print(f"Processing video {video_path}")

        dlc_scorer = get_super_animal_scorer(
            superanimal_name, model_snapshot_path, detector_snapshot_path
        )

        output_prefix = f"{Path(video_path).stem}_{dlc_scorer}"
        output_path = Path(dest_folder)
        output_h5 = Path(output_path) / f"{output_prefix}.h5"

        output_json = output_h5.with_suffix(".json")
        if len(output_suffix) > 0:
            output_json = output_json.with_stem(output_h5.stem + output_suffix)

        video = VideoIterator(video_path, cropping=cropping)
        predictions = video_inference(
            video,
            pose_runner=pose_runner,
            detector_runner=detector_runner,
        )

        bbox_keys_in_predictions = {"bboxes", "bbox_scores"}
        bboxes_list = [
            {key: value for key, value in p.items() if key in bbox_keys_in_predictions}
            for i, p in enumerate(predictions)
        ]

        bbox = cropping
        if cropping is None:
            vid_w, vid_h = video.dimensions
            bbox = (0, vid_w, 0, vid_h)

        print(f"Saving results to {dest_folder}")
        df = create_df_from_prediction(
            predictions=predictions,
            dlc_scorer=dlc_scorer,
            multi_animal=True,
            model_cfg=model_cfg,
            output_path=output_path,
            output_prefix=output_prefix,
        )

        results[video_path] = df
        with open(output_json, "w") as f:
            json.dump(predictions, f, cls=NumpyEncoder)

        output_video = output_path / f"{output_prefix}_labeled.mp4"
        if len(output_suffix) > 0:
            output_video = output_video.with_stem(output_video.stem + output_suffix)

        superanimal_colormaps = get_superanimal_colormaps()
        colormap = superanimal_colormaps[superanimal_name]
        create_video(
            video_path,
            output_h5,
            pcutoff=pcutoff,
            fps=video.fps,
            bbox=bbox,
            cmap=colormap,
            output_path=str(output_video),
            plot_bboxes=plot_bboxes,
            bboxes_list=bboxes_list,
            bboxes_pcutoff=bboxes_pcutoff,
        )
        print(f"Video with predictions was saved as {output_path}")

    return results


--- File: deeplabcut/pose_estimation_pytorch/runners/snapshots.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Code to handle storing models"""
from __future__ import annotations

import re
import warnings
from dataclasses import dataclass, field
from pathlib import Path

import numpy as np
import torch


@dataclass(frozen=True)
class Snapshot:
    best: bool
    epochs: int | None
    path: Path

    def uid(self) -> str:
        return self.path.stem.split("-")[-1]

    @staticmethod
    def from_path(path: Path) -> "Snapshot":
        best = "-best" in path.stem
        epochs = int(path.stem.split("-")[-1])
        return Snapshot(best=best, epochs=epochs, path=path)


@dataclass
class TorchSnapshotManager:
    """Class handling model checkpoint I/O

    Attributes:
        snapshot_prefix: The prefix to use when saving snapshots.
        model_folder: The path to the directory where model snapshots should be stored.
        key_metric: If defined, the metric is used to save the best model. Otherwise no
            best model is used.
        key_metric_asc: Whether the key metric is ascending (larger values are better).
        max_snapshots: The maximum number of snapshots to store for the training run.
            This does not include the best model (e.g., setting max_snapshots=5 will
            mean that the 5 latest models will be kept, plus the best model)
        save_epochs: The number of epochs between each model save
        save_optimizer_state: Whether to store the optimizer state. This makes snapshots
            much heavier, but allows to resume training as if it was never stopped.

    Examples:
        # Storing snapshots while training
        model: nn.Module
        loader = DLCLoader(...)
        snapshot_manager = TorchSnapshotManager(
            "snapshot",
            loader.model_folder,
            key_metric="test.mAP",
        )
        ...
        for epoch in range(num_epochs):
            train_epoch(model, data)
            snapshot_manager.update({
                "metadata": {
                    "metrics": {"mAP": ...}
                },
                "model": model.state_dict(),
                "optimizer": optimizer.state_dict()
            })
    """

    snapshot_prefix: str
    model_folder: Path
    key_metric: str | None = None
    key_metric_asc: bool = True
    max_snapshots: int = 5
    save_epochs: int = 25
    save_optimizer_state: bool = False
    _best_model_epochs: int = -1
    _best_metric: float | None = None
    _key: str = field(init=False)

    def __post_init__(self):
        assert self.max_snapshots > 0, f"max_snapshots must be a positive integer"
        self._key = f"metrics/{self.key_metric}"

    def update(self, epoch: int, state_dict: dict, last: bool = False) -> None:
        """Saves the model state dict if the epoch is one that requires a save

        Args:
            epoch: the number of epochs the model was trained for
            state_dict: the state dict to store
            last: whether this is the last epoch in the training run, which forces a
                model save no matter the epoch number

        Returns:
            the path to the saved snapshot if one
        """
        metrics = state_dict["metadata"]["metrics"]
        if (
            self._key in metrics
            and not np.isnan(metrics[self._key])
            and (
                self._best_metric is None
                or (self.key_metric_asc and self._best_metric < metrics[self._key])
                or (not self.key_metric_asc and self._best_metric > metrics[self._key])
            )
        ):
            current_best = self.best()
            self._best_metric = metrics[self._key]

            # Save the new best model
            save_path = self.snapshot_path(epoch, best=True)
            parsed_state_dict = {
                k: v
                for k, v in state_dict.items()
                if self.save_optimizer_state or k != "optimizer"
            }
            torch.save(parsed_state_dict, save_path)

            # Handle previous best model
            if current_best is not None:
                if current_best.epochs % self.save_epochs == 0:
                    new_name = self.snapshot_path(epoch=current_best.epochs)
                    current_best.path.rename(new_name)
                else:
                    current_best.path.unlink(missing_ok=False)
        elif last or epoch % self.save_epochs == 0:
            # Save regular snapshot if needed
            save_path = self.snapshot_path(epoch=epoch)
            parsed_state_dict = {
                k: v
                for k, v in state_dict.items()
                if self.save_optimizer_state or k != "optimizer"
            }
            torch.save(parsed_state_dict, save_path)

        # Clean up old snapshots if needed
        existing_snapshots = [s for s in self.snapshots() if not s.best]
        if len(existing_snapshots) >= self.max_snapshots:
            num_to_delete = len(existing_snapshots) - self.max_snapshots
            to_delete = existing_snapshots[:num_to_delete]
            for snapshot in to_delete:
                snapshot.path.unlink(missing_ok=False)

    def best(self) -> Snapshot | None:
        """Returns: the path to the best snapshot, if it exists"""
        snapshots = self.snapshots()
        best_snapshots = [s for s in snapshots if s.best]
        if len(best_snapshots) == 0:
            return None

        if len(best_snapshots) > 1:
            warnings.warn(
                f"TorchSnapshotManager.best(): found multiple best snapshots ("
                f"{best_snapshots}), returning the last one."
            )

        best_snapshot = best_snapshots[-1]
        return best_snapshot

    def last(self) -> Snapshot | None:
        """Returns: path to the last snapshot that was saved, if any snapshot exists"""
        snapshots = self.snapshots(best_in_last=False)
        if len(snapshots) == 0:
            return None
        return snapshots[-1]

    def snapshots(self, best_in_last: bool = True) -> list[Snapshot]:
        """
        Args:
            best_in_last: Whether to place the snapshot with the best performance in the
                last position in the list, even if it wasn't the last epoch.

        Returns:
            The snapshots for a training run, sorted by the number of epochs they were
            trained for. If ``best_in_last=True`` and a best snapshot exists, it will be
            the last one in the list.
        """

        def _sort_key(snapshot: Snapshot) -> int:
            return snapshot.epochs

        def _sort_key_best_as_last(snapshot: Snapshot) -> tuple[int, int]:
            return 1 if snapshot.best else 0, snapshot.epochs

        pattern = r"^(" + self.snapshot_prefix + r"(-best)?-\d+\.pt)$"
        snapshots = [
            Snapshot.from_path(f)
            for f in self.model_folder.iterdir()
            if re.match(pattern, f.name)
        ]

        sort_key = _sort_key
        if best_in_last:
            sort_key = _sort_key_best_as_last
        snapshots.sort(key=sort_key)
        return snapshots

    def snapshot_path(self, epoch: int, best: bool = False) -> Path:
        """
        Args:
            epoch: the number of epochs for which a snapshot was trained
            best: whether this is the best performing model for the training run

        Returns:
            the path where the model should be stored
        """
        uid = f"{epoch:03}"
        if best:
            uid = f"best-{uid}"
        return self.model_folder / f"{self.snapshot_prefix}-{uid}.pt"


--- File: deeplabcut/pose_estimation_pytorch/runners/shelving.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Modules used to read/write shelve data during video analysis in DeepLabCut 3.0"""
import pickle
import shelve
from abc import ABC
from pathlib import Path

import numpy as np


class ShelfManager(ABC):
    """Class to manage shelf data"""

    def __init__(self, filepath: str | Path, flag: str = "r") -> None:
        self.filepath = Path(filepath)
        self.flag = flag

        self._db: shelve.Shelf | None = None
        self._open: bool = False

    def open(self) -> None:
        """Opens the shelf"""
        self._db = shelve.open(
            str(self.filepath),
            flag=self.flag,
            protocol=pickle.DEFAULT_PROTOCOL,
        )
        self._open = True

    def close(self) -> None:
        """Closes the shelf"""
        if not self._open:
            return

        try:
            self._db.close()
        except AttributeError:
            pass

        self._open = False

    def keys(self) -> list[str]:
        if not self._open:
            raise ValueError(f"You must call open() before reading keys!")

        return [k for k in self._db]


class ShelfReader(ShelfManager):
    """Reads data from a shelf"""

    def __getitem__(self, item: str) -> dict:
        """Reads an item from the shelf.

        Args:
            item: The key of the item to read.

        Returns:
            The item.
        """
        if not self._open:
            raise ValueError(f"You must call open() before reading data!")

        return self._db[item]


class ShelfWriter(ShelfManager):
    """Writes data to a shelf on-the-fly during video analysis.

    Args:
        pose_cfg: The test pose config for the model.
        filepath: The path where the data should be saved.
        num_frames: The number of frames in the video. Used to set the number of
            leading 0s in the keys of the dictionary. Default is 5 if the number of
            frames is not given.

    Attributes:
        filepath: The path to the shelf.
    """

    def __init__(
        self, pose_cfg: dict, filepath: str | Path, num_frames: int | None = None
    ):
        super().__init__(filepath, flag="c")
        self._pose_cfg = pose_cfg
        self._num_frames = num_frames
        self._frame_index = 0

        self._str_width = 5
        if num_frames is not None:
            self._str_width = int(np.ceil(np.log10(num_frames)))

    def add_prediction(
        self,
        bodyparts: np.ndarray,
        unique_bodyparts: np.ndarray | None = None,
        identity_scores: np.ndarray | None = None,
        **kwargs,
    ) -> None:
        """Adds the prediction for a frame to the shelf

        Args:
            bodyparts: The predicted bodyparts.
            unique_bodyparts: The predicted unique bodyparts, if there are any.
            identity_scores: The predicted identities, if there are any.
        """
        if not self._open:
            raise ValueError(f"You must call open() before adding data!")

        key = "frame" + str(self._frame_index).zfill(self._str_width)

        # convert bodyparts to shape (num_bpts, num_assemblies, 3)
        bodyparts = bodyparts.transpose((1, 0, 2))
        coordinates = [bpt[:, :2] for bpt in bodyparts]
        scores = [bpt[:, 2:3] for bpt in bodyparts]

        # full pickle has bodyparts and unique bodyparts in same array
        unique_bodyparts = kwargs.get("unique_bodyparts", None)
        if unique_bodyparts is not None:
            unique_bpts = unique_bodyparts.transpose((1, 0, 2))
            coordinates += [bpt[:, :2] for bpt in unique_bpts]
            scores += [bpt[:, 2:] for bpt in unique_bpts]

        output = dict(coordinates=(coordinates,), confidence=scores, costs=None)

        identity_scores = kwargs.get("identity_scores", None)
        if identity_scores is not None:
            # Reshape id scores from (num_assemblies, num_bpts, num_individuals)
            # to the original DLC full pickle format: (num_bpts, num_assem, num_ind)
            id_scores = identity_scores.transpose((1, 0, 2))
            output["identity"] = [bpt_id_scores for bpt_id_scores in id_scores]

            if unique_bodyparts is not None:
                # needed for create_video_with_all_detections to display unique bpts
                num_unique = unique_bodyparts.shape[1]
                num_assem, num_ind = id_scores.shape[1:]
                output["identity"] += [
                    -1 * np.ones((num_assem, num_ind)) for i in range(num_unique)
                ]

        self._db[key] = output
        self._frame_index += 1

    def close(self) -> None:
        """Opens the shelf"""
        if self._open and self._frame_index > 0:
            self._db["metadata"]["nframes"] = self._frame_index

        super().close()

    def open(self) -> None:
        """Opens the shelf"""
        super().open()
        self._frame_index = 0

        all_joints = self._pose_cfg["all_joints"]
        paf_graph = self._pose_cfg.get("partaffinityfield_graph", [])

        self._db["metadata"] = {
            "nms radius": self._pose_cfg.get("nmsradius"),
            "minimal confidence": self._pose_cfg.get("minconfidence"),
            "sigma": self._pose_cfg.get("sigma", 1),
            "PAFgraph": paf_graph,
            "PAFinds": self._pose_cfg.get("paf_best", np.arange(len(paf_graph))),
            "all_joints": [[i] for i in range(len(all_joints))],
            "all_joints_names": [
                self._pose_cfg["all_joints_names"][i] for i in range(len(all_joints))
            ],
            "nframes": self._num_frames,
            "key_str_width": self._str_width,
        }


class FeatureShelfWriter(ShelfWriter):
    """Writes bodypart features to a shelf on-the-fly for ReID model training.

    Args:
        pose_cfg: The test pose config for the model.
        filepath: The path where the data should be saved.
        num_frames: The number of frames in the video. Used to set the number of
            leading 0s in the keys of the dictionary. Default is 5 if the number of
            frames is not given.

    Attributes:
        filepath: The path to the shelf.
    """

    def __init__(
        self, pose_cfg: dict, filepath: str | Path, num_frames: int | None = None
    ):
        super().__init__(pose_cfg, filepath, num_frames)

    def add_prediction(
        self,
        bodyparts: np.ndarray,
        features: np.ndarray | None = None,
        **kwargs,
    ) -> None:
        """Adds the prediction for a frame to the shelf

        Args:
            bodyparts: The predicted bodyparts.
            features: The features for the bodyparts.
        """
        if not self._open:
            raise ValueError(f"You must call open() before adding data!")

        key = "frame" + str(self._frame_index).zfill(self._str_width)

        # bodyparts to shape (num_assemblies, num_bpts, xy)
        coordinates = bodyparts[:, :, :2]
        if features is None:
            raise ValueError(
                "Backbone features must be given to the FeatureShelfWriter"
            )

        self._db[key] = dict(coordinates=coordinates, features=features)
        self._frame_index += 1


--- File: deeplabcut/pose_estimation_pytorch/runners/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from deeplabcut.pose_estimation_pytorch.runners.base import (
    attempt_snapshot_load,
    get_load_weights_only,
    fix_snapshot_metadata,
    Runner,
    set_load_weights_only,
)
from deeplabcut.pose_estimation_pytorch.runners.dynamic_cropping import DynamicCropper
from deeplabcut.pose_estimation_pytorch.runners.inference import (
    build_inference_runner,
    DetectorInferenceRunner,
    InferenceRunner,
    PoseInferenceRunner,
)
from deeplabcut.pose_estimation_pytorch.runners.logger import LOGGER
from deeplabcut.pose_estimation_pytorch.runners.snapshots import TorchSnapshotManager
from deeplabcut.pose_estimation_pytorch.runners.train import (
    build_training_runner,
    DetectorTrainingRunner,
    PoseTrainingRunner,
    TrainingRunner,
)


--- File: deeplabcut/pose_estimation_pytorch/runners/logger.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import csv
import logging
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Optional

import numpy as np
import torch
import torchvision.transforms as transforms
import torchvision.transforms.functional as F
from torch.utils.data import DataLoader
from torchvision.utils import draw_bounding_boxes, draw_keypoints

try:
    import wandb
    has_wandb = True
except ImportError:
    has_wandb = False

import deeplabcut.pose_estimation_pytorch.registry as deeplabcut_pose_estimation_pytorch_registry
from deeplabcut.pose_estimation_pytorch.models.model import PoseModel

LOGGER = deeplabcut_pose_estimation_pytorch_registry.Registry(
    "loggers", build_func=deeplabcut_pose_estimation_pytorch_registry.build_from_cfg
)


def setup_file_logging(filepath: Path) -> None:
    """
    Sets up logging to a file

    Args:
        filepath: the path where logs should be saved
    """
    logging.basicConfig(
        filename=filepath,
        filemode="a",
        datefmt="%Y-%m-%d %H:%M:%S",
        level=logging.INFO,
        format="%(asctime)-15s %(message)s",
        force=True,
    )
    console_logger = logging.StreamHandler()
    console_logger.setLevel(logging.INFO)
    root = logging.getLogger()
    root.addHandler(console_logger)


def destroy_file_logging() -> None:
    """Resets the logging module to log everything to the console"""
    root = logging.getLogger()
    handlers = [h for h in root.handlers]
    for handler in handlers:
        root.removeHandler(handler)


class BaseLogger(ABC):
    """Base class for logging training runs"""

    @abstractmethod
    def log_config(self, config: dict = None) -> None:
        """Logs the configuration data for a training run

        Args:
            config: the training configuration used for the run
        """

    @abstractmethod
    def log(self, metrics: dict[str, Any], step: Optional[int] = None) -> None:
        """Logs data from a training run

        Args:
            metrics: the metrics to log
            step: The global step in processing. Defaults to None.
        """

    @abstractmethod
    def save(self) -> None:
        """Saves the current training logs"""


class ImageLoggerMixin(ABC):
    """Mixin for loggers that can log images

    Before starting training, you should call `select_images_to_log`, which will
    select a train and a test image for which inputs/outputs will always be logged.
    Then logger.log_images should be called at every step - the logger will check if
    anything needs to be uploaded, and take care of it.

    Example:
        project_name = "example"
        run_name = "run-1"
        logger = WandbLogger(project_name, run_name)
        logger.select_images_to_log(train_loader, test_loader)

        for i in range(epochs):
            for batch_inputs in train_loader:
                batch_labels = batch_data["annotations"]
                batch_inputs = batch_data["image"]
                batch_outputs = model(batch_inputs)
                batch_targets = model.get_target(batch_outputs, batch_labels)
                loss = criterion(batch_targets, batch_outputs)
                loss.backwards()
                optim.step()

                logger.log_images(batch_inputs, batch_outputs, batch_targets)

            for batch_inputs in train_loader:
                ...
                logger.log_images(batch_inputs, batch_outputs, batch_targets)
    """

    def __init__(self, image_log_interval: int | None = None, *args, **kwargs):
        """"""
        super().__init__(*args, **kwargs)
        self.image_log_interval = image_log_interval
        self._logged = {}
        self._denormalize = transforms.Compose(
            [
                transforms.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),
                transforms.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),
            ]
        )
        self._softmax = torch.nn.Softmax2d()

    @abstractmethod
    def log_images(
        self,
        inputs: dict[str, Any],
        outputs: dict[str, torch.Tensor],
        targets: dict[str, dict[str, torch.Tensor]],
        step: int,
    ) -> None:
        """Log images for a batch

        Args:
            inputs: the inputs for the model, containing at least an "image" key
            outputs: the outputs of each model head
            targets: the targets for each model head
            step: the current step
        """
        pass

    def select_images_to_log(self, train: DataLoader, valid: DataLoader) -> None:
        """Selects the train and test images to log

        Args:
            train: the training dataloader
            valid: the inference dataloader
        """
        def _caption(image_path: str) -> str:
            p = Path(image_path)
            return f"{p.parent.name}.{p.stem}"

        train_image = train.dataset[0]["path"]
        test_image = valid.dataset[0]["path"]
        self._logged = {
            train_image: {"name": "train-0", "caption": _caption(train_image)},
            test_image: {"name": "test-0", "caption": _caption(test_image)},
        }

    def _prepare_image(
        self,
        image: torch.Tensor,
        denormalize: bool = False,
        keypoints: torch.Tensor | None = None,
        bboxes: torch.Tensor | None = None,
    ) -> np.ndarray:
        """
        Args:
            image: the image to log, of shape (C, H, W), of any data type
            denormalize: whether to remove ImageNet channel normalization
            keypoints: size (num_instances, K, 2) the K keypoints location
            bboxes: size (N, 4) containing bboxes in (xmin, ymin, xmax, ymax)

        Returns:
            an uint8 array with keypoints and bounding boxes drawn
        """
        if denormalize:
            image = self._denormalize(image.unsqueeze(0)).squeeze()

        image = F.convert_image_dtype(image.detach().cpu(), dtype=torch.uint8)
        if keypoints is not None and len(keypoints) > 0:
            assert len(keypoints.shape) == 3
            # Use visibility and force torchvision >= 0.18
            # pytorch.org/vision/0.18/generated/torchvision.utils.draw_keypoints.html
            # pytorch.org/vision/0.17/generated/torchvision.utils.draw_keypoints.html
            keypoints[torch.any(torch.isnan(keypoints), dim=-1)] = -1
            image = draw_keypoints(
                image, keypoints=keypoints[..., :2], colors="red", radius=5
            )

        if bboxes is not None and len(bboxes) > 0:
            assert len(bboxes.shape) == 2
            image = draw_bounding_boxes(image, boxes=bboxes[:, :4], width=1)

        return image.permute(1, 2, 0).numpy()

    def _heatmap_softmax(self, heatmaps: torch.Tensor) -> torch.Tensor:
        """Applies a softmax to the heatmap channels"""
        return self._softmax(heatmaps.detach().cpu())

    def _prepare_images(
        self,
        inputs: dict[str, Any],
        outputs: dict[str, dict[str, torch.Tensor]],
        targets: dict[str, dict[str, dict[str, torch.Tensor]]],
    ) -> dict[str, np.ndarray]:
        """Prepares images for logging"""
        image_logs = {}
        paths = inputs["path"]
        images_to_log = [(i, p) for i, p in enumerate(paths) if p in self._logged]
        for idx, path in images_to_log:
            base = self._logged[path]["name"]
            keypoints = inputs.get("annotations", {}).get("keypoints")
            if keypoints is not None:
                keypoints = keypoints[idx]
            image_logs[f"{base}.input"] = self._prepare_image(
                inputs["image"][idx], keypoints=keypoints, denormalize=True,
            )

            for head, head_outputs in outputs.items():
                if "heatmap" in head_outputs:
                    head_heatmaps = self._heatmap_softmax(head_outputs["heatmap"][idx])
                    head_targets = targets[head]["heatmap"]["target"][idx]
                    for j, (h, t) in enumerate(zip(head_heatmaps, head_targets)):
                        h = self._prepare_image(h.unsqueeze(0))
                        t = self._prepare_image(t.unsqueeze(0))
                        image_logs[f"{base}.heatmap.{j}"] = np.concatenate([h, t])

        return image_logs


@LOGGER.register_module
class WandbLogger(ImageLoggerMixin, BaseLogger):
    """Wandb logger to track experiments and log data.

    Refer to: https://docs.wandb.ai/guides for more information on wandb.

    Attributes:
        run (wandb.Run): The wandb run object associated with the current experiment.
    """

    def __init__(
        self,
        project_name: str = "deeplabcut",
        run_name: str = "tmp",
        image_log_interval: int | None = None,
        model: PoseModel = None,
        **wandb_kwargs,
    ) -> None:
        """Initialize the WandbLogger class.

        Args:
            project_name: The name of the wandb project. Defaults to "deeplabcut".
            run_name: The name of the wandb run. Defaults to "tmp".
            image_log_interval: How often train/test images are logged in epochs (if
                None, train/test inputs are never logged).
            model: The model to log. Defaults to None.
            wandb_kwargs: extra arguments to pass to ``wb.init``

        Example:
            logger = WandbLogger(project_name="mice", run_name="exp1", model=my_model)

        """
        super().__init__(image_log_interval=image_log_interval)

        if not has_wandb:
            raise ValueError(
                "Cannot use ``WandbLogger`` as wandb is not installed. Please run"
                "``pip install wandb`` if you want to log to wandb"
            )

        if wandb.run is not None:
            wandb.finish()

        self.run = wandb.init(
            project=project_name,
            name=run_name,
            **wandb_kwargs,
        )
        if model is None:
            raise ValueError("Specify the model to track!")
        self.run.watch(model)

    def log(self, metrics: dict[str, Any], step: Optional[int] = None) -> None:
        """Logs metrics from runs

        Args:
            metrics: the metrics to log
            step: The global step in processing. Defaults to None.

        Example:
            logger = WandbLogger()
            logger.log({"loss": 0.123}, step=100)
        """
        self.run.log(metrics, step=step)

    def log_images(
        self,
        inputs: dict[str, Any],
        outputs: dict[str, dict[str, torch.Tensor]],
        targets: dict[str, dict[str, dict[str, torch.Tensor]]],
        step: int,
    ) -> None:
        """Log images for a batch

        Args:
            inputs: the inputs for the model, containing at least an "image" key
            outputs: the outputs of each model head
            targets: the targets for each model head
            step: the current step
        """
        if self.image_log_interval is None or step % self.image_log_interval != 0:
            return

        images = self._prepare_images(inputs, outputs, targets)
        if len(images) > 0:
            self.run.log(
                {name: wandb.Image(image) for name, image in images.items()},
                step=step,
            )

    def save(self):
        """Syncs all files to wandb with the policy specified.

        Notes:
            self.run: A run is a unit of computation logged by wandb.
            self.run.run.dir: The directory where files associated with the run are saved.

        Example:
            logger = WandbLogger()
            # Training and logging
            logger.save()
        """
        self.run.save(self.run.dir)

    def log_config(self, config: dict = None) -> None:
        """Updates the current run with the given config dict.

        Notes:
            self.run: A run is a unit of computation logged by wandb.
            self.run.config: Config object associated with this run.

        Args:
            config: Experiment config file.

        Example:
            logger = WandbLogger()
            config = {"learning_rate": 0.001, "batch_size": 32}
            logger.log_config(config)

        """
        self.run.config.update(config)


@LOGGER.register_module
class CSVLogger(BaseLogger):
    """Logger saving stats and metrics to a CSV file"""

    def __init__(self, train_folder: Path, log_filename: str) -> None:
        """Initialize the WandbLogger class.

        Args:
            train_folder: The path of the folder containing training files.
            log_filename: The name of the file in which to store training stats
        """
        super().__init__()
        self.train_folder = train_folder
        self.log_filename = log_filename
        self.log_file = train_folder / log_filename

        self._steps: list[int] = []
        self._metric_store: list[dict] = []
        self._logged_metrics: set[str] = set()

    def log(self, metrics: dict[str, Any], step: Optional[int] = None) -> None:
        """Logs metrics from runs

        Args:
            metrics: the metrics to log
            step: The global step in processing. Defaults to None.
        """
        if step is None:
            if len(self._steps) == 0:
                step = 0
            else:
                step = self._steps[-1] + 1

        self._logged_metrics = self._logged_metrics.union(metrics.keys())
        if len(self._steps) > 0 and step == self._steps[-1]:
            self._metric_store[-1].update(metrics)
        else:
            self._steps.append(step)
            self._metric_store.append(metrics)

        self.save()

    def save(self):
        """Saves the metrics to the file system"""
        logs = self._prepare_logs()
        with open(self.log_file, 'w', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(logs)

    def log_config(self, config: dict = None) -> None:
        """Does not do anything as the config should already be saved

        Args:
            config: Experiment config file.
        """
        pass

    def _prepare_logs(self) -> list[list]:
        """Prepares the data to log as a list of strings"""
        if len(self._metric_store) == 0:
            return []

        metrics = list(sorted(self._logged_metrics))
        logs = [["step"] + metrics]
        for step, step_metrics in zip(self._steps, self._metric_store):
            logs.append([step] + [step_metrics.get(m) for m in metrics])

        return logs


--- File: deeplabcut/pose_estimation_pytorch/runners/train.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import logging
from abc import ABCMeta, abstractmethod
from collections import defaultdict
from pathlib import Path
from typing import Any, Generic

import numpy as np
import torch
import torch.nn as nn
from torch.nn.parallel import DataParallel
from torch.utils.data import DataLoader

import deeplabcut.core.metrics as metrics
import deeplabcut.pose_estimation_pytorch.runners.schedulers as schedulers
from deeplabcut.pose_estimation_pytorch.models.detectors import BaseDetector
from deeplabcut.pose_estimation_pytorch.models.model import PoseModel
from deeplabcut.pose_estimation_pytorch.runners.base import (
    attempt_snapshot_load,
    ModelType,
    Runner,
)
from deeplabcut.pose_estimation_pytorch.runners.logger import (
    BaseLogger,
    CSVLogger,
    ImageLoggerMixin,
)
from deeplabcut.pose_estimation_pytorch.runners.snapshots import TorchSnapshotManager
from deeplabcut.pose_estimation_pytorch.task import Task


class TrainingRunner(Runner, Generic[ModelType], metaclass=ABCMeta):
    """Base TrainingRunner class.

    A TrainingRunner is used to fit models to datasets. Subclasses must implement the
    ``step(self, batch, mode)`` method, which performs a single training or validation
    step on a batch of data. The step is different depending on the model type (e.g.
    a pose model step vs. an object detector step).

    Args:
        model: The model to fit.
        optimizer: The optimizer to use to fit the model.
        snapshot_manager: Manages how snapshots are saved to disk during training.
        device: The device on which to run training (e.g. 'cpu', 'cuda', 'cuda:0').
        gpus: Used to specify the GPU indices for multi-GPU training (e.g. [0, 1, 2, 3]
            to train on 4 GPUs). When a GPUs list is given, the device must be 'cuda'.
        eval_interval: The interval at which the model will be evaluated while training
            (e.g. `eval_interva=5` means the model will be evaluated every 5 epochs).
        snapshot_path: If continuing to train a model, the path to the snapshot to
            resume training from.
        scheduler: The learning rate scheduler (or it's configuration), if one should be
            used.
        load_scheduler_state_dict: When resuming training (snapshot_path is not None),
            attempts to load the scheduler state dict from the snapshot. If you've
            modified your scheduler, set this to False or the old scheduler parameters
            might be used.
        logger: Logger to monitor training (e.g. a WandBLogger).
        log_filename: Name of the file in which to store training stats.
        load_weights_only: Value for the torch.load() `weights_only` parameter if
            `snapshot_path` is not None.
            If False, the python pickle module is used implicitly, which is known to
            be insecure. Only set to False if you're loading data that you trust
            (e.g. snapshots that you created yourself). For more information, see:
                https://pytorch.org/docs/stable/generated/torch.load.html
            If None, the default value is used:
                `deeplabcut.pose_estimation_pytorch.get_load_weights_only()`
    """

    def __init__(
        self,
        model: ModelType,
        optimizer: dict | torch.optim.Optimizer,
        snapshot_manager: TorchSnapshotManager,
        device: str = "cpu",
        gpus: list[int] | None = None,
        eval_interval: int = 1,
        snapshot_path: str | Path | None = None,
        scheduler: dict | torch.optim.lr_scheduler.LRScheduler | None = None,
        load_scheduler_state_dict: bool = True,
        logger: BaseLogger | None = None,
        log_filename: str = "learning_stats.csv",
        load_weights_only: bool | None = None,
    ):
        super().__init__(
            model=model, device=device, gpus=gpus, snapshot_path=snapshot_path
        )
        if isinstance(optimizer, dict):
            optimizer = build_optimizer(model, optimizer)
        if isinstance(scheduler, dict):
            scheduler = schedulers.build_scheduler(scheduler, optimizer)

        self.eval_interval = eval_interval
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.snapshot_manager = snapshot_manager
        self.history: dict[str, list] = dict(train_loss=[], eval_loss=[])
        self.csv_logger = CSVLogger(
            train_folder=snapshot_manager.model_folder,
            log_filename=log_filename,
        )
        self.logger = logger
        self.starting_epoch = 0
        self.current_epoch = 0

        # some models cannot compute a validation loss (e.g. detectors)
        self._print_valid_loss = True

        if self.snapshot_path:
            snapshot = self.load_snapshot(
                self.snapshot_path,
                self.device,
                self.model,
                weights_only=load_weights_only,
            )
            self.starting_epoch = snapshot.get("metadata", {}).get("epoch", 0)

            if "optimizer" in snapshot:
                self.optimizer.load_state_dict(snapshot["optimizer"])

            self._load_scheduler_state_dict(load_scheduler_state_dict, snapshot)

        self._metadata = dict(epoch=self.starting_epoch, metrics=dict(), losses=dict())
        self._epoch_ground_truth = {}
        self._epoch_predictions = {}

    def state_dict(self) -> dict:
        """Returns: the state dict for the runner"""
        model = self.model
        if self._data_parallel:
            model = self.model.module

        state_dict_ = dict(
            metadata=self._metadata,
            model=model.state_dict(),
            optimizer=self.optimizer.state_dict(),
        )
        if self.scheduler is not None:
            state_dict_["scheduler"] = self.scheduler.state_dict()

        return state_dict_

    @abstractmethod
    def step(
        self, batch: dict[str, Any], mode: str = "train"
    ) -> dict[str, torch.Tensor]:
        """Perform a single epoch gradient update or validation step

        Args:
            batch: the batch data on which to run a step
            mode: "train" or "eval". Defaults to "train".

        Raises:
            ValueError: if mode is not in {"train", "eval"}

        Returns:
            A dictionary containing the different losses for the step
        """

    @abstractmethod
    def _compute_epoch_metrics(self) -> dict[str, float]:
        """Computes the metrics using the data accumulated during an epoch

        Returns:
            A dictionary containing the different losses for the step
        """
        raise NotImplementedError

    def fit(
        self,
        train_loader: DataLoader,
        valid_loader: DataLoader,
        epochs: int,
        display_iters: int,
    ) -> None:
        """Train model for the specified number of steps.

        Args:
            train_loader: Data loader, which is an iterator over train instances.
                Each batch contains image tensor and heat maps tensor input samples.
            valid_loader: Data loader used for validation of the model.
            epochs: The number of training epochs.
            display_iters: The number of iterations between each loss print

        Example:
           runner = Runner(model, optimizer, cfg, device='cuda')
           runner.fit(train_loader, valid_loader, "example/models" epochs=50)
        """
        if self._data_parallel:
            self.model = DataParallel(self.model, device_ids=self._gpus).cuda()
        else:
            self.model.to(self.device)

        if isinstance(self.logger, ImageLoggerMixin):
            self.logger.select_images_to_log(train_loader, valid_loader)

        # continuing to train a model: either total epochs or extra epochs
        if self.starting_epoch > 0:
            epochs = self.starting_epoch + epochs

        for e in range(self.starting_epoch + 1, epochs + 1):
            self.current_epoch = e
            self._metadata["epoch"] = e
            train_loss = self._epoch(
                train_loader, mode="train", display_iters=display_iters
            )
            if self.scheduler:
                self.scheduler.step()

            lr = self.optimizer.param_groups[0]["lr"]
            msg = f"Epoch {e}/{epochs} (lr={lr}), train loss {float(train_loss):.5f}"
            if e % self.eval_interval == 0:
                with torch.no_grad():
                    logging.info(f"Training for epoch {e} done, starting evaluation")
                    valid_loss = self._epoch(
                        valid_loader, mode="eval", display_iters=display_iters
                    )
                    if self._print_valid_loss:
                        msg += f", valid loss {float(valid_loss):.5f}"

            self.snapshot_manager.update(e, self.state_dict(), last=(e == epochs))
            logging.info(msg)

            epoch_metrics = self._metadata.get("metrics")
            if (
                e % self.eval_interval == 0
                and epoch_metrics is not None
                and len(epoch_metrics) > 0
            ):
                logging.info(f"Model performance:")
                line_length = max([len(name) for name in epoch_metrics.keys()]) + 2
                for name, score in epoch_metrics.items():
                    logging.info(f"  {(name + ':').ljust(line_length)}{score:6.2f}")

    def _epoch(
        self,
        loader: torch.utils.data.DataLoader,
        mode: str = "train",
        display_iters: int = 500,
    ) -> float:
        """Facilitates training over an epoch. Returns the loss over the batches.

        Args:
            loader: Data loader, which is an iterator over instances.
                Each batch contains image tensor and heat maps tensor input samples.
            mode: str identifier to instruct the Runner whether to train or evaluate.
                Possible values are: "train" or "eval".
            display_iters: the number of iterations between each loss print

        Raises:
            ValueError: When the given mode is invalid

        Returns:
            epoch_loss: Average of the loss over the batches.
        """
        if mode == "train":
            self.model.train()
        elif mode == "eval" or mode == "inference":
            self.model.eval()
        else:
            raise ValueError(f"Runner mode must be train or eval, found mode={mode}.")

        epoch_loss = []
        loss_metrics = defaultdict(list)
        for i, batch in enumerate(loader):
            losses_dict = self.step(batch, mode)
            if "total_loss" in losses_dict:
                epoch_loss.append(losses_dict["total_loss"])
                if (i + 1) % display_iters == 0 and mode != "eval":
                    logging.info(
                        f"Number of iterations: {i + 1}, "
                        f"loss: {losses_dict['total_loss']:.5f}, "
                        f"lr: {self.optimizer.param_groups[0]['lr']}"
                    )

            for key in losses_dict.keys():
                loss_metrics[key].append(losses_dict[key])

        perf_metrics = None
        if mode == "eval":
            perf_metrics = self._compute_epoch_metrics()
            self._metadata["metrics"] = perf_metrics
            self._epoch_predictions = {}
            self._epoch_ground_truth = {}

        if len(epoch_loss) > 0:
            epoch_loss = np.mean(epoch_loss).item()
        else:
            epoch_loss = 0
        self.history[f"{mode}_loss"].append(epoch_loss)

        metrics_to_log = {}
        if perf_metrics:
            for name, score in perf_metrics.items():
                if not isinstance(score, (int, float)):
                    score = 0.0
                metrics_to_log[name] = score

        for key in loss_metrics:
            name = f"{mode}.{key}"
            val = float("nan")
            if np.sum(~np.isnan(loss_metrics[key])) > 0:
                val = np.nanmean(loss_metrics[key]).item()
            self._metadata["losses"][name] = val
            metrics_to_log[f"losses/{name}"] = val

        self.csv_logger.log(metrics_to_log, step=self.current_epoch)
        if self.logger:
            self.logger.log(metrics_to_log, step=self.current_epoch)

        return epoch_loss

    def _load_scheduler_state_dict(self, load_state_dict: bool, snapshot: dict) -> None:
        if self.scheduler is None:
            return

        loaded_state_dict = False
        if load_state_dict and "scheduler" in snapshot:
            try:
                schedulers.load_scheduler_state(self.scheduler, snapshot["scheduler"])
                loaded_state_dict = True
            except ValueError as err:
                logging.warning(
                    "Failed to load the scheduler state_dict. The scheduler will "
                    "restart at epoch 0. This is expected if the scheduler "
                    "configuration was edited since the original snapshot was "
                    f"trained. Error: {err}"
                )

        if not loaded_state_dict and self.starting_epoch > 0:
            logging.info(
                f"Setting the scheduler starting epoch to {self.starting_epoch}"
            )
            self.scheduler.last_epoch = self.starting_epoch


class PoseTrainingRunner(TrainingRunner[PoseModel]):
    """Runner to train pose estimation models"""

    def __init__(
        self,
        model: PoseModel,
        optimizer: torch.optim.Optimizer,
        load_head_weights: bool = True,
        **kwargs,
    ):
        """
        Args:
            model: The neural network for solving pose estimation task.
            optimizer: A PyTorch optimizer for updating model parameters.
            load_head_weights: When `snapshot_path` is not None, whether to load the
                head weights from the saved snapshot or just the backbone weights.
            **kwargs: TrainingRunner kwargs
        """
        self._load_head_weights = load_head_weights
        super().__init__(model, optimizer, **kwargs)

    def load_snapshot(
        self,
        snapshot_path: str | Path,
        device: str,
        model: PoseModel,
        weights_only: bool | None = None,
    ) -> dict:
        """Loads the state dict for a model from a file

        This method loads a file containing a DeepLabCut PyTorch model snapshot onto
        a given device, and sets the model weights using the state_dict.

        Args:
            snapshot_path: the path containing the model weights to load
            device: the device on which the model should be loaded
            model: the model for which the weights are loaded
            weights_only: Value for torch.load() `weights_only` parameter.
                If False, the python pickle module is used implicitly, which is known to
                be insecure. Only set to False if you're loading data that you trust
                (e.g. snapshots that you created yourself). For more information, see:
                    https://pytorch.org/docs/stable/generated/torch.load.html
                If None, the default value is used:
                    `deeplabcut.pose_estimation_pytorch.get_load_weights_only()`

        Returns:
            The content of the snapshot file.
        """
        snapshot = attempt_snapshot_load(snapshot_path, device, weights_only)
        if self._load_head_weights:
            model.load_state_dict(snapshot["model"])
        else:
            backbone_prefix = "backbone."
            backbone_weights = {
                k[len(backbone_prefix) :]: v
                for k, v in snapshot["model"].items()
                if k.startswith(backbone_prefix)
            }
            model.backbone.load_state_dict(backbone_weights)

        return snapshot

    def step(
        self, batch: dict[str, Any], mode: str = "train"
    ) -> dict[str, torch.Tensor]:
        """Perform a single epoch gradient update or validation step.

        Args:
            batch: Tuple of input image(s) and target(s) for train or valid single step.
            mode: `train` or `eval`. Defaults to "train".

        Raises:
            ValueError: "Runner must be in train or eval mode, but {mode} was found."

        Returns:
            dict: {
                "total_loss": aggregate_loss,
                "aux_loss_1": loss_value,
                ...,
            }
        """
        if mode not in ["train", "eval"]:
            raise ValueError(
                f"BottomUpSolver must be in train or eval mode, but {mode} was found."
            )

        if mode == "train":
            self.optimizer.zero_grad()

        inputs = batch["image"]
        inputs = inputs.to(self.device).float()
        outputs = self.model(inputs)

        if self._data_parallel:
            underlying_model = self.model.module
        else:
            underlying_model = self.model

        target = underlying_model.get_target(outputs, batch["annotations"])
        losses_dict = underlying_model.get_loss(outputs, target)
        if mode == "train":
            losses_dict["total_loss"].backward()
            self.optimizer.step()

        if isinstance(self.logger, ImageLoggerMixin):
            self.logger.log_images(batch, outputs, target, step=self.current_epoch)

        if mode == "eval":
            predictions = {
                name: {k: v.detach().cpu().numpy() for k, v in pred.items()}
                for name, pred in underlying_model.get_predictions(outputs).items()
            }

            ground_truth = batch["annotations"]["keypoints"]
            if batch["annotations"]["with_center_keypoints"][0]:
                ground_truth = ground_truth[..., :-1, :]

            self._update_epoch_predictions(
                name="bodyparts",
                gt_keypoints=ground_truth,
                pred_keypoints=predictions["bodypart"]["poses"],
                offsets=batch["offsets"],
                scales=batch["scales"],
            )
            if "unique_bodypart" in predictions:
                self._update_epoch_predictions(
                    name="unique_bodyparts",
                    gt_keypoints=batch["annotations"]["keypoints_unique"],
                    pred_keypoints=predictions["unique_bodypart"]["poses"],
                    offsets=batch["offsets"],
                    scales=batch["scales"],
                )

        return {k: v.detach().cpu().numpy() for k, v in losses_dict.items()}

    def _compute_epoch_metrics(self) -> dict[str, float]:
        """Computes the metrics using the data accumulated during an epoch
        Returns:
            A dictionary containing the different losses for the step
        """
        scores = metrics.compute_metrics(
            ground_truth=self._epoch_ground_truth["bodyparts"],
            predictions=self._epoch_predictions["bodyparts"],
            single_animal=False,
            unique_bodypart_gt=self._epoch_ground_truth.get("unique_bodyparts"),
            unique_bodypart_poses=self._epoch_predictions.get("unique_bodyparts"),
            pcutoff=0.6,
            compute_detection_rmse=False,
        )
        return {f"metrics/test.{metric}": value for metric, value in scores.items()}

    def _update_epoch_predictions(
        self,
        name: str,
        gt_keypoints: torch.Tensor,
        pred_keypoints: torch.Tensor,
        scales: torch.Tensor,
        offsets: torch.Tensor,
    ) -> None:
        """Updates the stored predictions with a new batch"""
        epoch_gt_metric = self._epoch_ground_truth.get(name, {})
        epoch_metric = self._epoch_predictions.get(name, {})
        assert len(gt_keypoints) == len(pred_keypoints)
        assert len(offsets) == len(scales)
        scales = scales.detach().cpu().numpy()
        offsets = offsets.detach().cpu().numpy()

        for gt, pred, scale, offset in zip(
            gt_keypoints,
            pred_keypoints,
            scales,
            offsets,
        ):
            ground_truth = gt.detach().cpu().numpy()
            pred = pred.copy()

            # rescale to the full image for TD or CTD
            ground_truth[..., :2] = (ground_truth[..., :2] * scale) + offset
            pred[..., :2] = (pred[..., :2] * scale) + offset

            # we don't care about image paths here - use a default index
            index = len(epoch_metric) + 1
            epoch_gt_metric[f"sample{index:09}"] = ground_truth
            epoch_metric[f"sample{index:09}"] = pred

        self._epoch_ground_truth[name] = epoch_gt_metric
        self._epoch_predictions[name] = epoch_metric


class DetectorTrainingRunner(TrainingRunner[BaseDetector]):
    """Runner to train object detection models"""

    def __init__(self, model: BaseDetector, optimizer: torch.optim.Optimizer, **kwargs):
        """
        Args:
            model: The detector model to train.
            optimizer: The optimizer to use to train the model.
            **kwargs: TrainingRunner kwargs
        """
        log_filename = "learning_stats_detector.csv"
        if "log_filename" in kwargs:
            log_filename = kwargs.pop("log_filename")

        super().__init__(model, optimizer, log_filename=log_filename, **kwargs)
        self._pycoco_warning_displayed = False
        self._print_valid_loss = False

    def step(
        self, batch: dict[str, Any], mode: str = "train"
    ) -> dict[str, torch.Tensor]:
        """Perform a single epoch gradient update or validation step.

        Args:
            batch: Tuple of input image(s) and target(s) for train or valid single step.
            mode: `train` or `eval`. Defaults to "train".

        Raises:
            ValueError: "Runner must be in train or eval mode, but {mode} was found."

        Returns:
            dict: {
                'total_loss': torch.Tensor,
                'aux_loss_1': torch.Tensor,
                ...,
            }
        """
        if mode not in ["train", "eval"]:
            raise ValueError(
                f"DetectorSolver must be in train or eval mode, but {mode} was found."
            )

        if mode == "train":
            self.optimizer.zero_grad()
            self.model.train()
        else:
            self.model.eval()

        images = batch["image"]
        images = images.to(self.device)

        if self._data_parallel:
            underlying_model = self.model.module
        else:
            underlying_model = self.model

        target = underlying_model.get_target(batch["annotations"])
        for item in target:  # target is a list here
            for key in item:
                if item[key] is not None:
                    item[key] = item[key].to(self.device)

        losses, predictions = self.model(images, target)

        # losses only returned during training, not evaluation
        if mode == "train":
            losses["total_loss"] = sum(loss_part for loss_part in losses.values())
            losses["total_loss"].backward()
            self.optimizer.step()
            losses = {k: v.detach().cpu().numpy() for k, v in losses.items()}

        elif mode == "eval":
            losses["total_loss"] = float("nan")
            self._update_epoch_predictions(
                paths=batch["path"],
                sizes=batch["original_size"],
                bboxes=batch["annotations"]["boxes"],
                predictions=predictions,
                offsets=batch["offsets"],
                scales=batch["scales"],
            )

        return losses

    def _compute_epoch_metrics(self) -> dict[str, float]:
        """Returns: bounding box metrics, if"""
        try:
            return {
                f"metrics/test.{k}": v
                for k, v in metrics.compute_bbox_metrics(
                    self._epoch_ground_truth, self._epoch_predictions
                ).items()
            }
        except ModuleNotFoundError:
            if not self._pycoco_warning_displayed:
                logging.info(
                    "\nNote:\n"
                    "Cannot compute bounding box metrics as ``pycocotools`` is not "
                    "installed. If you want bounding box mAP metrics when training "
                    "detectors for top-down models, please run ``pip install "
                    "pycocotools``.\n"
                )
                self._pycoco_warning_displayed = True

        return {}

    def _update_epoch_predictions(
        self,
        paths: torch.Tensor,
        sizes: torch.Tensor,
        bboxes: torch.Tensor,
        predictions: list[dict[str, torch.Tensor]],
        scales: torch.Tensor,
        offsets: torch.Tensor,
    ) -> None:
        """Updates the stored predictions with a new batch"""
        for img_path, img_size, img_bboxes, img_pred, scale, offset in zip(
            paths, sizes, bboxes, predictions, scales, offsets
        ):
            scale_x, scale_y = scale
            scale_factors = np.array([scale_x, scale_y, scale_x, scale_y])
            offset = np.array(offset)

            # remove bboxes that are not visible
            img_bbox_mask = (img_bboxes[:, 2] > 0.0) & (img_bboxes[:, 3] > 0.0)
            img_bboxes = img_bboxes[img_bbox_mask]

            # rescale ground truth bounding boxes
            gt_rescaled = img_bboxes.cpu().numpy() * scale_factors
            gt_rescaled[..., :2] = gt_rescaled[..., :2] + offset

            # convert to COCO format (xywh) before rescaling
            pred_rescaled = img_pred["boxes"].detach().cpu().numpy()
            pred_rescaled[:, 2] -= pred_rescaled[:, 0]
            pred_rescaled[:, 3] -= pred_rescaled[:, 1]
            pred_rescaled[..., :4] = pred_rescaled[..., :4] * scale_factors
            pred_rescaled[..., :2] = pred_rescaled[..., :2] + offset

            self._epoch_ground_truth[img_path] = {
                "bboxes": gt_rescaled,
                "width": img_size[1],
                "height": img_size[0],
            }
            self._epoch_predictions[img_path] = {
                "bboxes": pred_rescaled,
                "scores": img_pred["scores"].detach().cpu().numpy(),
            }


def build_training_runner(
    runner_config: dict,
    model_folder: Path,
    task: Task,
    model: nn.Module,
    device: str,
    gpus: list[int] | None = None,
    snapshot_path: str | Path | None = None,
    load_head_weights: bool = True,
    logger: BaseLogger | None = None,
) -> TrainingRunner:
    """
    Build a runner object according to a pytorch configuration file

    Args:
        runner_config: the configuration for the runner
        model_folder: the folder where models should be saved
        task: the task the runner will perform
        model: the model to run
        device: the device to use (e.g. {'cpu', 'cuda:0', 'mps'})
        gpus: the list of GPU indices to use for multi-GPU training
        snapshot_path: the snapshot from which to load the weights
        load_head_weights: When `snapshot_path` is not None and a pose model is being
            trained, whether to load the head weights from the saved snapshot.
        logger: the logger to use, if any

    Returns:
        the runner that was built
    """
    optimizer = build_optimizer(model, runner_config["optimizer"])
    scheduler = schedulers.build_scheduler(runner_config.get("scheduler"), optimizer)

    # if no custom snapshot prefix is defined, use the default one
    snapshot_prefix = runner_config.get("snapshot_prefix")
    if snapshot_prefix is None or len(snapshot_prefix) == 0:
        snapshot_prefix = task.snapshot_prefix

    kwargs = dict(
        model=model,
        optimizer=optimizer,
        snapshot_manager=TorchSnapshotManager(
            snapshot_prefix=snapshot_prefix,
            model_folder=model_folder,
            key_metric=runner_config.get("key_metric"),
            key_metric_asc=runner_config.get("key_metric_asc"),
            max_snapshots=runner_config["snapshots"]["max_snapshots"],
            save_epochs=runner_config["snapshots"]["save_epochs"],
            save_optimizer_state=runner_config["snapshots"]["save_optimizer_state"],
        ),
        device=device,
        gpus=gpus,
        eval_interval=runner_config.get("eval_interval"),
        snapshot_path=snapshot_path,
        scheduler=scheduler,
        load_scheduler_state_dict=runner_config.get("load_scheduler_state_dict", True),
        logger=logger,
        load_weights_only=runner_config.get("load_weights_only", None),
    )
    if task == Task.DETECT:
        return DetectorTrainingRunner(**kwargs)

    kwargs["load_head_weights"] = load_head_weights
    return PoseTrainingRunner(**kwargs)


def build_optimizer(
    model: nn.Module,
    optimizer_config: dict,
) -> torch.optim.Optimizer:
    """Builds an optimizer from a configuration.

    Args:
        model: The model to optimize.
        optimizer_config: The configuration for the optimizer.

    Returns:
        The optimizer for the model built according to the given configuration.
    """
    optim_cls = getattr(torch.optim, optimizer_config["type"])
    optimizer = optim_cls(params=model.parameters(), **optimizer_config["params"])
    return optimizer


--- File: deeplabcut/pose_estimation_pytorch/runners/inference.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from abc import ABCMeta, abstractmethod
from pathlib import Path
from typing import Any, Generic, Iterable

import numpy as np
import torch
import torch.nn as nn

import deeplabcut.pose_estimation_pytorch.runners.shelving as shelving
from deeplabcut.pose_estimation_pytorch.data.postprocessor import Postprocessor
from deeplabcut.pose_estimation_pytorch.data.preprocessor import Preprocessor
from deeplabcut.pose_estimation_pytorch.models.detectors import BaseDetector
from deeplabcut.pose_estimation_pytorch.models.model import PoseModel
from deeplabcut.pose_estimation_pytorch.runners.base import ModelType, Runner
from deeplabcut.pose_estimation_pytorch.runners.dynamic_cropping import DynamicCropper
from deeplabcut.pose_estimation_pytorch.task import Task


class InferenceRunner(Runner, Generic[ModelType], metaclass=ABCMeta):
    """Base class for inference runners

    A runner takes a model and runs actions on it, such as training or inference
    """

    def __init__(
        self,
        model: ModelType,
        batch_size: int = 1,
        device: str = "cpu",
        snapshot_path: str | Path | None = None,
        preprocessor: Preprocessor | None = None,
        postprocessor: Postprocessor | None = None,
        load_weights_only: bool | None = None,
    ):
        """
        Args:
            model: The model to run actions on
            device: The device to use (e.g. {'cpu', 'cuda:0', 'mps'})
            snapshot_path: If defined, the path of a snapshot from which to load
                pretrained weights
            preprocessor: The preprocessor to use on images before inference
            postprocessor: The postprocessor to use on images after inference
            load_weights_only: Value for the torch.load() `weights_only` parameter.
                If False, the python pickle module is used implicitly, which is known to
                    be insecure. Only set to False if you're loading data that you trust
                    (e.g. snapshots that you created). For more information, see:
                        https://pytorch.org/docs/stable/generated/torch.load.html
                If None, the default value is used:
                    `deeplabcut.pose_estimation_pytorch.get_load_weights_only()`
        """
        super().__init__(model=model, device=device, snapshot_path=snapshot_path)
        if not isinstance(batch_size, int) or batch_size <= 0:
            raise ValueError(f"batch_size must be a positive integer; is {batch_size}")

        self.batch_size = batch_size
        self.preprocessor = preprocessor
        self.postprocessor = postprocessor

        if self.snapshot_path is not None and self.snapshot_path != "":
            self.load_snapshot(
                self.snapshot_path,
                self.device,
                self.model,
                weights_only=load_weights_only,
            )

        self._batch: torch.Tensor | None = None
        self._contexts: list[dict] = []
        self._image_batch_sizes: list[int] = []
        self._predictions: list = []

    @abstractmethod
    def predict(self, inputs: torch.Tensor) -> list[dict[str, dict[str, np.ndarray]]]:
        """Makes predictions from a model input and output

        Args:
            the inputs to the model, of shape (batch_size, ...)

        Returns:
            the predictions for each of the 'batch_size' inputs
        """

    @torch.no_grad()
    def inference(
        self,
        images: (
            Iterable[str | Path | np.ndarray]
            | Iterable[tuple[str | Path | np.ndarray, dict[str, Any]]]
        ),
        shelf_writer: shelving.ShelfWriter | None = None,
    ) -> list[dict[str, np.ndarray]]:
        """Run model inference on the given dataset

        TODO: Add an option to also return head outputs (such as heatmaps)? Can be
         super useful for debugging

        Args:
            images: the images to run inference on, optionally with context
            shelf_writer: by default, data are saved in a list and returned at the end
                of inference. Passing a shelf manager writes data to disk on-the-fly
                using a "shelf" (a pickle-based, persistent, database-like object by
                default, resulting in constant memory footprint). The returned list is
                then empty.

        Returns:
            a dict containing head predictions for each image
            [
                {
                    "bodypart": {"poses": np.array},
                    "unique_bodypart": {"poses": np.array},
                }
            ]
        """
        self.model.to(self.device)
        self.model.eval()

        results = []
        for data in images:
            self._prepare_inputs(data)
            self._process_full_batches()
            results += self._extract_results(shelf_writer)

        # Process the last batch even if not full
        if self._inputs_waiting_for_processing():
            self._process_batch()
            results += self._extract_results(shelf_writer)

        return results

    def _prepare_inputs(
        self,
        data: str | Path | np.ndarray | tuple[str | Path | np.ndarray, dict],
    ) -> None:
        """
        Prepares inputs for an image and adds them to the data ready to be processed
        """
        if isinstance(data, (str, Path, np.ndarray)):
            inputs, context = data, {}
        else:
            inputs, context = data

        if self.preprocessor is not None:
            inputs, context = self.preprocessor(inputs, context)
        else:
            inputs = torch.as_tensor(inputs)

        self._contexts.append(context)
        self._image_batch_sizes.append(len(inputs))

        # skip when there are no inputs for an image
        if len(inputs) == 0:
            return

        if self._batch is None:
            self._batch = inputs
        else:
            self._batch = torch.cat([self._batch, inputs], dim=0)

    def _process_full_batches(self) -> None:
        """Processes prepared inputs in batches of the desired batch size."""
        while self._batch is not None and len(self._batch) >= self.batch_size:
            self._process_batch()

    def _extract_results(self, shelf_writer: shelving.ShelfWriter) -> list:
        """Obtains results that were obtained from processing a batch."""
        results = []
        while (
            len(self._image_batch_sizes) > 0
            and len(self._predictions) >= self._image_batch_sizes[0]
        ):
            num_predictions = self._image_batch_sizes[0]
            image_predictions = self._predictions[:num_predictions]
            context = self._contexts[0]

            if self.postprocessor is not None:
                # TODO: Should we return context?
                # TODO: typing update - the post-processor can remove a dict level
                image_predictions, _ = self.postprocessor(image_predictions, context)

            if shelf_writer is not None:
                shelf_writer.add_prediction(
                    bodyparts=image_predictions["bodyparts"],
                    unique_bodyparts=image_predictions.get("unique_bodyparts"),
                    identity_scores=image_predictions.get("identity_scores"),
                    features=image_predictions.get("features"),
                )
            else:
                results.append(image_predictions)

            self._contexts = self._contexts[1:]
            self._image_batch_sizes = self._image_batch_sizes[1:]
            self._predictions = self._predictions[num_predictions:]

        return results

    def _process_batch(self) -> None:
        """
        Processes a batch. There must be inputs waiting to be processed before this is
        called, otherwise this method will raise an error.
        """
        batch = self._batch[: self.batch_size]
        self._predictions += self.predict(batch)

        # remove processed inputs from batch
        if len(self._batch) <= self.batch_size:
            self._batch = None
        else:
            self._batch = self._batch[self.batch_size :]

    def _inputs_waiting_for_processing(self) -> bool:
        """Returns: Whether there are inputs which have not yet been processed"""
        return self._batch is not None and len(self._batch) > 0


class PoseInferenceRunner(InferenceRunner[PoseModel]):
    """Runner for pose estimation inference"""

    def __init__(
        self,
        model: PoseModel,
        dynamic: DynamicCropper | None = None,
        **kwargs,
    ):
        super().__init__(model, **kwargs)
        self.dynamic = dynamic
        if dynamic is not None:
            print(
                f"Inference runner using dynamic cropping: {self.dynamic}.\n"
                "Note that dynamic cropping should only be used to analyze videos with "
                "bottom-up pose estimation models."
            )
            if self.batch_size != 1:
                raise ValueError(
                    "Dynamic cropping can only be used with batch size 1. Please set "
                    "your batch size to 1."
                )

    def predict(self, inputs: torch.Tensor) -> list[dict[str, dict[str, np.ndarray]]]:
        """Makes predictions from a model input and output

        Args:
            the inputs to the model, of shape (batch_size, ...)

        Returns:
            predictions for each of the 'batch_size' inputs, made by each head, e.g.
            [
                {
                    "bodypart": {"poses": np.ndarray},
                    "unique_bodypart": {"poses": np.ndarray},
                }
            ]
        """
        if self.dynamic is not None:
            inputs = self.dynamic.crop(inputs)

        outputs = self.model(inputs.to(self.device))
        raw_predictions = self.model.get_predictions(outputs)

        if self.dynamic is not None:
            self.dynamic.update(raw_predictions["bodypart"]["poses"])

        predictions = [
            {
                head: {
                    pred_name: pred[b].cpu().numpy()
                    for pred_name, pred in head_outputs.items()
                }
                for head, head_outputs in raw_predictions.items()
            }
            for b in range(len(inputs))
        ]
        return predictions


class DetectorInferenceRunner(InferenceRunner[BaseDetector]):
    """Runner for object detection inference"""

    def __init__(self, model: BaseDetector, **kwargs):
        """
        Args:
            model: The detector to use for inference.
            **kwargs: Inference runner kwargs.
        """
        super().__init__(model, **kwargs)

    def predict(self, inputs: torch.Tensor) -> list[dict[str, dict[str, np.ndarray]]]:
        """Makes predictions from a model input and output

        Args:
            the inputs to the model, of shape (batch_size, ...)

        Returns:
            predictions for each of the 'batch_size' inputs, made by each head, e.g.
            [
                {
                    "bodypart": {"poses": np.ndarray},
                    "unique_bodypart": "poses": np.ndarray},
            ]
        """
        _, raw_predictions = self.model(inputs.to(self.device))
        predictions = [
            {
                "detection": {
                    "bboxes": item["boxes"].cpu().numpy().reshape(-1, 4),
                    "scores": item["scores"].cpu().numpy().reshape(-1),
                }
            }
            for item in raw_predictions
        ]
        return predictions


def build_inference_runner(
    task: Task,
    model: nn.Module,
    device: str,
    snapshot_path: str | Path,
    batch_size: int = 1,
    preprocessor: Preprocessor | None = None,
    postprocessor: Postprocessor | None = None,
    dynamic: DynamicCropper | None = None,
    load_weights_only: bool | None = None,
) -> InferenceRunner:
    """
    Build a runner object according to a pytorch configuration file

    Args:
        task: the inference task to run
        model: the model to run
        device: the device to use (e.g. {'cpu', 'cuda:0', 'mps'})
        snapshot_path: the snapshot from which to load the weights
        batch_size: the batch size to use to run inference
        preprocessor: the preprocessor to use on images before inference
        postprocessor: the postprocessor to use on images after inference
        dynamic: The DynamicCropper used for video inference, or None if dynamic
            cropping should not be used. Only for bottom-up pose estimation models.
            Should only be used when creating inference runners for video pose
            estimation with batch size 1.
        load_weights_only: Value for the torch.load() `weights_only` parameter.
            If False, the python pickle module is used implicitly, which is known to
            be insecure. Only set to False if you're loading data that you trust (e.g.
            snapshots that you created). For more information, see:
                https://pytorch.org/docs/stable/generated/torch.load.html
            If None, the default value is used:
                `deeplabcut.pose_estimation_pytorch.get_load_weights_only()`

    Returns:
        The inference runner.
    """
    kwargs = dict(
        model=model,
        device=device,
        snapshot_path=snapshot_path,
        batch_size=batch_size,
        preprocessor=preprocessor,
        postprocessor=postprocessor,
        load_weights_only=load_weights_only,
    )
    if task == Task.DETECT:
        if dynamic is not None:
            raise ValueError(
                f"The DynamicCropper can only be used for pose estimation; not object "
                f"detection. Please turn off dynamic cropping."
            )
        return DetectorInferenceRunner(**kwargs)

    if task != Task.BOTTOM_UP:
        if dynamic is not None:
            print(
                "Turning off dynamic cropping. It should only be used for bottom-up "
                f"pose estimation models, but you are using a {task} model."
            )
        dynamic = None

    return PoseInferenceRunner(dynamic=dynamic, **kwargs)


--- File: deeplabcut/pose_estimation_pytorch/runners/dynamic_cropping.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Modules to dynamically crop individuals out of videos to improve video analysis"""
import math
from dataclasses import dataclass, field
from typing import Optional

import torch


@dataclass
class DynamicCropper:
    """
    If the state is true, then dynamic cropping will be performed. That means that
    if an object is detected (i.e. any body part > detection threshold), then object
    boundaries are computed according to the smallest/largest x position and
    smallest/largest y position of all body parts. This window is expanded by the
    margin and from then on only the posture within this crop is analyzed (until the
    object is lost, i.e. < detection threshold). The current position is utilized for
    updating the crop window for the next frame (this is why the margin is important
    and should be set large enough given the movement of the animal).

    Attributes:
        threshold: float
            The threshold score for bodyparts above which an individual is deemed to
            have been detected.
        margin: int
            The margin used to expand an individuals bounding box before cropping it.

    Examples:
        >>> import deeplabcut.pose_estimation_pytorch.models as models
        >>>
        >>> model: models.PoseModel
        >>> frames: torch.Tensor  # shape (num_frames, 3, H, W)
        >>>
        >>> dynamic = DynamicCropper(threshold=0.6, margin=25)
        >>> predictions = []
        >>> for image in frames:
        >>>     image = dynamic.crop(image)
        >>>
        >>>     outputs = model(image)
        >>>     preds = model.get_predictions(outputs)
        >>>     pose = preds["bodypart"]["poses"]
        >>>
        >>>     dynamic.update(pose)
        >>>     predictions.append(pose)
        >>>
    """
    threshold: float
    margin: int
    _crop: tuple[int, int, int, int] | None = field(default=None, repr=False)
    _shape: tuple[int, int] | None = field(default=None, repr=False)

    def crop(self, image: torch.Tensor) -> torch.Tensor:
        """Crops an input image according to the dynamic cropping parameters.

        Args:
            image: The image to crop, of shape (1, C, H, W).

        Returns:
            The cropped image of shape (1, C, H', W'), where [H', W'] is the size of
            the crop.

        Raises:
            RuntimeError: if there is not exactly one image in the batch to crop, or if
                `crop` was previously called with an image of a different width or
                height.
        """
        if len(image) != 1:
            raise RuntimeError(
                "DynamicCropper can only be used with batch size 1 (found image "
                f"shape: {image.shape})"
            )

        if self._shape is None:
            self._shape = image.shape[3], image.shape[2]

        if image.shape[3] != self._shape[0] or image.shape[2] != self._shape[1]:
            raise RuntimeError(
                "All frames must have the same shape; The first frame had (W, H) "
                f"{self._shape} but the current frame has shape {image.shape}."
            )

        if self._crop is None:
            return image

        x0, y0, x1, y1 = self._crop
        return image[:, :, y0:y1, x0:x1]

    def update(self, pose: torch.Tensor) -> None:
        """Updates the dynamic crop according to the pose model output.

        Uses the pose predicted by the model to update the dynamic crop parameters for
        the next frame. Scales the pose predicted in the cropped image back to the
        original image space and returns it.

        This method modifies the pose tensor in-place; so pass a copy of the tensor if
        you need to keep the original values.

        Args:
            pose: The pose that was predicted by the pose estimation model in the
                cropped image coordinate space.
        """
        if self._shape is None:
            raise RuntimeError(f"You must call `crop` before calling `update`.")

        # offset the pose to the original image space
        offset_x, offset_y = 0, 0
        if self._crop is not None:
            offset_x, offset_y = self._crop[:2]
        pose[..., 0] = pose[..., 0] + offset_x
        pose[..., 1] = pose[..., 1] + offset_y

        # check whether keypoints can be used for dynamic cropping
        keypoints = pose[..., :3].reshape(-1, 3)
        keypoints = keypoints[~torch.any(torch.isnan(keypoints), dim=1)]
        if len(keypoints) == 0:
            self.reset()
            return

        mask = keypoints[:, 2] >= self.threshold
        if torch.all(~mask):
            self.reset()
            return

        # set the crop coordinates
        x0 = self._min_value(keypoints[:, 0], self._shape[0])
        x1 = self._max_value(keypoints[:, 0], self._shape[0])
        y0 = self._min_value(keypoints[:, 1], self._shape[1])
        y1 = self._max_value(keypoints[:, 1], self._shape[1])
        crop_w, crop_h = x1 - x0, y1 - y0
        if crop_w == 0 or crop_h == 0:
            self.reset()
            return

        self._crop = x0, y0, x1, y1

    def reset(self) -> None:
        """Resets the DynamicCropper to not crop the next frame"""
        self._crop = None

    @staticmethod
    def build(
        dynamic: bool, threshold: float, margin: int
    ) -> Optional["DynamicCropper"]:
        """Builds the DynamicCropper based on the given parameters

        Args:
            dynamic: Whether dynamic cropping should be used
            threshold: The threshold score for bodyparts above which an individual is
                deemed to have been detected.
            margin: The margin used to expand an individuals bounding box before
                cropping it.

        Returns:
            None if dynamic is False
            DynamicCropper to use if dynamic is True
        """
        if not dynamic:
            return None

        return DynamicCropper(threshold, margin)

    def _min_value(self, coordinates: torch.Tensor, maximum: int) -> int:
        """Returns: min(coordinates - margin), clipped to [0, maximum]"""
        return self._clip(
            int(math.floor(torch.min(coordinates).item() - self.margin)),
            maximum,
        )

    def _max_value(self, coordinates: torch.Tensor, maximum: int) -> int:
        """Returns: max(coordinates + margin), clipped to [0, maximum]"""
        return self._clip(
            int(math.ceil(torch.max(coordinates).item() + self.margin)),
            maximum,
        )

    def _clip(self, value: int, maximum: int) -> int:
        """Returns: The value clipped to [0, maximum]"""
        return min(max(value, 0), maximum)


--- File: deeplabcut/pose_estimation_pytorch/runners/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import logging
import os
import pickle
from abc import ABC
from pathlib import Path
from typing import Generic, TypeVar

import numpy as np
import torch
import torch.nn as nn

ModelType = TypeVar("ModelType", bound=nn.Module)

_load_weights_only: bool = (
    os.getenv("TORCH_LOAD_WEIGHTS_ONLY", "true").lower() in ("true", "1")
)


def get_load_weights_only() -> bool:
    """Gets the default value to use when loading snapshots with `torch.load(...)`.

    Returns:
        The default `weights_only` value when loading snapshots using `torch.load(...)`.
    """
    global _load_weights_only
    return _load_weights_only


def set_load_weights_only(value: bool) -> None:
    """Sets the default value to use when loading snapshots with `torch.load(...)`.

    Args:
        value: The default `weights_only` value to use when loading snapshots using
            `torch.load(...)`.
    """
    global _load_weights_only
    _load_weights_only = value


class Runner(ABC, Generic[ModelType]):
    """Runner base class

    A runner takes a model and runs actions on it, such as training or inference
    """

    def __init__(
        self,
        model: ModelType,
        device: str = "cpu",
        gpus: list[int] | None = None,
        snapshot_path: str | Path | None = None,
    ):
        """
        Args:
            model: the model to run
            device: the device to use (e.g. {'cpu', 'cuda:0', 'mps'})
            gpus: the list of GPU indices to use for multi-GPU training
            snapshot_path: the path of a snapshot from which to load model weights
        """
        if gpus is None:
            gpus = []

        if len(gpus) == 1:
            if device != "cuda":
                raise ValueError(
                    "When specifying a GPU index to train on, the device must be set "
                    f"to 'cuda'. Found {device}"
                )
            device = f"cuda:{gpus[0]}"

        self.model = model
        self.device = device
        self.snapshot_path = snapshot_path
        self._gpus = gpus
        self._data_parallel = len(gpus) > 1

    @staticmethod
    def load_snapshot(
        snapshot_path: str | Path,
        device: str,
        model: ModelType,
        weights_only: bool | None = None,
    ) -> dict:
        """Loads the state dict for a model from a file

        This method loads a file containing a DeepLabCut PyTorch model snapshot onto
        a given device, and sets the model weights using the state_dict.

        Args:
            snapshot_path: The path containing the model weights to load
            device: The device on which the model should be loaded
            model: The model for which the weights are loaded
            weights_only: Value for torch.load() `weights_only` parameter.
                If False, the python pickle module is used implicitly, which is known to
                be insecure. Only set to False if you're loading data that you trust
                (e.g. snapshots that you created yourself). For more information, see:
                    https://pytorch.org/docs/stable/generated/torch.load.html
                If None, the default value is used:
                    `deeplabcut.pose_estimation_pytorch.get_load_weights_only()`

        Returns:
            The content of the snapshot file.
        """
        snapshot = attempt_snapshot_load(snapshot_path, device, weights_only)
        model.load_state_dict(snapshot["model"])
        return snapshot


def attempt_snapshot_load(
    path: str | Path,
    device: str,
    weights_only: bool | None = None,
) -> dict:
    """Attempts to load a snapshot using `torch.load(...)`.

    Args:
        path: The path of the snapshot to try to load..
        device: The device to use for the `map_location`.
        weights_only: Value for torch.load() `weights_only` parameter.
            If False, the python pickle module is used implicitly, which is known to be
            insecure. Only set to False if you're loading data that you trust (e.g.
            snapshots that you created yourself). For more information, see:
                https://pytorch.org/docs/stable/generated/torch.load.html
            If None, the default value is used:
                `deeplabcut.pose_estimation_pytorch.get_load_weights_only()`

    Returns:
        The loaded snapshot.

    Raises:
        pickle.UnpicklingError: If `weights_only=True` but the snapshot failed to load
            with `weights_only=True`.
    """
    try:
        if weights_only is None:
            weights_only = get_load_weights_only()

        snapshot = torch.load(path, map_location=device, weights_only=weights_only)
    except pickle.UnpicklingError as err:
        logging.error(
            f"\nFailed to load the snapshot: {path}.\n\n"
            "If you trust the snapshot that you're trying to load, you can try\n"
            "calling `Runner.load_snapshot` with `weights_only=False`. See the \n"
            "error message below for more information and warnings.\n"
            "You can set the `weights_only` parameter in the model configuration (\n"
            "the content of the pytorch_config.yaml), as:\n\n```\n"
            "runner:\n"
            "  load_weights_only: False\n```\n\n"
            "If it's the detector snapshot that's failing to load, place the\n"
            "`load_weights_only` key under the detector runner:\n\n```\n"
            "detector:\n"
            "    runner:\n"
            "      load_weights_only: False\n```\n\n"
            "You can also set the default `load_weights_only` that will be used when\n"
            "the `load_weights_only` variable is not set in the `pytorch_config.yaml`\n"
            "using `deeplabcut.pose_estimation_pytorch.set_load_weights_only(value)`:\n"
            "\n```\n"
            "from deeplabcut.pose_estimation_pytorch import set_load_weights_only\n"
            "set_load_weights_only(True)\n"
            "```\n\n"
            "You can also set the value for `load_weights_only` with a \n"
            "`TORCH_LOAD_WEIGHTS_ONLY` environment variable. If you call \n"
            "`TORCH_LOAD_WEIGHTS_ONLY=False python -m deeplabcut`, it will launch the\n"
            "DeepLabCut GUI with the default `load_weights_only` value to False.\n"
            "If you set this value to `False`, make sure you only load snapshots that\n"
            "you trust.\n\n"
        )
        raise err

    return snapshot


def fix_snapshot_metadata(path: str | Path) -> None:
    """Replace numpy floats in snapshot metrics

    Only call this method with snapshots that you trust, as torch.load(...) is called
    with `weights_only=False`. For more information, see:
        https://pytorch.org/docs/stable/generated/torch.load.html

    DeepLabCut PyTorch snapshots trained with older releases may have `numpy` floats in
    the stored metrics. This method opens the snapshots (with `weights_only=False`),
    replaces the numpy floats with python floats (allowing to load with
    `weights_only=True`), and saves the new snapshot data.

    Warning: This overwrites your existing snapshot. If you want to ensure that no data
    is lost, copy your snapshot before calling `fix_snapshot_metadata`.

    Args:
        path: The path of the snapshot to fix.
    """
    snapshot = torch.load(path, map_location="cpu", weights_only=False)
    metrics = snapshot.get("metadata", {}).get("metrics")
    if metrics is not None:
        snapshot["metadata"]["metrics"] = {k: float(v) for k, v in metrics.items()}

    torch.save(snapshot, path)


def _add_numpy_to_torch_safe_globals():
    """
    Attempts tot add numpy classes allowing snapshots containing numpy floats in the
    metrics to be loaded without needing to change the `weights_only` argument.

    This fix only works for `numpy>=1.25.0`.
    """
    try:
        from numpy.core.multiarray import scalar
        from numpy.dtypes import Float64DType
        torch.serialization.add_safe_globals([np.dtype, Float64DType, scalar])
    except Exception:
        pass


_add_numpy_to_torch_safe_globals()


--- File: deeplabcut/pose_estimation_pytorch/runners/schedulers.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from typing import Any

import torch
from torch.optim.lr_scheduler import _LRScheduler


class LRListScheduler(_LRScheduler):
    """
    You can achieve increased performance and faster training by using a learning rate
    that changes during training. A scheduler makes the learning rate adaptive. Given a
    list of learning rates and milestones modifies the learning rate accordingly during
    training.
    """

    def __init__(self, optimizer, milestones, lr_list, last_epoch=-1) -> None:
        """
        Args:
            optimizer: optimizer used for learning.
            milestones: number of epochs.
            lr_list: learning rate list.
            last_epoch: where to start the scheduler. (-1: start from beginning)

        Examples:
            input:
                last_epoch = -1
                verbose = False
                milestones = [10, 30, 40]
                lr_list = [[0.00001],[0.000005],[0.000001]]
        """
        self.milestones = milestones
        self.lr_list = lr_list
        super().__init__(optimizer, last_epoch)

    def get_lr(self):
        """Summary:
        Given a milestones, get the corresponding learning rate.

        Returns:
            lr: learning rate value

        Examples:
            input: LRListScheduler object
            output: learning rate (lr) = [0.001]
        """
        if self.last_epoch not in self.milestones:
            return [group["lr"] for group in self.optimizer.param_groups]
        return [lr for lr in self.lr_list[self.milestones.index(self.last_epoch)]]


def build_scheduler(
    scheduler_cfg: dict | None, optimizer: torch.optim.Optimizer
) -> torch.optim.lr_scheduler.LRScheduler | None:
    """Builds a scheduler from a configuration, if defined

    Args:
        scheduler_cfg: the configuration of the scheduler to build
        optimizer: the optimizer the scheduler will be built for

    Returns:
        None if scheduler_cfg is None, otherwise the scheduler
    """
    if scheduler_cfg is None:
        return None

    if scheduler_cfg["type"] == "LRListScheduler":
        scheduler = LRListScheduler
    else:
        scheduler = getattr(torch.optim.lr_scheduler, scheduler_cfg["type"])

    parsed_params = {}
    for param_name, param in scheduler_cfg["params"].items():
        if isinstance(param, list):
            param = [_parse_scheduler_param(p, optimizer) for p in param]
        else:
            param = _parse_scheduler_param(param, optimizer)

        parsed_params[param_name] = param

    return scheduler(optimizer=optimizer, **parsed_params)


def _parse_scheduler_param(param: Any, optimizer: torch.optim.Optimizer) -> Any:
    """Parses parameters so they're built as schedulers if they're configured as one"""
    if isinstance(param, dict) and "type" in param:
        param = build_scheduler(param, optimizer)

    return param


def load_scheduler_state(
    scheduler: torch.optim.lr_scheduler.LRScheduler,
    state_dict: dict,
) -> None:
    """
    Args:
        scheduler: The scheduler for which to load the state dict.
        state_dict: The state dict to load

    Raises:
        ValueError: if the state dict fails to load.
    """
    try:
        scheduler.load_state_dict(state_dict)
    except Exception as err:
        raise ValueError(f"Failed to load state dict: {err}")

    param_groups = scheduler.optimizer.param_groups
    resume_lrs = scheduler.get_last_lr()

    if len(param_groups) != len(resume_lrs):
        raise ValueError(
            f"Number of optimizer parameter groups ({len(param_groups)}) did not match "
            f"number of learning rates to resume from ({len(scheduler.get_last_lr())})."
        )

    # Update the learning rate for the optimizer based on the scheduler
    for group, resume_lr in zip(param_groups, resume_lrs):
        group['lr'] = resume_lr


--- File: deeplabcut/pose_estimation_pytorch/data/transforms.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import warnings
from typing import Any, Iterable, Sequence

import albumentations as A
import cv2
import numpy as np
from albumentations.augmentations.geometric import functional as F
from numpy.typing import NDArray
from scipy.spatial.distance import pdist, squareform
from scipy.stats import truncnorm


def build_transforms(augmentations: dict) -> A.BaseCompose:
    transforms = []

    if resize_aug := augmentations.get("resize", False):
        transforms += build_resize_transforms(resize_aug)

    if (lms_cfg := augmentations.get("longest_max_size")) is not None:
        transforms.append(A.LongestMaxSize(lms_cfg))

    if hflip_cfg := augmentations.get("hflip"):
        hflip_proba = 0.5
        symmetries = None
        if isinstance(hflip_cfg, float):
            hflip_proba = hflip_cfg
        elif isinstance(hflip_cfg, dict):
            if "p" in hflip_cfg:
                hflip_proba = float(hflip_cfg["p"])

            if "symmetries" in hflip_cfg:
                symmetries = []
                for kpt_a, kpt_b in hflip_cfg["symmetries"]:
                    symmetries.append((int(kpt_a), int(kpt_b)))

        if symmetries is not None:
            transforms.append(HFlip(symmetries=symmetries, p=hflip_proba))
        else:
            warnings.warn(
                "Be careful! Do not train pose models with horizontal flips if you have"
                " symmetric keypoints!"
            )
            transforms.append(A.HorizontalFlip(p=hflip_proba))

    if (affine := augmentations.get("affine")) is not None:
        scaling = affine.get("scaling")
        rotation = affine.get("rotation")
        translation = affine.get("translation")
        if rotation is not None:
            rotation = (-rotation, rotation)
        if translation is not None:
            translation = (-translation, translation)

        transforms.append(
            A.Affine(
                scale=scaling,
                rotate=rotation,
                translate_px=translation,
                p=affine.get("p", 0.9),
                keep_ratio=True,
            )
        )

    if bbox_tfm := augmentations.get("random_bbox_transform", False):
        transforms.append(
            RandomBBoxTransform(
                shift_factor=bbox_tfm.get("shift_factor", 0.1),
                shift_prob=bbox_tfm.get("shift_prob", 0.25),
                scale_factor=bbox_tfm.get("scale_factor", (0.75, 1.25)),
                scale_prob=bbox_tfm.get("scale_prob", 1.0),
                p=bbox_tfm.get("p", 1.0),
            )
        )

    if crop_sampling := augmentations.get("crop_sampling"):
        transforms.append(
            A.PadIfNeeded(
                min_height=crop_sampling["height"],
                min_width=crop_sampling["width"],
                border_mode=cv2.BORDER_CONSTANT,
                always_apply=True,
            )
        )
        transforms.append(
            KeypointAwareCrop(
                crop_sampling["width"],
                crop_sampling["height"],
                crop_sampling["max_shift"],
                crop_sampling["method"],
            )
        )

    if augmentations.get("hist_eq", False):
        transforms.append(A.Equalize(p=0.5))
    if augmentations.get("motion_blur", False):
        transforms.append(A.MotionBlur(p=0.5))
    if augmentations.get("covering", False):
        transforms.append(
            CoarseDropout(
                max_holes=10,
                max_height=0.05,
                min_height=0.01,
                max_width=0.05,
                min_width=0.01,
                p=0.5,
            )
        )
    if augmentations.get("elastic_transform", False):
        transforms.append(ElasticTransform(sigma=5, p=0.5))
    if augmentations.get("grayscale", False):
        transforms.append(Grayscale(alpha=(0.5, 1.0)))
    if noise := augmentations.get("gaussian_noise", False):
        # TODO inherit custom gaussian transform to support per_channel = 0.5
        if not isinstance(noise, (int, float)):
            noise = 0.05 * 255
        transforms.append(
            A.GaussNoise(
                var_limit=(0, noise ** 2),
                mean=0,
                per_channel=True,
                # Albumentations doesn't support per_channel = 0.5
                p=0.5,
            )
        )

    if augmentations.get("auto_padding"):
        transforms.append(build_auto_padding(**augmentations["auto_padding"]))

    if augmentations.get("normalize_images"):
        transforms.append(
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        )

    return A.Compose(
        transforms,
        keypoint_params=A.KeypointParams(
            "xy", remove_invisible=False, label_fields=["class_labels"]
        ),
        bbox_params=A.BboxParams(format="coco", label_fields=["bbox_labels"]),
    )


def build_auto_padding(
    min_height: int | None = None,
    min_width: int | None = None,
    pad_height_divisor: int | None = 1,
    pad_width_divisor: int | None = 1,
    position: str = "random",  # TODO: Which default to set?
    border_mode: str = "reflect_101",  # TODO: Which default to set?
    border_value: float | None = None,
    border_mask_value: float | None = None,
) -> A.PadIfNeeded:
    """
    Create an albumentations PadIfNeeded transform from a config

    Args:
        min_height: the minimum height of the image
        min_width: the minimum width of the image
        pad_height_divisor: if not None, ensures height is dividable by value of this argument
        pad_width_divisor: if not None, ensures width is dividable by value of this argument
        position: position of the image, one of the possible PadIfNeeded
        border_mode: 'constant' or 'reflect_101' (see cv2.BORDER modes)
        border_value: padding value if border_mode is 'constant'
        border_mask_value: padding value for mask if border_mode is 'constant'

    Raises:
        ValueError:
            Only one of 'min_height' and 'pad_height_divisor' parameters must be set
            Only one of 'min_width' and 'pad_width_divisor' parameters must be set

    Returns:
        the auto-padding transform
    """
    border_modes = {
        "constant": cv2.BORDER_CONSTANT,
        "reflect_101": cv2.BORDER_REFLECT_101,
    }
    if border_mode not in border_modes:
        raise ValueError(
            f"Unknown border mode for auto_padding: {border_mode} "
            f"(valid values are: {border_modes.keys()})"
        )

    return A.PadIfNeeded(
        min_height=min_height,
        min_width=min_width,
        pad_height_divisor=pad_height_divisor,
        pad_width_divisor=pad_width_divisor,
        position=position,
        border_mode=border_modes[border_mode],
        value=border_value,
        mask_value=border_mask_value,
    )


def build_resize_transforms(resize_cfg: dict) -> list[A.BasicTransform]:
    height, width = resize_cfg["height"], resize_cfg["width"]

    transforms = []
    if resize_cfg.get("keep_ratio", True):
        transforms.append(KeepAspectRatioResize(width=width, height=height, mode="pad"))
        transforms.append(
            A.PadIfNeeded(
                min_height=height,
                min_width=width,
                border_mode=cv2.BORDER_CONSTANT,
                position=A.PadIfNeeded.PositionType.TOP_LEFT,
            )
        )
    else:
        transforms.append(A.Resize(height, width))
    return transforms


class HFlip(A.HorizontalFlip):
    """Horizontal Flip which swaps symmetric keypoints"""

    def __init__(self, symmetries: list[tuple[int, int]], *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        self._symmetries = {}
        for i, j in symmetries:
            self._symmetries[i] = j
            self._symmetries[j] = i

    def apply_to_keypoints(self, keypoints, **params):
        swapped_keypoints = [
            keypoints[self._symmetries.get(kpt_idx, kpt_idx)]
            for kpt_idx in range(len(keypoints))
        ]
        return super().apply_to_keypoints(swapped_keypoints, **params)


class KeypointAwareCrop(A.RandomCrop):
    """Random crop for an image around keypoints

    Args:
        width: Crop images down to this maximum width.
        height: Crop images down to this maximum height.
        max_shift: Maximum allowed shift of the cropping center position
            as a fraction of the crop size.
        crop_sampling: Crop centers sampling method. Must be either:
            "uniform" (randomly over the image),
            "keypoints" (randomly over the annotated keypoints),
            "density" (weighing preferentially dense regions of keypoints),
            "hybrid" (alternating randomly between "uniform" and "density").
    """

    def __init__(
        self,
        width: int,
        height: int,
        max_shift: float = 0.4,
        crop_sampling: str = "hybrid",
    ):
        super().__init__(height, width, always_apply=True)
        # Clamp to 40% of crop size to ensure that at least
        # the center keypoint remains visible after the offset is applied.
        self.max_shift = max(0.0, min(max_shift, 0.4))
        if crop_sampling not in ("uniform", "keypoints", "density", "hybrid"):
            raise ValueError(
                f"Invalid sampling {crop_sampling}. Must be "
                f"either 'uniform', 'keypoints', 'density', or 'hybrid."
            )
        self.crop_sampling = crop_sampling

    @staticmethod
    def calc_n_neighbors(xy: NDArray, radius: float) -> NDArray:
        d = pdist(xy, "sqeuclidean")
        mat = squareform(d <= radius * radius, checks=False)
        return np.sum(mat, axis=0)

    @property
    def targets_as_params(self) -> list[str]:
        return ["image", "keypoints"]

    def get_params_dependent_on_targets(self, params: dict[str, Any]) -> dict[str, Any]:
        img = params["image"]
        kpts = params["keypoints"]
        shift_factors = np.random.random(2)
        shift = self.max_shift * shift_factors * np.array([self.width, self.height])
        sampling = self.crop_sampling
        if self.crop_sampling == "hybrid":
            sampling = np.random.choice(["uniform", "density"])
        if len(kpts) == 0:
            sampling = "uniform"
        if sampling == "uniform":
            center = np.random.random(2)
        else:
            h, w = img.shape[:2]
            kpts = np.array([[k[0], k[1]] for k in kpts])
            kpts = kpts[~np.isnan(kpts).all(axis=1)]
            n_kpts = kpts.shape[0]
            inds = np.arange(n_kpts)
            if sampling == "density":
                # Points located close to one another are sampled preferentially
                # in order to augment crowded regions.
                radius = 0.1 * min(h, w)
                n_neighbors = self.calc_n_neighbors(kpts, radius)
                # Include keypoints in the count to avoid null probabilities
                n_neighbors += 1
                p = n_neighbors / n_neighbors.sum()
            else:
                p = np.ones_like(inds) / n_kpts
            center = kpts[np.random.choice(inds, p=p)]
            # Shift the crop center in both dimensions by random amounts
            # and normalize to the original image dimensions.
            center = (center + shift) / [w, h]
            center = np.clip(center, 0, np.nextafter(1, 0))  # Clip to 1 exclusive
        return {"h_start": center[1], "w_start": center[0]}

    def apply_to_keypoints(
        self,
        keypoints,
        **params,
    ) -> list[tuple[float]]:
        keypoints = super().apply_to_keypoints(keypoints, **params)
        new_keypoints = []
        for kp in keypoints:
            x, y = kp[:2]
            if not (0 <= x < self.width and 0 <= y < self.height):
                kp = list(kp)
                kp[:2] = np.nan, np.nan
                kp = tuple(kp)
            new_keypoints.append(kp)
        return new_keypoints

    def get_transform_init_args_names(self) -> tuple[str, ...]:
        return "width", "height", "max_shift", "crop_sampling"


class KeepAspectRatioResize(A.DualTransform):
    """Resizes images while preserving their aspect ratio

    In 'pad' mode, the image will be rescaled to the largest possible size such that
    it can be padded to the correct size (with PadIfNeeded). So we'll have:
        output_width <= width, output_height <= height

    In 'crop' mode, the image will be rescaled to the smallest possible size such that
    it can be cropped to the correct size (with any random crop you want), so:
        output_width >= width, output_height >= height
    """

    def __init__(
        self,
        width: int,
        height: int,
        mode: str = "pad",
        interpolation: Any = cv2.INTER_LINEAR,
        p: float = 1.0,
        always_apply: bool = True,
    ) -> None:
        super().__init__(always_apply=always_apply, p=p)
        self.height = height
        self.width = width
        self.mode = mode
        self.interpolation = interpolation

    def apply(self, img, scale=0, interpolation=cv2.INTER_LINEAR, **params):
        return A.scale(img, scale, interpolation)

    def apply_to_bbox(self, bbox, **params):
        # Bounding box coordinates are scale invariant
        return bbox

    def apply_to_keypoint(self, keypoint, scale=0, **params):
        keypoint = A.keypoint_scale(keypoint, scale, scale)
        return keypoint

    @property
    def targets_as_params(self) -> list[str]:
        return ["image"]

    def get_params_dependent_on_targets(self, params: dict[str, Any]) -> dict[str, Any]:
        h, w, _ = params["image"].shape
        if self.mode == "pad":
            scale = min(self.height / h, self.width / w)
        else:
            scale = max(self.height / h, self.width / w)

        return {"scale": scale}

    def get_transform_init_args_names(self):
        return "height", "width", "mode", "interpolation"


class Grayscale(A.ToGray):
    def __init__(
        self,
        alpha: float | int | tuple[float, float] = 1.0,
        always_apply: bool = False,
        p: float = 0.5,
    ):
        """
        Args:
            alpha: int, float or tuple of floats, optional
            The alpha value of the new colorspace when overlaid over the
            old one. A value close to 1.0 means that mostly the new
            colorspace is visible. A value close to 0.0 means that mostly the
            old image is visible.

            * If a float, exactly that value will be used.
            * If a tuple ``(a, b)``, a random value from the range
              ``a <= x <= b`` will be sampled per image.
        """
        super().__init__(always_apply, p)
        if isinstance(alpha, (float, int)):
            self._alpha = self._validate_alpha(alpha)
        elif isinstance(alpha, tuple):
            if len(alpha) != 2:
                raise ValueError("`alpha` must be a tuple of two numbers.")
            self._alpha = tuple([self._validate_alpha(val) for val in alpha])
        else:
            raise ValueError("")

    @staticmethod
    def _validate_alpha(val: float) -> float:
        if not 0.0 <= val <= 1.0:
            warnings.warn("`alpha` will be clipped to the interval [0.0, 1.0].")
        return min(1.0, max(0.0, val))

    @property
    def alpha(self) -> float:
        if isinstance(self._alpha, float):
            return self._alpha
        return np.random.uniform(*self._alpha)

    def apply(self, img: NDArray, **params) -> NDArray:
        img_gray = super().apply(img, **params)
        alpha = self.alpha
        img_blend = img * (1 - alpha) + img_gray * alpha
        return img_blend.astype(img.dtype)


class ElasticTransform(A.ElasticTransform):
    def __init__(
        self,
        alpha: float = 20.0,
        sigma: float = 5.0,  # As in DLC TF
        alpha_affine: float = 0.0,  # Deactivate affine prior to elastic deformation
        interpolation: int = cv2.INTER_CUBIC,  # As in imgaug
        border_mode: int = cv2.BORDER_CONSTANT,  # As in imgaug
        value: float | None = None,
        mask_value: float | None = None,
        always_apply: bool = False,
        approximate: bool = True,  # Faster by a factor of 2
        same_dxdy: bool = True,  # Here too
        p: float = 0.5,
    ):
        super().__init__(
            alpha,
            sigma,
            alpha_affine,
            interpolation,
            border_mode,
            value,
            mask_value,
            always_apply,
            approximate,
            same_dxdy,
            p,
        )
        self._neighbor_dist = 3
        self._neighbor_dist_square = self._neighbor_dist ** 2

    def apply_to_keypoints(
        self, keypoints: Sequence[float], random_state: int | None = None, **params
    ) -> list[float]:
        heatmaps = np.zeros(
            (params["rows"], params["cols"], len(keypoints)), dtype=np.float32
        )
        grid = np.mgrid[: params["rows"], : params["cols"]].transpose((1, 2, 0))
        kpts = np.array([(k[1], k[0]) for k in keypoints])
        valid_kpts = np.all(kpts > 0.0, axis=1)
        dist = ((grid - kpts[:, None, None]) ** 2).sum(axis=3)
        mask = (dist <= self._neighbor_dist_square) & valid_kpts[:, None, None]
        heatmaps[mask.transpose(1, 2, 0)] = 1

        heatmaps_aug = F.elastic_transform(
            heatmaps,
            self.alpha,
            self.sigma,
            self.alpha_affine,
            cv2.INTER_NEAREST,
            self.border_mode,
            self.mask_value,
            np.random.RandomState(random_state),
            self.approximate,
            self.same_dxdy,
        )

        inds = np.indices(heatmaps_aug.shape[:2])[::-1]
        mask = np.transpose(heatmaps_aug == 1, (2, 0, 1))
        # Let's compute the average, rather than the median, coordinates
        div = np.sum(mask, axis=(1, 2))
        sum_indices = np.sum(inds[:, None] * mask[None], axis=(2, 3)).T
        xy = sum_indices / div[:, None]
        new_keypoints = []
        for kp, new_coords in zip(keypoints, xy):
            kp = list(kp)
            kp[:2] = new_coords
            new_keypoints.append(tuple(kp))
        return new_keypoints


class CoarseDropout(A.CoarseDropout):
    def __init__(
        self,
        max_holes: int = 8,
        max_height: int | float = 8,
        max_width: int | float = 8,
        min_holes: int | None = None,
        min_height: int | float | None = None,
        min_width: int | float | None = None,
        fill_value: int = 0,
        mask_fill_value: int | None = None,
        always_apply: bool = False,
        p: float = 0.5,
    ):
        super().__init__(
            max_holes,
            max_height,
            max_width,
            min_holes,
            min_height,
            min_width,
            fill_value,
            mask_fill_value,
            always_apply,
            p,
        )

    def apply_to_bboxes(self, bboxes: Sequence[float], **params) -> list[float]:
        return list(bboxes)

    def apply_to_keypoints(
        self,
        keypoints: Sequence[float],
        holes: Iterable[tuple[int, int, int, int]] = (),
        **params,
    ) -> list[float]:
        new_keypoints = []
        for kp in keypoints:
            in_hole = False
            for hole in holes:
                if self._keypoint_in_hole(kp, hole):
                    in_hole = True
                    break
            if in_hole:
                kp = list(kp)
                kp[:2] = np.nan, np.nan
                kp = tuple(kp)
            new_keypoints.append(kp)
        return new_keypoints

    def _keypoint_in_hole(self, keypoint, hole: tuple[int, int, int, int]) -> bool:
        """Reimplemented from Albumentations as was removed in v1.4.0"""
        x1, y1, x2, y2 = hole
        x, y = keypoint[:2]
        return x1 <= x < x2 and y1 <= y < y2


class RandomBBoxTransform(A.DualTransform):
    """Random jittering for bounding boxes for top-down pose estimation models.

    Implementation based on the mmpose `RandomBBoxTransform`. For more information,
    see <https://github.com/open-mmlab/mmpose>.
    """

    def __init__(
        self,
        shift_factor: float = 0.1,
        shift_prob: float = 0.25,
        scale_factor: tuple[float, float] = (0.5, 1.5),
        scale_prob: float = 1.0,
        sampling: str = "truncnorm",
        p: float = 1.0,
    ):
        super().__init__(p=p)
        self.shift_factor = shift_factor
        self.shift_prob = shift_prob
        self.scale_factor = scale_factor
        self.scale_prob = scale_prob
        self.sampling = sampling

    def apply(self, img: np.ndarray, **params) -> np.ndarray:
        return img

    def apply_to_keypoints(self, keypoints: np.ndarray, **params) -> np.ndarray:
        return keypoints

    def apply_to_bboxes(self, bboxes, **params):
        if len(bboxes) == 0:
            return bboxes

        # Albumentations provides bounding boxes in normalized xyxy format internally
        bboxes_xyxy = np.asarray(bboxes)
        bboxes_extra = None
        if bboxes_xyxy.shape[1] > 4:
            # can't take from array - may have different dtype
            bboxes_extra = [bbox[4:] for bbox in bboxes]
            bboxes_xyxy = bboxes_xyxy[:, :4]

        # sample parameters
        bboxes_to_scale = np.random.random(len(bboxes)) < self.scale_prob
        num_bboxes_to_scale = np.sum(bboxes_to_scale).item()
        scale_factors = np.ones((len(bboxes), 2))
        if num_bboxes_to_scale > 0:
            scale_factors[bboxes_to_scale] = self._sample(
                (num_bboxes_to_scale, 2),
                low=self.scale_factor[0],
                high=self.scale_factor[1],
            )

        bboxes_to_shift = np.random.random(len(bboxes)) < self.shift_prob
        num_bboxes_to_shift = np.sum(bboxes_to_shift).item()
        shift_factors = np.zeros((len(bboxes), 2))
        if num_bboxes_to_shift > 0:
            shift_factors[bboxes_to_shift] = self._sample(
                (num_bboxes_to_shift, 2),
                low=-self.shift_factor,
                high=self.shift_factor,
            )

        bbox_wh = bboxes_xyxy[:, 2:] - bboxes_xyxy[:, :2]
        bbox_cxcy = bboxes_xyxy[:, :2] + (0.5 * bbox_wh)

        # scale + shift bounding boxes
        bbox_cxcy = bbox_cxcy + (shift_factors * bbox_wh)
        bbox_wh = bbox_wh * scale_factors

        # convert to xyxy, clip so all bounding boxes are in the image
        bbox_half_wh = 0.5 * bbox_wh
        bbox_xyxy = np.empty((len(bboxes), 4))
        bbox_xyxy[:, :2] = bbox_cxcy - bbox_half_wh
        bbox_xyxy[:, 2:] = bbox_cxcy + bbox_half_wh
        bbox_xyxy = np.clip(bbox_xyxy, 0, 1)

        # add the extra information back; tuples for albumentations<=1.4.3
        bboxes_out = [tuple(bbox) for bbox in bbox_xyxy]
        if bboxes_extra is not None:
            bboxes_out = [bbox + extra for bbox, extra in zip(bboxes_out, bboxes_extra)]
        return bboxes_out

    def get_transform_init_args_names(self):
        return "shift_factor", "shift_prob", "scale_factor", "scale_prob", "sampling"

    def _sample(
        self,
        size: tuple[int, ...],
        low: float = -1.0,
        high: float = 1.0,
    ) -> np.ndarray:
        if self.sampling == "truncnorm":
            return truncnorm.rvs(low, high, size=size).astype(np.float32)
        elif self.sampling == "uniform":
            delta = high - low
            return low + (delta * np.random.random(size))

        raise ValueError(f"Unknown sampling: {self.sampling}")


--- File: deeplabcut/pose_estimation_pytorch/data/collate.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Custom collate functions"""
from __future__ import annotations

from abc import ABC, abstractmethod

import numpy as np
from torch.utils.data import default_collate

from deeplabcut.pose_estimation_pytorch.data.image import resize_and_random_crop
from deeplabcut.pose_estimation_pytorch.registry import build_from_cfg, Registry


COLLATE_FUNCTIONS = Registry("collate_functions", build_func=build_from_cfg)


class CollateFunction(ABC):
    """A class that can be called as a collate function"""

    @abstractmethod
    def __call__(self, batch) -> dict | list:
        """Returns: the collated batch"""
        raise NotImplementedError()


class ResizeCollate(CollateFunction, ABC):
    """A collate function which resizes all images in a batch to the same size

    Args:
        max_shift: The maximum shift, in pixels, to add to the random crop (this means
            there can be a slight border around the image)
        max_size: The maximum size of the long edge of the image when resized. If the
            longest side will be greater than this value, resizes such that the longest
            side is this size, and the shortest side is smaller than the desired size.
            This is useful to keep some information from images with extreme aspect
            ratios.
        seed: The random seed to use to sample scales/sizes.
    """

    def __init__(
        self,
        max_shift: int = 10,
        max_size: int = 2048,
        seed: int = 0,
    ) -> None:
        self.generator = np.random.default_rng(seed=seed)
        self.max_size = max_size
        self.max_shift = max_shift
        self._current_batch = []

    @abstractmethod
    def _sample_scale(self) -> int | tuple[int, int]:
        """Returns: the target shape for images in the batch"""
        raise NotImplementedError()

    def __call__(self, batch) -> dict | list:
        """Returns: the collated batch"""
        self._current_batch = batch
        new_size = self._sample_scale()
        updated_batch = []
        for item in batch:
            image, new_targets = resize_and_random_crop(
                image=item["image"],
                targets=item,
                size=new_size,
                max_size=self.max_size,
                max_shift=self.max_shift,
            )
            new_targets["image"] = image
            updated_batch.append(new_targets)

        return default_collate(updated_batch)


@COLLATE_FUNCTIONS.register_module
class ResizeFromDataSizeCollate(ResizeCollate):
    """A collate function which resizes all images in a batch to the same size

    The target size is obtained by taking the size of the first image in the batch, and
    multiplying it by a scale taken uniformly at random from (min_scale, max_scale).

    The aspect ratio of all images in the batch is preserved, with cropping/padding used
    to generate images of the correct shapes.

    If to_square:
        The images will be resized to squares, where the side is the short side of the
        original image.
    else:
        The images will be resized to a scaled version of the shape of the first image.

    Args:
        min_scale: The minimum scale factor to apply to the image size
        max_scale: The maximum scale factor to apply to the image size
        min_short_side: The smallest size for the target short side.
        max_short_side: The largest size for the target short side.
        max_ratio: The largest aspect ratio allowed for a target (longSide / shortSide).
            If the aspect ratio is larger, it will be clamped to max_ratio. Must be >=1.
        multiple_of: If defined, the height and width of all target sizes will be a
            multiple of this value.
        to_square: Whether images should be resized to squares.
    """

    def __init__(
        self,
        min_scale: float,
        max_scale: float,
        min_short_side: int = 128,
        max_short_side: int = 1152,
        max_ratio: float = 2.0,
        multiple_of: int | None = None,
        to_square: bool = False,
        **kwargs
    ) -> None:
        super().__init__(**kwargs)
        self.min_scale = min_scale
        self.max_scale = max_scale
        self.min_short_side = min_short_side
        self.max_short_side = max_short_side
        self.max_ratio = max_ratio
        self.multiple_of = multiple_of
        self.to_square = to_square

    def _sample_scale(self) -> int | tuple[int, int]:
        if len(self._current_batch) == 0:
            raise ValueError("Cannot sample frame shape: no items in current batch")

        h, w = self._current_batch[0]["image"].shape[1:]
        scale = self.generator.uniform(self.min_scale, self.max_scale)
        if self.to_square:
            short_side = min(h, w)
            size = int(round(
                min(self.max_short_side, max(self.min_short_side, scale * short_side))
            ))
            if self.multiple_of is not None:
                size = _to_multiple(size, self.multiple_of)
            return size

        short, long = min(h, w), max(h, w)
        ratio = long / short
        if ratio > self.max_ratio:
            ratio = self.max_ratio

        short_size = int(
            round(min(self.max_short_side, max(self.min_short_side, scale * short)))
        )
        if h < w:
            h = short_size
            w = int(ratio * short_size)
        else:
            h = int(ratio * short_size)
            w = short_size

        if self.multiple_of is not None:
            w = _to_multiple(w, self.multiple_of)
            h = _to_multiple(h, self.multiple_of)

        return h, w


@COLLATE_FUNCTIONS.register_module
class ResizeFromListCollate(ResizeCollate):
    """A collate function which resizes all images in a batch to the same size

    The target size image size is sampled from a list. If it's a list of integers,
    all images will be resized into squares. If it's a list of tuples, that will be the
    target (h, w) for images.

    Args:
        scales: The target sizes to resize the images to.
    """

    def __init__(self, scales: list[int] | list[tuple[int, int]], **kwargs) -> None:
        super().__init__(**kwargs)
        self.scales = scales

    def _sample_scale(self) -> int | tuple[int, int]:
        return self.generator.choice(self.scales)


def _to_multiple(value: int, of: int) -> int:
    """Returns: the smallest integer >= ``value`` which is a multiple of ``of``"""
    return of * ((value + of - 1) // of)


--- File: deeplabcut/pose_estimation_pytorch/data/dlcloader.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Class implementing the Loader for DeepLabCut projects"""
from __future__ import annotations

import logging
import pickle
from pathlib import Path

import numpy as np
import pandas as pd
import scipy.io as sio

import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.core.engine import Engine
from deeplabcut.pose_estimation_pytorch.data.base import Loader
from deeplabcut.pose_estimation_pytorch.data.dataset import PoseDatasetParameters
from deeplabcut.pose_estimation_pytorch.data.utils import read_image_shape_fast


class DLCLoader(Loader):
    """A Loader for DeepLabCut projects"""

    def __init__(
        self,
        config: str | Path | dict,
        trainset_index: int = 0,
        shuffle: int = 0,
        modelprefix: str = "",
    ):
        """
        Args:
            config: Path to the DeepLabCut project config, or the project config itself
            trainset_index: the index of the TrainingsetFraction for which to load data
            shuffle: the index of the shuffle for which to load data
            modelprefix: the modelprefix for the shuffle
        """
        if isinstance(config, (str, Path)):
            self._project_root = Path(config).parent
            self._project_config = af.read_config(str(config))
        else:
            self._project_root = Path(config["project_path"])
            self._project_config = config

        self._shuffle = shuffle
        self._trainset_index = trainset_index
        self._train_frac = self._project_config["TrainingFraction"][trainset_index]
        self._model_folder = af.get_model_folder(
            self._train_frac,
            shuffle,
            self._project_config,
            engine=Engine.PYTORCH,
            modelprefix=modelprefix,
        )
        self._evaluation_folder = af.get_evaluation_folder(
            trainFraction=self._train_frac,
            shuffle=shuffle,
            cfg=self._project_config,
            engine=Engine.PYTORCH,
            modelprefix=modelprefix,
        )

        super().__init__(
            self._project_root
            / self._model_folder
            / "train"
            / Engine.PYTORCH.pose_cfg_name
        )

        # lazy-load split and DataFrames
        self._split: dict[str, list[int]] | None = None
        self._loaded_df: dict[str, pd.DataFrame] | None = None
        self._resolutions = set()

    @property
    def project_cfg(self) -> dict:
        """Returns: the configuration for the DeepLabCut project"""
        return self._project_config

    @property
    def df(self) -> pd.DataFrame:
        """Returns: The ground truth dataframe. Should not be modified."""
        return self._dfs["full"]

    @property
    def df_test(self) -> pd.DataFrame:
        """Returns: A copy of the DataFrame containing the test data."""
        return self._dfs["test"].copy()

    @property
    def df_train(self) -> pd.DataFrame:
        """Returns: A copy of the DataFrame containing the training data."""
        return self._dfs["train"].copy()

    def image_resolutions(self) -> set[tuple[int, int]]:
        """Returns: The collection of image resolutions present in the dataset"""
        return self._resolutions

    @property
    def evaluation_folder(self) -> Path:
        """Returns: The path to the evaluation folder"""
        return self._project_root / self._evaluation_folder

    @property
    def project_path(self) -> Path:
        """Returns: The path to the DeepLabCut project"""
        return self._project_root

    @property
    def shuffle(self) -> int:
        """Returns: the shuffle being loaded"""
        return self._shuffle

    @property
    def train_fraction(self) -> float:
        """Returns: the fraction of the dataset used for training"""
        return self._train_frac

    @property
    def split(self) -> dict[str, list[int]]:
        if self._split is None:
            self._split = self.load_split(
                self._project_config, self._trainset_index, self.shuffle
            )

        return self._split

    def get_dataset_parameters(self) -> PoseDatasetParameters:
        """Retrieves dataset parameters based on the instance's configuration.

        Returns:
            An instance of the PoseDatasetParameters with the parameters set.
        """
        crop_cfg = self.model_cfg["data"]["train"].get("top_down_crop", {})
        crop_w, crop_h = crop_cfg.get("width", 256), crop_cfg.get("height", 256)
        crop_margin = crop_cfg.get("margin", 0)

        return PoseDatasetParameters(
            bodyparts=self.model_cfg["metadata"]["bodyparts"],
            unique_bpts=self.model_cfg["metadata"]["unique_bodyparts"],
            individuals=self.model_cfg["metadata"]["individuals"],
            with_center_keypoints=self.model_cfg.get("with_center_keypoints", False),
            color_mode=self.model_cfg.get("color_mode", "RGB"),
            top_down_crop_size=(crop_w, crop_h),
            top_down_crop_margin=crop_margin,
        )

    def load_data(self, mode: str = "train") -> dict:
        """Loads DeepLabCut data into COCO-style annotations

        This function reads data from h5 file, split the data and returns it in
        COCO-like format

        Args:
            mode: mode indicating whether to use 'train' or 'test' data.

        Raises:
            AttributeError: if the specified mode (train or test) does not exist.

        Returns:
            the coco-style annotations
        """
        if mode not in ["train", "test"]:
            raise AttributeError(f"Unknown mode: {mode}")
        if mode not in self._dfs:
            raise ValueError(f"No split for: {mode} (found {self._dfs.keys()})")
        if self._dfs[mode] is None:
            raise ValueError(f"No data in {mode} split for this shuffle!")

        params = self.get_dataset_parameters()
        data = self.to_coco(str(self._project_root), self._dfs[mode], params)
        with_bbox = self._compute_bboxes(
            data["images"],
            data["annotations"],
            method="keypoints",
            bbox_margin=self.model_cfg["data"].get("bbox_margin", 20),
        )
        data["annotations"] = with_bbox
        return data

    def load_ground_truth(
        self,
        config: dict,
        trainset_index: int,
        shuffle: int,
    ) -> tuple[dict[str, pd.DataFrame], set[tuple[int, int]]]:
        """Loads the ground truth dataset for a DeepLabCut project.

        Args:
            config: the DeepLabCut project configuration file
            trainset_index: the TrainingsetFraction for which to load data
            shuffle: the index of the shuffle for which to load data

        Returns: ground_truth_dataframes, image_resolutions
            ground_truth_dataframes: a dictionary containing the different DataFrames
                for the annotated DeepLabCut data for the current iteration
            image_resolutions: all possible image resolutions in the dataset

        Raises:
            ValueError: if the data contained in the ground truth HDF does not contain
                a dataframe.
        """
        trainset_dir = Path(config["project_path"]) / af.get_training_set_folder(config)
        dataset_path = f"CollectedData_{config['scorer']}.h5"
        train_frac = int(100 * config["TrainingFraction"][trainset_index])
        project_id = f"{config['Task']}_{config['scorer']}"
        dataset_file = trainset_dir / f"{project_id}{train_frac}shuffle{shuffle}"
        params = self.get_dataset_parameters()

        # as in TF DeepLabCut, load the training data from the .mat/.pickle file
        if config.get("multianimalproject", False):
            image_sizes, df_train = _load_pickle_dataset(
                dataset_file.with_suffix(".pickle"),
                config["scorer"],
                params=params,
            )
        else:
            image_sizes, df_train = _load_mat_dataset(
                dataset_file.with_suffix(".mat"),
                config["scorer"],
                params=params,
            )

        # load the full dataset file
        df = pd.read_hdf(trainset_dir / dataset_path)
        if not isinstance(df, pd.DataFrame):
            raise ValueError(
                f"The ground truth data in {trainset_dir} must contain a DataFrame! "
                f"Found {df}"
            )

        # load the data splits, check that there's nothing suspect
        dfs = self.split_data(df, self.split)
        dfs["full"] = df
        # let's not validate for now
        # dfs = _validate_dataframes(dfs, df_train)
        return dfs, image_sizes

    @staticmethod
    def load_split(
        config: dict,
        trainset_index: int = 0,
        shuffle: int = 0,
    ) -> dict[str, list[int]]:
        """Loads the train/test split for a DeepLabCut shuffle

        Args:
            config: the DeepLabCut project config
            trainset_index: the TrainingsetFraction for which to load data
            shuffle: the index of the shuffle for which to load data

        Return:
            the {"train": [train_ids], "test": [test_ids]} data split
        """
        trainset_dir = Path(config["project_path"]) / af.get_training_set_folder(config)
        train_frac = int(100 * config["TrainingFraction"][trainset_index])
        shuffle_id = f"{config['Task']}_{train_frac}shuffle{shuffle}.pickle"
        doc_path = trainset_dir / f"Documentation_data-{shuffle_id}"

        with open(doc_path, "rb") as f:
            meta = pickle.load(f)

        train_ids = [int(i) for i in meta[1]]
        test_ids = [int(i) for i in meta[2]]
        return {"train": train_ids, "test": test_ids}

    @staticmethod
    def split_data(
        dlc_df: pd.DataFrame,
        split: dict[str, list[int]],
    ) -> dict[str, pd.DataFrame | None]:
        """
        Splits a DeepLabCut DataFrame into train/test dataframes

        Args:
            dlc_df: the dataframe containing the labeled data
            split: the train/test indices

        Returns:
            a dictionary containing the same keys as the split dictionary, where the
            values are the rows of dlc_df with index in the split, or None if there are
            no indices in that split
        """
        split_dfs = {}
        for k, indices in split.items():
            if len(indices) == 0:
                split_dfs[k] = None
            else:
                split_dfs[k] = dlc_df.iloc[indices]
        return split_dfs

    @staticmethod
    def to_coco(
        project_root: str | Path,
        df: pd.DataFrame,
        parameters: PoseDatasetParameters,
    ) -> dict:
        """Formerly Shaokai's function

        Args:
            project_root: the path to the project root
            df: the DLC-format annotation dataframe to convert to a COCO-format dict
            parameters: the parameters for pose estimation

        Returns:
            the coco format data
        """
        with_individuals = "individuals" in df.columns.names
        if not with_individuals and (
            len(parameters.individuals) > 1 or len(parameters.unique_bpts) > 0
        ):
            raise ValueError(
                "The DataFrame contains single-animal annotations (for a single, "
                "individual), but the parameters suggest this is a multi-animal project"
                f": {parameters} (with multiple individuals or unique bodyparts)"
            )

        categories = [
            {
                "id": 1,
                "name": "animals",
                "supercategory": "animal",
                "keypoints": parameters.bodyparts,
            },
        ]
        individuals = [idv for idv in parameters.individuals]
        if len(parameters.unique_bpts) > 0:
            individuals += ["single"]
            categories.append(
                {
                    "id": 2,
                    "name": "unique_bodypart",
                    "supercategory": "animal",
                    "keypoints": parameters.unique_bpts,
                }
            )

        anns, images = [], []
        base_path = Path(project_root)
        for idx, row in df.iterrows():
            image_id = len(images) + 1
            rel_path = Path(*idx) if isinstance(idx, tuple) else Path(str(idx))
            path = str(base_path / rel_path)
            _, height, width = read_image_shape_fast(path)
            images.append(
                {
                    "id": image_id,
                    "file_name": path,
                    "width": width,
                    "height": height,
                }
            )

            for idv_idx, idv in enumerate(individuals):
                category_id = 1
                individual_id = idv_idx
                if with_individuals:
                    if idv == "single":
                        category_id = 2
                        individual_id = -1
                    data = row.xs(idv, level="individuals")
                else:
                    data = row

                raw_keypoints = data.to_numpy().reshape((-1, 2))
                keypoints = np.zeros((len(raw_keypoints), 3))
                keypoints[:, :2] = raw_keypoints
                is_visible = np.logical_and(
                    ~pd.isnull(raw_keypoints).all(axis=1),
                    np.logical_and(
                        np.logical_and(
                            0 < keypoints[..., 0],
                            keypoints[..., 0] < width,
                        ),
                        np.logical_and(
                            0 < keypoints[..., 1],
                            keypoints[..., 1] < height,
                        ),
                    ),
                )
                keypoints[:, 2] = np.where(is_visible, 2, 0)
                num_keypoints = is_visible.sum()
                if num_keypoints > 0:
                    anns.append(
                        {
                            "id": len(anns) + 1,
                            "image_id": image_id,
                            "category_id": category_id,
                            "individual": idv,
                            "individual_id": individual_id,
                            "num_keypoints": num_keypoints,
                            "keypoints": keypoints,
                            "iscrowd": 0,
                        }
                    )

        return {"annotations": anns, "categories": categories, "images": images}

    @property
    def _dfs(self) -> dict[str, pd.DataFrame]:
        """Lazy-loading of the training dataset dataframes"""
        if self._loaded_df is None:
            self._loaded_df, image_sizes = self.load_ground_truth(
                self._project_config,
                trainset_index=self._trainset_index,
                shuffle=self.shuffle,
            )
            self._resolutions = self._resolutions.union(image_sizes)

        return self._loaded_df


def _load_mat_dataset(
    file: Path,
    scorer: str,
    params: PoseDatasetParameters,
) -> tuple[set[tuple[int, int]], pd.DataFrame]:
    """Loads the training dataset stored as a .mat file

    Returns: images_sizes, dlc_dataset
        images_sizes: all possible images sizes in the dataset
        dlc_dataset: the dataset in a DLC-format DataFrame
    """
    if not params.max_num_animals == 1:
        raise RuntimeError(
            f"Cannot load a multi-animal pose dataset from a `.mat` file ({file})"
        )

    raw_data = sio.loadmat(str(file))
    dataset = raw_data["dataset"]
    num_images = dataset.shape[1]

    image_sizes = set()
    index, data = [], []
    for i in range(num_images):
        item = dataset[0, i]

        # add the image size
        c, h, w = item[1][0]
        image_sizes.add((h, w))

        # parse image path
        raw_path = item[0][0]
        if isinstance(raw_path, str):
            image_path = Path(raw_path).parts[-3:]
        else:
            image_path = tuple([p.strip() for p in raw_path])
        index.append(image_path)

        # parse data
        keypoints = np.zeros((1, params.num_joints, 2))
        keypoints.fill(np.nan)
        if len(item) >= 3:
            joints = item[2][0][0]
            for joint_id, x, y in joints:
                keypoints[0, joint_id, 0] = x
                keypoints[0, joint_id, 1] = y

            joint_id = joints[:, 0]
            if joint_id.size != 0:  # make sure joint ids are 0-indexed
                assert (joint_id < params.num_joints).any()
            joints[:, 0] = joint_id

        data.append(keypoints)

    dataframe = pd.DataFrame(
        data=np.stack(data, axis=0).reshape((num_images, -1)),
        index=pd.MultiIndex.from_tuples(index),
        columns=build_dlc_dataframe_columns(scorer, params, False),
    )
    dataframe = dataframe.sort_index(axis=0)
    return image_sizes, dataframe


def _load_pickle_dataset(
    file: Path,
    scorer: str,
    params: PoseDatasetParameters,
) -> tuple[set[tuple[int, int]], pd.DataFrame]:
    """Loads the training dataset stored as a .mat file

    Returns: images_sizes, dlc_dataset
        images_sizes: all possible images sizes in the dataset
        dlc_dataset: the dataset in a DLC-format DataFrame
    """
    with open(file, "rb") as f:
        raw_data = pickle.load(f)

    num_images = len(raw_data)
    image_sizes = set()
    index, data = [], []
    data_unique = None
    if params.num_unique_bpts > 0:
        data_unique = []

    for image_data in raw_data:
        # add image path
        index.append(image_data["image"])

        # add image size
        c, h, w = image_data["size"]
        image_sizes.add((h, w))

        # add keypoints
        keypoints = np.zeros((params.max_num_animals, params.num_joints, 2))
        keypoints.fill(np.nan)
        keypoints_unique = None
        for idv_idx, idv_bodyparts in image_data.get("joints", {}).items():
            if idv_idx < params.max_num_animals:
                for joint_id, x, y in idv_bodyparts:
                    bodypart = int(joint_id)
                    keypoints[idv_idx, bodypart, 0] = x
                    keypoints[idv_idx, bodypart, 1] = y

            elif (
                idv_idx == params.max_num_animals
                and data_unique is not None
                and keypoints_unique is None
            ):
                keypoints_unique = np.zeros((params.num_unique_bpts, 2))
                keypoints_unique.fill(np.nan)
                for joint_id, x, y in idv_bodyparts:
                    unique_bpt_id = int(joint_id) - params.num_joints
                    keypoints_unique[unique_bpt_id, 0] = x
                    keypoints_unique[unique_bpt_id, 1] = y

            else:
                raise ValueError(f"Malformed dataset: {params}, {image_data}")

        data.append(keypoints)
        if data_unique is not None:
            if keypoints_unique is None:
                keypoints_unique = np.zeros((params.num_unique_bpts, 2))
                keypoints_unique.fill(np.nan)
            data_unique.append(keypoints_unique)

    data = np.stack(data, axis=0).reshape((num_images, -1))
    if data_unique is not None:
        data_unique = np.stack(data_unique, axis=0).reshape((num_images, -1))
        data = np.concatenate([data, data_unique], axis=1)

    dataframe = pd.DataFrame(
        data=data,
        index=pd.MultiIndex.from_tuples(index),
        columns=build_dlc_dataframe_columns(scorer, params, False),
    )
    dataframe = dataframe.sort_index(axis=0)
    return image_sizes, dataframe


def _validate_dataframes(
    dfs: dict[str, pd.DataFrame],
    df_train: pd.DataFrame,
    strict: bool = False,
) -> dict[str, pd.DataFrame]:
    """Validates the training/test DataFrames

    Performs the following validation steps:
        1. Checks that the training data loaded from CollectedData.h5 matches the
            training data stored in the ".mat" or ".pickle" file.
        2. Checks that there are no duplicate entries in the DataFrames (if there are
            any, removes them)
        3. Checks that there is no data leak between the training and test set (if there
            is, prints a warning)

    Args:
        dfs: the "full" and split DataFrames loaded from the H5 file
        df_train: the training data loaded from the ".mat" or ".pickle" file
        strict: Whether to fail if the data does not pass validation (instead of
            attempting a fix).

    Returns:
        The validated and sanitized DataFrames

    Raises:
        ValueError: if strict and there is a small fixable error, or if there are images
        that are present in both the training and test set.
    """
    error = False

    # checks that all images in the .pickle/.mat file are in the HDF
    pickle_train_images = set(df_train.index)
    hdf_train_images = set(dfs["train"].index)
    missing_images = pickle_train_images - hdf_train_images
    extra_images = hdf_train_images - pickle_train_images
    if len(missing_images) > 0:
        error = True
        logging.debug(
            f"Found images in the dataset file which were not in H5: {missing_images}"
        )
    if len(extra_images) > 0:
        error = True
        logging.debug(
            f"Found images in the H5 file which were not in the dataset: {extra_images}"
        )

    # checks that the data is close for the similar images
    train_index = list(hdf_train_images.intersection(pickle_train_images))
    data_h5 = np.nan_to_num(dfs["full"].loc[train_index], nan=-1)
    data_pickle_mat = np.nan_to_num(df_train, nan=-1)
    if not np.isclose(data_h5, data_pickle_mat, atol=0.1).all():
        error = True
        logging.debug(
            "Found differences between the training-dataset HDF (.h5) data and the "
            "training data found. This might be the case if you refined your data "
            "after creating the dataset, and then created a new shuffle."
        )

    # checks that there are no duplicate entries
    dfs_clean = {}
    for split, df in dfs.items():
        dup = df.index.duplicated(keep="first")
        num_dup = dup.sum()
        if dup.sum() > 0:
            error = True
            logging.debug(f"Found {num_dup} duplicates in {split}: {df[dup].index}")
            dfs_clean[split] = df[~dup]
        else:
            dfs_clean[split] = df[~dup]

    # check for leaks
    if dfs["test"] is not None:
        train_images = set(dfs["train"].index)
        test_images = set(dfs["test"].index)
        leak = train_images.intersection(test_images)
        if len(leak) > 0:
            logging.warning(
                f"Found images both in the training and test set: {leak}! To resolve "
                "this issue please try the following:\n"
                f"  1. Check that each video is listed exactly once in your project's"
                f"`config.yaml`\n"
                f"  2. Make sure all of your videos have different names."
                f"  3. You can use `dropduplicatesinannotatinfiles` and "
                f"`comparevideolistsanddatafolders` to ensure that there are no more "
                f"duplicates"
                f"  3. Switch to a new iteration and create a fresh training dataset"
            )

    if error and strict:
        raise ValueError(f"Found errors when validating the dataset")

    return dfs


def build_dlc_dataframe_columns(
    scorer: str,
    parameters: PoseDatasetParameters,
    with_likelihood: bool,
) -> pd.MultiIndex:
    """Builds the columns for a DeepLabCut DataFrame

    Args:
        scorer: the scorer name
        parameters: the parameters for the project
        with_likelihood: whether the DataFrame contains pose likelihood

    Returns:
        the multi-index columns for the DataFrame
    """
    levels = ["scorer", "individuals", "bodyparts", "coords"]
    kpt_entries = ["x", "y"]
    if with_likelihood:
        kpt_entries.append("likelihood")

    columns = []
    for i in parameters.individuals:
        for b in parameters.bodyparts:
            columns += [(scorer, i, b, entry) for entry in kpt_entries]

    for unique_bpt in parameters.unique_bpts:
        columns += [(scorer, "single", unique_bpt, entry) for entry in kpt_entries]

    return pd.MultiIndex.from_tuples(columns, names=levels)


--- File: deeplabcut/pose_estimation_pytorch/data/postprocessor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Post-process predictions made by models"""
from __future__ import annotations

from abc import ABC, abstractmethod
from enum import Enum
from typing import Any

import numpy as np

from deeplabcut.pose_estimation_pytorch.data.preprocessor import Context
from deeplabcut.pose_estimation_pytorch.post_processing.identity import assign_identity


class Postprocessor(ABC):
    """A post-processor can be called on the output of a model
    TODO: Documentation
    """

    @abstractmethod
    def __call__(self, predictions: Any, context: Context) -> Any:
        """
        Post-processes the outputs of a model into a single prediction.

        Args:
            predictions: the predictions made by the model on a single image
            context: the context returned by the pre-processor with the image

        Returns:
            a single post-processed prediction
        """
        pass


def build_bottom_up_postprocessor(
    max_individuals: int,
    num_bodyparts: int,
    num_unique_bodyparts: int,
    with_identity: bool = False,
    with_backbone_features: bool = False,
) -> ComposePostprocessor:
    """Creates a postprocessor for bottom-up pose estimation (or object detection)

    Args:
        max_individuals: the maximum number of individuals in a single image
        num_bodyparts: the number of bodyparts output by the model
        num_unique_bodyparts: the number of unique_bodyparts output by the model
        with_identity: whether the model has an identity head
        with_backbone_features: When True, the backbone features are extracted from
            the output and saved in a `features` key. The `PoseModel` must have its
            `output_features` attribute set to True, or this will raise an Exception.

    Returns:
        A default bottom-up Postprocessor
    """
    keys_to_concatenate = {"bodyparts": ("bodypart", "poses")}
    empty_shapes = {"bodyparts": (num_bodyparts, 3)}
    keys_to_rescale = ["bodyparts"]

    if num_unique_bodyparts > 0:
        keys_to_concatenate["unique_bodyparts"] = ("unique_bodypart", "poses")
        empty_shapes["unique_bodyparts"] = (num_bodyparts, 3)
        keys_to_rescale.append("unique_bodyparts")

    if with_identity:
        keys_to_concatenate["identity_heatmap"] = ("identity", "heatmap")
        empty_shapes["identity_heatmap"] = (1, 1, max_individuals)

    if with_backbone_features:
        keys_to_concatenate["features"] = ("backbone", "features")
        empty_shapes["features"] = (num_bodyparts, 0, 1)

    components = [
        ConcatenateOutputs(
            keys_to_concatenate=keys_to_concatenate,
            empty_shapes=empty_shapes,
            create_empty_outputs=True,
        ),
    ]

    if with_identity:
        components.append(
            PredictKeypointIdentities(
                identity_key="identity_scores",
                identity_map_key="identity_heatmap",
                pose_key="bodyparts",
                keep_id_maps=False,
            )
        )

    components += [
        RescaleAndOffset(
            keys_to_rescale=keys_to_rescale,
            mode=RescaleAndOffset.Mode.KEYPOINT,
        ),
        PadOutputs(
            max_individuals={
                "bodyparts": max_individuals,
                "identity_scores": max_individuals,
            },
            pad_value=-1,
        ),
    ]

    if with_identity:
        components.append(
            AssignIndividualIdentities(
                identity_key="identity_scores", pose_key="bodyparts",
            )
        )

    return ComposePostprocessor(components=components)


def build_top_down_postprocessor(
    max_individuals: int,
    num_bodyparts: int,
    num_unique_bodyparts: int,
    with_backbone_features: bool = False,
) -> Postprocessor:
    """Creates a postprocessor for top-down pose estimation

    Args:
        max_individuals: the maximum number of individuals in a single image
        num_bodyparts: the number of bodyparts output by the model
        num_unique_bodyparts: the number of unique_bodyparts output by the model
        with_backbone_features: When True, the backbone features are extracted from
            the output and saved in a `features` key. The `PoseModel` must have its
            `output_features` attribute set to True, or this will raise an Exception.

    Returns:
        A default top-down Postprocessor
    """
    keys_to_concatenate = {"bodyparts": ("bodypart", "poses")}
    empty_shapes = {"bodyparts": (num_bodyparts, 3)}
    keys_to_rescale = ["bodyparts"]
    if num_unique_bodyparts > 0:
        keys_to_concatenate["unique_bodyparts"] = ("unique_bodypart", "poses")
        empty_shapes["unique_bodyparts"] = (num_unique_bodyparts, 3)
        keys_to_rescale.append("unique_bodyparts")

    if with_backbone_features:
        keys_to_concatenate["features"] = ("backbone", "features")
        empty_shapes["features"] = (num_bodyparts, 0, 1)

    return ComposePostprocessor(
        components=[
            ConcatenateOutputs(
                keys_to_concatenate=keys_to_concatenate,
                empty_shapes=empty_shapes,
                create_empty_outputs=True,
            ),
            RescaleAndOffset(
                keys_to_rescale=keys_to_rescale,
                mode=RescaleAndOffset.Mode.KEYPOINT_TD,
            ),
            AddContextToOutput(keys=["bboxes", "bbox_scores"]),
            PadOutputs(
                max_individuals={
                    "bodyparts": max_individuals,
                    "bboxes": max_individuals,
                    "bbox_scores": max_individuals,
                },
                pad_value=-1,
            ),
        ]
    )


def build_detector_postprocessor(max_individuals: int) -> Postprocessor:
    """Creates a postprocessor for top-down pose estimation

    Args:
        max_individuals: the maximum number of detections to keep in a single image

    Returns:
        A default top-down Postprocessor
    """
    return ComposePostprocessor(
        components=[
            ConcatenateOutputs(
                keys_to_concatenate={
                    "bboxes": ("detection", "bboxes"),
                    "bbox_scores": ("detection", "scores"),
                }
            ),
            TrimOutputs(
                max_individuals={
                    "bboxes": max_individuals,
                    "bbox_scores": max_individuals,
                },
            ),
            BboxToCoco(bounding_box_keys=["bboxes"]),
            RescaleAndOffset(
                keys_to_rescale=["bboxes"],
                mode=RescaleAndOffset.Mode.BBOX_XYWH,
            ),
        ]
    )


class ComposePostprocessor(Postprocessor):
    """
    Class to preprocess an image and turn it into a batch of
    inputs before running inference
    """

    def __init__(self, components: list[Postprocessor]) -> None:
        self.components = components

    def __call__(self, predictions: Any, context: Context) -> tuple[Any, Context]:
        for postprocessor in self.components:
            predictions, context = postprocessor(predictions, context)
        return predictions, context


class ConcatenateOutputs(Postprocessor):
    """Checks that there is a single prediction for the image and returns it"""

    def __init__(
        self,
        keys_to_concatenate: dict[str, tuple[str, str]],
        empty_shapes: dict[str, tuple[int, ...]] | None = None,
        create_empty_outputs: bool = False,
    ):
        self.keys_to_concatenate = keys_to_concatenate
        self.empty_shapes = empty_shapes
        self.create_empty_outputs = create_empty_outputs

        if self.create_empty_outputs:
            if not all([k in self.empty_shapes for k in self.keys_to_concatenate]):
                raise ValueError(
                    "You must provide the expected shape for all keys to concatenate"
                    f" when create_empty_outputs is true, found {self.empty_shapes}"
                )

    def __call__(
        self, predictions: Any, context: Context
    ) -> tuple[dict[str, np.ndarray], Context]:
        if len(predictions) == 0:
            outputs = {
                name: np.zeros((0, *self.empty_shapes[name]))
                for name in self.keys_to_concatenate.keys()
            }
            return outputs, context

        outputs = {}
        for output_name, head_key in self.keys_to_concatenate.items():
            head_name, val_name = head_key
            outputs[output_name] = np.concatenate(
                [p[head_name][val_name] for p in predictions]
            )

        return outputs, context


class PadOutputs(Postprocessor):
    """Pads the outputs to have the maximum number of individuals"""

    def __init__(
        self,
        max_individuals: dict[str, int],
        pad_value: int,
    ):
        self.max_individuals = max_individuals
        self.pad_value = pad_value

    def __call__(
        self, predictions: dict[str, np.ndarray], context: Context
    ) -> tuple[dict[str, np.ndarray], Context]:
        for name in predictions:
            output = predictions[name]
            if (
                name in self.max_individuals
                and len(output) < self.max_individuals[name]
            ):
                pad_size = self.max_individuals[name] - len(output)
                tail_shape = output.shape[1:]
                padding = self.pad_value * np.ones((pad_size, *tail_shape))
                predictions[name] = np.concatenate([output, padding])

        return predictions, context


class TrimOutputs(Postprocessor):
    """Ensures all outputs have at most `max_individuals` detections

    Assumes that the outputs are sorted by decreasing score, such that the first
    `max_individuals` predictions are the ones to keep.
    """

    def __init__(self, max_individuals: dict[str, int]):
        self.max_individuals = max_individuals

    def __call__(
        self, predictions: dict[str, np.ndarray], context: Context
    ) -> tuple[dict[str, np.ndarray], Context]:
        for name in predictions:
            output = predictions[name]
            if len(output) > self.max_individuals[name]:
                predictions[name] = output[:self.max_individuals[name]]

        return predictions, context


class RescaleAndOffset(Postprocessor):
    """Rescales and offsets predictions back to their position in the original image

    This can be done in 3 ways:
        BBOX_XYWH: the data has shape (num_individuals, 4), in xywh format, and there
            is a single scale and offset for all bounding boxes (e.g., because the image
            was resized before being passed to a detector)
        KEYPOINT: the data has shape (num_individuals, num_keypoints, 2/3), and there
            is a single scale and offset for all individuals (e.g., because the image
            was resized before being passed to a BU pose model)
        KEYPOINT_TD: the data has shape (num_individuals, num_keypoints, 2/3), and there
            are num_individuals scales and offsets (one for each individual, as TD crops
            one image per individual)

    If no scale and no offsets are given, then this postprocessor simply forwards the
    predictions and context.
    """

    class Mode(Enum):
        BBOX_XYWH = "bbox_xywh"
        KEYPOINT = "keypoint"
        KEYPOINT_TD = "keypoint_td"

    def __init__(
        self,
        keys_to_rescale: list[str],
        mode: RescaleAndOffset.Mode,
    ) -> None:
        super().__init__()
        self.keys_to_rescale = keys_to_rescale
        self.mode = mode

    def __call__(
        self, predictions: dict[str, np.ndarray], context: Context
    ) -> tuple[dict[str, np.ndarray], Context]:
        if "scales" not in context and "offsets" not in context:
            # no rescaling needed
            return predictions, context

        updated_predictions = {}
        scales, offsets = context["scales"], context["offsets"]
        for name, outputs in predictions.items():
            if name in self.keys_to_rescale:
                if self.mode == self.Mode.BBOX_XYWH:
                    rescaled = outputs.copy()
                    rescaled[:, 0] = outputs[:, 0] * scales[0] + offsets[0]
                    rescaled[:, 1] = outputs[:, 1] * scales[1] + offsets[1]
                    rescaled[:, 2] = outputs[:, 2] * scales[0]
                    rescaled[:, 3] = outputs[:, 3] * scales[1]
                elif self.mode == self.Mode.KEYPOINT:
                    rescaled = outputs.copy()
                    rescaled[..., :2] = outputs[..., :2] * scales + offsets
                else:  # Mode.KEYPOINT_TD
                    if not len(outputs) == len(scales) == len(offsets):
                        raise ValueError(
                            "There must be as many 'scales' and 'offsets' as outputs, found "
                            f"{len(outputs)}, {len(scales)}, {len(offsets)}"
                        )

                    if len(outputs) == 0:
                        rescaled = outputs
                    else:
                        rescaled_individuals = []
                        for output, scale, offset in zip(outputs, scales, offsets):
                            output_rescaled = output.copy()
                            output_rescaled[:, :2] = output[:, :2] * scale + offset
                            rescaled_individuals.append(output_rescaled)
                        rescaled = np.stack(rescaled_individuals)

                updated_predictions[name] = rescaled
            else:
                updated_predictions[name] = outputs.copy()

        return updated_predictions, context


class BboxToCoco(Postprocessor):
    """Transforms bounding boxes from xyxy to COCO format (xywh)"""

    def __init__(self, bounding_box_keys: list[str]) -> None:
        super().__init__()
        self.bounding_box_keys = bounding_box_keys

    def __call__(
        self, predictions: dict[str, np.ndarray], context: Context
    ) -> tuple[dict[str, np.ndarray], Context]:
        for bbox_key in self.bounding_box_keys:
            predictions[bbox_key][:, 2] -= predictions[bbox_key][:, 0]
            predictions[bbox_key][:, 3] -= predictions[bbox_key][:, 1]

        return predictions, context


class AddContextToOutput(Postprocessor):
    """
    Adds items from the context to the output, such as the bounding boxes contained
    during top-down inference.
    """

    def __init__(self, keys: list[str]) -> None:
        super().__init__()
        self.keys = keys

    def __call__(
        self,
        predictions: dict[str, np.ndarray],
        context: Context,
    ) -> tuple[dict[str, np.ndarray], Context]:
        for k in self.keys:
            if k in context:
                predictions[k] = context[k].copy()
        return predictions, context


class PredictKeypointIdentities(Postprocessor):
    """Assigns predicted identities to keypoints

    The identity maps have shape (h, w, num_ids).

    Attributes:
        identity_key: Key with which to add predicted identities in the predictions dict
        identity_map_key: Key for the identity maps in the predictions dict
        pose_key: Key for the bodyparts in the predictions dict
        keep_id_maps: Whether to keep identity heatmaps in the output dictionary.
            Setting this value to True can be useful for debugging, but can lead to
            memory issues when running video analysis on long videos.
    """

    def __init__(
        self,
        identity_key: str,
        identity_map_key: str,
        pose_key: str,
        keep_id_maps: bool = False,
    ) -> None:
        self.identity_key = identity_key
        self.identity_map_key = identity_map_key
        self.pose_key = pose_key
        self.keep_id_maps = keep_id_maps

    def __call__(
        self, predictions: dict[str, np.ndarray], context: Context
    ) -> tuple[dict[str, np.ndarray], Context]:
        pose = predictions[self.pose_key]
        num_preds, num_keypoints, _ = pose.shape

        identity_heatmap = predictions[self.identity_map_key]  # (h, w, num_ids)
        h, w, num_ids = identity_heatmap.shape

        id_score_matrix = np.zeros((num_preds, num_keypoints, num_ids))
        for pred_idx, individual_keypoints in enumerate(pose):
            heatmap_indices = np.rint(individual_keypoints).astype(int)
            xs = np.clip(heatmap_indices[:, 0], 0, w - 1)
            ys = np.clip(heatmap_indices[:, 1], 0, h - 1)

            # get the score from each identity heatmap at each predicted keypoint
            for kpt_idx, (x, y) in enumerate(zip(xs, ys)):
                id_score_matrix[pred_idx, kpt_idx] = identity_heatmap[y, x, :]

        predictions[self.identity_key] = id_score_matrix
        if not self.keep_id_maps:
            # delete the heatmaps as this saves memory
            id_heatmaps = predictions.pop(self.identity_map_key)
            del id_heatmaps

        return predictions, context


class AssignIndividualIdentities(Postprocessor):
    """Assigns predicted identities to individuals

    Attributes:
        identity_key: Key with which to add predicted identities in the predictions dict
        pose_key: Key for the bodyparts in the predictions dict
    """

    def __init__(self, identity_key: str, pose_key: str) -> None:
        self.identity_key = identity_key
        self.pose_key = pose_key

    def __call__(
        self, predictions: dict[str, np.ndarray], context: Context
    ) -> tuple[dict[str, np.ndarray], Context]:
        map_ = assign_identity(predictions["bodyparts"], predictions["identity_scores"])
        predictions["bodyparts"] = predictions["bodyparts"][map_]
        predictions["identity_scores"] = predictions["identity_scores"][map_]
        return predictions, context


class PrepareBackboneFeatures(Postprocessor):
    """Adds backbone features for each individual and keypoint to the outputs

    Attributes:
        top_down: Whether the model is a top-down model.
    """

    def __init__(self, top_down: bool) -> None:
        self.top_down = top_down

    def __call__(self, predictions: Any, context: Context) -> tuple[Any, Context]:
        if self.top_down:
            input_w, input_h = context["top_down_crop_size"]
        else:
            input_w, input_h = context["image_size"]

        for pred in predictions:
            features: np.ndarray = pred["backbone"]["features"]
            pose: np.ndarray = pred["bodypart"]["poses"]

            # only extract features from valid pose
            mask = ~np.all((pose < 0) | np.isnan(pose), axis=(1, 2))
            pose = pose[mask]
            pred["bodypart"]["poses"] = pose.copy()

            pose = np.nan_to_num(pose, nan=0)

            num_features, h, w = features.shape
            backbone_stride = input_w / w, input_h / h

            num_preds, num_keypoints, _ = pose.shape

            bodypart_features = np.zeros((num_preds, num_keypoints, num_features))
            indices = np.rint(pose[..., :2] / backbone_stride).astype(int)
            indices[..., 0] = np.clip(indices[..., 0], 0, w - 1)
            indices[..., 1] = np.clip(indices[..., 1], 0, h - 1)

            for idv, idv_indices in enumerate(indices):
                for kpt, (x, y) in enumerate(idv_indices):
                    # only assign features if the pose was defined
                    if np.sum(x + y) > 0:
                        bodypart_features[idv, kpt] = features[:, y, x]

            pred["backbone"]["bodypart_features"] = bodypart_features

        return predictions, context


--- File: deeplabcut/pose_estimation_pytorch/data/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.data.base import Loader
from deeplabcut.pose_estimation_pytorch.data.cocoloader import COCOLoader
from deeplabcut.pose_estimation_pytorch.data.collate import COLLATE_FUNCTIONS
from deeplabcut.pose_estimation_pytorch.data.dlcloader import DLCLoader
from deeplabcut.pose_estimation_pytorch.data.dataset import (
    PoseDatasetParameters,
    PoseDataset,
)
from deeplabcut.pose_estimation_pytorch.data.image import top_down_crop
from deeplabcut.pose_estimation_pytorch.data.postprocessor import (
    build_bottom_up_postprocessor,
    build_detector_postprocessor,
    build_top_down_postprocessor,
    Postprocessor,
)
from deeplabcut.pose_estimation_pytorch.data.preprocessor import (
    build_bottom_up_preprocessor,
    build_top_down_preprocessor,
    Preprocessor,
)
from deeplabcut.pose_estimation_pytorch.data.transforms import build_transforms


--- File: deeplabcut/pose_estimation_pytorch/data/preprocessor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Helpers to run preprocess data before running inference"""
from __future__ import annotations

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, TypeVar

import albumentations as A
import cv2
import numpy as np
import torch

from deeplabcut.pose_estimation_pytorch.data.image import top_down_crop


Image = TypeVar("Image", torch.Tensor, np.ndarray, str, Path)
Context = TypeVar("Context", dict[str, Any], None)


class Preprocessor(ABC):
    """
    Class to preprocess an image and turn it into a batch of inputs before running
    inference.

    As an example, a pre-processor can load an image, use a "bboxes" key from context
    to crop bounding boxes for individuals (going from a (h, w, 3) array to a
    (num_individuals, h, w, 3) array), and convert it into a tensor ready for inference.
    """

    @abstractmethod
    def __call__(self, image: Image, context: Context) -> tuple[Image, Context]:
        """Pre-processes an image

        Args:
            image: an image (containing height, width and channel dimensions) or a
                batch of images linked to a single input (containing an extra batch
                dimension)
            context: the context for this image or batch of images (such as bounding
                boxes, conditional pose, ...)

        Returns:
            the pre-processed image (or batch of images) and their context
        """
        pass


def build_bottom_up_preprocessor(
    color_mode: str, transform: A.BaseCompose
) -> Preprocessor:
    """Creates a preprocessor for bottom-up pose estimation (or object detection)

    Creates a preprocessor that loads an image, runs some transform on it (such as
    normalization), creates a tensor from the numpy array (going from (h, w, 3) to
    (3, h, w)) and adds a batch dimension (so the final tensor shape is (1, 3, h, w))

    Args:
        color_mode: whether to load the image as an RGB or BGR
        transform: the transform to apply to the image

    Returns:
        A default bottom-up Preprocessor
    """
    return ComposePreprocessor(
        components=[
            LoadImage(color_mode),
            AugmentImage(transform),
            ToTensor(),
            ToBatch(),
        ]
    )


def build_top_down_preprocessor(
    color_mode: str,
    transform: A.BaseCompose,
    top_down_crop_size: tuple[int, int],
    top_down_crop_margin: int = 0,
) -> Preprocessor:
    """Creates a preprocessor for top-down pose estimation

    Creates a preprocessor that loads an image, crops all bounding boxes given as a
    context (through a "bboxes" key), runs some transforms on each cropped image (such
    as normalization), creates a tensor from the numpy array (going from
    (num_ind, h, w, 3) to (num_ind, 3, h, w)).

    Args:
        color_mode: whether to load the image as an RGB or BGR
        transform: the transform to apply to the image
        top_down_crop_size: the (width, height) to resize cropped bboxes to
        top_down_crop_margin: the margin to add around detected bboxes for the crop

    Returns:
        A default top-down Preprocessor
    """
    return ComposePreprocessor(
        components=[
            LoadImage(color_mode),
            TopDownCrop(output_size=top_down_crop_size, margin=top_down_crop_margin),
            AugmentImage(transform),
            ToTensor(),
        ]
    )


class ComposePreprocessor(Preprocessor):
    """
    Class to preprocess an image and turn it into a batch of
    inputs before running inference
    """

    def __init__(self, components: list[Preprocessor]) -> None:
        self.components = components

    def __call__(self, image: Image, context: Context) -> tuple[Image, Context]:
        for preprocessor in self.components:
            image, context = preprocessor(image, context)
        return image, context


class LoadImage(Preprocessor):
    """Loads an image from a file, if not yet loaded"""

    def __init__(self, color_mode: str = "RBG") -> None:
        self.color_mode = color_mode

    def __call__(self, image: Image, context: Context) -> tuple[np.ndarray, Context]:
        if isinstance(image, (str, Path)):
            image_ = cv2.imread(str(image))
            if self.color_mode == "RGB":
                image_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)
        else:
            image_ = image

        h, w = image_.shape[:2]
        context["image_size"] = w, h
        return image_, context


class AugmentImage(Preprocessor):
    """

    Adds an offset and scale key to the context:
        offset: (x, y) position of the pixel in the top left corner of the augmented
            image in the original image
        scale: size of the original image divided by the size of the new image

    This allows to map the position of predictions in the transformed image back to the
    original image space.
        p_original = p_transformed * scale + offset
        p_transformed = (p_original - offset) / scale
    """

    def __init__(self, transform: A.BaseCompose) -> None:
        self.transform = transform

    @staticmethod
    def get_offsets_and_scales(
        h: int,
        w: int,
        output_bboxes: list[tuple[float, float, float, float]],
    ) -> tuple[list[tuple[float, float]], list[tuple[float, float]]]:
        offsets, scales = [], []
        for bbox in output_bboxes:
            x_origin, y_origin, w_out, h_out = bbox
            x_scale, y_scale = w / w_out, h / h_out
            x_offset = -x_origin * x_scale
            y_offset = -y_origin * y_scale
            offsets.append((x_offset, y_offset))
            scales.append((x_scale, y_scale))

        return offsets, scales

    @staticmethod
    def update_offset(
        offset: tuple[float, float],
        scale: tuple[float, float],
        new_offset: tuple[float, float],
    ) -> tuple[float, float]:
        return (
            scale[0] * new_offset[0] + offset[0],
            scale[1] * new_offset[1] + offset[1],
        )

    @staticmethod
    def update_scale(
        scale: tuple[float, float], new_scale: tuple[float, float]
    ) -> tuple[float, float]:
        return scale[0] * new_scale[0], scale[1] * new_scale[1]

    @staticmethod
    def update_offsets_and_scales(context, new_offsets, new_scales) -> tuple:
        """
        x = x' * scale' + offset'
        x' = x'' * scale'' + offset''
        -> x = x'' * (scale' * scale'') + (scale' * offset'' + offset')
        """
        # scales and offsets are either both lists or both tuples
        offsets = context.get("offsets", (0, 0))
        scales = context.get("scales", (1, 1))
        if isinstance(offsets, tuple):
            if isinstance(new_offsets, list):
                updated_offsets = [
                    AugmentImage.update_offset(offsets, scales, new_offset)
                    for new_offset in new_offsets
                ]
                updated_scales = [
                    AugmentImage.update_scale(scales, new_scale)
                    for new_scale in new_scales
                ]
            else:
                if not len(offsets) == len(new_offsets):
                    raise ValueError("Cannot rescale lists when not same length")

                updated_offsets = AugmentImage.update_offset(
                    offsets, scales, new_offsets
                )
                updated_scales = AugmentImage.update_scale(scales, new_scales)
        else:
            if isinstance(new_offsets, list):
                if not len(offsets) == len(new_offsets):
                    raise ValueError("Cannot rescale lists when not same length")

                updated_offsets = [
                    AugmentImage.update_offset(offset, scale, new_offset)
                    for offset, scale, new_offset in zip(offsets, scales, new_offsets)
                ]
                updated_scales = [
                    AugmentImage.update_scale(scale, new_scale)
                    for scale, new_scale in zip(scales, new_scales)
                ]
            else:
                updated_offsets = [
                    AugmentImage.update_offset(offset, scale, new_offsets)
                    for offset, scale in zip(offsets, scales)
                ]
                updated_scales = [
                    AugmentImage.update_scale(scale, new_scales) for scale in scales
                ]
        return updated_offsets, updated_scales

    def __call__(self, image: Image, context: Context) -> tuple[np.ndarray, Context]:
        # If the image is a batch, process each entry
        if len(image.shape) == 4:
            batch_size, h, w, _ = image.shape
            if batch_size == 0:
                # no images in top-down when no detections
                offsets, scales = (0, 0), (1, 1)
            else:
                transformed = [
                    self.transform(
                        image=img,
                        keypoints=[],
                        class_labels=[],
                        bboxes=[[0, 0, w, h]],
                        bbox_labels=["image"],
                    )
                    for img in image
                ]
                image = np.stack([t["image"] for t in transformed])
                output_bboxes = [t["bboxes"][0] for t in transformed]
                offsets, scales = self.get_offsets_and_scales(h, w, output_bboxes)
        else:
            h, w, _ = image.shape
            transformed = self.transform(
                image=image,
                keypoints=[],
                class_labels=[],
                bboxes=[[0, 0, w, h]],
                bbox_labels=["image"],
            )
            image = transformed["image"]
            output_bboxes = [transformed["bboxes"][0]]
            offsets, scales = self.get_offsets_and_scales(h, w, output_bboxes)
            offsets = offsets[0]
            scales = scales[0]

        offsets, scales = self.update_offsets_and_scales(context, offsets, scales)
        context["offsets"] = offsets
        context["scales"] = scales
        return image, context


class ToTensor(Preprocessor):
    """Transforms lists and numpy arrays into tensors"""

    def __call__(self, image: Image, context: Context) -> tuple[np.ndarray, Context]:
        image = torch.tensor(image, dtype=torch.float)
        if len(image.shape) == 4:
            image = image.permute(0, 3, 1, 2)
        else:
            image = image.permute(2, 0, 1)
        return image, context


class ToBatch(Preprocessor):
    """TODO"""

    def __call__(self, image: Image, context: Context) -> tuple[np.ndarray, Context]:
        return image.unsqueeze(0), context


class TopDownCrop(Preprocessor):
    """Crops bounding boxes out of images for top-down pose estimation

    Args:
        output_size: The (width, height) of crops to output
        margin: The margin to add around detected bounding boxes before cropping
    """

    def __init__(self, output_size: int | tuple[int, int], margin: int = 0) -> None:
        if isinstance(output_size, int):
            output_size = (output_size, output_size)

        self.output_size = output_size
        self.margin = margin

    def __call__(
        self, image: np.ndarray, context: Context
    ) -> tuple[np.ndarray, Context]:
        """TODO: numpy implementation"""
        if "bboxes" not in context:
            raise ValueError(f"Must include bboxes to CropDetections, found {context}")

        images, offsets, scales = [], [], []
        for bbox in context["bboxes"]:
            crop, offset, scale = top_down_crop(
                image, bbox, self.output_size, margin=self.margin
            )
            images.append(crop)
            offsets.append(offset)
            scales.append(scale)

        context["offsets"] = np.array(offsets)
        context["scales"] = np.array(scales)

        # can have no bounding boxes if detector made no detections
        if len(images) == 0:
            images = np.zeros((0, *image.shape))
        else:
            images = np.stack(images, axis=0)

        context["top_down_crop_size"] = self.output_size
        return images, context


--- File: deeplabcut/pose_estimation_pytorch/data/dataset.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from dataclasses import dataclass

import albumentations as A
import cv2
import numpy as np
from torch.utils.data import Dataset

from deeplabcut.pose_estimation_pytorch.data.image import top_down_crop
from deeplabcut.pose_estimation_pytorch.data.utils import (
    _crop_image_keypoints,
    _extract_keypoints_and_bboxes,
    apply_transform,
    map_id_to_annotations,
    map_image_path_to_id,
    out_of_bounds_keypoints,
    pad_to_length,
)
from deeplabcut.pose_estimation_pytorch.task import Task


@dataclass(frozen=True)
class PoseDatasetParameters:
    """Parameters for a pose dataset

    Attributes:
        bodyparts: the names of bodyparts in the dataset
        unique_bpts: the names of unique bodyparts, or an empty list
        individuals: the names of individuals
        with_center_keypoints: whether to compute center keypoints for individuals
        color_mode: {"RGB", "BGR"} the mode to load images in
        top_down_crop_size: for top-down models, the (width, height) to crop bboxes to
        top_down_crop_margin: for top-down models, the margin to add around bboxes
    """

    bodyparts: list[str]
    unique_bpts: list[str]
    individuals: list[str]
    with_center_keypoints: bool = False
    color_mode: str = "RGB"
    top_down_crop_size: tuple[int, int] | None = None
    top_down_crop_margin: int | None = None

    @property
    def num_joints(self) -> int:
        return len(self.bodyparts)

    @property
    def num_unique_bpts(self) -> int:
        return len(self.unique_bpts)

    @property
    def max_num_animals(self) -> int:
        return len(self.individuals)


@dataclass
class PoseDataset(Dataset):
    """A pose dataset"""

    images: list[dict]
    annotations: list[dict]
    parameters: PoseDatasetParameters
    transform: A.BaseCompose | None = None
    mode: str = "train"
    task: Task = Task.BOTTOM_UP

    def __post_init__(self):
        self.image_path_id_map = map_image_path_to_id(self.images)
        self.annotation_idx_map = map_id_to_annotations(self.annotations)
        self.img_id_to_index = {
            img["id"]: index for index, img in enumerate(self.images)
        }
        if self.task == Task.TOP_DOWN and (
            self.parameters.top_down_crop_size is None
            or self.parameters.top_down_crop_margin is None
        ):
            raise ValueError(
                "You must specify a ``top_down_crop_size`` and ``top_down_crop_margin``"
                "in your PoseDatasetParameters when the task is TOP_DOWN."
            )

        self.td_crop_size = self.parameters.top_down_crop_size
        self.td_crop_margin = self.parameters.top_down_crop_margin

    def __len__(self):
        # TODO: TD should only return the number of annotations that aren't unique_bodyparts
        if self.task in (Task.BOTTOM_UP, Task.DETECT):
            return len(self.images)

        return len(self.annotations)

    def _get_raw_item(self, index: int) -> tuple[str, list[dict], int]:
        """
        Retrieve the image path and annotations for the specified index.

        Args:
            index (int): The index of the item to retrieve.

        Returns:
            tuple[str, list]: A tuple containing the image path and annotations.

        Note:
            This method is used by the __getitem__ method to fetch raw data from the dataset storage.
            If `self.crop` is True, it returns the image path and a list with a single annotation.
            Otherwise, it returns the image path and a list of annotations for all instances in the image.
        """
        img = self.images[index]

        anns = [self.annotations[idx] for idx in self.annotation_idx_map[img["id"]]]

        return img["file_name"], anns, img["id"]

    def _get_raw_item_crop(self, index: int) -> tuple[str, list[dict], int]:
        ann = self.annotations[index]

        img = self.images[self.img_id_to_index[ann["image_id"]]]
        return img["file_name"], [ann], img["id"]

    def __getitem__(self, index: int) -> dict:
        """
        Gets the item at the specified index from the dataset.

        Args:
            index: ordered number of the items in the dataset

        Returns:
            dict: corresponding to the image annotations, with keys:
            {
                "image": image tensor of shape (c, h, w),
                "image_id": the ID of the image,
                "path": the filepath to the image,
                "original_size": the original (h, w) size before transforms
                "offsets": the (x, y) offsets to apply to the keypoints in TD mode
                "scales": the (x, y) scales to apply to the keypoints in TD mode
                "annotations": {
                    "keypoints": array of keypoints, invisible keypoints appear as (-1,-1)
                    "keypoints_unique": the unique keypoints, if there are any
                    "area": array of animals area in this image
                    "boxes": the bounding boxes in this image
                    "is_crowd": is_crowd annotations
                    "labels": category_id annotations for boxes
                },
            }
        """
        image_path, anns, image_id = self._get_data_based_on_task(index)

        image, original_size = self._load_image(image_path)
        (
            keypoints,
            keypoints_unique,
            bboxes,
            annotations_merged,
        ) = self.extract_keypoints_and_bboxes(anns, image.shape)

        # this is applying data augmentations before the cropping
        # though normalization should be applied after the cropping
        transformed = self.apply_transform_all_keypoints(
            image, keypoints, keypoints_unique, bboxes
        )
        image = transformed["image"]
        keypoints = transformed["keypoints"]
        keypoints_unique = transformed["keypoints_unique"]
        bboxes = transformed["bboxes"]
        offsets = (0, 0)
        scales = (1.0, 1.0)

        if self.task == Task.TOP_DOWN:
            if len(bboxes) > 1:
                raise ValueError(
                    "There can only be one bbox per item in TD datasets, found "
                    f"{bboxes} for {index} (image {image_path})"
                )
            bboxes = bboxes.astype(int)

            if bboxes[0, 2] == 0 or bboxes[0, 3] == 0:
                # bbox was augmented out of the image; blank image, no keypoints
                keypoints[..., 2] = 0.0
                image = np.zeros(
                    (self.td_crop_size[1], self.td_crop_size[0], image.shape[-1]),
                    dtype=image.dtype,
                )
            else:
                image, offsets, scales = top_down_crop(
                    image, bboxes[0], self.td_crop_size, margin=self.td_crop_margin,
                )
                keypoints[:, :, 0] = (keypoints[:, :, 0] - offsets[0]) / scales[0]
                keypoints[:, :, 1] = (keypoints[:, :, 1] - offsets[1]) / scales[1]
                bboxes = bboxes[:1]
                bboxes[..., 0] = (bboxes[..., 0] - offsets[0]) / scales[0]
                bboxes[..., 1] = (bboxes[..., 1] - offsets[1]) / scales[1]
                bboxes[..., 2] = bboxes[..., 2] / scales[0]
                bboxes[..., 3] = bboxes[..., 3] / scales[1]

                # as a RandomBBoxTransform can be added, keypoints may be outside of the
                #   image after the crop
                oob_mask = out_of_bounds_keypoints(keypoints, self.td_crop_size)
                if np.sum(oob_mask) > 0:
                    keypoints[oob_mask, 2] = 0.0

        if self.parameters.with_center_keypoints:
            keypoints = self.add_center_keypoints(keypoints)

        return self._prepare_final_data_dict(
            image,
            keypoints,
            keypoints_unique,
            original_size,
            image_path,
            bboxes,
            image_id,
            annotations_merged,
            offsets,
            scales,
        )

    def _prepare_final_data_dict(
        self,
        image: np.ndarray,
        keypoints: np.ndarray,
        keypoints_unique: np.ndarray,
        original_size: tuple[int, int],
        image_path: str,
        bboxes: np.array,
        image_id: int,
        annotations_merged: dict,
        offsets: tuple[int, int],
        scales: tuple[float, float],
    ) -> dict[str, np.ndarray | dict[str, np.ndarray]]:
        return {
            "image": image.transpose((2, 0, 1)),
            "image_id": image_id,
            "path": image_path,
            "original_size": np.array(original_size),
            "offsets": np.array(offsets, dtype=int),
            "scales": np.array(scales, dtype=float),
            "annotations": self._prepare_final_annotation_dict(
                keypoints, keypoints_unique, bboxes, annotations_merged
            ),
        }

    def _prepare_final_annotation_dict(
        self,
        keypoints: np.ndarray,
        keypoints_unique: np.ndarray,
        bboxes: np.array,
        anns: dict,
    ) -> dict[str, np.ndarray]:
        num_animals = self.parameters.max_num_animals
        if self.task == Task.TOP_DOWN:
            num_animals = 1

        bbox_widths = np.maximum(1, bboxes[..., 2])
        bbox_heights = np.maximum(1, bboxes[..., 3])
        area = bbox_widths * bbox_heights
        if "individual_id" not in anns:
            anns["individual_id"] = -np.ones(len(anns["category_id"]), dtype=int)

        # we use ..., :3 to pass the visibility flag along
        return {
            "keypoints": pad_to_length(keypoints[..., :3], num_animals, 0).astype(
                np.single
            ),
            "keypoints_unique": keypoints_unique[..., :3].astype(np.single),
            "with_center_keypoints": self.parameters.with_center_keypoints,
            "area": pad_to_length(area, num_animals, 0).astype(np.single),
            "boxes": pad_to_length(bboxes, num_animals, 0).astype(np.single),
            "is_crowd": pad_to_length(anns["iscrowd"], num_animals, 0).astype(int),
            "labels": pad_to_length(anns["category_id"], num_animals, -1).astype(int),
            "individual_ids": pad_to_length(
                anns["individual_id"], num_animals, -1
            ).astype(int),
        }

    def _load_image(self, image_path):
        image = cv2.imread(image_path)
        if self.parameters.color_mode == "RGB":
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        return image, image.shape

    def _get_data_based_on_task(self, index: int) -> tuple[str, list[dict], int]:
        """
        Retrieve data based on the specified task.

        For the 'TD' (top-down pose estimation) task:
        - Provides a cropped image and its annotations.
        - The shape of annotations['keypoints'] is (1, num_joints, 2).

        For 'BU' and 'DT' tasks:
        - Provides the full, non-cropped image and its annotations.
        - The shape of annotations['keypoints'] is (max_num_animals, num_joints, 2).

        Args:
            index: Index of the item in the dataset.

        Returns:
            tuple: Tuple containing the image path, annotations, and image ID.
        """
        if self.task == Task.TOP_DOWN:
            return self._get_raw_item_crop(index)
        elif self.task in [Task.BOTTOM_UP, Task.DETECT]:
            return self._get_raw_item(index)

        raise ValueError(f"Unknown task: {self.task}")

    def apply_transform_all_keypoints(
        self,
        image: np.ndarray,
        keypoints: np.ndarray,
        keypoints_unique: np.ndarray,
        bboxes: np.ndarray,
    ) -> dict[str, np.ndarray]:
        """Transforms the image using this class's transform

        Args:
            image: the image to transform
            keypoints: an array of shape (num_individuals, num_joints, 3) containing
                the keypoints in the image
            keypoints_unique: an array of shape (num_unique_bodyparts, 3) containing
                the unique keypoints in the image
            bboxes: the bounding boxes in the image

        Returns:
            the augmented image, keypoints and bboxes, in format
            {
                "image": (h, w, c),
                "keypoints": (num_individuals, num_joints, 3),
                "keypoints_unique": (num_unique_bodyparts, 3),
                "bboxes": (4,),
            }
        """
        class_labels = [
            f"individual{i}_{bpt}"
            for i in range(len(keypoints))
            for bpt in self.parameters.bodyparts
        ] + [f"unique_{bpt}" for bpt in self.parameters.unique_bpts]

        all_keypoints = keypoints.reshape(-1, 3)
        if self.parameters.num_unique_bpts > 0:
            all_keypoints = np.concatenate([all_keypoints, keypoints_unique], axis=0)

        transformed = apply_transform(
            self.transform, image, all_keypoints, bboxes, class_labels=class_labels
        )
        if self.parameters.num_unique_bpts > 0:
            keypoints = transformed["keypoints"][
                : -self.parameters.num_unique_bpts
            ].reshape(*keypoints.shape)
            keypoints_unique = transformed["keypoints"][
                -self.parameters.num_unique_bpts :
            ]
            keypoints_unique = keypoints_unique.reshape(
                self.parameters.num_unique_bpts, 3
            )
        else:
            keypoints = transformed["keypoints"].reshape(*keypoints.shape)
            keypoints_unique = np.zeros((0,))

        transformed["keypoints"] = keypoints
        transformed["keypoints_unique"] = keypoints_unique
        transformed["bboxes"] = np.array(transformed["bboxes"])
        if len(transformed["bboxes"]) == 0:
            transformed["bboxes"] = np.zeros((0, 4))

        return transformed

    @staticmethod
    def crop(
        image: np.ndarray,
        keypoints,
        coords: tuple[tuple[int, int], tuple[int, int]],
        output_size: tuple[int, int],
    ) -> tuple[np.ndarray, np.ndarray, tuple[int, int], tuple[int, int]]:
        """
        Crop the image based on a given bounding box and resize it to the desired output size.

        Args:
            image: the image to transform
            keypoints: an array of shape (num_individuals, num_joints, 3) containing
                the keypoints in the image
            coords: A bounding box defined as ((x_center, y_center), (width, height)).
            output_size: Desired size for the output cropped, padded and resized image.

        Returns:
            Cropped (and possibly padded) and resized image.
            Offsets used for cropping.
            Padding sizes.
            Scale factor used to resize the image.
        """
        return _crop_image_keypoints(image, keypoints, coords, output_size)

    def extract_keypoints_and_bboxes(
        self, anns: list[dict], image_shape: tuple[int, int, int]
    ) -> tuple[np.ndarray, np.ndarray, np.ndarray, dict[str, np.ndarray]]:
        """
        Args:
            anns: COCO-style annotations
            image_shape: the (h, w, c) shape of the image for which to get annotations

        Returns:
            keypoints with shape (n_annotation, num_joints, 3)
            unique_keypoints with shape (num_unique_bpts, 3)
            bboxes in xywh format with shape (n_annotation, 4)
            annotations_merged, where each key contains n_annotation values
        """
        return _extract_keypoints_and_bboxes(
            anns,
            image_shape,
            self.parameters.num_joints,
            self.parameters.num_unique_bpts,
        )

    @staticmethod
    def add_center_keypoints(keypoints: np.ndarray) -> np.ndarray:
        """Adds a keypoint in the mean of each individual

        Args:
            keypoints: shape (num_idv, num_kpts, 3)

        Returns:
            keypoints with centers, of shape (num_idv, num_kpts + 1, 3)
        """
        num_idv = keypoints.shape[0]
        centers = np.full((num_idv, 1, 3), np.nan)

        keypoints_xy = keypoints.copy()[..., :2]
        keypoints_xy[keypoints[..., 2] <= 0] = np.nan

        # only set centers for individuals where at least 1 bodypart is visible
        vis_mask = (~np.isnan(keypoints_xy) > 0).all(axis=2).any(axis=1)
        if np.any(vis_mask):
            centers[vis_mask, 0, :2] = np.nanmean(keypoints_xy[vis_mask], axis=1)

        masked_centers = np.any(np.isnan(centers[:, 0, :2]), axis=1)
        centers[masked_centers, 0, 2] = 0
        centers[~masked_centers, 0, 2] = 2
        np.nan_to_num(centers, copy=False, nan=0)

        return np.concatenate((keypoints, centers), axis=1)


--- File: deeplabcut/pose_estimation_pytorch/data/cocoloader.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import json
import os
import warnings
from pathlib import Path

import numpy as np

from deeplabcut.pose_estimation_pytorch.data.base import Loader
from deeplabcut.pose_estimation_pytorch.data.dataset import PoseDatasetParameters
from deeplabcut.pose_estimation_pytorch.data.utils import (
    map_id_to_annotations,
    map_image_path_to_id,
)


class COCOLoader(Loader):
    """
    Attributes:
        project_root: root directory path of the COCO project.
        train_json_filename: the name of the json file containing the train annotations
        test_json_filename: the name of the json file containing the train annotations.
            None if there is no test set.

    Examples:
        loader = COCOLoader(
            project_root='/path/to/project/',
            model_config_path='/path/to/project/experiments/train/pytorch_config.yaml'
            train_json_filename="train.json",
            test_json_filename="test.json",
        )
    """

    def __init__(
        self,
        project_root: str | Path,
        model_config_path: str | Path,
        train_json_filename: str = "train.json",
        test_json_filename: str = "test.json",
    ):
        super().__init__(Path(model_config_path))
        self.project_root = Path(project_root)
        self.train_json_filename = train_json_filename
        self.test_json_filename = test_json_filename
        self._dataset_parameters = None

        self.train_json = self.load_json(self.project_root, self.train_json_filename)
        self.test_json = None
        if self.test_json_filename:
            self.test_json = self.load_json(self.project_root, self.test_json_filename)

    def get_dataset_parameters(self) -> PoseDatasetParameters:
        """
        Retrieves dataset parameters based on the instance's configuration.

        Returns:
            An instance of the PoseDatasetParameters with the parameters set.
        """
        if self._dataset_parameters is None:
            num_individuals, bodyparts = self.get_project_parameters(self.train_json)

            crop_cfg = self.model_cfg["data"]["train"].get("top_down_crop", {})
            crop_w, crop_h = crop_cfg.get("width", 256), crop_cfg.get("height", 256)
            crop_margin = crop_cfg.get("margin", 0)

            self._dataset_parameters = PoseDatasetParameters(
                bodyparts=bodyparts,
                unique_bpts=[],
                individuals=[f"individual{i}" for i in range(num_individuals)],
                with_center_keypoints=self.model_cfg.get("with_center_keypoints", False),
                color_mode=self.model_cfg.get("color_mode", "RGB"),
                top_down_crop_size=(crop_w, crop_h),
                top_down_crop_margin=crop_margin,
            )

        return self._dataset_parameters

    @staticmethod
    def load_json(project_root: str | Path, filename: str) -> dict:
        """Load a JSON file from the annotations directory.

        Args:
            project_root: path to the root directory for the project
            filename: filename of JSON file to load

        Returns:
            json_obj: JSON object loaded from the file

        Raises:
            FileNotFoundError if the file does not exist
            ValueError if the object stored in the file is not a dict

        Examples:
            Check https://docs.trainingdata.io/v1.0/Export%20Format/COCO/ to see
            examples of how a json file looks like.
        """
        json_path = os.path.join(project_root, "annotations", filename)
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"File {json_path} does not exist.")

        with open(json_path, "r") as f:
            json_obj = json.load(f)

        if not isinstance(json_obj, dict):
            raise ValueError("COCO datasets need to be saved in JSON Objects")

        return json_obj

    @staticmethod
    def validate_categories(coco_json: dict) -> dict:
        """Checks that the categories for the COCO project are valid.

        Checks that there is no category with ID 0 in the dataset, as this causes issues
        with torchvision object detectors (label 0 is reserved for background
        detections). If that's the case, all category IDs are shifted by 1 such that
        there is no longer a category 0.

        Currently, detectors can only be trained with a single category. This also
        ensures that all annotations have `category_id` set to 1.

        Args:
            coco_json: the COCO dictionary containing the annotations

        Returns:
            the validated COCO object
        """
        cat_0 = False
        for cat in coco_json["categories"]:
            if cat["id"] == 0:
                cat_0 = cat
                warnings.warn(
                    f"Found a category with ID 0 ({cat}) in the COCO dataset. This is not"
                    f" allowed, as category ID 0 is reserved as the background ID for"
                    f" torchvision detectors. All category IDs have been shifted by 1."
                )

        if len(coco_json["categories"]) > 1:
            warnings.warn(
                f"Found more than 1 category in the project. This is currently not"
                f" supported in DeepLabCut. All annotations will be given category 1"
            )

        if cat_0:
            for cat in coco_json["categories"]:
                cat["id"] = 1

        if cat_0 or len(coco_json["categories"]) > 1:
            for ann in coco_json["annotations"]:
                ann["category_id"] = 1

        return coco_json

    @staticmethod
    def validate_images(project_root: str | Path, coco_json: dict) -> dict:
        """Goes over images and annotations to look for potential errors

        This code tries to ensure that training a model on this project does not crash
        down the line

        Completes relative image filepaths to '/project_root/images/file_name'. Absolute
        filepaths are not updated (which allows storing images to be stored in a folder
        other than the project root) Then checks that all images files exist in the file
        system.

        Args:
            project_root: the root path of the COCO project
            coco_json: the COCO dictionary containing the annotations

        Returns:
            the validated COCO object
        """
        image_ids = set()
        missing_images = {}
        validated_images = []
        for image in coco_json["images"]:
            image_filename = Path(image["file_name"])
            if image_filename.is_absolute():
                image_path = image_filename
            else:
                image_path = Path(project_root) / "images" / image["file_name"]
                image["file_name"] = str(image_path)

            if not image_path.exists():
                missing_images[image["id"]] = image["file_name"]
            else:
                validated_images.append(image)
                image_ids.add(image["id"])

        if len(missing_images) > 0:
            warnings.warn(
                f"There are {len(missing_images)} images that cannot be found (here"
                " are some):"
            )
            for img_id, file_name in missing_images.items():
                print(f"  * {img_id}: {file_name}")

        coco_json["images"] = validated_images

        if len(missing_images) > 0:
            validated_annotations = []
            for ann in coco_json["annotations"]:
                if ann["image_id"] not in missing_images:
                    validated_annotations.append(ann)

            coco_json["annotations"] = validated_annotations

        validated_annotations = []
        for ann in coco_json["annotations"]:
            if ann["image_id"] in image_ids:
                validated_annotations.append(ann)

        if len(coco_json["annotations"]) < len(validated_annotations):
            warnings.warn(
                f"Found some annotations for which the image ID was not in the images."
                f" Removing them from the dataset."
            )
            print(f"  All annotations: {len(coco_json['annotations'])}")
            print(f"  Annotations with correct image IDs: {len(validated_annotations)}")
            coco_json["annotations"] = validated_annotations

        return coco_json

    def load_data(self, mode: str = "train") -> dict:
        """Convert data from JSON object to dictionary.
        Args:
            mode: indicates which JSON object to convert. Defaults to "train".

        Returns:
            the train or test data
        """
        # todo: add validation
        if mode == "train":
            data = self.train_json
        elif mode == "test":
            data = self.test_json
        else:
            raise AttributeError(f"Unknown mode: {mode}")

        data = COCOLoader.validate_categories(data)
        data = COCOLoader.validate_images(self.project_root, data)

        annotations_per_image = {}
        for annotation in data["annotations"]:
            annotation["keypoints"] = np.array(annotation["keypoints"], dtype=float)
            annotation["bbox"] = np.array(annotation["bbox"], dtype=float)

            # set individual index
            image_id = annotation["image_id"]
            individual_idx = annotations_per_image.get(image_id, 0)
            annotation["individual"] = f"individual{individual_idx}"
            annotations_per_image[image_id] = individual_idx + 1

        filter_annotations = []
        for annotation in data['annotations']:
            keypoints = annotation['keypoints']
            bbox = annotation['bbox']
            if np.all(keypoints <= 0) or len(bbox) == 0:
                continue
            filter_annotations.append(annotation)

        data["annotations"] = filter_annotations        
        
        # FIXME: why estimating bbox when there are already bbox?
        annotations_with_bbox = self._compute_bboxes(
            data["images"],
            data["annotations"],
            method="gt",
        )
        data["annotations"] = annotations_with_bbox
        return data

    @staticmethod
    def get_project_parameters(train_json: dict) -> tuple[int, list[str]]:
        """
        Loads the parameters for the project from the train json file
        TODO: Should this compute the number also using the test json?

        Args:
            train_json: the json dictionary containing the data for training

        Returns:
            int: the maximum number of individuals in a single image
            list[str]: the name of keypoints annotated in this project
        """
        # TODO: Check that there's a single category
        bodyparts = train_json["categories"][0]["keypoints"]

        img_to_annotations = map_id_to_annotations(train_json["annotations"])
        if len(img_to_annotations) == 0:
            raise ValueError(f"No images found in the dataset: {train_json}!")
        elif len(img_to_annotations) == 1:
            num_individuals = len(list(img_to_annotations.values())[0])
        else:
            num_individuals = max(
                *[len(a_ids) for a_ids in img_to_annotations.values()]
            )

        return num_individuals, bodyparts

    def predictions_to_coco(
        self,
        predictions: dict[str, dict[str, np.ndarray]],
        mode: str = "train",
    ) -> list[dict]:
        """Converts detections to COCO format

        Args:
            predictions: a dictionary mapping image name to the predictions made for it
            mode: {"train", "test"} the mode that the predictions were made with

        Returns:
            The COCO-format predictions
        """
        data = self.load_data(mode)
        image_path_to_id = map_image_path_to_id(data["images"])

        # TODO: no unique bodyparts for COCO
        coco_predictions = []
        for image_path, pred in predictions.items():
            image_id = image_path_to_id[image_path]

            # Shape (num_individuals, num_keypoints, 3)
            individuals = pred["bodyparts"]
            for idx, keypoints in enumerate(individuals):
                if not np.all(keypoints == -1):
                    score = np.mean(keypoints[:, 2]).item()
                    keypoints = keypoints.copy()
                    keypoints[:, 2] = 2  # set visibility instead of score
                    coco_pred = {
                        "image_id": int(image_id),
                        "category_id": 1,  # TODO: get category ID from prediction?
                        "keypoints": keypoints.reshape(-1).tolist(),
                        "score": float(score),
                    }
                    if "bboxes" in pred:
                        coco_pred["bbox"] = pred["bboxes"][idx].reshape(-1).tolist()
                    if "bbox_scores" in pred:
                        coco_pred["bbox_scores"] = (
                            pred["bbox_scores"][idx].reshape(-1).tolist()
                        )

                    coco_predictions.append(coco_pred)

        return coco_predictions


--- File: deeplabcut/pose_estimation_pytorch/data/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from collections import defaultdict
from functools import reduce, lru_cache
from pathlib import Path

import albumentations as A
import numpy as np
import torch
from PIL import Image
from torchvision.ops import box_convert
from torchvision.transforms import functional as F


@lru_cache(maxsize=None)
def read_image_shape_fast(path: str | Path) -> tuple[int, int, int]:
    """Blazing fast and does not load the image into memory"""
    with Image.open(path) as img:
        width, height = img.size
        return len(img.getbands()), height, width


def bbox_from_keypoints(
    keypoints: np.ndarray,
    image_h: int,
    image_w: int,
    margin: int,
) -> np.ndarray:
    """
    Computes bounding boxes from keypoints.

    Args:
        keypoints: (..., num_keypoints, xy) the keypoints from which to get bboxes
        image_h: the height of the image
        image_w: the width of the image
        margin: the bounding box margin

    Returns:
        the bounding boxes for the keypoints, of shape (..., 4) in the xywh format
    """
    squeeze = False

    # we do not estimate bbox on keypoints that have 0 or -1 flag
    keypoints = np.copy(keypoints)
    keypoints[keypoints[..., -1] <= 0] = np.nan

    if len(keypoints.shape) == 2:
        squeeze = True
        keypoints = np.expand_dims(keypoints, axis=0)

    bboxes = np.full((keypoints.shape[0], 4), np.nan)
    bboxes[:, :2] = np.nanmin(keypoints[..., :2], axis=1) - margin  # X1, Y1
    bboxes[:, 2:4] = np.nanmax(keypoints[..., :2], axis=1) + margin  # X2, Y2

    # can have NaNs if some individuals have no visible keypoints
    bboxes = np.nan_to_num(bboxes, nan=0)

    bboxes = np.clip(
        bboxes,
        a_min=[0, 0, 0, 0],
        a_max=[image_w, image_h, image_w, image_h],
    )
    bboxes[..., 2] = bboxes[..., 2] - bboxes[..., 0]  # to width
    bboxes[..., 3] = bboxes[..., 3] - bboxes[..., 1]  # to height
    if squeeze:
        return bboxes[0]

    return bboxes


def merge_list_of_dicts(
    list_of_dicts: list[dict], keys_to_include: list[str]
) -> dict[str, list]:
    """
    Flattens a list of dictionaries into a dictionary with the lists concatenated.

    Args:
        list_of_dicts: the dictionaries to merge
        keys_to_include: the keys to include in the new dictionary

    Returns:
        the merged dictionary

    Examples:
        input:
            list_of_dicts: [{"id": 0, "num": 1}, {"id": 1, "num": 10}]
            keys_to_include: ["id", "num"]
        output:
            {"id": [0, 1], "num": [1, 10]}
    """
    return reduce(
        lambda acc, d: {
            key: acc.get(key, []) + [value]
            for key, value in d.items()
            if key in keys_to_include
        },
        list_of_dicts,
        defaultdict(list),
    )


def map_image_path_to_id(images: list[dict]) -> dict[str, int]:
    """
    Binds the image paths to their respective IDs.

    Args:
        images: List of dictionaries containing image data in COCO-like format.
            Each dictionary should have 'file_name' and 'id' keys.

    Returns:
        A dictionary mapping image paths to their respective IDs.

    Examples:
        images = [{"file_name": "path/to/image1.jpg", "id": 1}, ...]
    """

    return {image["file_name"]: image["id"] for image in images}


def map_id_to_annotations(annotations: list[dict]) -> dict[int, list[int]]:
    """
    Maps image IDs to their corresponding annotation indices.

    Args:
        annotations: List of dictionaries containing annotation data. Each dictionary
            should have 'image_id' key.

    Returns:
        A dictionary mapping image IDs to lists of corresponding annotation indices.

    Examples:
        annotations = [{"image_id": 1, ...}, ...]
    """

    annotation_idx_map = defaultdict(list)
    for idx, annotation in enumerate(annotations):
        annotation_idx_map[annotation["image_id"]].append(idx)

    return annotation_idx_map


def _crop_and_pad_image(
    image: np.ndarray,
    coords: tuple[tuple[int, int], tuple[int, int]],
    output_size: tuple[int, int],
) -> tuple[np.ndarray, tuple[int, int]]:
    """
    Crop the image using the given coordinates and pad the larger dimension to change
    the aspect ratio.

    Args:
        image: Image to crop, of shape (height, width, channels).
        coords: Coordinates for cropping as [(xmin, xmax), (ymin, ymax)].
        output_size: The (output_h, output_w) that this cropped image will be resized
            to. Used to compute padding to keep aspect ratios.

    Returns:
        Cropped (and possibly padded) image
        Padding (pad_h, pad_w)
    """
    cropped_image = image[coords[1][0] : coords[1][1], coords[0][0] : coords[0][1], :]

    crop_h, crop_w, c = cropped_image.shape
    pad_h, pad_w = 0, 0
    target_ratio_h = output_size[0] / crop_h
    target_ratio_w = output_size[1] / crop_w

    if target_ratio_h != target_ratio_w:
        if crop_h < crop_w:
            # Pad the height
            new_h = int(crop_w * output_size[0] / output_size[1])
            pad_h = new_h - crop_h
            pad_image = np.zeros((new_h, crop_w, c))
            y_offset = pad_h // 2
            pad_image[y_offset : y_offset + crop_h, :] = cropped_image
        else:
            # Pad the width
            new_w = int(crop_h * output_size[1] / output_size[0])
            pad_w = new_w - crop_w
            pad_image = np.zeros((crop_h, new_w, c))
            x_offset = pad_w // 2
            pad_image[:, x_offset : x_offset + crop_w] = cropped_image
    else:
        pad_image = cropped_image

    return pad_image, (pad_h, pad_w)


def _crop_and_pad_keypoints(
    keypoints: np.ndarray, coords: tuple[int, int], pad_size: tuple[int, int]
):
    """
    Adjust the keypoints after cropping and padding.

    Parameters:
        keypoints: The original keypoints, typically a 2D array of shape (..., 2).
        coords: The (xmin, ymin) crop coordinates used for cropping the image.
        pad_size: The padding sizes added to the cropped image, in the format (pad_h, pad_w).

    Returns:
        Adjusted keypoints.
    """
    keypoints[..., 0] -= coords[0]
    keypoints[..., 1] -= coords[1]
    keypoints[..., 0] += pad_size[1] // 2
    keypoints[..., 1] += pad_size[0] // 2
    return keypoints


def _crop_image_keypoints(
    image, keypoints, coords, output_size
) -> tuple[np.ndarray, np.ndarray, tuple[int, int], tuple[int, int]]:
    """TODO: Requires fixing
    Crop the image based on a given bounding box and resize it to the desired output
    size. Returns offsets and scales to map keypoints in the resized image to
    coordinates in the original image:

        x_original = (x_cropped * x_scale) + x_offset
        y_original = (y_cropped * y_scale) + y_offset

    Args:
        image: Image to crop, of shape (height, width, channels).
        coords: Coordinates for cropping as ((xmin, xmax), (ymin, ymax)).
        output_size: The (h, w) that the cropped image should be resized to.

    Returns:
        Cropped, possibly padded, and resized image.
        The position of the keypoints in the cropped, resized image
        Offsets used for cropping.
        The offsets to map predicted keypoints back to the original image
        The scale to map predicted keypoints back to the original image
    """

    cropped_image, pad_size = _crop_and_pad_image(image, coords, output_size)
    cropped_keypoints = _crop_and_pad_keypoints(
        keypoints, (coords[0][0], coords[1][0]), pad_size
    )

    offsets = (coords[0][0], coords[1][0])
    scales = [
        output_size[0] / cropped_image.shape[0],
        output_size[1] / cropped_image.shape[1],
    ]

    # TODO: Fix resizing, use OpenCV
    cropped_resized_image = np.resize(
        cropped_image, (*output_size, cropped_image.shape[2])
    )

    cropped_resized_keypoints = np.array(cropped_keypoints) * np.array(scales + [1])

    return cropped_resized_image, cropped_resized_keypoints, offsets, scales


def _compute_crop_bounds(
    bboxes: np.ndarray,
    image_shape: tuple[int, int, int],
    remove_empty: bool = True,
) -> np.ndarray:
    """
    Compute the boundaries for cropping an image based on a COCO-format bounding box
    and image shape by clipping values so the bounding boxes are entirely in the image.

    Args:
        bboxes: COCO-format bounding box of shape (b, xywh)
        image_shape: Shape of the image defined as (height, width, channels).

    Returns:
        The bounding boxes, clipped to be entirely inside the image
    """
    h, w = image_shape[:2]
    # to xyxy
    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]
    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]
    # clip
    bboxes = np.clip(bboxes, 0, np.array([w, h, w, h]))
    # to xywh
    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]
    bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]
    # filter
    if remove_empty:
        squashed_bbox_mask = np.logical_or(bboxes[:, 2] <= 0, bboxes[:, 3] <= 0)
        bboxes = bboxes[~squashed_bbox_mask]
    return bboxes


def _extract_keypoints_and_bboxes(
    anns: list[dict],
    image_shape: tuple[int, int, int],
    num_joints: int,
    num_unique_bodyparts: int,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, dict[str, np.ndarray]]:
    """
    Args:
        anns: COCO-style annotations
        image_shape: the (h, w, c) shape of the image for which to get annotations
        num_joints: the number of joints in the annotations

    Returns:
        keypoints, unique_keypoints, bboxes in xywh format, annotations_merged
    """
    keypoints = []
    original_bboxes = []
    anns_to_merge = []
    unique_keypoints = None
    h, w = image_shape[:2]
    for i, annotation in enumerate(anns):
        keypoints_individual = _annotation_to_keypoints(annotation, h, w)
        if annotation["individual"] != "single":
            bbox_individual = annotation["bbox"]
            original_bboxes.append(bbox_individual)
            keypoints.append(keypoints_individual)
            anns_to_merge.append(annotation)
        else:
            unique_keypoints = keypoints_individual

    if unique_keypoints is None:
        unique_keypoints = -1 * np.ones((num_unique_bodyparts, 3), dtype=float)

    keypoints = safe_stack(keypoints, (0, num_joints, 3))
    original_bboxes = safe_stack(original_bboxes, (0, 4))
    bboxes = _compute_crop_bounds(original_bboxes, image_shape, remove_empty=False)

    # at least 1 visible joint to keep individuals
    vis_mask = (keypoints[..., 2] > 0).any(axis=1)
    keypoints = keypoints[vis_mask]
    bboxes = bboxes[vis_mask]

    keys_to_merge = ["area", "category_id", "iscrowd", "individual_id"]
    anns_merged = {k: [] for k in keys_to_merge}
    if len(anns_to_merge) > 0:
        anns_merged = merge_list_of_dicts(anns_to_merge, keys_to_include=keys_to_merge)
    anns_merged = {k: np.array(v)[vis_mask] for k, v in anns_merged.items()}

    if len(anns_merged["area"]) != len(keypoints):
        raise ValueError(f"Missing area values! {anns_merged}, {keypoints.shape}")

    return keypoints, unique_keypoints, bboxes, anns_merged


def calc_area_from_keypoints(keypoints: np.ndarray) -> np.ndarray:
    """
    Calculate the area from keypoints

    TODO: in the pups benchmark, there are 5 keypoints perfectly aligned so
     the area is 0.
     How do we deal with that?
     Makes more sense to compute the area from the bboxes (they are padded)
     Below is a temporary fix, which sets a min height and width to 5
     Suggestion: compute min height/width using labeled data

    Args:
        keypoints (np.ndarray): array of keypoints

    Returns:
        np.ndarray: array containing the computed areas based on the keypoints
    """
    w = np.maximum(keypoints[:, :, 0].max(axis=1) - keypoints[:, :, 0].min(axis=1), 1)
    h = np.maximum(keypoints[:, :, 1].max(axis=1) - keypoints[:, :, 1].min(axis=1), 1)
    return w * h


def _annotation_to_keypoints(annotation: dict, h: int, w: int) -> np.array:
    """
    Convert the coco annotations into array of keypoints returns the array of the
    keypoints' visibility. If keypoint is not visible, the value for (x,y) coordinates
    is set to 0. If the keypoints are outside of the image, they are also set to 0.

    Args:
        annotation: dictionary containing coco-like annotations with essential
            `keypoints` field
        h: the image height
        w: the image width

    Returns:
        keypoints: np.array where the first two columns are x and y coordinates of the
    
    """
    # we don't mess up visibility flags here
    return annotation["keypoints"].reshape(-1, 3)


def apply_transform(
    transform: A.BaseCompose,
    image: np.ndarray,
    keypoints: np.ndarray,
    bboxes: np.ndarray,
    class_labels: list[str],
) -> dict[str, np.ndarray]:
    """
    Applies a transformation to the provided image and keypoints.

    Args:
        transform: The transformation to apply.
        image: The input image to which the transformation will be applied.
        keypoints: List of keypoints to be transformed along with the image. Each keypoint
            is expected to be a tuple or list with at least three values,
            where the third value indicates the class label index.
        bboxes: List of bounding boxes to be transformed along with the image.
        class_labels: List of class labels corresponding to the keypoints.

    Returns:
        transformed: A dictionary containing the transformed image and keypoints.
    """

    if transform:
        oob_mask = out_of_bounds_keypoints(keypoints, image.shape)
        transformed = _apply_transform(
            transform, image, keypoints, bboxes, class_labels
        )

        transformed["keypoints"] = np.array(transformed["keypoints"])

        # out-of-bound keypoints have visibility flag 0. But we don't touch coordinates
        if np.sum(oob_mask) > 0:
            transformed["keypoints"][oob_mask, 2] = 0.0

        out_shape = transformed["image"].shape
        if len(transformed["keypoints"]) > 0:
            oob_mask = out_of_bounds_keypoints(transformed["keypoints"], out_shape)
            # out-of-bound keypoints have visibility flag 0. Don't touch coordinates
            if np.sum(oob_mask) > 0:
                transformed["keypoints"][oob_mask, 2] = 0.0

        # TODO: Check that the transformed bboxes are still within the image
        if len(transformed["bboxes"]) > 0:
            transformed["bboxes"] = np.array(transformed["bboxes"])
        else:
            transformed["bboxes"] = np.zeros(shape=(0, 4))

    else:
        transformed = {"keypoints": keypoints, "image": image}

    # do we ever need to do this if we had check_keypoints_within_bounds above?
    # np.nan_to_num(transformed["keypoints"], copy=False, nan=-1)
    return transformed


def _apply_transform(
    transform: A.BaseCompose,
    image: np.ndarray,
    keypoints: np.ndarray,
    bboxes: np.ndarray,
    class_labels: list[str],
) -> dict[str, np.ndarray]:
    """
    Applies a transformation to the provided image and keypoints.

    Args:
        image : np.array or similar image data format
            The input image to which the transformation will be applied.

        keypoints : list or similar data format
            List of keypoints to be transformed along with the image. Each keypoint
            is expected to be a tuple or list with at least three values,
            where the third value indicates the class label index.

    Returns:
        dict
            A dictionary containing the transformed image and keypoints.
    """
    transformed = transform(
        image=image,
        keypoints=keypoints,
        class_labels=class_labels,
        bboxes=bboxes,
        bbox_labels=np.arange(len(bboxes)),
    )

    bboxes_out = np.zeros(bboxes.shape)
    for bbox, bbox_id in zip(transformed["bboxes"], transformed["bbox_labels"]):
        bboxes_out[bbox_id] = bbox

    transformed["bboxes"] = bboxes_out
    return transformed


def out_of_bounds_keypoints(keypoints: np.ndarray, shape: tuple) -> np.ndarray:
    """Computes which visible keypoints are outside an image

    Args:
        keypoints: A (N, 3) shaped array where N is the number of keypoints and each
            keypoint is represented as (x, y, visibility).
        shape: A tuple representing the shape or bounds as (height, width).

    Returns:
        A boolean array of shape (N,) where each element corresponds to whether
        the respective keypoint is visible (visibility > 0) and outside the image
        bounds. This mask can be used to set the visibility bit to 0 for keypoints that
        were kicked off an image due to augmentation.
    """
    return (keypoints[..., 2] > 0) & (
        np.isnan(keypoints[..., 0])
        | np.isnan(keypoints[..., 1])
        | (keypoints[..., 0] < 0)
        | (keypoints[..., 0] > shape[1])
        | (keypoints[..., 1] < 0)
        | (keypoints[..., 1] > shape[0])
    )


def pad_to_length(data: np.array, length: int, value: float) -> np.array:
    """
    Pads the first dimension of an array with a given value

    Args:
        data: the array to pad, of shape (l, ...), where l <= length
        length: the desired length of the tensor
        value: the value to pad with

    Returns:
        the padded array of shape (length, ...)
    """
    pad_length = length - len(data)
    if pad_length == 0:
        return data
    elif pad_length > 0:
        padding = value * np.ones((pad_length, *data.shape[1:]), dtype=data.dtype)
        return np.concatenate([data, padding])

    raise ValueError(f"Cannot pad! data.shape={data.shape} > length={length}")


def safe_stack(data: list[np.ndarray], default_shape: tuple[int, ...]) -> np.ndarray:
    """
    Stacks a list of arrays if there are any, otherwise returns an array of zeros
    of a desired shape.

    Args:
        data: the list of arrays to stack
        default_shape: the shape of the array to return if the list is empty

    Returns:
        the stacked data or empty array
    """
    if len(data) == 0:
        return np.zeros(default_shape, dtype=float)

    return np.stack(data, axis=0)


--- File: deeplabcut/pose_estimation_pytorch/data/helper.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from abc import ABCMeta


def cfg_getter(key, default=None):
    def _getter(cfg):
        return cfg.get(key, default)

    return _getter


def class_property(func, arg_func):
    """
    Decorator to create a class property.

    Parameters:
    - func: Callable that represents the logic of the property.
    - arg_func: Callable that provides the arguments for `func`.

    Returns:
    - A property with the logic encapsulated in `func` and arguments derived from `arg_func`.
    """

    def decorator_wrapper(method):
        def wrapper(self):
            return func(arg_func(self))

        return property(wrapper)

    return decorator_wrapper


class PropertyMeta(type):
    """
    Metaclass for creating class properties in a more organized and systematic manner.

    This metaclass allows a class to define its properties using a simple dictionary
    structure (`properties`). The dictionary keys represent the property names,
    while the values are tuples containing two callables:
    1. The function that represents the logic of the property.
    2. The function that provides the arguments for the logic function.

    Usage:
    class MyClass(metaclass=PropertyMeta):
        properties = {
            'property_name': (logic_function, arguments_function),
            # ... more properties ...
        }

    For each property specified in the `properties` dictionary, the metaclass will
    generate a real property that uses the logic from `logic_function` and
    arguments from `arguments_function`.

    Attributes:
    - properties (dict): Dictionary containing property names as keys and tuples
      of (logic_function, arguments_function) as values.
    """

    def __new__(cls, name, bases, attrs):
        if "properties" not in attrs:
            raise AttributeError(f"{name} must define a 'properties' dictionary.")
        properties = attrs.get("properties", {})
        for prop_name, (func, arg_func) in properties.items():
            attrs[prop_name] = class_property(func, arg_func)(lambda self: None)
        return super().__new__(cls, name, bases, attrs)


class CombinedPropertyMeta(ABCMeta, PropertyMeta):
    """
    Combined metaclass that integrates the functionalities of both `ABCMeta` and `BasePropertyMeta`.

    This metaclass is useful in scenarios where a class needs to use both abstract methods (from `ABCMeta`)
    and the property definition utilities provided by `BasePropertyMeta`.

    By using this metaclass, a class can be both an abstract class (with abstract methods and/or properties)
    and can also define properties in the structured manner facilitated by `PropertyMeta`.

    Inherits:
    - ABCMeta: Metaclass for base classes that include abstract methods.
    - PropertyMeta: Metaclass that facilitates structured property definitions.

    Note:
    When defining a class using `CombinedPropertyMeta`, ensure that the class also inherits
    from `ABC` to make it compatible with the `ABCMeta` behavior.
    """


--- File: deeplabcut/pose_estimation_pytorch/data/image.py ---
from __future__ import annotations

import copy

import cv2
import numpy as np
import torch
import torchvision.transforms.functional as F

from deeplabcut.pose_estimation_pytorch.data.utils import _compute_crop_bounds


def resize_and_random_crop(
    image: np.ndarray,
    targets: dict,
    size: int | tuple[int, int],
    max_size: int | None = None,
    max_shift: int | None = None,
) -> tuple[torch.tensor, dict]:
    """Resizes images while preserving their aspect ratio

    If size is an integer: resizes to square images.
        First, resizes the image so that it's short side is equal to `size`. If this
        makes its long side greater than `max_size`, resizes the long side to `max_size`
        and the short side to the corresponding value to preserve the aspect ratio.

        Then, the image is cropped to a size-by-size square with a random crop.

    If size is a tuple, resize images to (w=size[1], h=size[0])
        First, rescales the image while preserving the aspect ratio such that both its
        width and height are greater or equal to the target width/height for the image
        (where either the width/height is the target width/height). If this makes its
        long side greater than `max_size`, resizes the long side to `max_size`.

        Then, the image is cropped to (w=size[1], h=size[0]) with a random crop.

    Args:
        image: an image of shape (C, H, W)
        targets: the dictionary containing targets
        size: the size of the output image (it will be square)
        max_size: if defined, the maximum size of any side of the output image
        max_shift: the maximum shift for the crop after resizing

    Returns: image, targets
        the resized image as a PyTorch tensor
        the updated targets in the resized image
    """

    def get_resize_hw(
        original_size: tuple[int, int], tgt_short_side: int, max_long_side: int | None
    ) -> tuple[int, int]:
        short_side, long_side = min(*original_size), max(*original_size)
        tgt_long_side = int((tgt_short_side / short_side) * long_side)

        # if the image's long side will be too big, make the image smaller
        if max_long_side is not None and tgt_long_side > max_long_side:
            tgt_long_side = max_long_side
            tgt_short_side = int((tgt_long_side / long_side) * short_side)

        # height is the short side
        if original_size[0] < original_size[1]:
            return tgt_short_side, tgt_long_side

        # width is the short side
        return tgt_long_side, tgt_short_side

    def get_resize_preserve_ratio(
        oh: int, ow: int, tgt_h: int, tgt_w: int, max_long_side: int | None
    ) -> tuple[int, int]:
        w_scale = ow / tgt_w
        h_scale = oh / tgt_h
        if h_scale <= w_scale:
            h = tgt_h
            w = int(ow * (tgt_h / oh))
        else:
            h = int(oh * (tgt_w / ow))
            w = tgt_w

        # if the image's long side will be too big, make the image smaller
        long_side = max(h, w)
        if max_long_side is not None and long_side > max_long_side:
            if h <= w:
                w = max_long_side
                h = int(oh * (max_long_side / ow))
            else:
                w = int(ow * (max_long_side / oh))
                h = max_long_side

        return h, w

    oh, ow = image.shape[1:]
    if isinstance(size, int):
        h, w = get_resize_hw((oh, ow), tgt_short_side=size, max_long_side=max_size)
        tgt_h, tgt_w = size, size
    else:
        h, w = get_resize_preserve_ratio(
            oh, ow, size[0], size[1], max_long_side=max_size
            )
        tgt_h, tgt_w = size

    scale_x, scale_y = ow / w, oh / h
    scaled_image = F.resize(torch.tensor(image), [h, w])

    # shift the image
    if max_shift is None:
        max_shift = 0
    extra_x, extra_y = max(0, w - tgt_w), max(0, h - tgt_h)
    offset_x = np.random.randint(
        max(-tgt_w // 2, -max(0, tgt_w - w) - max_shift),
        min(max_shift + extra_x, extra_x + (min(w, tgt_w) // 2)),
    )
    offset_y = np.random.randint(
        max(-tgt_h // 2, -max(0, tgt_h - h) - max_shift),
        min(max_shift + extra_y, extra_y + (min(h, tgt_h) // 2)),
    )

    # 0-pads, then crops if image size is smaller than output size along any edge
    scaled_cropped_image = F.crop(scaled_image, offset_y, offset_x, tgt_h, tgt_w)

    # update targets
    targets = copy.deepcopy(targets)

    # update scales and offsets
    sx, sy = targets["scales"]
    ox, oy = targets["offsets"]
    targets["offsets"] = ox + (offset_x * sx), oy + (offset_y * sy)
    targets["scales"] = sx * scale_x, sy * scale_y

    # update annotations
    anns = targets.get("annotations", {})

    kpt_scale = np.array([scale_x, scale_y])
    kpt_offset = np.array([offset_x, offset_y])
    for kpt_key in ["keypoints", "keypoints_unique"]:
        keypoints = anns.get(kpt_key)
        if keypoints is not None and len(keypoints) > 0:
            scaled_kpts = keypoints.copy()
            scaled_kpts[..., :2] = (scaled_kpts[..., :2] / kpt_scale) - kpt_offset
            scaled_kpts[(scaled_kpts[..., 0] >= tgt_w)] = -1
            scaled_kpts[(scaled_kpts[..., 1] >= tgt_h)] = -1
            scaled_kpts[(scaled_kpts[..., :2] < 0).any(axis=-1)] = -1
            anns[kpt_key] = scaled_kpts

    bbox_scale = np.array([scale_x, scale_y, scale_x, scale_y])
    bbox_offset = np.array([offset_x, offset_y, 0, 0])
    for bbox_key in ["boxes"]:
        boxes = anns.get(bbox_key)
        if boxes is not None and len(boxes) > 0:
            scaled_boxes = (boxes / bbox_scale) - bbox_offset
            scaled_boxes = _compute_crop_bounds(
                scaled_boxes, (tgt_h, tgt_w, 3), remove_empty=False,
            )
            anns[bbox_key] = scaled_boxes

    area = anns.get("area")
    if area is not None:
        if "boxes" in anns:  # recompute areas from the new bounding boxes
            widths = np.maximum(anns["boxes"][..., 2], 1)
            heights = np.maximum(anns["boxes"][..., 3], 1)
            anns["area"] = widths * heights
        else:  # just rescale
            scaled_area = area * (scale_x * scale_y)
            anns["area"] = scaled_area

    return scaled_cropped_image, targets


def top_down_crop(
    image: np.ndarray,
    bbox: np.ndarray,
    output_size: tuple[int, int],
    margin: int = 0,
    center_padding: bool = False,
) -> tuple[np.array, tuple[int, int], tuple[float, float]]:
    """
    Crops images around bounding boxes for top-down pose estimation. Computes offsets so
    that coordinates in the original image can be mapped to the cropped one;

        x_cropped = (x - offset_x) / scale_x
        x_cropped = (y - offset_y) / scale_y

    Bounding boxes are expected to be in COCO-format (xywh).

    Args:
        image: (h, w, c) the image to crop
        bbox: (4,) the bounding box to crop around
        output_size: the (width, height) of the output cropped image
        margin: a margin to add around the bounding box before cropping
        center_padding: whether to center the image in the padding if any is needed

    Returns:
        cropped_image, (offset_x, offset_y), (scale_x, scale_y)
    """
    image_h, image_w, c = image.shape
    out_w, out_h = output_size
    x, y, w, h = bbox

    cx = x + w / 2
    cy = y + h / 2
    w += 2 * margin
    h += 2 * margin

    input_ratio = w / h
    output_ratio = out_w / out_h
    if input_ratio > output_ratio:  # h/w < h0/w0 => h' = w * h0/w0
        h = w / output_ratio
    elif input_ratio < output_ratio:  # w/h < w0/h0 => w' = h * w0/h0
        w = h * output_ratio

    # cx,cy,w,h will now give the right ratio -> check if padding is needed
    x1, y1 = int(round(cx - (w / 2))), int(round(cy - (h / 2)))
    x2, y2 = int(round(cx + (w / 2))), int(round(cy + (h / 2)))

    # pad symmetrically - compute total padding across axis
    pad_left, pad_right, pad_top, pad_bottom = 0, 0, 0, 0
    if x1 < 0:
        pad_left = -x1
        x1 = 0
    if x2 > image_w:
        pad_right = x2 - image_w
        x2 = image_w
    if y1 < 0:
        pad_top = -y1
        y1 = 0
    if y2 > image_h:
        pad_bottom = y2 - image_h
        y2 = image_h

    w, h = x2 - x1, y2 - y1
    pad_x = pad_left + pad_right
    pad_y = pad_top + pad_bottom
    if center_padding:
        pad_left = pad_x // 2
        pad_top = pad_y // 2

    # crop the pixels we care about
    image_crop = np.zeros((h + pad_y, w + pad_x, c), dtype=image.dtype)
    image_crop[pad_top:pad_top + h, pad_left:pad_left + w] = image[y1:y2, x1:x2]

    # resize the cropped image
    image = cv2.resize(image_crop, (out_w, out_h), interpolation=cv2.INTER_LINEAR)

    # compute scale and offset
    offset = x1 - pad_left, y1 - pad_top
    scale = (w + pad_x) / out_w, (h + pad_y) / out_h
    return image, offset, scale


--- File: deeplabcut/pose_estimation_pytorch/data/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

from abc import ABC, abstractmethod
from pathlib import Path

import albumentations as A
import numpy as np

import deeplabcut.core.config as config_utils
import deeplabcut.pose_estimation_pytorch.config as config
from deeplabcut.pose_estimation_pytorch.data.dataset import (
    PoseDataset,
    PoseDatasetParameters,
)
from deeplabcut.pose_estimation_pytorch.data.utils import (
    _compute_crop_bounds,
    bbox_from_keypoints,
    map_id_to_annotations,
)
from deeplabcut.pose_estimation_pytorch.task import Task


class Loader(ABC):
    """
    Abstract class that represents a blueprint for loading and processing dataset information.

    Methods:
        load_data(mode: str = 'train') -> dict:
            Abstract method to convert the project configuration to a standard COCO format.
        create_dataset(images: dict = None, annotations: dict = None, transform: object = None,
            mode: str = "train", task: Task = Task.BOTTOM_UP) -> PoseDataset:
            Creates and returns a PoseDataset given a set of images, annotations, and other parameters.
        _compute_bboxes(images, annotations, method: str = 'gt') -> dict:
            Retrieves all bounding boxes based on the specified method.
        get_dataset_parameters(*args, **kwargs) -> dict:
            Returns a dictionary containing dataset parameters derived from the configuration.
    """

    def __init__(self, model_config_path: str | Path) -> None:
        self.model_config_path = Path(model_config_path)
        self.model_cfg = config_utils.read_config_as_dict(str(model_config_path))
        self.pose_task = Task(self.model_cfg["method"])
        self._loaded_data: dict[str, dict[str, list[dict]]] = {}

    @property
    def model_folder(self) -> Path:
        """Returns: The path of the folder containing the model data"""
        return self.model_config_path.parent

    def update_model_cfg(self, updates: dict) -> None:
        """Updates the model configuration

        Args:
            updates: the items to update in the model configuration
        """
        self.model_cfg = config.update_config_by_dotpath(self.model_cfg, updates)
        config_utils.write_config(self.model_config_path, self.model_cfg)

    @abstractmethod
    def load_data(self, mode: str = "train") -> dict[str, list[dict]]:
        """Abstract method to convert the project configuration to a standard coco format.

        Raises:
            NotImplementedError: This method must be implemented in the derived classes.
        """
        raise NotImplementedError

    def image_filenames(self, mode: str = "train") -> list[str]:
        """
        Args:
            mode: {"train", "test"} whether to load train or test data

        Returns:
            the image paths for this mode
        """
        if mode not in self._loaded_data:
            self._loaded_data[mode] = self.load_data(mode)

        data = self._loaded_data[mode]
        return [image["file_name"] for image in data["images"]]

    def ground_truth_keypoints(
        self, mode: str = "train", unique_bodypart: bool = False
    ) -> dict[str, np.ndarray]:
        """
        Creates a dictionary containing the ground truth data

        TODO: make more efficient

        Args:
            mode: {"train", "test"} whether to load train or test data
            unique_bodypart: returns the ground truth for unique bodyparts

        Raises:
            ValueError if unique_bodypart=True but there are no unique bodyparts

        Returns:
            A dict mapping image paths to the ground truth annotations for the mode in
            the format:
                {'image': keypoints with shape (num_individuals, num_keypoints, 2)}
        """
        parameters = self.get_dataset_parameters()
        if unique_bodypart:
            if not parameters.num_unique_bpts > 0:
                raise ValueError("There are no unique bodyparts in this dataset!")
            individuals = ["single"]
            num_bodyparts = parameters.num_unique_bpts
        else:
            individuals = parameters.individuals
            num_bodyparts = parameters.num_joints

        if "weight_init" in self.model_cfg["train_settings"]:
            weight_init_cfg = self.model_cfg["train_settings"]["weight_init"]
            if weight_init_cfg["memory_replay"]:
                conversion_array = weight_init_cfg["conversion_array"]
                num_bodyparts = len(conversion_array)

        if mode not in self._loaded_data:
            self._loaded_data[mode] = self.load_data(mode)
        data = self._loaded_data[mode]

        annotations = self.filter_annotations(data["annotations"], task=Task.BOTTOM_UP)
        img_to_ann_map = map_id_to_annotations(annotations)

        ground_truth_dict = {}
        for image in data["images"]:
            image_path = image["file_name"]
            individual_keypoints = {
                annotations[i]["individual"]: annotations[i]["keypoints"]
                for i in img_to_ann_map[image["id"]]
            }
            gt_array = np.zeros((len(individuals), num_bodyparts, 3))
            # Keep the shape of the ground truth
            for idv_idx, idv in enumerate(individuals):
                if idv in individual_keypoints:
                    keypoints = individual_keypoints[idv].reshape(num_bodyparts, -1)
                    gt_array[idv_idx, :, :] = keypoints[:, :3]

            ground_truth_dict[image_path] = gt_array

        return ground_truth_dict

    def ground_truth_bboxes(self, mode: str = "train") -> dict[str, dict]:
        """Creates a dictionary containing the ground truth bounding boxes

        Args:
            mode: {"train", "test"} whether to load train or test data

        Returns:
            A dict mapping image paths to the ground truth annotations for the mode in
            the format:
                {
                    'path/to/image000.png': {
                        "width": (int) the width of the image, in pixels
                        "height": (int) the height of the image, in pixels
                        "bboxes": (np.ndarray) bboxes with shape (num_individuals, xywh)
                    },
                    'path/to/image000.png': {...},
                }
        """
        if mode not in self._loaded_data:
            self._loaded_data[mode] = self.load_data(mode)
        data = self._loaded_data[mode]

        annotations = self.filter_annotations(data["annotations"], task=Task.DETECT)
        img_to_ann_map = map_id_to_annotations(annotations)

        ground_truth_dict = {}
        for image in data["images"]:
            image_path = image["file_name"]
            img_shape = image["height"], image["width"], 3
            bboxes = [annotations[i]["bbox"] for i in img_to_ann_map[image["id"]]]
            if len(bboxes) == 0:
                bboxes = np.zeros((0, 4))
            else:
                bboxes = _compute_crop_bounds(np.stack(bboxes, axis=0), img_shape)

            ground_truth_dict[image_path] = dict(
                width=image["width"],
                height=image["height"],
                bboxes=bboxes,
            )

        return ground_truth_dict

    def create_dataset(
        self,
        transform: A.BaseCompose | None = None,
        mode: str = "train",
        task: Task = Task.BOTTOM_UP,
    ) -> PoseDataset:
        """
        Creates a PoseDataset based on provided arguments.

        Args:
            transform: Transformation to be applied on dataset. Defaults to None.
            mode: Mode in which dataset is to be used (e.g., 'train', 'test'). Defaults to 'train'.
            task: Task for which the dataset is being used. Defaults to 'BU'.

        Returns:
            PoseDataset: An instance of the PoseDataset class.

        Raises:
            Any exception raised by `get_dataset_parameters` or `load_data` methods.
        """
        parameters = self.get_dataset_parameters()
        data = self.load_data(mode)
        data["annotations"] = self.filter_annotations(data["annotations"], task)
        dataset = PoseDataset(
            images=data["images"],
            annotations=data["annotations"],
            transform=transform,
            mode=mode,
            task=task,
            parameters=parameters,
        )
        return dataset

    @abstractmethod
    def get_dataset_parameters(self) -> PoseDatasetParameters:
        """
        Retrieves dataset parameters based on the instance's configuration.

        Returns:
            An instance of the PoseDatasetParameters with the parameters set.
        """
        raise NotImplementedError

    @staticmethod
    def filter_annotations(annotations: list[dict], task: Task) -> list[dict]:
        """Filters annotations based on the task, removing empty annotations

        For pose estimation tasks, annotations with empty keypoints are removed. For
        detection task, annotations with no bounding boxes are removed

        Args:
            annotations: the annotations to filter
            task: the task for which to filter

        Returns:
            list: the filtered annotations
        """
        filtered_annotations = []
        for annotation in annotations:
            keypoints = annotation["keypoints"].reshape(-1, 3)
            if task in (Task.DETECT, Task.TOP_DOWN) and (
                annotation["bbox"][2] <= 0 or annotation["bbox"][3] <= 0
            ):
                continue
            elif task != Task.DETECT and np.all(keypoints[:, :2] <= 0):
                continue

            filtered_annotations.append(annotation)

        return filtered_annotations

    @staticmethod
    def _compute_bboxes(
        images: list[dict],
        annotations: list[dict],
        method: str = "gt",
        bbox_margin: int = 20,
    ):
        """TODO: Nastya method of bbox computation (detection bbox, seg. mask, ...)
        Retrieves all bounding boxes based on the given method.

        Args:
            images: A list of images.
            annotations: A list of annotations corresponding to images.
            method (str, optional): Method to use for retrieving bounding boxes. Defaults to 'gt'.
                - 'gt': Ground truth bounding boxes.
                - 'detection bbox': Bounding boxes from detection.
                - 'keypoints': Bounding boxes from keypoints.
                - 'segmentation mask': Bounding boxes from segmentation masks.
            bbox_margin: Margin to add around keypoints when generating bounding boxes.

        Returns:
            list: Updated annotations based on the given method.

        Raises:
            ValueError: If 'bbox' is not found in annotation when method is 'gt'.
            ValueError: If method is not one of 'gt', 'detection bbox', 'keypoints', or 'segmentation mask'.
        """

        if not method:
            return annotations

        elif method == "gt":
            for i, annotation in enumerate(annotations):
                if "bbox" not in annotation:
                    # or do something else?
                    raise ValueError(
                        f"Bounding box not found in annotation {annotation}, please "
                        "chose another bbox computation method"
                    )
            return annotations

        elif method == "detection bbox":
            raise NotImplementedError

        elif method == "keypoints":
            min_area = 1  # TODO: should not be hardcoded
            img_id_to_annotations = map_id_to_annotations(annotations)
            for img in images:
                anns = [annotations[idx] for idx in img_id_to_annotations[img["id"]]]
                for a in anns:
                    a["bbox"] = bbox_from_keypoints(
                        keypoints=a["keypoints"],
                        image_h=img["height"],
                        image_w=img["width"],
                        margin=bbox_margin,
                    )
                    a["area"] = max(min_area, (a["bbox"][2] * a["bbox"][3]).item())
            return annotations

        elif method == "segmentation mask":
            raise NotImplementedError

        else:
            raise ValueError(f"Unknown method: {method}")


--- File: deeplabcut/pose_estimation_tensorflow/config.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#

import logging
import pprint

import yaml


def _merge_a_into_b(a, b):
    """
    Merge config dictionary a into config dictionary b, clobbering the
    options in b whenever they are also specified in a.
    """
    for k, v in a.items():
        # a must specify keys that are in b
        # if k not in b:
        #    raise KeyError('{} is not a valid config key'.format(k))

        # recursively merge dicts
        if isinstance(v, dict):
            if not b.get(k, False):
                b[k] = v
            else:
                try:
                    _merge_a_into_b(a[k], b[k])
                except:
                    print("Error under config key: {}".format(k))
                    raise
        else:
            b[k] = v


def cfg_from_file(filename):
    """
    Load a config from file filename and merge it into the default options.
    """
    with open(filename, "r") as f:
        yaml_cfg = yaml.load(f, Loader=yaml.SafeLoader)

    # Update the snapshot path to the corresponding path!
    trainpath = str(filename).split("pose_cfg.yaml")[0]
    yaml_cfg["snapshot_prefix"] = trainpath + "snapshot"
    # the default is: "./snapshot"

    # reloading defaults, as they can bleed over from a previous run otherwise
    import importlib
    from . import default_config

    importlib.reload(default_config)

    default_cfg = default_config.cfg
    _merge_a_into_b(yaml_cfg, default_cfg)

    logging.info("Config:\n" + pprint.pformat(default_cfg))
    return default_cfg  # updated


def load_config(filename="pose_cfg.yaml"):
    return cfg_from_file(filename)


if __name__ == "__main__":
    print(load_config())


--- File: deeplabcut/pose_estimation_tensorflow/predict_multianimal.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import os
import pickle
import shelve
import time
from pathlib import Path

import numpy as np
from skimage.color import rgba2rgb
from skimage.util import img_as_ubyte
from tqdm import tqdm

from deeplabcut.pose_estimation_tensorflow.core import predict_multianimal as predict
from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal
from deeplabcut.utils.auxfun_videos import VideoWriter
import pickle


def extract_bpt_feature_from_video(
    video,
    DLCscorer,
    trainFraction,
    cfg,
    dlc_cfg,
    sess,
    inputs,
    outputs,
    extra_dict,
    destfolder=None,
    robust_nframes=False,
):
    print("Starting to analyze % ", video)
    vname = Path(video).stem
    videofolder = str(Path(video).parents[0])
    if destfolder is None:
        destfolder = videofolder
    auxiliaryfunctions.attempt_to_make_folder(destfolder)
    dataname = os.path.join(destfolder, vname + DLCscorer + ".h5")

    assemble_filename = dataname.split(".h5")[0] + "_assemblies.pickle"

    feature_dict = shelve.open(
        dataname.split(".h5")[0] + "_bpt_features.pickle",
        protocol=pickle.DEFAULT_PROTOCOL,
    )

    with open(assemble_filename, "rb") as f:
        assemblies = pickle.load(f)
        print("Loading ", video)
        vid = VideoWriter(video)
        if robust_nframes:
            nframes = vid.get_n_frames(robust=True)
            duration = vid.calc_duration(robust=True)
            fps = nframes / duration
        else:
            nframes = len(vid)
            duration = vid.calc_duration(robust=False)
            fps = vid.fps

        nx, ny = vid.dimensions
        print(
            "Duration of video [s]: ",
            round(duration, 2),
            ", recorded with ",
            round(fps, 2),
            "fps!",
        )
        print(
            "Overall # of frames: ",
            nframes,
            " found with (before cropping) frame dimensions: ",
            nx,
            ny,
        )
        start = time.time()

        print("Starting to extract posture")
        if int(dlc_cfg["batch_size"]) > 1:
            # for multi animal, seems only this is used
            PredicteData, nframes = GetPoseandCostsF_from_assemblies(
                cfg,
                dlc_cfg,
                sess,
                inputs,
                outputs,
                vid,
                nframes,
                int(dlc_cfg["batch_size"]),
                assemblies,
                feature_dict,
                extra_dict,
            )
        else:
            raise NotImplementedError(
                "Not implemented yet, please raise an GitHub issue if you need this."
            )


def AnalyzeMultiAnimalVideo(
    video,
    DLCscorer,
    trainFraction,
    cfg,
    dlc_cfg,
    sess,
    inputs,
    outputs,
    destfolder=None,
    robust_nframes=False,
    use_shelve=False,
):
    """Helper function for analyzing a video with multiple individuals"""

    print("Starting to analyze % ", video)
    vname = Path(video).stem
    videofolder = str(Path(video).parents[0])
    if destfolder is None:
        destfolder = videofolder
    auxiliaryfunctions.attempt_to_make_folder(destfolder)
    dataname = os.path.join(destfolder, vname + DLCscorer + ".h5")

    if os.path.isfile(dataname.split(".h5")[0] + "_full.pickle"):
        print("Video already analyzed!", dataname)
    else:
        print("Loading ", video)
        vid = VideoWriter(video)
        if robust_nframes:
            nframes = vid.get_n_frames(robust=True)
            duration = vid.calc_duration(robust=True)
            fps = nframes / duration
        else:
            nframes = len(vid)
            duration = vid.calc_duration(robust=False)
            fps = vid.fps

        nx, ny = vid.dimensions
        print(
            "Duration of video [s]: ",
            round(duration, 2),
            ", recorded with ",
            round(fps, 2),
            "fps!",
        )
        print(
            "Overall # of frames: ",
            nframes,
            " found with (before cropping) frame dimensions: ",
            nx,
            ny,
        )
        start = time.time()

        print(
            "Starting to extract posture from the video(s) with batchsize:",
            dlc_cfg["batch_size"],
        )
        if use_shelve:
            shelf_path = dataname.split(".h5")[0] + "_full.pickle"
        else:
            shelf_path = ""
        if int(dlc_cfg["batch_size"]) > 1:
            PredicteData, nframes = GetPoseandCostsF(
                cfg,
                dlc_cfg,
                sess,
                inputs,
                outputs,
                vid,
                nframes,
                int(dlc_cfg["batch_size"]),
                shelf_path,
            )
        else:
            PredicteData, nframes = GetPoseandCostsS(
                cfg,
                dlc_cfg,
                sess,
                inputs,
                outputs,
                vid,
                nframes,
                shelf_path,
            )

        stop = time.time()

        if cfg["cropping"] == True:
            coords = [cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"]]
        else:
            coords = [0, nx, 0, ny]

        dictionary = {
            "start": start,
            "stop": stop,
            "run_duration": stop - start,
            "Scorer": DLCscorer,
            "DLC-model-config file": dlc_cfg,
            "fps": fps,
            "batch_size": dlc_cfg["batch_size"],
            "frame_dimensions": (ny, nx),
            "nframes": nframes,
            "iteration (active-learning)": cfg["iteration"],
            "training set fraction": trainFraction,
            "cropping": cfg["cropping"],
            "cropping_parameters": coords,
        }
        metadata = {"data": dictionary}
        print("Video Analyzed. Saving results in %s..." % (destfolder))

        if use_shelve:
            metadata_path = dataname.split(".h5")[0] + "_meta.pickle"
            with open(metadata_path, "wb") as f:
                pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)
        else:
            _ = auxfun_multianimal.SaveFullMultiAnimalData(
                PredicteData, metadata, dataname
            )


def _get_features_dict(raw_coords, features, stride):
    from deeplabcut.pose_tracking_pytorch import (
        load_features_from_coord,
        convert_coord_from_img_space_to_feature_space,
    )

    coords_img_space = np.array(
        [coord[:, :2] for coord in raw_coords]
    )  # only first two columns are useful

    coords_feature_space = convert_coord_from_img_space_to_feature_space(
        coords_img_space,
        stride,
    )

    bpt_features = load_features_from_coord(
        features.astype(np.float16), coords_feature_space
    )
    return {"features": bpt_features, "coordinates": coords_img_space}


def GetPoseandCostsF_from_assemblies(
    cfg,
    dlc_cfg,
    sess,
    inputs,
    outputs,
    cap,
    nframes,
    batchsize,
    assemblies,
    feature_dict,
    extra_dict,
):
    """Batchwise prediction of pose"""
    strwidth = int(np.ceil(np.log10(nframes)))  # width for strings
    batch_ind = 0  # keeps track of which image within a batch should be written to
    batch_num = 0  # keeps track of which batch you are at
    if cfg["cropping"]:
        cap.set_bbox(cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"])
    nx, ny = cap.dimensions

    frames = np.empty(
        (batchsize, ny, nx, 3), dtype="ubyte"
    )  # this keeps all frames in a batch
    pbar = tqdm(total=nframes)
    counter = 0
    inds = []

    PredicteData = {}

    while cap.video.isOpened():
        frame = cap.read_frame(crop=cfg["cropping"])
        key = "frame" + str(counter).zfill(strwidth)
        if frame is not None:
            # Avoid overwriting data already on the shelf
            if key in feature_dict:
                continue

            frame = img_as_ubyte(frame)
            if frame.shape[-1] == 4:
                frame = rgba2rgb(frame)
            frames[batch_ind] = frame
            inds.append(counter)

            if batch_ind == batchsize - 1:
                preds = predict.predict_batched_peaks_and_costs(
                    dlc_cfg, frames, sess, inputs, outputs, extra_dict=extra_dict
                )
                if not preds:
                    continue

                D, features = preds
                for i, (ind, data) in enumerate(zip(inds, D)):
                    PredicteData["frame" + str(ind).zfill(strwidth)] = data
                    raw_coords = assemblies.get(ind)
                    if raw_coords is None:
                        continue
                    fname = "frame" + str(ind).zfill(strwidth)
                    feature_dict[fname] = _get_features_dict(
                        raw_coords,
                        features[i],
                        dlc_cfg["stride"],
                    )

                batch_ind = 0
                inds.clear()
                batch_num += 1
            else:
                batch_ind += 1
        elif counter >= nframes:
            if batch_ind > 0:
                preds = predict.predict_batched_peaks_and_costs(
                    dlc_cfg, frames, sess, inputs, outputs, extra_dict=extra_dict
                )
                if not preds:
                    continue

                D, features = preds
                for i, (ind, data) in enumerate(zip(inds, D)):
                    PredicteData["frame" + str(ind).zfill(strwidth)] = data
                    raw_coords = assemblies.get(ind)
                    if raw_coords is None:
                        continue
                    fname = "frame" + str(ind).zfill(strwidth)
                    feature_dict[fname] = _get_features_dict(
                        raw_coords,
                        features[i],
                        dlc_cfg["stride"],
                    )

            break
        counter += 1
        pbar.update(1)

    cap.close()
    pbar.close()
    feature_dict.close()
    PredicteData["metadata"] = {
        "nms radius": dlc_cfg["nmsradius"],
        "minimal confidence": dlc_cfg["minconfidence"],
        "sigma": dlc_cfg.get("sigma", 1),
        "PAFgraph": dlc_cfg["partaffinityfield_graph"],
        "PAFinds": dlc_cfg.get(
            "paf_best", np.arange(len(dlc_cfg["partaffinityfield_graph"]))
        ),
        "all_joints": [[i] for i in range(len(dlc_cfg["all_joints"]))],
        "all_joints_names": [
            dlc_cfg["all_joints_names"][i] for i in range(len(dlc_cfg["all_joints"]))
        ],
        "nframes": nframes,
    }
    return PredicteData, nframes


def GetPoseandCostsF(
    cfg,
    dlc_cfg,
    sess,
    inputs,
    outputs,
    cap,
    nframes,
    batchsize,
    shelf_path,
):
    """Batchwise prediction of pose"""
    strwidth = int(np.ceil(np.log10(nframes)))  # width for strings
    batch_ind = 0  # keeps track of which image within a batch should be written to
    batch_num = 0  # keeps track of which batch you are at
    if cfg["cropping"]:
        cap.set_bbox(cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"])
    nx, ny = cap.dimensions

    frames = np.empty(
        (batchsize, ny, nx, 3), dtype="ubyte"
    )  # this keeps all frames in a batch
    pbar = tqdm(total=nframes)
    counter = 0
    inds = []

    if shelf_path:
        db = shelve.open(
            shelf_path,
            protocol=pickle.DEFAULT_PROTOCOL,
        )
    else:
        db = dict()
    db["metadata"] = {
        "nms radius": dlc_cfg["nmsradius"],
        "minimal confidence": dlc_cfg["minconfidence"],
        "sigma": dlc_cfg.get("sigma", 1),
        "PAFgraph": dlc_cfg["partaffinityfield_graph"],
        "PAFinds": dlc_cfg.get(
            "paf_best", np.arange(len(dlc_cfg["partaffinityfield_graph"]))
        ),
        "all_joints": [[i] for i in range(len(dlc_cfg["all_joints"]))],
        "all_joints_names": [
            dlc_cfg["all_joints_names"][i] for i in range(len(dlc_cfg["all_joints"]))
        ],
        "nframes": nframes,
    }
    while cap.video.isOpened():
        frame = cap.read_frame(crop=cfg["cropping"])
        key = "frame" + str(counter).zfill(strwidth)
        if frame is not None:
            # Avoid overwriting data already on the shelf
            if isinstance(db, shelve.Shelf) and key in db:
                continue
            frame = img_as_ubyte(frame)
            if frame.shape[-1] == 4:
                frame = rgba2rgb(frame)
            frames[batch_ind] = frame
            inds.append(counter)
            if batch_ind == batchsize - 1:
                D = predict.predict_batched_peaks_and_costs(
                    dlc_cfg,
                    frames,
                    sess,
                    inputs,
                    outputs,
                )
                for ind, data in zip(inds, D):
                    db["frame" + str(ind).zfill(strwidth)] = data
                del D
                batch_ind = 0
                inds.clear()
                batch_num += 1
            else:
                batch_ind += 1
        elif counter >= nframes:
            if batch_ind > 0:
                D = predict.predict_batched_peaks_and_costs(
                    dlc_cfg,
                    frames,
                    sess,
                    inputs,
                    outputs,
                )
                for ind, data in zip(inds, D):
                    db["frame" + str(ind).zfill(strwidth)] = data
                del D
            break
        counter += 1
        pbar.update(1)

    cap.close()
    pbar.close()
    try:
        db.close()
    except AttributeError:
        pass
    return db, nframes


def GetPoseandCostsS(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, shelf_path):
    """Non batch wise pose estimation for video cap."""
    strwidth = int(np.ceil(np.log10(nframes)))  # width for strings
    if cfg["cropping"]:
        cap.set_bbox(cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"])

    if shelf_path:
        db = shelve.open(
            shelf_path,
            protocol=pickle.DEFAULT_PROTOCOL,
        )
    else:
        db = dict()
    db["metadata"] = {
        "nms radius": dlc_cfg["nmsradius"],
        "minimal confidence": dlc_cfg["minconfidence"],
        "sigma": dlc_cfg.get("sigma", 1),
        "PAFgraph": dlc_cfg["partaffinityfield_graph"],
        "PAFinds": dlc_cfg.get(
            "paf_best", np.arange(len(dlc_cfg["partaffinityfield_graph"]))
        ),
        "all_joints": [[i] for i in range(len(dlc_cfg["all_joints"]))],
        "all_joints_names": [
            dlc_cfg["all_joints_names"][i] for i in range(len(dlc_cfg["all_joints"]))
        ],
        "nframes": nframes,
    }
    pbar = tqdm(total=nframes)
    counter = 0
    while cap.video.isOpened():
        frame = cap.read_frame(crop=cfg["cropping"])
        key = "frame" + str(counter).zfill(strwidth)
        if frame is not None:
            # Avoid overwriting data already on the shelf
            if isinstance(db, shelve.Shelf) and key in db:
                continue
            frame = img_as_ubyte(frame)
            if frame.shape[-1] == 4:
                frame = rgba2rgb(frame)
            dets = predict.predict_batched_peaks_and_costs(
                dlc_cfg,
                np.expand_dims(frame, axis=0),
                sess,
                inputs,
                outputs,
            )
            if not dets:
                continue
            db[key] = dets[0]
            del dets
        elif counter >= nframes:
            break
        counter += 1
        pbar.update(1)

    pbar.close()
    try:
        db.close()
    except AttributeError:
        pass
    return db, nframes


--- File: deeplabcut/pose_estimation_tensorflow/LICENSE ---
                   GNU LESSER GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.


  This version of the GNU Lesser General Public License incorporates
the terms and conditions of version 3 of the GNU General Public
License, supplemented by the additional permissions listed below.

  0. Additional Definitions.

  As used herein, "this License" refers to version 3 of the GNU Lesser
General Public License, and the "GNU GPL" refers to version 3 of the GNU
General Public License.

  "The Library" refers to a covered work governed by this License,
other than an Application or a Combined Work as defined below.

  An "Application" is any work that makes use of an interface provided
by the Library, but which is not otherwise based on the Library.
Defining a subclass of a class defined by the Library is deemed a mode
of using an interface provided by the Library.

  A "Combined Work" is a work produced by combining or linking an
Application with the Library.  The particular version of the Library
with which the Combined Work was made is also called the "Linked
Version".

  The "Minimal Corresponding Source" for a Combined Work means the
Corresponding Source for the Combined Work, excluding any source code
for portions of the Combined Work that, considered in isolation, are
based on the Application, and not on the Linked Version.

  The "Corresponding Application Code" for a Combined Work means the
object code and/or source code for the Application, including any data
and utility programs needed for reproducing the Combined Work from the
Application, but excluding the System Libraries of the Combined Work.

  1. Exception to Section 3 of the GNU GPL.

  You may convey a covered work under sections 3 and 4 of this License
without being bound by section 3 of the GNU GPL.

  2. Conveying Modified Versions.

  If you modify a copy of the Library, and, in your modifications, a
facility refers to a function or data to be supplied by an Application
that uses the facility (other than as an argument passed when the
facility is invoked), then you may convey a copy of the modified
version:

   a) under this License, provided that you make a good faith effort to
   ensure that, in the event an Application does not supply the
   function or data, the facility still operates, and performs
   whatever part of its purpose remains meaningful, or

   b) under the GNU GPL, with none of the additional permissions of
   this License applicable to that copy.

  3. Object Code Incorporating Material from Library Header Files.

  The object code form of an Application may incorporate material from
a header file that is part of the Library.  You may convey such object
code under terms of your choice, provided that, if the incorporated
material is not limited to numerical parameters, data structure
layouts and accessors, or small macros, inline functions and templates
(ten or fewer lines in length), you do both of the following:

   a) Give prominent notice with each copy of the object code that the
   Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the object code with a copy of the GNU GPL and this license
   document.

  4. Combined Works.

  You may convey a Combined Work under terms of your choice that,
taken together, effectively do not restrict modification of the
portions of the Library contained in the Combined Work and reverse
engineering for debugging such modifications, if you also do each of
the following:

   a) Give prominent notice with each copy of the Combined Work that
   the Library is used in it and that the Library and its use are
   covered by this License.

   b) Accompany the Combined Work with a copy of the GNU GPL and this license
   document.

   c) For a Combined Work that displays copyright notices during
   execution, include the copyright notice for the Library among
   these notices, as well as a reference directing the user to the
   copies of the GNU GPL and this license document.

   d) Do one of the following:

       0) Convey the Minimal Corresponding Source under the terms of this
       License, and the Corresponding Application Code in a form
       suitable for, and under terms that permit, the user to
       recombine or relink the Application with a modified version of
       the Linked Version to produce a modified Combined Work, in the
       manner specified by section 6 of the GNU GPL for conveying
       Corresponding Source.

       1) Use a suitable shared library mechanism for linking with the
       Library.  A suitable mechanism is one that (a) uses at run time
       a copy of the Library already present on the user's computer
       system, and (b) will operate properly with a modified version
       of the Library that is interface-compatible with the Linked
       Version.

   e) Provide Installation Information, but only if you would otherwise
   be required to provide such information under section 6 of the
   GNU GPL, and only to the extent that such information is
   necessary to install and execute a modified version of the
   Combined Work produced by recombining or relinking the
   Application with a modified version of the Linked Version. (If
   you use option 4d0, the Installation Information must accompany
   the Minimal Corresponding Source and Corresponding Application
   Code. If you use option 4d1, you must provide the Installation
   Information in the manner specified by section 6 of the GNU GPL
   for conveying Corresponding Source.)

  5. Combined Libraries.

  You may place library facilities that are a work based on the
Library side by side in a single library together with other library
facilities that are not Applications and are not covered by this
License, and convey such a combined library under terms of your
choice, if you do both of the following:

   a) Accompany the combined library with a copy of the same work based
   on the Library, uncombined with any other library facilities,
   conveyed under the terms of this License.

   b) Give prominent notice with the combined library that part of it
   is a work based on the Library, and explaining where to find the
   accompanying uncombined form of the same work.

  6. Revised Versions of the GNU Lesser General Public License.

  The Free Software Foundation may publish revised and/or new versions
of the GNU Lesser General Public License from time to time. Such new
versions will be similar in spirit to the present version, but may
differ in detail to address new problems or concerns.

  Each version is given a distinguishing version number. If the
Library as you received it specifies that a certain numbered version
of the GNU Lesser General Public License "or any later version"
applies to it, you have the option of following the terms and
conditions either of that published version or of any later version
published by the Free Software Foundation. If the Library as you
received it does not specify a version number of the GNU Lesser
General Public License, you may choose any version of the GNU Lesser
General Public License ever published by the Free Software Foundation.

  If the Library as you received it specifies that a proxy can decide
whether future versions of the GNU Lesser General Public License shall
apply, that proxy's public statement of acceptance of any version is
permanent authorization for you to choose that version for the
Library.



--- File: deeplabcut/pose_estimation_tensorflow/predict_videos.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


####################################################
# Dependencies
####################################################

import argparse
import os
import os.path
import pickle
import re
import time
import warnings
from pathlib import Path

import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
from scipy.optimize import linear_sum_assignment
from skimage.util import img_as_ubyte
from tqdm import tqdm

from deeplabcut.core import trackingutils, inferenceutils
from deeplabcut.pose_estimation_tensorflow.config import load_config
from deeplabcut.pose_estimation_tensorflow.core import predict

from deeplabcut.refine_training_dataset.stitch import stitch_tracklets
from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal, auxfun_models
from deeplabcut.pose_estimation_tensorflow.core.openvino.session import (
    GetPoseF_OV,
    is_openvino_available,
)


####################################################
# Loading data, and defining model folder
####################################################


def create_tracking_dataset(
    config,
    videos,
    track_method,
    videotype="",
    shuffle=1,
    trainingsetindex=0,
    gputouse=None,
    save_as_csv=False,
    destfolder=None,
    batchsize=None,
    cropping=None,
    TFGPUinference=True,
    dynamic=(False, 0.5, 10),
    modelprefix="",
    robust_nframes=False,
    n_triplets=1000,
):
    try:
        from deeplabcut.pose_tracking_pytorch import create_triplets_dataset
    except ModuleNotFoundError:
        raise ModuleNotFoundError(
            "Unsupervised identity learning requires PyTorch. Please run `pip install torch`."
        )

    from deeplabcut.pose_estimation_tensorflow.predict_multianimal import (
        extract_bpt_feature_from_video,
    )

    # allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in
    allow_growth = True

    if "TF_CUDNN_USE_AUTOTUNE" in os.environ:
        del os.environ["TF_CUDNN_USE_AUTOTUNE"]  # was potentially set during training

    if gputouse is not None:  # gpu selection
        auxfun_models.set_visible_devices(gputouse)

    tf.compat.v1.reset_default_graph()
    start_path = os.getcwd()  # record cwd to return to this directory in the end

    cfg = auxiliaryfunctions.read_config(config)
    trainFraction = cfg["TrainingFraction"][trainingsetindex]

    if cropping is not None:
        cfg["cropping"] = True
        cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"] = cropping
        print("Overwriting cropping parameters:", cropping)
        print("These are used for all videos, but won't be save to the cfg file.")

    modelfolder = os.path.join(
        cfg["project_path"],
        str(
            auxiliaryfunctions.get_model_folder(
                trainFraction, shuffle, cfg, modelprefix=modelprefix
            )
        ),
    )
    path_test_config = Path(modelfolder) / "test" / "pose_cfg.yaml"
    try:
        dlc_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError(
            "It seems the model for shuffle %s and trainFraction %s does not exist."
            % (shuffle, trainFraction)
        )

    Snapshots = auxiliaryfunctions.get_snapshots_from_folder(
        train_folder=Path(modelfolder) / "train",
    )

    if cfg["snapshotindex"] == "all":
        print(
            "Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!"
        )
        snapshotindex = -1
    else:
        snapshotindex = cfg["snapshotindex"]

    print("Using %s" % Snapshots[snapshotindex], "for model", modelfolder)

    ##################################################
    # Load and setup CNN part detector
    ##################################################

    # Check if data already was generated:
    dlc_cfg["init_weights"] = os.path.join(
        modelfolder, "train", Snapshots[snapshotindex]
    )
    trainingsiterations = (dlc_cfg["init_weights"].split(os.sep)[-1]).split("-")[-1]
    # Update number of output and batchsize
    dlc_cfg["num_outputs"] = cfg.get("num_outputs", dlc_cfg.get("num_outputs", 1))

    if batchsize is None:
        # update batchsize (based on parameters in config.yaml)
        dlc_cfg["batch_size"] = cfg["batch_size"]
    else:
        dlc_cfg["batch_size"] = batchsize
        cfg["batch_size"] = batchsize

    if "multi-animal" in dlc_cfg["dataset_type"]:
        dynamic = (False, 0.5, 10)  # setting dynamic mode to false
        TFGPUinference = False

    if dynamic[0]:  # state=true
        # (state,detectiontreshold,margin)=dynamic
        print("Starting analysis in dynamic cropping mode with parameters:", dynamic)
        dlc_cfg["num_outputs"] = 1
        TFGPUinference = False
        dlc_cfg["batch_size"] = 1
        print(
            "Switching batchsize to 1, num_outputs (per animal) to 1 and TFGPUinference to False (all these features are not supported in this mode)."
        )

    # Name for scorer:
    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
        cfg,
        shuffle,
        trainFraction,
        trainingsiterations=trainingsiterations,
        modelprefix=modelprefix,
    )
    if dlc_cfg["num_outputs"] > 1:
        if TFGPUinference:
            print(
                "Switching to numpy-based keypoint extraction code, as multiple point extraction is not supported by TF code currently."
            )
            TFGPUinference = False
        print("Extracting ", dlc_cfg["num_outputs"], "instances per bodypart")
        xyz_labs_orig = ["x", "y", "likelihood"]
        suffix = [str(s + 1) for s in range(dlc_cfg["num_outputs"])]
        suffix[0] = ""  # first one has empty suffix for backwards compatibility
        xyz_labs = [x + s for s in suffix for x in xyz_labs_orig]
    else:
        xyz_labs = ["x", "y", "likelihood"]

    if TFGPUinference:
        sess, inputs, outputs = predict.setup_GPUpose_prediction(
            dlc_cfg, allow_growth=allow_growth
        )
    else:
        sess, inputs, outputs, extra_dict = predict.setup_pose_prediction(
            dlc_cfg, allow_growth=allow_growth, collect_extra=True
        )

    pdindex = pd.MultiIndex.from_product(
        [[DLCscorer], dlc_cfg["all_joints_names"], xyz_labs],
        names=["scorer", "bodyparts", "coords"],
    )

    ##################################################
    # Looping over videos
    ##################################################
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if len(Videos) > 0:
        if "multi-animal" in dlc_cfg["dataset_type"]:
            for video in Videos:
                extract_bpt_feature_from_video(
                    video,
                    DLCscorer,
                    trainFraction,
                    cfg,
                    dlc_cfg,
                    sess,
                    inputs,
                    outputs,
                    extra_dict,
                    destfolder=destfolder,
                    robust_nframes=robust_nframes,
                )

            # should close tensorflow session here in order to free gpu
            sess.close()
            tf.keras.backend.clear_session()
            create_triplets_dataset(
                Videos,
                DLCscorer,
                track_method,
                n_triplets=n_triplets,
                destfolder=destfolder,
            )

        else:
            raise NotImplementedError("not implemented")

        os.chdir(str(start_path))
        if "multi-animal" in dlc_cfg["dataset_type"]:
            print(
                "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames."
            )
        else:
            print(
                "The videos are analyzed. Now your research can truly start! \n You can create labeled videos with 'create_labeled_video'"
            )
            print(
                "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames."
            )
        return DLCscorer  # note: this is either DLCscorer or DLCscorerlegacy depending on what was used!
    else:
        print("No video(s) were found. Please check your paths and/or 'videotype'.")
        return DLCscorer


def analyze_videos(
    config,
    videos,
    videotype="",
    shuffle=1,
    trainingsetindex=0,
    gputouse=None,
    save_as_csv=False,
    in_random_order=True,
    destfolder=None,
    batchsize=None,
    cropping=None,
    TFGPUinference=True,
    dynamic=(False, 0.5, 10),
    modelprefix="",
    robust_nframes=False,
    allow_growth=False,
    use_shelve=False,
    auto_track=True,
    n_tracks=None,
    animal_names=None,
    calibrate=False,
    identity_only=False,
    use_openvino="CPU" if is_openvino_available else None,
):
    """Makes prediction based on a trained network.

    The index of the trained network is specified by parameters in the config file
    (in particular the variable 'snapshotindex').

    The labels are stored as MultiIndex Pandas Array, which contains the name of
    the network, body part name, (x, y) label position in pixels, and the
    likelihood for each frame per body part. These arrays are stored in an
    efficient Hierarchical Data Format (HDF) in the same directory where the video
    is stored. However, if the flag save_as_csv is set to True, the data can also
    be exported in comma-separated values format (.csv), which in turn can be
    imported in many programs, such as MATLAB, R, Prism, etc.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos: list[str]
        A list of strings containing the full paths to videos for analysis or a path to
        the directory, where all the videos with same extension are stored.

    videotype: str, optional, default=""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed. If left unspecified,
        videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional, default=1
        An integer specifying the shuffle index of the training dataset used for
        training the network.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        By default the first (note that TrainingFraction is a list in config.yaml).

    gputouse: int or None, optional, default=None
        Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a
        GPU put ``None``.
        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    save_as_csv: bool, optional, default=False
        Saves the predictions in a .csv file.

    in_random_order: bool, optional (default=True)
        Whether or not to analyze videos in a random order.
        This is only relevant when specifying a video directory in `videos`.

    destfolder: string or None, optional, default=None
        Specifies the destination folder for analysis data. If ``None``, the path of
        the video is used. Note that for subsequent analysis this folder also needs to
        be passed.

    batchsize: int or None, optional, default=None
        Change batch size for inference; if given overwrites value in ``pose_cfg.yaml``.

    cropping: list or None, optional, default=None
        List of cropping coordinates as [x1, x2, y1, y2].
        Note that the same cropping parameters will then be used for all videos.
        If different video crops are desired, run ``analyze_videos`` on individual
        videos with the corresponding cropping coordinates.

    TFGPUinference: bool, optional, default=True
        Perform inference on GPU with TensorFlow code. Introduced in "Pretraining
        boosts out-of-domain robustness for pose estimation" by Alexander Mathis,
        Mert Yüksekgönül, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis.
        Source: https://arxiv.org/abs/1909.11229

    dynamic: tuple(bool, float, int) triple containing (state, detectiontreshold, margin)
        If the state is true, then dynamic cropping will be performed. That means that if an object is detected (i.e. any body part > detectiontreshold),
        then object boundaries are computed according to the smallest/largest x position and smallest/largest y position of all body parts. This  window is
        expanded by the margin and from then on only the posture within this crop is analyzed (until the object is lost, i.e. <detectiontreshold). The
        current position is utilized for updating the crop window for the next frame (this is why the margin is important and should be set large
        enough given the movement of the animal).

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    robust_nframes: bool, optional, default=False
        Evaluate a video's number of frames in a robust manner.
        This option is slower (as the whole video is read frame-by-frame),
        but does not rely on metadata, hence its robustness against file corruption.

    allow_growth: bool, optional, default=False.
        For some smaller GPUs the memory issues happen. If ``True``, the memory
        allocator does not pre-allocate the entire specified GPU memory region, instead
        starting small and growing as needed.
        See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2

    use_shelve: bool, optional, default=False
        By default, data are dumped in a pickle file at the end of the video analysis.
        Otherwise, data are written to disk on the fly using a "shelf"; i.e., a
        pickle-based, persistent, database-like object by default, resulting in
        constant memory footprint.

    The following parameters are only relevant for multi-animal projects:

    auto_track: bool, optional, default=True
        By default, tracking and stitching are automatically performed, producing the
        final h5 data file. This is equivalent to the behavior for single-animal
        projects.

        If ``False``, one must run ``convert_detections2tracklets`` and
        ``stitch_tracklets`` afterwards, in order to obtain the h5 file.

    This function has 3 related sub-calls:

    identity_only: bool, optional, default=False
        If ``True`` and animal identity was learned by the model, assembly and tracking
        rely exclusively on identity prediction.

    calibrate: bool, optional, default=False
        If ``True``, use training data to calibrate the animal assembly procedure. This
        improves its robustness to wrong body part links, but requires very little
        missing data.

    n_tracks: int or None, optional, default=None
        Number of tracks to reconstruct. By default, taken as the number of individuals
        defined in the config.yaml. Another number can be passed if the number of
        animals in the video is different from the number of animals the model was
        trained on.

    animal_names: list[str], optional
        If you want the names given to individuals in the labeled data file, you can
        specify those names as a list here. If given and `n_tracks` is None, `n_tracks`
        will be set to `len(animal_names)`. If `n_tracks` is not None, then it must be
        equal to `len(animal_names)`. If it is not given, then `animal_names` will
        be loaded from the `individuals` in the project config.yaml file.

    use_openvino: str, optional
        Use "CPU" for inference if OpenVINO is available in the Python environment.

    Returns
    -------
    DLCScorer: str
        the scorer used to analyze the videos

    Examples
    --------

    Analyzing a single video on Windows

    >>> deeplabcut.analyze_videos(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi'],
        )

    Analyzing a single video on Linux/MacOS

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/reachingvideo1.avi'],
        )

    Analyze all videos of type ``avi`` in a folder

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos'],
            videotype='.avi',
        )

    Analyze multiple videos

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
        )

    Analyze multiple videos with ``shuffle=2``

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
            shuffle=2,
        )

    Analyze multiple videos with ``shuffle=2``, save results as an additional csv file

    >>> deeplabcut.analyze_videos(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
            shuffle=2,
            save_as_csv=True,
        )
    """
    if "TF_CUDNN_USE_AUTOTUNE" in os.environ:
        del os.environ["TF_CUDNN_USE_AUTOTUNE"]  # was potentially set during training

    if gputouse is not None:  # gpu selection
        auxfun_models.set_visible_devices(gputouse)

    tf.compat.v1.reset_default_graph()
    start_path = os.getcwd()  # record cwd to return to this directory in the end

    cfg = auxiliaryfunctions.read_config(config)
    trainFraction = cfg["TrainingFraction"][trainingsetindex]
    iteration = cfg["iteration"]

    if cropping is not None:
        cfg["cropping"] = True
        cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"] = cropping
        print("Overwriting cropping parameters:", cropping)
        print("These are used for all videos, but won't be save to the cfg file.")

    modelfolder = os.path.join(
        cfg["project_path"],
        str(
            auxiliaryfunctions.get_model_folder(
                trainFraction, shuffle, cfg, modelprefix=modelprefix
            )
        ),
    )
    path_test_config = Path(modelfolder) / "test" / "pose_cfg.yaml"
    try:
        dlc_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError(
            "It seems the model for iteration %s and shuffle %s and trainFraction %s does not exist."
            % (iteration, shuffle, trainFraction)
        )

    Snapshots = auxiliaryfunctions.get_snapshots_from_folder(
        train_folder=Path(modelfolder) / "train",
    )

    if cfg["snapshotindex"] == "all":
        print(
            "Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!"
        )
        snapshotindex = -1
    else:
        snapshotindex = cfg["snapshotindex"]

    print("Using %s" % Snapshots[snapshotindex], "for model", modelfolder)

    ##################################################
    # Load and setup CNN part detector
    ##################################################

    # Check if data already was generated:
    dlc_cfg["init_weights"] = os.path.join(
        modelfolder, "train", Snapshots[snapshotindex]
    )
    trainingsiterations = (dlc_cfg["init_weights"].split(os.sep)[-1]).split("-")[-1]
    # Update number of output and batchsize
    dlc_cfg["num_outputs"] = cfg.get("num_outputs", dlc_cfg.get("num_outputs", 1))

    if batchsize is None:
        # update batchsize (based on parameters in config.yaml)
        dlc_cfg["batch_size"] = cfg["batch_size"]
    else:
        dlc_cfg["batch_size"] = batchsize
        cfg["batch_size"] = batchsize

    if "multi-animal" in dlc_cfg["dataset_type"]:
        dynamic = (False, 0.5, 10)  # setting dynamic mode to false
        TFGPUinference = False

    if dynamic[0]:  # state=true
        # (state,detectiontreshold,margin)=dynamic
        print("Starting analysis in dynamic cropping mode with parameters:", dynamic)
        dlc_cfg["num_outputs"] = 1
        TFGPUinference = False
        dlc_cfg["batch_size"] = 1
        print(
            "Switching batchsize to 1, num_outputs (per animal) to 1 and TFGPUinference to False (all these features are not supported in this mode)."
        )

    # Name for scorer:
    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
        cfg,
        shuffle,
        trainFraction,
        trainingsiterations=trainingsiterations,
        modelprefix=modelprefix,
    )
    if dlc_cfg["num_outputs"] > 1:
        if TFGPUinference:
            print(
                "Switching to numpy-based keypoint extraction code, as multiple point extraction is not supported by TF code currently."
            )
            TFGPUinference = False
        print("Extracting ", dlc_cfg["num_outputs"], "instances per bodypart")
        xyz_labs_orig = ["x", "y", "likelihood"]
        suffix = [str(s + 1) for s in range(dlc_cfg["num_outputs"])]
        suffix[0] = ""  # first one has empty suffix for backwards compatibility
        xyz_labs = [x + s for s in suffix for x in xyz_labs_orig]
    else:
        xyz_labs = ["x", "y", "likelihood"]

    if use_openvino:
        sess, inputs, outputs = predict.setup_openvino_pose_prediction(
            dlc_cfg, device=use_openvino
        )
    elif TFGPUinference:
        sess, inputs, outputs = predict.setup_GPUpose_prediction(
            dlc_cfg, allow_growth=allow_growth
        )
    else:
        sess, inputs, outputs = predict.setup_pose_prediction(
            dlc_cfg, allow_growth=allow_growth
        )

    pdindex = pd.MultiIndex.from_product(
        [[DLCscorer], dlc_cfg["all_joints_names"], xyz_labs],
        names=["scorer", "bodyparts", "coords"],
    )

    ##################################################
    # Looping over videos
    ##################################################
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype, in_random_order)
    if len(Videos) > 0:
        if "multi-animal" in dlc_cfg["dataset_type"]:
            from deeplabcut.pose_estimation_tensorflow.predict_multianimal import (
                AnalyzeMultiAnimalVideo,
            )

            for video in Videos:
                AnalyzeMultiAnimalVideo(
                    video,
                    DLCscorer,
                    trainFraction,
                    cfg,
                    dlc_cfg,
                    sess,
                    inputs,
                    outputs,
                    destfolder,
                    robust_nframes=robust_nframes,
                    use_shelve=use_shelve,
                )
                if auto_track:  # tracker type is taken from default in cfg
                    convert_detections2tracklets(
                        config,
                        [video],
                        videotype,
                        shuffle,
                        trainingsetindex,
                        destfolder=destfolder,
                        modelprefix=modelprefix,
                        calibrate=calibrate,
                        identity_only=identity_only,
                    )
                    stitch_tracklets(
                        config,
                        [video],
                        videotype,
                        shuffle,
                        trainingsetindex,
                        destfolder=destfolder,
                        n_tracks=n_tracks,
                        animal_names=animal_names,
                        modelprefix=modelprefix,
                        save_as_csv=save_as_csv,
                    )
        else:
            for video in Videos:
                DLCscorer = AnalyzeVideo(
                    video,
                    DLCscorer,
                    DLCscorerlegacy,
                    trainFraction,
                    cfg,
                    dlc_cfg,
                    sess,
                    inputs,
                    outputs,
                    pdindex,
                    save_as_csv,
                    destfolder,
                    TFGPUinference,
                    dynamic,
                    use_openvino,
                )

        os.chdir(str(start_path))
        if "multi-animal" in dlc_cfg["dataset_type"]:
            print(
                "The videos are analyzed. Time to assemble animals and track 'em... \n Call 'create_video_with_all_detections' to check multi-animal detection quality before tracking."
            )
            print(
                "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames."
            )
        else:
            print(
                "The videos are analyzed. Now your research can truly start! \n You can create labeled videos with 'create_labeled_video'"
            )
            print(
                "If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract a few representative outlier frames."
            )
        return DLCscorer  # note: this is either DLCscorer or DLCscorerlegacy depending on what was used!
    else:
        print("No video(s) were found. Please check your paths and/or 'video_type'.")
        return DLCscorer


def checkcropping(cfg, cap):
    print(
        "Cropping based on the x1 = %s x2 = %s y1 = %s y2 = %s. You can adjust the cropping coordinates in the config.yaml file."
        % (cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"])
    )
    nx = cfg["x2"] - cfg["x1"]
    ny = cfg["y2"] - cfg["y1"]
    if nx > 0 and ny > 0:
        pass
    else:
        raise Exception("Please check the order of cropping parameter!")
    if (
        cfg["x1"] >= 0
        and cfg["x2"] < int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) + 1)
        and cfg["y1"] >= 0
        and cfg["y2"] < int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) + 1)
    ):
        pass  # good cropping box
    else:
        raise Exception("Please check the boundary of cropping!")
    return int(ny), int(nx)


def GetPoseF(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, batchsize):
    """Batchwise prediction of pose"""
    PredictedData = np.zeros(
        (nframes, dlc_cfg["num_outputs"] * 3 * len(dlc_cfg["all_joints_names"]))
    )
    batch_ind = 0  # keeps track of which image within a batch should be written to
    batch_num = 0  # keeps track of which batch you are at
    ny, nx = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(
        cap.get(cv2.CAP_PROP_FRAME_WIDTH)
    )
    if cfg["cropping"]:
        ny, nx = checkcropping(cfg, cap)

    frames = np.empty(
        (batchsize, ny, nx, 3), dtype="ubyte"
    )  # this keeps all frames in a batch
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))
    inds = []
    while cap.isOpened():
        if counter != 0 and counter % step == 0:
            pbar.update(step)
        ret, frame = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if cfg["cropping"]:
                frames[batch_ind] = img_as_ubyte(
                    frame[cfg["y1"] : cfg["y2"], cfg["x1"] : cfg["x2"]]
                )
            else:
                frames[batch_ind] = img_as_ubyte(frame)
            inds.append(counter)
            if batch_ind == batchsize - 1:
                pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs)
                PredictedData[inds] = pose
                batch_ind = 0
                inds.clear()
                batch_num += 1
            else:
                batch_ind += 1
        elif counter >= nframes:
            if batch_ind > 0:
                pose = predict.getposeNP(
                    frames, dlc_cfg, sess, inputs, outputs
                )  # process the whole batch (some frames might be from previous batch!)
                PredictedData[inds[:batch_ind]] = pose[:batch_ind]
            break
        counter += 1

    pbar.close()
    return PredictedData, nframes


def GetPoseS(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes):
    """Non batch wise pose estimation for video cap."""
    if cfg["cropping"]:
        ny, nx = checkcropping(cfg, cap)

    PredictedData = np.zeros(
        (nframes, dlc_cfg["num_outputs"] * 3 * len(dlc_cfg["all_joints_names"]))
    )
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))
    while cap.isOpened():
        if counter != 0 and counter % step == 0:
            pbar.update(step)

        ret, frame = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if cfg["cropping"]:
                frame = img_as_ubyte(
                    frame[cfg["y1"] : cfg["y2"], cfg["x1"] : cfg["x2"]]
                )
            else:
                frame = img_as_ubyte(frame)
            pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)
            PredictedData[counter, :] = (
                pose.flatten()
            )  # NOTE: thereby cfg['all_joints_names'] should be same order as bodyparts!
        elif counter >= nframes:
            break
        counter += 1

    pbar.close()
    return PredictedData, nframes


def GetPoseS_GTF(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes):
    """Non batch wise pose estimation for video cap."""
    if cfg["cropping"]:
        ny, nx = checkcropping(cfg, cap)

    pose_tensor = predict.extract_GPUprediction(
        outputs, dlc_cfg
    )  # extract_output_tensor(outputs, dlc_cfg)
    PredictedData = np.zeros((nframes, 3 * len(dlc_cfg["all_joints_names"])))
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))
    while cap.isOpened():
        if counter != 0 and counter % step == 0:
            pbar.update(step)

        ret, frame = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if cfg["cropping"]:
                frame = img_as_ubyte(
                    frame[cfg["y1"] : cfg["y2"], cfg["x1"] : cfg["x2"]]
                )
            else:
                frame = img_as_ubyte(frame)

            pose = sess.run(
                pose_tensor,
                feed_dict={inputs: np.expand_dims(frame, axis=0).astype(float)},
            )
            pose[:, [0, 1, 2]] = pose[:, [1, 0, 2]]
            # pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)
            PredictedData[counter, :] = (
                pose.flatten()
            )  # NOTE: thereby cfg['all_joints_names'] should be same order as bodyparts!
        elif counter >= nframes:
            break
        counter += 1

    pbar.close()
    return PredictedData, nframes


def GetPoseF_GTF(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, batchsize):
    """Batchwise prediction of pose"""
    PredictedData = np.zeros((nframes, 3 * len(dlc_cfg["all_joints_names"])))
    batch_ind = 0  # keeps track of which image within a batch should be written to
    batch_num = 0  # keeps track of which batch you are at
    ny = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    nx = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    if cfg["cropping"]:
        ny, nx = checkcropping(cfg, cap)

    # Flip x, y, confidence and reshape
    pose_tensor = predict.extract_GPUprediction(outputs, dlc_cfg)
    pose_tensor = tf.gather(pose_tensor, [1, 0, 2], axis=1)
    pose_tensor = tf.reshape(pose_tensor, (batchsize, -1))

    frames = np.empty((batchsize, ny, nx, 3), dtype="ubyte")
    pbar = tqdm(total=nframes)
    counter = -1
    inds = []
    while cap.isOpened() and counter < nframes - 1:
        ret, frame = cap.read()
        counter += 1
        if not ret:
            warnings.warn(f"Could not decode frame #{counter}.")
            continue

        if cfg["cropping"]:
            frame = img_as_ubyte(frame[cfg["y1"] : cfg["y2"], cfg["x1"] : cfg["x2"]])
        else:
            frame = img_as_ubyte(frame)
        frames[batch_ind] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        inds.append(counter)
        if batch_ind == batchsize - 1:
            pose = sess.run(pose_tensor, feed_dict={inputs: frames})
            PredictedData[inds] = pose
            batch_ind = 0
            batch_num += 1
            inds.clear()
            pbar.update(batchsize)
        else:
            batch_ind += 1

    if batch_ind > 0:
        pose = sess.run(pose_tensor, feed_dict={inputs: frames})
        PredictedData[inds[:batch_ind]] = pose[:batch_ind]
        pbar.update(batch_ind)

    pbar.close()
    return PredictedData, nframes


def getboundingbox(x, y, nx, ny, margin):
    x1 = max([0, int(np.amin(x)) - margin])
    x2 = min([nx, int(np.amax(x)) + margin])
    y1 = max([0, int(np.amin(y)) - margin])
    y2 = min([ny, int(np.amax(y)) + margin])
    return x1, x2, y1, y2


def GetPoseDynamic(
    cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, detectiontreshold, margin
):
    """Non batch wise pose estimation for video cap by dynamically cropping around previously detected parts."""
    if cfg["cropping"]:
        ny, nx = checkcropping(cfg, cap)
    else:
        ny, nx = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)), int(
            cap.get(cv2.CAP_PROP_FRAME_WIDTH)
        )
    x1, x2, y1, y2 = 0, nx, 0, ny
    detected = False
    # TODO: perform detection on resized image (For speed)

    PredictedData = np.zeros((nframes, 3 * len(dlc_cfg["all_joints_names"])))
    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))
    while cap.isOpened():
        if counter != 0 and counter % step == 0:
            pbar.update(step)

        ret, frame = cap.read()
        if ret:
            # print(counter,x1,x2,y1,y2,detected)
            originalframe = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            if cfg["cropping"]:
                frame = img_as_ubyte(
                    originalframe[cfg["y1"] : cfg["y2"], cfg["x1"] : cfg["x2"]]
                )[y1:y2, x1:x2]
            else:
                frame = img_as_ubyte(originalframe[y1:y2, x1:x2])

            pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs).flatten()
            detection = np.any(pose[2::3] > detectiontreshold)  # is anything detected?
            if detection:
                pose[0::3], pose[1::3] = (
                    pose[0::3] + x1,
                    pose[1::3] + y1,
                )  # offset according to last bounding box
                x1, x2, y1, y2 = getboundingbox(
                    pose[0::3], pose[1::3], nx, ny, margin
                )  # coordinates for next iteration
                if not detected:
                    detected = True  # object detected
            else:
                if (
                    detected and (x1 + y1 + y2 - ny + x2 - nx) != 0
                ):  # was detected in last frame and dyn. cropping was performed >> but object lost in cropped variant >> re-run on full frame!
                    # print("looking again, lost!")
                    if cfg["cropping"]:
                        frame = img_as_ubyte(
                            originalframe[cfg["y1"] : cfg["y2"], cfg["x1"] : cfg["x2"]]
                        )
                    else:
                        frame = img_as_ubyte(originalframe)
                    pose = predict.getpose(
                        frame, dlc_cfg, sess, inputs, outputs
                    ).flatten()  # no offset is necessary

                x0, y0 = x1, y1
                x1, x2, y1, y2 = 0, nx, 0, ny
                detected = False

            PredictedData[counter, :] = pose
        elif counter >= nframes:
            break
        counter += 1

    pbar.close()
    return PredictedData, nframes


def AnalyzeVideo(
    video,
    DLCscorer,
    DLCscorerlegacy,
    trainFraction,
    cfg,
    dlc_cfg,
    sess,
    inputs,
    outputs,
    pdindex,
    save_as_csv,
    destfolder=None,
    TFGPUinference=True,
    dynamic=(False, 0.5, 10),
    use_openvino="CPU" if is_openvino_available else None,
):
    """Helper function for analyzing a video."""
    print("Starting to analyze % ", video)

    if destfolder is None:
        destfolder = str(Path(video).parents[0])
    auxiliaryfunctions.attempt_to_make_folder(destfolder)
    vname = Path(video).stem
    try:
        _ = auxiliaryfunctions.load_analyzed_data(destfolder, vname, DLCscorer)
    except FileNotFoundError:
        print("Loading ", video)
        cap = cv2.VideoCapture(video)
        if not cap.isOpened():
            raise IOError(
                "Video could not be opened. Please check that the the file integrity."
            )
        # https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get
        fps = cap.get(cv2.CAP_PROP_FPS)
        nframes = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = nframes * 1.0 / fps
        size = (
            int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
            int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
        )
        ny, nx = size
        print(
            "Duration of video [s]: ",
            round(duration, 2),
            ", recorded with ",
            round(fps, 2),
            "fps!",
        )
        print(
            "Overall # of frames: ",
            nframes,
            " found with (before cropping) frame dimensions: ",
            nx,
            ny,
        )

        dynamic_analysis_state, detectiontreshold, margin = dynamic
        start = time.time()
        print("Starting to extract posture")
        if dynamic_analysis_state:
            PredictedData, nframes = GetPoseDynamic(
                cfg,
                dlc_cfg,
                sess,
                inputs,
                outputs,
                cap,
                nframes,
                detectiontreshold,
                margin,
            )
            # GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg["batch_size"]))
        else:
            if int(dlc_cfg["batch_size"]) > 1:
                args = (
                    cfg,
                    dlc_cfg,
                    sess,
                    inputs,
                    outputs,
                    cap,
                    nframes,
                    int(dlc_cfg["batch_size"]),
                )
                if use_openvino:
                    PredictedData, nframes = GetPoseF_OV(*args)
                elif TFGPUinference:
                    PredictedData, nframes = GetPoseF_GTF(*args)
                else:
                    PredictedData, nframes = GetPoseF(*args)
            else:
                if TFGPUinference:
                    PredictedData, nframes = GetPoseS_GTF(
                        cfg, dlc_cfg, sess, inputs, outputs, cap, nframes
                    )
                else:
                    PredictedData, nframes = GetPoseS(
                        cfg, dlc_cfg, sess, inputs, outputs, cap, nframes
                    )

        stop = time.time()
        if cfg["cropping"] == True:
            coords = [cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"]]
        else:
            coords = [0, nx, 0, ny]

        dictionary = {
            "start": start,
            "stop": stop,
            "run_duration": stop - start,
            "Scorer": DLCscorer,
            "DLC-model-config file": dlc_cfg,
            "fps": fps,
            "batch_size": dlc_cfg["batch_size"],
            "frame_dimensions": (ny, nx),
            "nframes": nframes,
            "iteration (active-learning)": cfg["iteration"],
            "training set fraction": trainFraction,
            "cropping": cfg["cropping"],
            "cropping_parameters": coords,
            # "gpu_info": device_lib.list_local_devices()
        }
        metadata = {"data": dictionary}

        print(f"Saving results in {destfolder}...")
        dataname = os.path.join(destfolder, vname + DLCscorer + ".h5")
        auxiliaryfunctions.save_data(
            PredictedData[:nframes, :],
            metadata,
            dataname,
            pdindex,
            range(nframes),
            save_as_csv,
        )
    finally:
        return DLCscorer


def GetPosesofFrames(
    cfg, dlc_cfg, sess, inputs, outputs, directory, framelist, nframes, batchsize
):
    """Batchwise prediction of pose for frame list in directory"""
    from deeplabcut.utils.auxfun_videos import imread

    print("Starting to extract posture")
    im = imread(os.path.join(directory, framelist[0]), mode="skimage")

    ny, nx, nc = np.shape(im)
    print(
        "Overall # of frames: ",
        nframes,
        " found with (before cropping) frame dimensions: ",
        nx,
        ny,
    )

    PredictedData = np.zeros(
        (nframes, dlc_cfg["num_outputs"] * 3 * len(dlc_cfg["all_joints_names"]))
    )
    batch_ind = 0  # keeps track of which image within a batch should be written to
    batch_num = 0  # keeps track of which batch you are at
    if cfg["cropping"]:
        print(
            "Cropping based on the x1 = %s x2 = %s y1 = %s y2 = %s. You can adjust the cropping coordinates in the config.yaml file."
            % (cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"])
        )
        nx, ny = cfg["x2"] - cfg["x1"], cfg["y2"] - cfg["y1"]
        if nx > 0 and ny > 0:
            pass
        else:
            raise Exception("Please check the order of cropping parameter!")
        if (
            cfg["x1"] >= 0
            and cfg["x2"] < int(np.shape(im)[1])
            and cfg["y1"] >= 0
            and cfg["y2"] < int(np.shape(im)[0])
        ):
            pass  # good cropping box
        else:
            raise Exception("Please check the boundary of cropping!")

    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))

    if batchsize == 1:
        for counter, framename in enumerate(framelist):
            im = imread(os.path.join(directory, framename), mode="skimage")

            if counter != 0 and counter % step == 0:
                pbar.update(step)

            if cfg["cropping"]:
                frame = img_as_ubyte(
                    im[cfg["y1"] : cfg["y2"], cfg["x1"] : cfg["x2"], :]
                )
            else:
                frame = img_as_ubyte(im)

            pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)
            PredictedData[counter, :] = pose.flatten()
    else:
        frames = np.empty(
            (batchsize, ny, nx, 3), dtype="ubyte"
        )  # this keeps all the frames of a batch
        for counter, framename in enumerate(framelist):
            im = imread(os.path.join(directory, framename), mode="skimage")

            if counter != 0 and counter % step == 0:
                pbar.update(step)

            if cfg["cropping"]:
                frames[batch_ind] = img_as_ubyte(
                    im[cfg["y1"] : cfg["y2"], cfg["x1"] : cfg["x2"], :]
                )
            else:
                frames[batch_ind] = img_as_ubyte(im)

            if batch_ind == batchsize - 1:
                pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs)
                PredictedData[
                    batch_num * batchsize : (batch_num + 1) * batchsize, :
                ] = pose
                batch_ind = 0
                batch_num += 1
            else:
                batch_ind += 1

        if (
            batch_ind > 0
        ):  # take care of the last frames (the batch that might have been processed)
            pose = predict.getposeNP(
                frames, dlc_cfg, sess, inputs, outputs
            )  # process the whole batch (some frames might be from previous batch!)
            PredictedData[
                batch_num * batchsize : batch_num * batchsize + batch_ind, :
            ] = pose[:batch_ind, :]

    pbar.close()
    return PredictedData, nframes, nx, ny


def analyze_time_lapse_frames(
    config,
    directory,
    frametype=".png",
    shuffle=1,
    trainingsetindex=0,
    gputouse=None,
    save_as_csv=False,
    modelprefix="",
):
    """
    Analyzed all images (of type = frametype) in a folder and stores the output in one file.

    You can crop the frames (before analysis), by changing 'cropping'=True and setting 'x1','x2','y1','y2' in the config file.

    Output: The labels are stored as MultiIndex Pandas Array, which contains the name of the network, body part name, (x, y) label position \n
            in pixels, and the likelihood for each frame per body part. These arrays are stored in an efficient Hierarchical Data Format (HDF) \n
            in the same directory, where the video is stored. However, if the flag save_as_csv is set to True, the data can also be exported in \n
            comma-separated values format (.csv), which in turn can be imported in many programs, such as MATLAB, R, Prism, etc.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    directory: string
        Full path to directory containing the frames that shall be analyzed

    frametype: string, optional
        Checks for the file extension of the frames. Only images with this extension are analyzed. The default is ``.png``

    shuffle: int, optional
        An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    gputouse: int, optional. Natural number indicating the number of your GPU (see number in nvidia-smi). If you do not have a GPU put None.
    See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    save_as_csv: bool, optional
        Saves the predictions in a .csv file. The default is ``False``; if provided it must be either ``True`` or ``False``

    Examples
    --------
    If you want to analyze all frames in /analysis/project/timelapseexperiment1
    >>> deeplabcut.analyze_videos('/analysis/project/reaching-task/config.yaml','/analysis/project/timelapseexperiment1')
    --------

    Note: for test purposes one can extract all frames from a video with ffmeg, e.g. ffmpeg -i testvideo.avi thumb%04d.png
    """
    if "TF_CUDNN_USE_AUTOTUNE" in os.environ:
        del os.environ["TF_CUDNN_USE_AUTOTUNE"]  # was potentially set during training

    if gputouse is not None:  # gpu selection
        auxfun_models.set_visible_devices(gputouse)

    tf.compat.v1.reset_default_graph()
    start_path = os.getcwd()  # record cwd to return to this directory in the end

    cfg = auxiliaryfunctions.read_config(config)
    trainFraction = cfg["TrainingFraction"][trainingsetindex]
    modelfolder = os.path.join(
        cfg["project_path"],
        str(
            auxiliaryfunctions.get_model_folder(
                trainFraction, shuffle, cfg, modelprefix=modelprefix
            )
        ),
    )
    path_test_config = Path(modelfolder) / "test" / "pose_cfg.yaml"
    try:
        dlc_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError(
            "It seems the model for shuffle %s and trainFraction %s does not exist."
            % (shuffle, trainFraction)
        )

    Snapshots = auxiliaryfunctions.get_snapshots_from_folder(
        train_folder=Path(modelfolder) / "train",
    )

    if cfg["snapshotindex"] == "all":
        print(
            "Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!"
        )
        snapshotindex = -1
    else:
        snapshotindex = cfg["snapshotindex"]

    print("Using %s" % Snapshots[snapshotindex], "for model", modelfolder)

    ##################################################
    # Load and setup CNN part detector
    ##################################################

    # Check if data already was generated:
    dlc_cfg["init_weights"] = os.path.join(
        modelfolder, "train", Snapshots[snapshotindex]
    )
    trainingsiterations = (dlc_cfg["init_weights"].split(os.sep)[-1]).split("-")[-1]

    # update batchsize (based on parameters in config.yaml)
    dlc_cfg["batch_size"] = cfg["batch_size"]

    # Name for scorer:
    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
        cfg,
        shuffle,
        trainFraction,
        trainingsiterations=trainingsiterations,
        modelprefix=modelprefix,
    )
    sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)

    # update number of outputs and adjust pandas indices
    dlc_cfg["num_outputs"] = cfg.get("num_outputs", 1)

    xyz_labs_orig = ["x", "y", "likelihood"]
    suffix = [str(s + 1) for s in range(dlc_cfg["num_outputs"])]
    suffix[0] = ""  # first one has empty suffix for backwards compatibility
    xyz_labs = [x + s for s in suffix for x in xyz_labs_orig]

    pdindex = pd.MultiIndex.from_product(
        [[DLCscorer], dlc_cfg["all_joints_names"], xyz_labs],
        names=["scorer", "bodyparts", "coords"],
    )

    if gputouse is not None:  # gpu selectinon
        auxfun_models.set_visible_devices(gputouse)

    ##################################################
    # Loading the images
    ##################################################
    # checks if input is a directory
    if os.path.isdir(directory) == True:
        """
        Analyzes all the frames in the directory.
        """
        print("Analyzing all frames in the directory: ", directory)
        os.chdir(directory)
        framelist = np.sort([fn for fn in os.listdir(os.curdir) if (frametype in fn)])
        vname = Path(directory).stem
        notanalyzed, dataname, DLCscorer = auxiliaryfunctions.check_if_not_analyzed(
            directory, vname, DLCscorer, DLCscorerlegacy, flag="framestack"
        )
        if notanalyzed:
            nframes = len(framelist)
            if nframes > 0:
                start = time.time()

                PredictedData, nframes, nx, ny = GetPosesofFrames(
                    cfg,
                    dlc_cfg,
                    sess,
                    inputs,
                    outputs,
                    directory,
                    framelist,
                    nframes,
                    dlc_cfg["batch_size"],
                )
                stop = time.time()

                if cfg["cropping"] == True:
                    coords = [cfg["x1"], cfg["x2"], cfg["y1"], cfg["y2"]]
                else:
                    coords = [0, nx, 0, ny]

                dictionary = {
                    "start": start,
                    "stop": stop,
                    "run_duration": stop - start,
                    "Scorer": DLCscorer,
                    "config file": dlc_cfg,
                    "batch_size": dlc_cfg["batch_size"],
                    "num_outputs": dlc_cfg["num_outputs"],
                    "frame_dimensions": (ny, nx),
                    "nframes": nframes,
                    "cropping": cfg["cropping"],
                    "cropping_parameters": coords,
                }
                metadata = {"data": dictionary}

                print("Saving results in %s..." % (directory))

                auxiliaryfunctions.save_data(
                    PredictedData[:nframes, :],
                    metadata,
                    dataname,
                    pdindex,
                    framelist,
                    save_as_csv,
                )
                print("The folder was analyzed. Now your research can truly start!")
                print(
                    "If the tracking is not satisfactory for some frame, consider expanding the training set."
                )
            else:
                print(
                    "No frames were found. Consider changing the path or the frametype."
                )

    os.chdir(str(start_path))


def _convert_detections_to_tracklets(
    cfg,
    inference_cfg,
    data,
    metadata,
    output_path,
    greedy=False,
    calibrate=False,
):
    track_method = cfg.get("default_track_method", "ellipse")
    if track_method not in trackingutils.TRACK_METHODS:
        raise ValueError(
            f"Invalid tracking method. Only {', '.join(trackingutils.TRACK_METHODS)} are currently supported."
        )

    joints = data["metadata"]["all_joints_names"]
    partaffinityfield_graph = data["metadata"]["PAFgraph"]
    paf_inds = data["metadata"]["PAFinds"]
    paf_graph = [partaffinityfield_graph[l] for l in paf_inds]
    if track_method == "box":
        mot_tracker = trackingutils.SORTBox(
            inference_cfg["max_age"],
            inference_cfg["min_hits"],
            inference_cfg.get("iou_threshold", 0.3),
        )
    elif track_method == "skeleton":
        mot_tracker = trackingutils.SORTSkeleton(
            len(joints),
            inference_cfg["max_age"],
            inference_cfg["min_hits"],
            inference_cfg.get("oks_threshold", 0.5),
        )
    else:
        mot_tracker = trackingutils.SORTEllipse(
            inference_cfg.get("max_age", 1),
            inference_cfg.get("min_hits", 1),
            inference_cfg.get("iou_threshold", 0.6),
        )
    tracklets = {}

    assembly_builder = inferenceutils.Assembler(
        data,
        max_n_individuals=inference_cfg["topktoretain"],
        n_multibodyparts=len(cfg["multianimalbodyparts"]),
        graph=paf_graph,
        paf_inds=list(paf_inds),
        greedy=greedy,
        pcutoff=inference_cfg.get("pcutoff", 0.1),
        min_affinity=inference_cfg.get("pafthreshold", 0.05),
        min_n_links=inference_cfg["minimalnumberofconnections"]
    )
    if calibrate:
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
        train_data_file = os.path.join(
            cfg["project_path"],
            str(trainingsetfolder),
            "CollectedData_" + cfg["scorer"] + ".h5",
        )
        assembly_builder.calibrate(train_data_file)
    assembly_builder.assemble()

    output_path, _ = os.path.splitext(output_path)
    output_path += ".pickle"
    assembly_builder.to_pickle(output_path.replace(".pickle", "_assemblies.pickle"))

    if cfg["uniquebodyparts"]:
        tracklets["single"] = {}
        tracklets["single"].update(assembly_builder.unique)

    for i, imname in tqdm(enumerate(assembly_builder.metadata["imnames"])):
        assemblies = assembly_builder.assemblies.get(i)
        if assemblies is None:
            continue
        animals = np.stack(
            [assembly_builder.data[:, :3] for assembly_builder in assemblies]
        )
        if track_method == "box":
            xy = trackingutils.calc_bboxes_from_keypoints(
                animals, inference_cfg.get("boundingboxslack", 0)
            )  # TODO: get cropping parameters and utilize!
        else:
            xy = animals[..., :2]
        trackers = mot_tracker.track(xy)
        trackingutils.fill_tracklets(tracklets, trackers, animals, imname)

    bodypartlabels = [joint for joint in joints for _ in range(3)]
    numentries = len(bodypartlabels)
    scorers = numentries * [metadata["data"]["Scorer"]]
    xylvalue = len(bodypartlabels) // 3 * ["x", "y", "likelihood"]
    pdindex = pd.MultiIndex.from_arrays(
        np.vstack([scorers, bodypartlabels, xylvalue]),
        names=["scorer", "bodyparts", "coords"],
    )
    tracklets["header"] = pdindex
    with open(output_path, "wb") as f:
        pickle.dump(tracklets, f, pickle.HIGHEST_PROTOCOL)


def convert_detections2tracklets(
    config,
    videos,
    videotype="",
    shuffle=1,
    trainingsetindex=0,
    overwrite=False,
    destfolder=None,
    ignore_bodyparts=None,
    inferencecfg=None,
    modelprefix="",
    greedy=False,
    calibrate=False,
    window_size=0,
    identity_only=False,
    track_method="",
):
    """
    This should be called at the end of deeplabcut.analyze_videos for multianimal projects!

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    videos : list
        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.\n Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional
        An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    overwrite: bool, optional.
        Overwrite tracks file i.e. recompute tracks from full detections and overwrite.

    destfolder: string, optional
        Specifies the destination folder for analysis data (default is the path of the video). Note that for subsequent analysis this
        folder also needs to be passed.

    ignore_bodyparts: optional
        List of body part names that should be ignored during tracking (advanced).
        By default, all the body parts are used.

    inferencecfg: Default is None.
        Configuration file for inference (assembly of individuals). Ideally
        should be obtained from cross validation (during evaluation). By default
        the parameters are loaded from inference_cfg.yaml, but these get_level_values
        can be overwritten.

    calibrate: bool, optional (default=False)
        If True, use training data to calibrate the animal assembly procedure.
        This improves its robustness to wrong body part links,
        but requires very little missing data.

    window_size: int, optional (default=0)
        Recurrent connections in the past `window_size` frames are
        prioritized during assembly. By default, no temporal coherence cost
        is added, and assembly is driven mainly by part affinity costs.

    identity_only: bool, optional (default=False)
        If True and animal identity was learned by the model,
        assembly and tracking rely exclusively on identity prediction.

    track_method: string, optional
         Specifies the tracker used to generate the pose estimation data.
         For multiple animals, must be either 'box', 'skeleton', or 'ellipse'
         and will be taken from the config.yaml file if none is given.


    Examples
    --------
    If you want to convert detections to tracklets:
    >>> deeplabcut.convert_detections2tracklets('/analysis/project/reaching-task/config.yaml',[]'/analysis/project/video1.mp4'], videotype='.mp4')

    If you want to convert detections to tracklets based on box_tracker:
    >>> deeplabcut.convert_detections2tracklets('/analysis/project/reaching-task/config.yaml',[]'/analysis/project/video1.mp4'], videotype='.mp4',track_method='box')

    --------

    """
    cfg = auxiliaryfunctions.read_config(config)
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)

    if len(cfg["multianimalbodyparts"]) == 1 and track_method != "box":
        warnings.warn("Switching to `box` tracker for single point tracking...")
        track_method = "box"
        cfg["default_track_method"] = track_method
        auxiliaryfunctions.write_config(config, cfg)

    trainFraction = cfg["TrainingFraction"][trainingsetindex]
    start_path = os.getcwd()  # record cwd to return to this directory in the end

    # TODO: add cropping as in video analysis!
    # if cropping is not None:
    #    cfg['cropping']=True
    #    cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping
    #    print("Overwriting cropping parameters:", cropping)
    #    print("These are used for all videos, but won't be save to the cfg file.")

    modelfolder = os.path.join(
        cfg["project_path"],
        str(
            auxiliaryfunctions.get_model_folder(
                trainFraction, shuffle, cfg, modelprefix=modelprefix
            )
        ),
    )
    path_test_config = Path(modelfolder) / "test" / "pose_cfg.yaml"
    try:
        dlc_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError(
            "It seems the model for shuffle %s and trainFraction %s does not exist."
            % (shuffle, trainFraction)
        )

    if "multi-animal" not in dlc_cfg["dataset_type"]:
        raise ValueError("This function is only required for multianimal projects!")

    path_inference_config = Path(modelfolder) / "test" / "inference_cfg.yaml"
    if inferencecfg is None:  # then load or initialize
        inferencecfg = auxfun_multianimal.read_inferencecfg(path_inference_config, cfg)
    else:
        auxfun_multianimal.check_inferencecfg_sanity(cfg, inferencecfg)

    if len(cfg["multianimalbodyparts"]) == 1 and track_method != "box":
        warnings.warn("Switching to `box` tracker for single point tracking...")
        track_method = "box"
        # Also ensure `boundingboxslack` is greater than zero, otherwise overlap
        # between trackers cannot be evaluated, resulting in empty tracklets.
        inferencecfg["boundingboxslack"] = max(inferencecfg["boundingboxslack"], 40)

    Snapshots = auxiliaryfunctions.get_snapshots_from_folder(
        train_folder=Path(modelfolder) / "train",
    )

    if cfg["snapshotindex"] == "all":
        print(
            "Snapshotindex is set to 'all' in the config.yaml file. Running video analysis with all snapshots is very costly! Use the function 'evaluate_network' to choose the best the snapshot. For now, changing snapshot index to -1!"
        )
        snapshotindex = -1
    else:
        snapshotindex = cfg["snapshotindex"]

    print("Using %s" % Snapshots[snapshotindex], "for model", modelfolder)
    dlc_cfg["init_weights"] = os.path.join(
        modelfolder, "train", Snapshots[snapshotindex]
    )
    trainingsiterations = (dlc_cfg["init_weights"].split(os.sep)[-1]).split("-")[-1]

    # Name for scorer:
    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
        cfg,
        shuffle,
        trainFraction,
        trainingsiterations=trainingsiterations,
        modelprefix=modelprefix,
    )

    ##################################################
    # Looping over videos
    ##################################################
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if len(Videos) > 0:
        for video in Videos:
            print("Processing... ", video)
            videofolder = str(Path(video).parents[0])
            if destfolder is None:
                destfolder = videofolder
            auxiliaryfunctions.attempt_to_make_folder(destfolder)
            vname = Path(video).stem
            dataname = os.path.join(destfolder, vname + DLCscorer + ".h5")
            data, metadata = auxfun_multianimal.LoadFullMultiAnimalData(dataname)
            if track_method == "ellipse":
                method = "el"
            elif track_method == "box":
                method = "bx"
            else:
                method = "sk"
            trackname = dataname.split(".h5")[0] + f"_{method}.pickle"
            # NOTE: If dataname line above is changed then line below is obsolete?
            # trackname = trackname.replace(videofolder, destfolder)
            if (
                os.path.isfile(trackname) and not overwrite
            ):  # TODO: check if metadata are identical (same parameters!)
                print("Tracklets already computed", trackname)
                print("Set overwrite = True to overwrite.")
            else:
                print("Analyzing", dataname)
                DLCscorer = metadata["data"]["Scorer"]
                all_jointnames = data["metadata"]["all_joints_names"]

                numjoints = len(all_jointnames)

                # TODO: adjust this for multi + unique bodyparts!
                # this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. )
                bodypartlabels = [
                    bpt for i, bpt in enumerate(all_jointnames) for _ in range(3)
                ]
                scorers = len(bodypartlabels) * [DLCscorer]
                xylvalue = int(len(bodypartlabels) / 3) * ["x", "y", "likelihood"]
                pdindex = pd.MultiIndex.from_arrays(
                    np.vstack([scorers, bodypartlabels, xylvalue]),
                    names=["scorer", "bodyparts", "coords"],
                )

                imnames = [fn for fn in data if fn != "metadata"]

                if track_method == "box":
                    mot_tracker = trackingutils.SORTBox(
                        inferencecfg["max_age"],
                        inferencecfg["min_hits"],
                        inferencecfg.get("iou_threshold", 0.3),
                    )
                elif track_method == "skeleton":
                    mot_tracker = trackingutils.SORTSkeleton(
                        numjoints,
                        inferencecfg["max_age"],
                        inferencecfg["min_hits"],
                        inferencecfg.get("oks_threshold", 0.5),
                    )
                else:
                    mot_tracker = trackingutils.SORTEllipse(
                        inferencecfg.get("max_age", 1),
                        inferencecfg.get("min_hits", 1),
                        inferencecfg.get("iou_threshold", 0.6),
                    )
                tracklets = {}
                multi_bpts = cfg["multianimalbodyparts"]
                assembly_builder = inferenceutils.Assembler(
                    data,
                    max_n_individuals=inferencecfg["topktoretain"],
                    n_multibodyparts=len(multi_bpts),
                    greedy=greedy,
                    pcutoff=inferencecfg.get("pcutoff", 0.1),
                    min_affinity=inferencecfg.get("pafthreshold", 0.05),
                    window_size=window_size,
                    identity_only=identity_only,
                    min_n_links=inferencecfg["minimalnumberofconnections"]
                )
                assemblies_filename = dataname.split(".h5")[0] + "_assemblies.pickle"
                if not os.path.exists(assemblies_filename) or overwrite:
                    if calibrate:
                        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(
                            cfg
                        )
                        train_data_file = os.path.join(
                            cfg["project_path"],
                            str(trainingsetfolder),
                            "CollectedData_" + cfg["scorer"] + ".h5",
                        )
                        assembly_builder.calibrate(train_data_file)
                    assembly_builder.assemble()
                    assembly_builder.to_pickle(assemblies_filename)
                else:
                    assembly_builder.from_pickle(assemblies_filename)
                    print(f"Loading assemblies from {assemblies_filename}")
                try:
                    data.close()
                except AttributeError:
                    pass

                if cfg[
                    "uniquebodyparts"
                ]:  # Initialize storage of the 'single' individual track
                    tracklets["single"] = {}
                    _single = {}
                    for index, imname in enumerate(imnames):
                        single_detection = assembly_builder.unique.get(index)
                        if single_detection is None:
                            continue
                        imindex = int(re.findall(r"\d+", imname)[0])
                        _single[imindex] = single_detection
                    tracklets["single"].update(_single)

                if inferencecfg["topktoretain"] == 1:
                    tracklets[0] = {}
                    for index, imname in tqdm(enumerate(imnames)):
                        assemblies = assembly_builder.assemblies.get(index)
                        if assemblies is None:
                            continue
                        tracklets[0][imname] = assemblies[0].data
                else:
                    keep = set(multi_bpts).difference(ignore_bodyparts or [])
                    keep_inds = sorted(multi_bpts.index(bpt) for bpt in keep)
                    for index, imname in tqdm(enumerate(imnames)):
                        assemblies = assembly_builder.assemblies.get(index)
                        if assemblies is None:
                            continue
                        animals = np.stack(
                            [assembly_builder.data for assembly_builder in assemblies]
                        )
                        if not identity_only:
                            if track_method == "box":
                                xy = trackingutils.calc_bboxes_from_keypoints(
                                    animals[:, keep_inds],
                                    inferencecfg["boundingboxslack"],
                                )  # TODO: get cropping parameters and utilize!
                            else:
                                xy = animals[:, keep_inds, :2]
                            trackers = mot_tracker.track(xy)
                        else:
                            # Optimal identity assignment based on soft voting
                            mat = np.zeros(
                                (len(assemblies), inferencecfg["topktoretain"])
                            )
                            for nrow, assembly in enumerate(assemblies):
                                for k, v in assembly.soft_identity.items():
                                    mat[nrow, k] = v
                            inds = linear_sum_assignment(mat, maximize=True)
                            trackers = np.c_[inds][:, ::-1]
                        trackingutils.fill_tracklets(
                            tracklets, trackers, animals, imname
                        )

                tracklets["header"] = pdindex
                with open(trackname, "wb") as f:
                    pickle.dump(tracklets, f, pickle.HIGHEST_PROTOCOL)

        os.chdir(str(start_path))

        print(
            "The tracklets were created (i.e., under the hood deeplabcut.convert_detections2tracklets was run). Now you can 'refine_tracklets' in the GUI, or run 'deeplabcut.stitch_tracklets'."
        )
    else:
        print("No video(s) found. Please check your path!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("video")
    parser.add_argument("config")
    cli_args = parser.parse_args()


--- File: deeplabcut/pose_estimation_tensorflow/visualizemaps.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import matplotlib.pyplot as plt
from skimage.transform import resize
from deeplabcut.core.visualization import (
    form_figure,  # for backwards compatibility
    visualize_scoremaps,
    visualize_locrefs,
    visualize_paf,
)


def extract_maps(
    config,
    shuffle=0,
    trainingsetindex=0,
    gputouse=None,
    rescale=False,
    Indices=None,
    modelprefix="",
):
    """
    Extracts the scoremap, locref, partaffinityfields (if available).

    Returns a dictionary indexed by: trainingsetfraction, snapshotindex, and imageindex
    for those keys, each item contains: (image,scmap,locref,paf,bpt names,partaffinity graph, imagename, True/False if this image was in trainingset)
    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 0.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to "all".

    rescale: bool, default False
        Evaluate the model at the 'global_scale' variable (as set in the test/pose_config.yaml file for a particular project). I.e. every
        image will be resized according to that scale and prediction will be compared to the resized ground truth. The error will be reported
        in pixels at rescaled to the *original* size. I.e. For a [200,200] pixel image evaluated at global_scale=.5, the predictions are calculated
        on [100,100] pixel images, compared to 1/2*ground truth and this error is then multiplied by 2!. The evaluation images are also shown for the
        original size!

    Examples
    --------
    If you want to extract the data for image 0 and 103 (of the training set) for model trained with shuffle 0.
    >>> deeplabcut.extract_maps(configfile,0,Indices=[0,103])

    """
    from deeplabcut.utils.auxfun_videos import imread, imresize
    from deeplabcut.pose_estimation_tensorflow.core import (
        predict,
        predict_multianimal as predictma,
    )
    from deeplabcut.pose_estimation_tensorflow.config import load_config
    from deeplabcut.pose_estimation_tensorflow.datasets.utils import data_to_input
    from deeplabcut.utils import auxiliaryfunctions
    from tqdm import tqdm
    import tensorflow as tf

    import pandas as pd
    from pathlib import Path
    import numpy as np

    tf.compat.v1.reset_default_graph()
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"  #
    #    tf.logging.set_verbosity(tf.logging.WARN)

    start_path = os.getcwd()
    # Read file path for pose_config file. >> pass it on
    cfg = auxiliaryfunctions.read_config(config)

    if gputouse is not None:  # gpu selectinon
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gputouse)

    if trainingsetindex == "all":
        TrainingFractions = cfg["TrainingFraction"]
    else:
        if trainingsetindex < len(cfg["TrainingFraction"]) and trainingsetindex >= 0:
            TrainingFractions = [cfg["TrainingFraction"][int(trainingsetindex)]]
        else:
            raise Exception(
                "Please check the trainingsetindex! ",
                trainingsetindex,
                " should be an integer from 0 .. ",
                int(len(cfg["TrainingFraction"]) - 1),
            )

    # Loading human annotatated data
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    Data = pd.read_hdf(
        os.path.join(
            cfg["project_path"],
            str(trainingsetfolder),
            "CollectedData_" + cfg["scorer"] + ".h5",
        )
    )

    # Make folder for evaluation
    auxiliaryfunctions.attempt_to_make_folder(
        str(cfg["project_path"] + "/evaluation-results/")
    )

    Maps = {}
    for trainFraction in TrainingFractions:
        Maps[trainFraction] = {}
        ##################################################
        # Load and setup CNN part detector
        ##################################################
        datafn, metadatafn = auxiliaryfunctions.get_data_and_metadata_filenames(
            trainingsetfolder, trainFraction, shuffle, cfg
        )

        modelfolder = os.path.join(
            cfg["project_path"],
            str(
                auxiliaryfunctions.get_model_folder(
                    trainFraction, shuffle, cfg, modelprefix=modelprefix
                )
            ),
        )
        path_test_config = Path(modelfolder) / "test" / "pose_cfg.yaml"
        # Load meta data
        (
            data,
            trainIndices,
            testIndices,
            trainFraction,
        ) = auxiliaryfunctions.load_metadata(
            os.path.join(cfg["project_path"], metadatafn)
        )
        try:
            dlc_cfg = load_config(str(path_test_config))
        except FileNotFoundError:
            raise FileNotFoundError(
                "It seems the model for shuffle %s and trainFraction %s does not exist."
                % (shuffle, trainFraction)
            )

        # change batch size, if it was edited during analysis!
        dlc_cfg["batch_size"] = 1  # in case this was edited for analysis.

        # Create folder structure to store results.
        evaluationfolder = os.path.join(
            cfg["project_path"],
            str(
                auxiliaryfunctions.get_evaluation_folder(
                    trainFraction, shuffle, cfg, modelprefix=modelprefix
                )
            ),
        )
        auxiliaryfunctions.attempt_to_make_folder(evaluationfolder, recursive=True)

        Snapshots = auxiliaryfunctions.get_snapshots_from_folder(
            train_folder=Path(modelfolder) / "train",
        )

        if cfg["snapshotindex"] == -1:
            snapindices = [-1]
        elif cfg["snapshotindex"] == "all":
            snapindices = range(len(Snapshots))
        elif cfg["snapshotindex"] < len(Snapshots):
            snapindices = [cfg["snapshotindex"]]
        else:
            print(
                "Invalid choice, only -1 (last), any integer up to last, or all (as string)!"
            )

        ########################### RESCALING (to global scale)
        scale = dlc_cfg["global_scale"] if rescale else 1
        Data *= scale

        bptnames = [
            dlc_cfg["all_joints_names"][i] for i in range(len(dlc_cfg["all_joints"]))
        ]

        for snapindex in snapindices:
            dlc_cfg["init_weights"] = os.path.join(
                str(modelfolder), "train", Snapshots[snapindex]
            )  # setting weights to corresponding snapshot.
            trainingsiterations = (dlc_cfg["init_weights"].split(os.sep)[-1]).split(
                "-"
            )[
                -1
            ]  # read how many training siterations that corresponds to.

            # Name for deeplabcut net (based on its parameters)
            # DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)
            # notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])
            # print("Extracting maps for ", DLCscorer, " with # of trainingiterations:", trainingsiterations)
            # if notanalyzed: #this only applies to ask if h5 exists...

            # Specifying state of model (snapshot / training state)
            sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)
            Numimages = len(Data.index)
            PredicteData = np.zeros((Numimages, 3 * len(dlc_cfg["all_joints_names"])))
            print("Analyzing data...")
            if Indices is None:
                Indices = enumerate(Data.index)
            else:
                Ind = [Data.index[j] for j in Indices]
                Indices = enumerate(Ind)

            DATA = {}
            for imageindex, imagename in tqdm(Indices):
                image = imread(
                    os.path.join(cfg["project_path"], *imagename), mode="skimage"
                )

                if scale != 1:
                    image = imresize(image, scale)

                image_batch = data_to_input(image)

                # Compute prediction with the CNN
                outputs_np = sess.run(outputs, feed_dict={inputs: image_batch})

                if cfg.get("multianimalproject", False):
                    scmap, locref, paf = predictma.extract_cnn_output(
                        outputs_np, dlc_cfg
                    )
                    pagraph = dlc_cfg["partaffinityfield_graph"]
                else:
                    scmap, locref = predict.extract_cnn_output(outputs_np, dlc_cfg)
                    paf = None
                    pagraph = []
                peaks = outputs_np[-1]

                if imageindex in testIndices:
                    trainingfram = False
                else:
                    trainingfram = True

                DATA[imageindex] = [
                    image,
                    scmap,
                    locref,
                    paf,
                    peaks,
                    bptnames,
                    pagraph,
                    imagename,
                    trainingfram,
                ]
            Maps[trainFraction][Snapshots[snapindex]] = DATA
    os.chdir(str(start_path))
    return Maps


def resize_to_same_shape(array, array_dest):
    shape_dest = array_dest.shape
    return resize(array, (shape_dest[0], shape_dest[1]))


def resize_all_maps(image, scmap, locref, paf):
    scmap = resize_to_same_shape(scmap, image)
    locref_x = resize_to_same_shape(locref[:, :, :, 0], image)
    locref_y = resize_to_same_shape(locref[:, :, :, 1], image)
    if paf is not None:
        paf = resize_to_same_shape(paf, image)
    return scmap, (locref_x, locref_y), paf


def _save_individual_subplots(fig, axes, labels, output_path):
    for ax, label in zip(axes, labels):
        extent = ax.get_tightbbox(fig.canvas.renderer).transformed(
            fig.dpi_scale_trans.inverted()
        )
        fig.savefig(output_path.format(bp=label), bbox_inches=extent)


def extract_save_all_maps(
    config,
    shuffle=1,
    trainingsetindex=0,
    comparisonbodyparts="all",
    extract_paf=True,
    all_paf_in_one=True,
    gputouse=None,
    rescale=False,
    Indices=None,
    modelprefix="",
    dest_folder=None,
):
    """
    Extracts the scoremap, location refinement field and part affinity field prediction of the model. The maps
    will be rescaled to the size of the input image and stored in the corresponding model folder in /evaluation-results.

    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to "all".

    comparisonbodyparts: list of bodyparts, Default is "all".
        The average error will be computed for those body parts only (Has to be a subset of the body parts).

    extract_paf : bool
        Extract part affinity fields by default.
        Note that turning it off will make the function much faster.

    all_paf_in_one : bool
        By default, all part affinity fields are displayed on a single frame.
        If false, individual fields are shown on separate frames.

    Indices: default None
        For which images shall the scmap/locref and paf be computed? Give a list of images

    nplots_per_row: int, optional (default=None)
        Number of plots per row in grid plots. By default, calculated to approximate a squared grid of plots

    Examples
    --------
    Calculated maps for images 0, 1 and 33.
    >>> deeplabcut.extract_save_all_maps('/analysis/project/reaching-task/config.yaml', shuffle=1,Indices=[0,1,33])

    """

    from deeplabcut.utils.auxiliaryfunctions import (
        read_config,
        attempt_to_make_folder,
        get_evaluation_folder,
        intersection_of_body_parts_and_ones_given_by_user,
    )
    from tqdm import tqdm

    cfg = read_config(config)
    data = extract_maps(
        config, shuffle, trainingsetindex, gputouse, rescale, Indices, modelprefix
    )

    comparisonbodyparts = intersection_of_body_parts_and_ones_given_by_user(
        cfg, comparisonbodyparts
    )

    print("Saving plots...")
    for frac, values in data.items():
        if not dest_folder:
            dest_folder = os.path.join(
                cfg["project_path"],
                str(get_evaluation_folder(frac, shuffle, cfg, modelprefix=modelprefix)),
                "maps",
            )
        attempt_to_make_folder(dest_folder)
        filepath = "{imname}_{map}_{label}_{shuffle}_{frac}_{snap}.png"
        dest_path = os.path.join(dest_folder, filepath)
        for snap, maps in values.items():
            for imagenr in tqdm(maps):
                (
                    image,
                    scmap,
                    locref,
                    paf,
                    peaks,
                    bptnames,
                    pafgraph,
                    impath,
                    trainingframe,
                ) = maps[imagenr]
                if not extract_paf:
                    paf = None
                label = "train" if trainingframe else "test"
                imname = impath[-1]
                scmap, (locref_x, locref_y), paf = resize_all_maps(
                    image, scmap, locref, paf
                )
                to_plot = [
                    i for i, bpt in enumerate(bptnames) if bpt in comparisonbodyparts
                ]
                list_of_inds = []
                for n, edge in enumerate(pafgraph):
                    if any(ind in to_plot for ind in edge):
                        list_of_inds.append(
                            [(2 * n, 2 * n + 1), (bptnames[edge[0]], bptnames[edge[1]])]
                        )
                if len(to_plot) > 1:
                    map_ = scmap[:, :, to_plot].sum(axis=2)
                    locref_x_ = locref_x[:, :, to_plot].sum(axis=2)
                    locref_y_ = locref_y[:, :, to_plot].sum(axis=2)
                elif len(to_plot) == 1 and len(bptnames) > 1:
                    map_ = scmap[:, :, to_plot]
                    locref_x_ = locref_x[:, :, to_plot]
                    locref_y_ = locref_y[:, :, to_plot]
                else:
                    map_ = scmap[..., 0]
                    locref_x_ = locref_x[..., 0]
                    locref_y_ = locref_y[..., 0]
                fig1, _ = visualize_scoremaps(image, map_)
                temp = dest_path.format(
                    imname=imname,
                    map="scmap",
                    label=label,
                    shuffle=shuffle,
                    frac=frac,
                    snap=snap,
                )
                fig1.savefig(temp)

                fig2, _ = visualize_locrefs(image, map_, locref_x_, locref_y_)
                temp = dest_path.format(
                    imname=imname,
                    map="locref",
                    label=label,
                    shuffle=shuffle,
                    frac=frac,
                    snap=snap,
                )
                fig2.savefig(temp)

                if paf is not None:
                    if not all_paf_in_one:
                        for inds, names in list_of_inds:
                            fig3, _ = visualize_paf(image, paf[:, :, [inds]])
                            temp = dest_path.format(
                                imname=imname,
                                map=f'paf_{"_".join(names)}',
                                label=label,
                                shuffle=shuffle,
                                frac=frac,
                                snap=snap,
                            )
                            fig3.savefig(temp)
                    else:
                        inds = [elem[0] for elem in list_of_inds]
                        n_inds = len(inds)
                        cmap = plt.cm.get_cmap(cfg["colormap"], n_inds)
                        colors = cmap(range(n_inds))
                        fig3, _ = visualize_paf(image, paf[:, :, inds], colors=colors)
                        temp = dest_path.format(
                            imname=imname,
                            map=f"paf",
                            label=label,
                            shuffle=shuffle,
                            frac=frac,
                            snap=snap,
                        )
                        fig3.savefig(temp)
                plt.close("all")


--- File: deeplabcut/pose_estimation_tensorflow/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#

# Suppress tensorflow warning messages
import tensorflow as tf
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

from deeplabcut.pose_estimation_tensorflow.config import *
from deeplabcut.pose_estimation_tensorflow.datasets import *
from deeplabcut.pose_estimation_tensorflow.default_config import *
from deeplabcut.pose_estimation_tensorflow.core.evaluate import *
from deeplabcut.pose_estimation_tensorflow.core.train import *
from deeplabcut.pose_estimation_tensorflow.core.test import *
from deeplabcut.pose_estimation_tensorflow.export import export_model
from deeplabcut.pose_estimation_tensorflow.models import *
from deeplabcut.pose_estimation_tensorflow.nnets import *
from deeplabcut.pose_estimation_tensorflow.predict_videos import *
from deeplabcut.pose_estimation_tensorflow.training import *
from deeplabcut.pose_estimation_tensorflow.util import *
from deeplabcut.pose_estimation_tensorflow.visualizemaps import *


--- File: deeplabcut/pose_estimation_tensorflow/export.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import glob
import os
import shutil
import tarfile
from pathlib import Path

import numpy as np
import ruamel.yaml
import tensorflow as tf

from deeplabcut.utils import auxiliaryfunctions
from deeplabcut.pose_estimation_tensorflow.config import load_config
from deeplabcut.pose_estimation_tensorflow.core import predict


def create_deploy_config_template():
    """

    TODO: WIP

    Creates a template for config.yaml file.
    This specific order is preserved while saving as yaml file.
    """

    yaml_str = """\
# Deploy config.yaml - info about project origin:
    Task:
    scorer:
    date:
    \n
# Project path
    project_path:
    \n
# Annotation data set configuration (and individual video cropping parameters)
    video_sets:
    bodyparts:
    \n
# Plotting configuration
    skeleton:
    skeleton_color:
    \n
    """

    ruamelFile = ruamel.yaml.YAML()
    cfg_file = ruamelFile.load(yaml_str)
    return cfg_file, ruamelFile


def write_deploy_config(configname, cfg):
    """

    CURRENTLY NOT IMPLEMENTED

    Write structured config file.
    """

    with open(configname, "w") as cf:
        ruamelFile = ruamel.yaml.YAML()
        cfg_file, ruamelFile = create_deploy_config_template()
        for key in cfg.keys():
            cfg_file[key] = cfg[key]

        # Adding default value for variable skeleton and skeleton_color for backward compatibility.
        if not "skeleton" in cfg.keys():
            cfg_file["skeleton"] = []
            cfg_file["skeleton_color"] = "black"
        ruamelFile.dump(cfg_file, cf)


def load_model(cfg, shuffle=1, trainingsetindex=0, TFGPUinference=True, modelprefix=""):
    """

    Loads a tensorflow session with a DLC model from the associated configuration
    Return a tensorflow session with DLC model given cfg and shuffle

    Parameters:
    -----------
    cfg : dict
        Configuration read from the project's main config.yaml file

    shuffle : int, optional
        which shuffle to use

    trainingsetindex : int. optional
        which training fraction to use, identified by its index

    TFGPUinference : bool, optional
        use tensorflow inference model? default = True

    Returns:
    --------
    sess : tensorflow session
        tensorflow session with DLC model from the provided configuration, shuffle, and trainingsetindex

    checkpoint file path : string
        the path to the checkpoint file associated with the loaded model
    """

    ########################
    ### find snapshot to use
    ########################

    train_fraction = cfg["TrainingFraction"][trainingsetindex]
    model_folder = os.path.join(
        cfg["project_path"],
        str(
            auxiliaryfunctions.get_model_folder(
                train_fraction, shuffle, cfg, modelprefix=modelprefix
            )
        ),
    )
    path_test_config = os.path.normpath(model_folder + "/test/pose_cfg.yaml")
    path_train_config = os.path.normpath(model_folder + "/train/pose_cfg.yaml")

    try:
        dlc_cfg = load_config(str(path_train_config))
        # dlc_cfg_train = load_config(str(path_train_config))
    except FileNotFoundError:
        raise FileNotFoundError(
            "It seems the model for shuffle %s and trainFraction %s does not exist."
            % (shuffle, train_fraction)
        )

    Snapshots = auxiliaryfunctions.get_snapshots_from_folder(
        train_folder=Path(model_folder) / "train",
    )

    if cfg["snapshotindex"] == "all":
        print(
            "Snapshotindex is set to 'all' in the config.yaml file. Changing snapshot index to -1!"
        )
        snapshotindex = -1
    else:
        snapshotindex = cfg["snapshotindex"]

    ####################################
    ### Load and setup CNN part detector
    ####################################

    # Check if data already was generated:
    dlc_cfg["init_weights"] = os.path.join(
        model_folder, "train", Snapshots[snapshotindex]
    )
    trainingsiterations = (dlc_cfg["init_weights"].split(os.sep)[-1]).split("-")[-1]
    dlc_cfg["num_outputs"] = cfg.get("num_outputs", dlc_cfg.get("num_outputs", 1))
    dlc_cfg["batch_size"] = None

    # load network
    if TFGPUinference:
        sess, _, _ = predict.setup_GPUpose_prediction(dlc_cfg)
        output = ["concat_1"]
    else:
        sess, _, _ = predict.setup_pose_prediction(dlc_cfg)
        if dlc_cfg["location_refinement"]:
            output = ["Sigmoid", "pose/locref_pred/block4/BiasAdd"]
        else:
            output = ["Sigmoid", "pose/part_pred/block4/BiasAdd"]

    input = tf.compat.v1.get_default_graph().get_operations()[0].name

    return sess, input, output, dlc_cfg


def tf_to_pb(sess, checkpoint, output, output_dir=None):
    """

    Saves a frozen tensorflow graph (a protobuf file).
    See also https://leimao.github.io/blog/Save-Load-Inference-From-TF-Frozen-Graph/

    Parameters
    ----------
    sess : tensorflow session
        session with graph to be saved

    checkpoint : string
        checkpoint of tensorflow model to be converted to protobuf (output will be <checkpoint>.pb)

    output : list of strings
        list of the names of output nodes (is returned by load_models)

    output_dir : string, optional
        path to the directory that exported models should be saved to.
        If None, will export to the directory of the checkpoint file.
    """

    output_dir = (
        os.path.expanduser(output_dir) if output_dir else os.path.dirname(checkpoint)
    )
    ckpt_base = os.path.basename(checkpoint)

    # save graph to pbtxt file
    pbtxt_file = os.path.normpath(output_dir + "/" + ckpt_base + ".pbtxt")
    tf.io.write_graph(sess.graph.as_graph_def(), "", pbtxt_file, as_text=True)

    # create frozen graph from pbtxt file
    pb_file = os.path.normpath(output_dir + "/" + ckpt_base + ".pb")
    frozen_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(
        sess,
        sess.graph_def,
        output,
    )
    with open(pb_file, "wb") as file:
        file.write(frozen_graph_def.SerializeToString())


def export_model(
    cfg_path,
    shuffle=1,
    trainingsetindex=0,
    snapshotindex=None,
    iteration=None,
    TFGPUinference=True,
    overwrite=False,
    make_tar=True,
    wipepaths=False,
    modelprefix="",
):
    """

    Export DeepLabCut models for the model zoo or for live inference.

    Saves the pose configuration, snapshot files, and frozen TF graph of the model to
    directory named exported-models within the project directory

    Parameters
    -----------

    cfg_path : string
        path to the DLC Project config.yaml file

    shuffle : int, optional
        the shuffle of the model to export. default = 1

    trainingsetindex : int, optional
        the index of the training fraction for the model you wish to export. default = 1

    snapshotindex : int, optional
        the snapshot index for the weights you wish to export. If None,
        uses the snapshotindex as defined in 'config.yaml'. Default = None

    iteration : int, optional
        The model iteration (active learning loop) you wish to export. If None,
        the iteration listed in the config file is used.

    TFGPUinference : bool, optional
        use the tensorflow inference model? Default = True
        For inference using DeepLabCut-live, it is recommended to set TFGPIinference=False

    overwrite : bool, optional
        if the model you wish to export has already been exported, whether to overwrite. default = False

    make_tar : bool, optional
        Do you want to compress the exported directory to a tar file? Default = True
        This is necessary to export to the model zoo, but not for live inference.

    wipepaths : bool, optional
        Removes the actual path of your project and the init_weights from pose_cfg.

    Example:
    --------
    Export the first stored snapshot for model trained with shuffle 3:
    >>> deeplabcut.export_model('/analysis/project/reaching-task/config.yaml',shuffle=3, snapshotindex=-1)
    --------
    """

    ### read config file

    try:
        cfg = auxiliaryfunctions.read_config(cfg_path)
    except FileNotFoundError:
        FileNotFoundError("The config.yaml file at %s does not exist." % cfg_path)

    cfg["project_path"] = os.path.dirname(os.path.realpath(cfg_path))
    cfg["iteration"] = iteration if iteration is not None else cfg["iteration"]
    cfg["batch_size"] = cfg["batch_size"] if cfg["batch_size"] > 1 else 2
    cfg["snapshotindex"] = (
        snapshotindex if snapshotindex is not None else cfg["snapshotindex"]
    )

    ### load model

    sess, input, output, dlc_cfg = load_model(
        cfg, shuffle, trainingsetindex, TFGPUinference, modelprefix
    )
    ckpt = dlc_cfg["init_weights"]
    model_dir = os.path.dirname(ckpt)

    ### set up export directory

    export_dir = os.path.normpath(cfg["project_path"] + "/" + "exported-models")
    if not os.path.isdir(export_dir):
        os.mkdir(export_dir)

    sub_dir_name = "DLC_%s_%s_iteration-%d_shuffle-%d" % (
        cfg["Task"],
        dlc_cfg["net_type"],
        cfg["iteration"],
        shuffle,
    )
    full_export_dir = os.path.normpath(export_dir + "/" + sub_dir_name)

    if os.path.isdir(full_export_dir):
        if not overwrite:
            raise FileExistsError(
                "Export directory %s already exists. Terminating export..."
                % full_export_dir
            )
    else:
        os.mkdir(full_export_dir)

    ### write pose config file

    # sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory
    dlc_cfg = dict(dlc_cfg)
    sorted_cfg = {}
    for key, value in sorted(dlc_cfg.items()):
        if wipepaths:
            if key in ["init_weights", "project_path", "snapshot_prefix"]:
                sorted_cfg[key] = "TBA"
            else:
                sorted_cfg[key] = value
        else:
            sorted_cfg[key] = value

    pose_cfg_file = os.path.normpath(full_export_dir + "/pose_cfg.yaml")
    ruamel_file = ruamel.yaml.YAML()
    ruamel_file.dump(sorted_cfg, open(pose_cfg_file, "w"))

    ### copy checkpoint to export directory

    ckpt_files = glob.glob(ckpt + "*")
    ckpt_dest = [
        os.path.normpath(full_export_dir + "/" + os.path.basename(ckf))
        for ckf in ckpt_files
    ]
    for ckf, ckd in zip(ckpt_files, ckpt_dest):
        shutil.copy(ckf, ckd)

    ### create pbtxt and pb files for checkpoint in export directory

    tf_to_pb(sess, ckpt, output, output_dir=full_export_dir)

    ### tar export directory

    if make_tar:
        tar_name = os.path.normpath(full_export_dir + ".tar.gz")
        with tarfile.open(tar_name, "w:gz") as tar:
            tar.add(full_export_dir, arcname=os.path.basename(full_export_dir))


--- File: deeplabcut/pose_estimation_tensorflow/README.md ---
# DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs

https://github.com/DeepLabCut/DeepLabCut

Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS

The code was built on Eldar's DeeperCut code: https://github.com/eldar/pose-tensorflow

In mid 2018, we adapted it to be integrated in the pip package of DeepLabCut, and among other updates added additional networks (MobileNets, EfficientNets, MultiFusion backbones and multi stage decoders), faster inference code, additional augmentation code. See DLC release history for more details on the updates. In the spring of 2021 we refactored the code to updated the code to TensorFlow 2.

Check out the following references for details:

@article{Mathisetal2018,
        title={DeepLabCut: markerless pose estimation of user-defined body parts with deep learning},
        author = {Alexander Mathis and Pranav Mamidanna and Kevin M. Cury and Taiga Abe  and Venkatesh N. Murthy and Mackenzie W. Mathis and Matthias Bethge},
        journal={Nature Neuroscience},
        year={2018},
        url={https://www.nature.com/articles/s41593-018-0209-y}
    }

@article{mathis2019pretraining,
    title={Pretraining boosts out-of-domain robustness for pose estimation},
    author={Alexander Mathis and Mert Yüksekgönül and Byron Rogers and Matthias Bethge and Mackenzie W. Mathis},
    year={2019},
    eprint={1909.11229},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{lauer2021multi,
  title={Multi-animal pose estimation and tracking with DeepLabCut},
  author={Lauer, Jessy and Zhou, Mu and Ye, Shaokai and Menegas, William and Nath, Tanmay and Rahman, Mohammed Mostafizur and Di Santo, Valentina and Soberanes, Daniel and Feng, Guoping and Murthy, Venkatesh N and others},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}


@article{insafutdinov2016deepercut,
    author = {Eldar Insafutdinov and Leonid Pishchulin and Bjoern Andres and Mykhaylo Andriluka and Bernt Schiele},
    url = {http://arxiv.org/abs/1605.03170}
    title = {DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation Model},
    year = {2016}
}

@inproceedings{pishchulin16cvpr,
    title = {DeepCut: Joint Subset Partition and Labeling for Multi Person Pose Estimation},
    booktitle = {CVPR'16},
    url = {https://arxiv.org/abs/1511.06645},
    author = {Leonid Pishchulin and Eldar Insafutdinov and Siyu Tang and Bjoern Andres and Mykhaylo Andriluka and Peter Gehler and Bernt Schiele}
}

# License:

This project (DeepLabCut and DeeperCut) is licensed under the GNU Lesser General Public License v3.0.


--- File: deeplabcut/pose_estimation_tensorflow/default_config.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#

cfg = dict()

cfg["stride"] = 8.0
cfg["weigh_part_predictions"] = False
cfg["weigh_negatives"] = False
cfg["fg_fraction"] = 0.25

# imagenet mean for resnet pretraining:
cfg["mean_pixel"] = [123.68, 116.779, 103.939]
cfg["shuffle"] = True
cfg["snapshot_prefix"] = "./snapshot"
cfg["log_dir"] = "log"
cfg["global_scale"] = 1.0
cfg["location_refinement"] = False
cfg["locref_stdev"] = 7.2801
cfg["locref_loss_weight"] = 1.0
cfg["locref_huber_loss"] = True
cfg["optimizer"] = "sgd"
cfg["intermediate_supervision"] = False
cfg["intermediate_supervision_layer"] = 12
cfg["regularize"] = False
cfg["weight_decay"] = 0.0001
cfg["crop_pad"] = 0
cfg["scoremap_dir"] = "test"

cfg["batch_size"] = 1

# types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py
cfg["dataset_type"] = "imgaug"  # >> imagaug default as of 2.2
# you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324
cfg["deterministic"] = False
cfg["mirror"] = False

# for DLC 2.2. (here all set False to not use PAFs/pairwise fields)
cfg["pairwise_huber_loss"] = True
cfg["weigh_only_present_joints"] = False
cfg["partaffinityfield_predict"] = False
cfg["pairwise_predict"] = False


--- File: deeplabcut/pose_estimation_tensorflow/vis_dataset.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#


import logging

import matplotlib.pyplot as plt
import numpy as np
import cv2


from deeplabcut.pose_estimation_tensorflow.config import load_config
from deeplabcut.pose_estimation_tensorflow.datasets import (
    Batch,
    PoseDatasetFactory,
)
from deeplabcut.utils.auxfun_videos import imresize


def display_dataset():
    logging.basicConfig(level=logging.DEBUG)

    cfg = load_config()
    dataset = PoseDatasetFactory.create(cfg)
    dataset.set_shuffle(False)

    while True:
        batch = dataset.next_batch()

        for frame_id in range(1):
            img = batch[Batch.inputs][frame_id, :, :, :]
            img = np.squeeze(img).astype("uint8")

            scmap = batch[Batch.part_score_targets][frame_id, :, :, :]
            scmap = np.squeeze(scmap)

            # scmask = batch[Batch.part_score_weights]
            # if scmask.size > 1:
            #     scmask = np.squeeze(scmask).astype('uint8')
            # else:
            #     scmask = np.zeros(img.shape)

            subplot_height = 4
            subplot_width = 5
            num_plots = subplot_width * subplot_height
            f, axarr = plt.subplots(subplot_height, subplot_width)

            for j in range(num_plots):
                plot_j = j // subplot_width
                plot_i = j % subplot_width

                curr_plot = axarr[plot_j, plot_i]
                curr_plot.axis("off")

                if j >= cfg["num_joints"]:
                    continue

                scmap_part = scmap[:, :, j]
                scmap_part = imresize(
                    scmap_part, 8.0, interpolationmethod=cv2.INTER_NEAREST
                )
                scmap_part = np.lib.pad(scmap_part, ((4, 0), (4, 0)), "minimum")

                curr_plot.set_title("{}".format(j + 1))
                curr_plot.imshow(img)
                curr_plot.hold(True)
                curr_plot.imshow(scmap_part, alpha=0.5)

        # figure(0)
        # plt.imshow(np.sum(scmap, axis=2))
        # plt.figure(100)
        # plt.imshow(img)
        # plt.figure(2)
        # plt.imshow(scmask)
        plt.show()
        plt.waitforbuttonpress()


if __name__ == "__main__":
    display_dataset()


--- File: deeplabcut/pose_estimation_tensorflow/training.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import os
from pathlib import Path


def return_train_network_path(config, shuffle=1, trainingsetindex=0, modelprefix=""):
    """Returns the training and test pose config file names as well as the folder where the snapshot is
    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: int
        Integer value specifying the shuffle index to select for training.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    Returns the triple: trainposeconfigfile, testposeconfigfile, snapshotfolder
    """
    from deeplabcut.utils import auxiliaryfunctions

    cfg = auxiliaryfunctions.read_config(config)
    modelfoldername = auxiliaryfunctions.get_model_folder(
        cfg["TrainingFraction"][trainingsetindex], shuffle, cfg, modelprefix=modelprefix
    )
    trainposeconfigfile = Path(
        os.path.join(
            cfg["project_path"], str(modelfoldername), "train", "pose_cfg.yaml"
        )
    )
    testposeconfigfile = Path(
        os.path.join(cfg["project_path"], str(modelfoldername), "test", "pose_cfg.yaml")
    )
    snapshotfolder = Path(
        os.path.join(cfg["project_path"], str(modelfoldername), "train")
    )

    return trainposeconfigfile, testposeconfigfile, snapshotfolder


def train_network(
    config,
    shuffle=1,
    trainingsetindex=0,
    max_snapshots_to_keep=5,
    displayiters=None,
    saveiters=None,
    maxiters=None,
    allow_growth=True,
    gputouse=None,
    autotune=False,
    keepdeconvweights=True,
    modelprefix="",
    superanimal_name="",
    superanimal_transfer_learning=False,
):
    """Trains the network with the labels in the training dataset.

        Parameters
        ----------
        config : string
            Full path of the config.yaml file as a string.

        shuffle: int, optional, default=1
            Integer value specifying the shuffle index to select for training.

        trainingsetindex: int, optional, default=0
            Integer specifying which TrainingsetFraction to use.
            Note that TrainingFraction is a list in config.yaml.

        max_snapshots_to_keep: int or None
            Sets how many snapshots are kept, i.e. states of the trained network. Every
            saving iteration many times a snapshot is stored, however only the last
            ``max_snapshots_to_keep`` many are kept! If you change this to None, then all
            are kept.
            See: https://github.com/DeepLabCut/DeepLabCut/issues/8#issuecomment-387404835

        displayiters: optional, default=None
            This variable is actually set in ``pose_config.yaml``. However, you can
            overwrite it with this hack. Don't use this regularly, just if you are too lazy
            to dig out the ``pose_config.yaml`` file for the corresponding project. If
            ``None``, the value from there is used, otherwise it is overwritten!

        saveiters: optional, default=None
            This variable is actually set in ``pose_config.yaml``. However, you can
            overwrite it with this hack. Don't use this regularly, just if you are too lazy
            to dig out the ``pose_config.yaml`` file for the corresponding project.
            If ``None``, the value from there is used, otherwise it is overwritten!

        maxiters: optional, default=None
            This variable is actually set in ``pose_config.yaml``. However, you can
            overwrite it with this hack. Don't use this regularly, just if you are too lazy
            to dig out the ``pose_config.yaml`` file for the corresponding project.
            If ``None``, the value from there is used, otherwise it is overwritten!

        allow_growth: bool, optional, default=True.
            For some smaller GPUs the memory issues happen. If ``True``, the memory
            allocator does not pre-allocate the entire specified GPU memory region, instead
            starting small and growing as needed.
            See issue: https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2

        gputouse: optional, default=None
            Natural number indicating the number of your GPU (see number in nvidia-smi).
            If you do not have a GPU put None.
            See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

        autotune: bool, optional, default=False
            Property of TensorFlow, somehow faster if ``False``
            (as Eldar found out, see https://github.com/tensorflow/tensorflow/issues/13317).

        keepdeconvweights: bool, optional, default=True
            Also restores the weights of the deconvolution layers (and the backbone) when
            training from a snapshot. Note that if you change the number of bodyparts, you
            need to set this to false for re-training.

        modelprefix: str, optional, default=""
            Directory containing the deeplabcut models to use when evaluating the network.
            By default, the models are assumed to exist in the project folder.

        superanimal_name: str, optional, default =""
            Specified if transfer learning with superanimal is desired

        superanimal_transfer_learning: bool, optional, default = False.
            If set true, the training is transfer learning (new decoding layer). If set false,
    and superanimal_name is True, then the training is fine-tuning (reusing the decoding layer)

        Returns
        -------
        None

        Examples
        --------
        To train the network for first shuffle of the training dataset

        >>> deeplabcut.train_network('/analysis/project/reaching-task/config.yaml')

        To train the network for second shuffle of the training dataset

        >>> deeplabcut.train_network(
                '/analysis/project/reaching-task/config.yaml',
                shuffle=2,
                keepdeconvweights=True,
            )
    """
    if allow_growth:
        os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"

    import tensorflow as tf

    # reload logger.
    import importlib
    import logging

    importlib.reload(logging)
    logging.shutdown()

    from deeplabcut.utils import auxiliaryfunctions

    tf.compat.v1.reset_default_graph()
    start_path = os.getcwd()

    # Read file path for pose_config file. >> pass it on
    cfg = auxiliaryfunctions.read_config(config)
    modelfoldername = auxiliaryfunctions.get_model_folder(
        cfg["TrainingFraction"][trainingsetindex], shuffle, cfg, modelprefix=modelprefix
    )
    poseconfigfile = Path(
        os.path.join(
            cfg["project_path"], str(modelfoldername), "train", "pose_cfg.yaml"
        )
    )
    if not poseconfigfile.is_file():
        print("The training datafile ", poseconfigfile, " is not present.")
        print(
            "Probably, the training dataset for this specific shuffle index was not created."
        )
        print(
            "Try with a different shuffle/trainingsetfraction or use function 'create_training_dataset' to create a new trainingdataset with this shuffle index."
        )
    else:
        # Set environment variables
        if (
            autotune is not False
        ):  # see: https://github.com/tensorflow/tensorflow/issues/13317
            os.environ["TF_CUDNN_USE_AUTOTUNE"] = "0"
        if gputouse is not None:
            os.environ["CUDA_VISIBLE_DEVICES"] = str(gputouse)
    try:
        cfg_dlc = auxiliaryfunctions.read_plainconfig(poseconfigfile)

        if superanimal_name != "":
            from deeplabcut.modelzoo.utils import parse_available_supermodels
            from dlclibrary.dlcmodelzoo.modelzoo_download import (
                download_huggingface_model,
                MODELOPTIONS,
            )
            import glob

            dlc_root_path = auxiliaryfunctions.get_deeplabcut_path()
            supermodels = parse_available_supermodels()
            weight_folder = str(
                Path(dlc_root_path)
                / "pose_estimation_tensorflow"
                / "models"
                / "pretrained"
                / (superanimal_name + "_weights")
            )

            if superanimal_name in MODELOPTIONS:
                if not os.path.exists(weight_folder):
                    download_huggingface_model(superanimal_name, weight_folder)
                else:
                    print(f"{weight_folder} exists, using the downloaded weights")
            else:
                print(
                    f"{superanimal_name} not available. Available ones are: ",
                    MODELOPTIONS,
                )

            snapshots = glob.glob(os.path.join(weight_folder, "snapshot-*.index"))
            init_weights = os.path.abspath(snapshots[0]).replace(".index", "")

            from deeplabcut.pose_estimation_tensorflow.core.train_multianimal import (
                train,
            )

            print("Selecting multi-animal trainer")
            train(
                str(poseconfigfile),
                displayiters,
                saveiters,
                maxiters,
                max_to_keep=max_snapshots_to_keep,
                keepdeconvweights=keepdeconvweights,
                allow_growth=allow_growth,
                init_weights=init_weights,
                remove_head=(
                    True
                    if superanimal_name != "" and superanimal_transfer_learning
                    else False
                ),
            )  # pass on path and file name for pose_cfg.yaml!

        elif "multi-animal" in cfg_dlc["dataset_type"]:
            from deeplabcut.pose_estimation_tensorflow.core.train_multianimal import (
                train,
            )

            print("Selecting multi-animal trainer")
            train(
                str(poseconfigfile),
                displayiters,
                saveiters,
                maxiters,
                max_to_keep=max_snapshots_to_keep,
                keepdeconvweights=keepdeconvweights,
                allow_growth=allow_growth,
            )  # pass on path and file name for pose_cfg.yaml!
        else:
            from deeplabcut.pose_estimation_tensorflow.core.train import train

            print("Selecting single-animal trainer")
            train(
                str(poseconfigfile),
                displayiters,
                saveiters,
                maxiters,
                max_to_keep=max_snapshots_to_keep,
                keepdeconvweights=keepdeconvweights,
                allow_growth=allow_growth,
            )  # pass on path and file name for pose_cfg.yaml!

    except BaseException as e:
        raise e
    finally:
        os.chdir(str(start_path))
    print(
        "The network is now trained and ready to evaluate. Use the function 'evaluate_network' to evaluate the network."
    )


--- File: deeplabcut/pose_estimation_tensorflow/core/predict_multianimal.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import numpy as np
import tensorflow as tf
from skimage.feature import peak_local_max
from scipy.ndimage import measurements


def extract_cnn_output(outputs_np, cfg):
    """extract locref, scmap and partaffinityfield from network"""
    scmap = outputs_np[0]
    scmap = np.squeeze(scmap)
    if cfg["location_refinement"]:
        locref = np.squeeze(outputs_np[1])
        shape = locref.shape
        locref = np.reshape(locref, (shape[0], shape[1], -1, 2))
        locref *= cfg["locref_stdev"]
    else:
        locref = None
    if cfg["partaffinityfield_predict"] and ("multi-animal" in cfg["dataset_type"]):
        paf = np.squeeze(outputs_np[2])
    else:
        paf = None

    if len(scmap.shape) == 2:  # for single body part!
        scmap = np.expand_dims(scmap, axis=2)
    return scmap, locref, paf


def extract_cnn_outputmulti(outputs_np, cfg):
    """extract locref + scmap from network
    Dimensions: image batch x imagedim1 x imagedim2 x bodypart"""
    scmap = outputs_np[0]
    if cfg["location_refinement"]:
        locref = outputs_np[1]
        shape = locref.shape
        locref = np.reshape(locref, (shape[0], shape[1], shape[2], -1, 2))
        locref *= cfg["locref_stdev"]
    else:
        locref = None
    if cfg["partaffinityfield_predict"] and ("multi-animal" in cfg["dataset_type"]):
        paf = outputs_np[2]
    else:
        paf = None

    if len(scmap.shape) == 2:  # for single body part!
        scmap = np.expand_dims(scmap, axis=2)
    return scmap, locref, paf


def compute_edge_costs(
    pafs,
    peak_inds_in_batch,
    graph,
    paf_inds,
    n_bodyparts,
    n_points=10,
    n_decimals=3,
):
    # Clip peak locations to PAFs dimensions
    h, w = pafs.shape[1:3]
    peak_inds_in_batch[:, 1] = np.clip(peak_inds_in_batch[:, 1], 0, h - 1)
    peak_inds_in_batch[:, 2] = np.clip(peak_inds_in_batch[:, 2], 0, w - 1)

    n_samples = pafs.shape[0]
    sample_inds = []
    edge_inds = []
    all_edges = []
    all_peaks = []
    for i in range(n_samples):
        samples_i = peak_inds_in_batch[:, 0] == i
        peak_inds = peak_inds_in_batch[samples_i, 1:]
        if not np.any(peak_inds):
            continue
        peaks = peak_inds[:, :2]
        bpt_inds = peak_inds[:, 2]
        idx = np.arange(peaks.shape[0])
        idx_per_bpt = {j: idx[bpt_inds == j].tolist() for j in range(n_bodyparts)}
        edges = []
        for k, (s, t) in zip(paf_inds, graph):
            inds_s = idx_per_bpt[s]
            inds_t = idx_per_bpt[t]
            if not (inds_s and inds_t):
                continue
            candidate_edges = ((i, j) for i in inds_s for j in inds_t)
            edges.extend(candidate_edges)
            edge_inds.extend([k] * len(inds_s) * len(inds_t))
        if not edges:
            continue
        sample_inds.extend([i] * len(edges))
        all_edges.extend(edges)
        all_peaks.append(peaks[np.asarray(edges)])
    if not all_peaks:
        return [dict() for _ in range(n_samples)]

    sample_inds = np.asarray(sample_inds, dtype=np.int32)
    edge_inds = np.asarray(edge_inds, dtype=np.int32)
    all_edges = np.asarray(all_edges, dtype=np.int32)
    all_peaks = np.concatenate(all_peaks)
    vecs_s = all_peaks[:, 0]
    vecs_t = all_peaks[:, 1]
    vecs = vecs_t - vecs_s
    lengths = np.linalg.norm(vecs, axis=1).astype(np.float32)
    lengths += np.spacing(1, dtype=np.float32)
    xy = np.linspace(vecs_s, vecs_t, n_points, axis=1, dtype=np.int32)
    y = pafs[
        sample_inds.reshape((-1, 1)),
        xy[..., 0],
        xy[..., 1],
        edge_inds.reshape((-1, 1)),
    ]
    integ = np.trapz(y, xy[..., ::-1], axis=1)
    affinities = np.linalg.norm(integ, axis=1).astype(np.float32)
    # unit_vecs = vecs / lengths[:, np.newaxis]
    # affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)
    affinities /= lengths
    np.round(affinities, decimals=n_decimals, out=affinities)
    np.round(lengths, decimals=n_decimals, out=lengths)

    # Form cost matrices
    all_costs = []
    for i in range(n_samples):
        samples_i_mask = sample_inds == i
        costs = dict()
        for k in paf_inds:
            edges_k_mask = edge_inds == k
            idx = np.flatnonzero(samples_i_mask & edges_k_mask)
            s, t = all_edges[idx].T
            n_sources = np.unique(s).size
            n_targets = np.unique(t).size
            costs[k] = dict()
            costs[k]["m1"] = affinities[idx].reshape((n_sources, n_targets))
            costs[k]["distance"] = lengths[idx].reshape((n_sources, n_targets))
        all_costs.append(costs)

    return all_costs


def compute_peaks_and_costs(
    scmaps,
    locrefs,
    pafs,
    peak_inds_in_batch,
    graph,
    paf_inds,
    stride,
    n_id_channels,
    n_points=10,
    n_decimals=3,
):
    n_samples, _, _, n_channels = np.shape(scmaps)
    n_bodyparts = n_channels - n_id_channels
    pos = calc_peak_locations(locrefs, peak_inds_in_batch, stride, n_decimals)
    if graph:
        costs = compute_edge_costs(
            pafs,
            peak_inds_in_batch,
            graph,
            paf_inds,
            n_bodyparts,
            n_points,
            n_decimals,
        )
    else:
        costs = None
    s, r, c, b = peak_inds_in_batch.T
    prob = np.round(scmaps[s, r, c, b], n_decimals).reshape((-1, 1))
    if n_id_channels:
        ids = np.round(scmaps[s, r, c, -n_id_channels:], n_decimals)

    peaks_and_costs = []
    for i in range(n_samples):
        xy = []
        p = []
        id_ = []
        samples_i_mask = peak_inds_in_batch[:, 0] == i
        for j in range(n_bodyparts):
            bpts_j_mask = peak_inds_in_batch[:, 3] == j
            idx = np.flatnonzero(samples_i_mask & bpts_j_mask)
            xy.append(pos[idx])
            p.append(prob[idx])
            if n_id_channels:
                id_.append(ids[idx])
        dict_ = {"coordinates": (xy,), "confidence": p}
        if costs is not None:
            dict_["costs"] = costs[i]
        if n_id_channels:
            dict_["identity"] = id_
        peaks_and_costs.append(dict_)

    return peaks_and_costs


def predict_batched_peaks_and_costs(
    pose_cfg,
    images_batch,
    sess,
    inputs,
    outputs,
    peaks_gt=None,
    n_points=10,
    n_decimals=3,
    extra_dict=None,
):
    if extra_dict:
        features = sess.run(extra_dict["features"], feed_dict={inputs: images_batch})

    scmaps, locrefs, *pafs, peaks = sess.run(outputs, feed_dict={inputs: images_batch})
    if ~np.any(peaks):
        return []

    locrefs = np.reshape(locrefs, (*locrefs.shape[:3], -1, 2))
    locrefs *= pose_cfg["locref_stdev"]
    if pafs:
        pafs = np.reshape(pafs[0], (*pafs[0].shape[:3], -1, 2))
    else:
        pafs = None
    graph = pose_cfg["partaffinityfield_graph"]
    limbs = pose_cfg.get("paf_best", np.arange(len(graph)))
    graph = [graph[l] for l in limbs]
    preds = compute_peaks_and_costs(
        scmaps,
        locrefs,
        pafs,
        peaks,
        graph,
        limbs,
        pose_cfg["stride"],
        pose_cfg.get("num_idchannel", 0),
        n_points,
        n_decimals,
    )
    if peaks_gt is not None and graph:
        costs_gt = compute_edge_costs(
            pafs,
            peaks_gt,
            graph,
            limbs,
            pose_cfg["num_joints"],
            n_points,
            n_decimals,
        )
        for i, costs in enumerate(costs_gt):
            preds[i]["groundtruth_costs"] = costs
    if extra_dict:
        return preds, features
    else:
        return preds


def find_local_maxima(scmap, radius, threshold):
    peak_idx = peak_local_max(
        scmap, min_distance=radius, threshold_abs=threshold, exclude_border=False
    )
    grid = np.zeros_like(scmap, dtype=bool)
    grid[tuple(peak_idx.T)] = True
    labels = measurements.label(grid)[0]
    xy = measurements.center_of_mass(grid, labels, range(1, np.max(labels) + 1))
    return np.asarray(xy, dtype=int).reshape((-1, 2))


def find_local_peak_indices_maxpool_nms(scmaps, radius, threshold):
    pooled = tf.nn.max_pool2d(scmaps, [radius, radius], strides=1, padding="SAME")
    maxima = scmaps * tf.cast(tf.equal(scmaps, pooled), tf.float32)
    return tf.cast(tf.where(maxima >= threshold), tf.int32)


def find_local_peak_indices_dilation(scmaps, radius, threshold):
    kernel = np.zeros((radius, radius, 1))
    mid = (radius - 1) // 2
    kernel[mid, mid] = -1
    kernel = tf.convert_to_tensor(kernel, dtype=tf.float32)

    height = tf.shape(scmaps)[1]
    width = tf.shape(scmaps)[2]
    depth = tf.shape(scmaps)[3]
    scmaps_flat = tf.reshape(
        tf.transpose(scmaps, [0, 3, 1, 2]),
        [-1, height, width, 1],
    )
    scmaps_dil = tf.nn.dilation2d(
        scmaps_flat,
        kernel,
        strides=[1, 1, 1, 1],
        rates=[1, 1, 1, 1],
        padding="SAME",
    )
    scmaps_dil = tf.transpose(
        tf.reshape(scmaps_dil, [-1, depth, height, width]),
        [0, 2, 3, 1],
    )
    argmax_and_thresh_img = (scmaps > scmaps_dil) & (scmaps > threshold)
    return tf.cast(tf.where(argmax_and_thresh_img), tf.int32)


def find_local_peak_indices_skimage(scmaps, radius, threshold):
    inds_gt = []
    for i in range(scmaps.shape[0]):
        for j in range(scmaps.shape[3]):
            scmap = scmaps[i, ..., j]
            peaks = find_local_maxima(scmap, radius, threshold)
            samples_i = np.ones(len(peaks), dtype=int).reshape((-1, 1)) * i
            bpts_j = np.ones_like(samples_i) * j
            inds_gt.append(np.c_[samples_i, peaks, bpts_j])
    return np.concatenate(inds_gt)


def calc_peak_locations(
    locrefs,
    peak_inds_in_batch,
    stride,
    n_decimals=3,
):
    s, r, c, b = peak_inds_in_batch.T
    off = locrefs[s, r, c, b]
    loc = stride * peak_inds_in_batch[:, [2, 1]] + stride // 2 + off
    return np.round(loc, decimals=n_decimals)


--- File: deeplabcut/pose_estimation_tensorflow/core/predict.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#

import numpy as np
import tensorflow as tf
from deeplabcut.pose_estimation_tensorflow.nnets.factory import PoseNetFactory
from .openvino.session import OpenVINOSession


def setup_pose_prediction(cfg, allow_growth=False, collect_extra=False):
    tf.compat.v1.reset_default_graph()
    inputs = tf.compat.v1.placeholder(
        tf.float32, shape=[cfg["batch_size"], None, None, 3]
    )
    net_heads = PoseNetFactory.create(cfg).test(inputs)
    extra_dict = {}
    outputs = [net_heads["part_prob"]]
    if cfg["location_refinement"]:
        outputs.append(net_heads["locref"])

    if ("multi-animal" in cfg["dataset_type"]) and cfg["partaffinityfield_predict"]:
        print("Activating extracting of PAFs")
        outputs.append(net_heads["pairwise_pred"])

    outputs.append(net_heads["peak_inds"])

    if collect_extra:
        extra_dict["features"] = net_heads["features"]

    restorer = tf.compat.v1.train.Saver()

    if allow_growth:
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.compat.v1.Session(config=config)
    else:
        sess = tf.compat.v1.Session()
    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(tf.compat.v1.local_variables_initializer())

    # Restore variables from disk.
    restorer.restore(sess, cfg["init_weights"])

    if collect_extra:
        return sess, inputs, outputs, extra_dict
    else:
        return sess, inputs, outputs


def extract_cnn_output(outputs_np, cfg):
    """extract locref + scmap from network"""
    scmap = outputs_np[0]
    scmap = np.squeeze(scmap)
    locref = None
    if cfg["location_refinement"]:
        locref = np.squeeze(outputs_np[1])
        shape = locref.shape
        locref = np.reshape(locref, (shape[0], shape[1], -1, 2))
        locref *= cfg["locref_stdev"]
    if len(scmap.shape) == 2:  # for single body part!
        scmap = np.expand_dims(scmap, axis=2)
    return scmap, locref


def argmax_pose_predict(scmap, offmat, stride):
    """Combine scoremat and offsets to the final pose."""
    num_joints = scmap.shape[2]
    pose = []
    for joint_idx in range(num_joints):
        maxloc = np.unravel_index(
            np.argmax(scmap[:, :, joint_idx]), scmap[:, :, joint_idx].shape
        )
        offset = np.array(offmat[maxloc][joint_idx])[::-1]
        pos_f8 = np.array(maxloc).astype("float") * stride + 0.5 * stride + offset
        pose.append(np.hstack((pos_f8[::-1], [scmap[maxloc][joint_idx]])))
    return np.array(pose)


def multi_pose_predict(scmap, locref, stride, num_outputs):
    Y, X = get_top_values(scmap[None], num_outputs)
    Y, X = Y[:, 0], X[:, 0]
    num_joints = scmap.shape[2]
    DZ = np.zeros((num_outputs, num_joints, 3))
    for m in range(num_outputs):
        for k in range(num_joints):
            x = X[m, k]
            y = Y[m, k]
            DZ[m, k, :2] = locref[y, x, k, :]
            DZ[m, k, 2] = scmap[y, x, k]

    X = X.astype("float32") * stride + 0.5 * stride + DZ[:, :, 0]
    Y = Y.astype("float32") * stride + 0.5 * stride + DZ[:, :, 1]
    P = DZ[:, :, 2]

    pose = np.empty((num_joints, num_outputs * 3), dtype="float32")
    pose[:, 0::3] = X.T
    pose[:, 1::3] = Y.T
    pose[:, 2::3] = P.T

    return pose


def getpose(image, cfg, sess, inputs, outputs, outall=False):
    """Extract pose"""
    im = np.expand_dims(image, axis=0).astype(float)
    outputs_np = sess.run(outputs, feed_dict={inputs: im})
    scmap, locref = extract_cnn_output(outputs_np, cfg)
    num_outputs = cfg.get("num_outputs", 1)
    if num_outputs > 1:
        pose = multi_pose_predict(scmap, locref, cfg["stride"], num_outputs)
    else:
        pose = argmax_pose_predict(scmap, locref, cfg["stride"])
    if outall:
        return scmap, locref, pose
    else:
        return pose


## Functions below implement are for batch sizes > 1:
def extract_cnn_outputmulti(outputs_np, cfg):
    """extract locref + scmap from network
    Dimensions: image batch x imagedim1 x imagedim2 x bodypart"""
    scmap = outputs_np[0]
    locref = None
    if cfg["location_refinement"]:
        locref = outputs_np[1]
        shape = locref.shape
        locref = np.reshape(locref, (shape[0], shape[1], shape[2], -1, 2))
        locref *= cfg["locref_stdev"]
    if len(scmap.shape) == 2:  # for single body part!
        scmap = np.expand_dims(scmap, axis=2)
    return scmap, locref


def get_top_values(scmap, n_top=5):
    batchsize, ny, nx, num_joints = scmap.shape
    scmap_flat = scmap.reshape(batchsize, nx * ny, num_joints)
    if n_top == 1:
        scmap_top = np.argmax(scmap_flat, axis=1)[None]
    else:
        scmap_top = np.argpartition(scmap_flat, -n_top, axis=1)[:, -n_top:]
        for ix in range(batchsize):
            vals = scmap_flat[ix, scmap_top[ix], np.arange(num_joints)]
            arg = np.argsort(-vals, axis=0)
            scmap_top[ix] = scmap_top[ix, arg, np.arange(num_joints)]
        scmap_top = scmap_top.swapaxes(0, 1)

    Y, X = np.unravel_index(scmap_top, (ny, nx))
    return Y, X


def getposeNP(image, cfg, sess, inputs, outputs, outall=False):
    """Adapted from DeeperCut, performs numpy-based faster inference on batches.
    Introduced in https://www.biorxiv.org/content/10.1101/457242v1"""

    num_outputs = cfg.get("num_outputs", 1)
    outputs_np = sess.run(outputs, feed_dict={inputs: image})

    scmap, locref = extract_cnn_outputmulti(outputs_np, cfg)  # processes image batch.
    batchsize, ny, nx, num_joints = scmap.shape

    Y, X = get_top_values(scmap, n_top=num_outputs)

    # Combine scoremat and offsets to the final pose.
    DZ = np.zeros((num_outputs, batchsize, num_joints, 3))
    for m in range(num_outputs):
        for l in range(batchsize):
            for k in range(num_joints):
                x = X[m, l, k]
                y = Y[m, l, k]
                DZ[m, l, k, :2] = locref[l, y, x, k, :]
                DZ[m, l, k, 2] = scmap[l, y, x, k]

    X = X.astype("float32") * cfg["stride"] + 0.5 * cfg["stride"] + DZ[:, :, :, 0]
    Y = Y.astype("float32") * cfg["stride"] + 0.5 * cfg["stride"] + DZ[:, :, :, 1]
    P = DZ[:, :, :, 2]

    Xs = X.swapaxes(0, 2).swapaxes(0, 1)
    Ys = Y.swapaxes(0, 2).swapaxes(0, 1)
    Ps = P.swapaxes(0, 2).swapaxes(0, 1)

    pose = np.empty(
        (cfg["batch_size"], num_outputs * cfg["num_joints"] * 3), dtype=X.dtype
    )
    pose[:, 0::3] = Xs.reshape(batchsize, -1)
    pose[:, 1::3] = Ys.reshape(batchsize, -1)
    pose[:, 2::3] = Ps.reshape(batchsize, -1)

    if outall:
        return scmap, locref, pose
    else:
        return pose


### Code for TF inference on GPU
def setup_GPUpose_prediction(cfg, allow_growth=False):
    tf.compat.v1.reset_default_graph()
    inputs = tf.compat.v1.placeholder(
        tf.float32, shape=[cfg["batch_size"], None, None, 3]
    )
    net_heads = PoseNetFactory.create(cfg).inference(inputs)
    outputs = [net_heads["pose"]]

    restorer = tf.compat.v1.train.Saver()

    if allow_growth:
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.compat.v1.Session(config=config)
    else:
        sess = tf.compat.v1.Session()

    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(tf.compat.v1.local_variables_initializer())

    # Restore variables from disk.
    restorer.restore(sess, cfg["init_weights"])

    return sess, inputs, outputs


def extract_GPUprediction(outputs, cfg):
    return outputs[0]


def setup_openvino_pose_prediction(cfg, device):
    sess = OpenVINOSession(cfg, device)
    return sess, sess.input_name, [sess.output_name]


--- File: deeplabcut/pose_estimation_tensorflow/core/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_estimation_tensorflow/core/test.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#

import argparse
import logging
import os

import numpy as np
import scipy.io
import scipy.ndimage

from deeplabcut.pose_estimation_tensorflow.config import load_config
from deeplabcut.pose_estimation_tensorflow.datasets.factory import PoseDatasetFactory
from deeplabcut.pose_estimation_tensorflow.datasets import Batch
from .predict import (
    setup_pose_prediction,
    extract_cnn_output,
    argmax_pose_predict,
)
from deeplabcut.pose_estimation_tensorflow.util import visualize


def test_net(visualise, cache_scoremaps):
    logging.basicConfig(level=logging.INFO)

    cfg = load_config()
    dataset = PoseDatasetFactory.create(cfg)
    dataset.set_shuffle(False)
    dataset.set_test_mode(True)

    sess, inputs, outputs = setup_pose_prediction(cfg)

    if cache_scoremaps:
        out_dir = cfg["scoremap_dir"]
        if not os.path.exists(out_dir):
            os.makedirs(out_dir)

    num_images = dataset.num_images
    predictions = np.zeros((num_images,), dtype=np.object)

    for k in range(num_images):
        print("processing image {}/{}".format(k, num_images - 1))

        batch = dataset.next_batch()

        outputs_np = sess.run(outputs, feed_dict={inputs: batch[Batch.inputs]})

        scmap, locref = extract_cnn_output(outputs_np, cfg)

        pose = argmax_pose_predict(scmap, locref, cfg["stride"])

        pose_refscale = np.copy(pose)
        pose_refscale[:, 0:2] /= cfg["global_scale"]
        predictions[k] = pose_refscale

        if visualise:
            img = np.squeeze(batch[Batch.inputs]).astype("uint8")
            visualize.show_heatmaps(cfg, img, scmap, pose)
            visualize.waitforbuttonpress()

        if cache_scoremaps:
            base = os.path.basename(batch[Batch.data_item].im_path)
            raw_name = os.path.splitext(base)[0]
            out_fn = os.path.join(out_dir, raw_name + ".mat")
            scipy.io.savemat(out_fn, mdict={"scoremaps": scmap.astype("float32")})

            out_fn = os.path.join(out_dir, raw_name + "_locreg" + ".mat")
            if cfg["location_refinement"]:
                scipy.io.savemat(
                    out_fn, mdict={"locreg_pred": locref.astype("float32")}
                )

    scipy.io.savemat("predictions.mat", mdict={"joints": predictions})

    sess.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--novis", default=False, action="store_true")
    parser.add_argument("--cache", default=False, action="store_true")
    args, unparsed = parser.parse_known_args()

    test_net(not args.novis, args.cache)


--- File: deeplabcut/pose_estimation_tensorflow/core/train_multianimal.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import argparse
import logging
import os
from pathlib import Path

import tensorflow as tf
import tf_slim as slim

from deeplabcut.pose_estimation_tensorflow.config import load_config
from deeplabcut.pose_estimation_tensorflow.datasets import PoseDatasetFactory
from deeplabcut.pose_estimation_tensorflow.nnets import PoseNetFactory
from deeplabcut.pose_estimation_tensorflow.nnets.utils import get_batch_spec
from deeplabcut.pose_estimation_tensorflow.util.logging import setup_logging
from deeplabcut.pose_estimation_tensorflow.core.train import (
    setup_preloading,
    start_preloading,
    get_optimizer,
    LearningRate,
)
from deeplabcut.utils import auxfun_models


def train(
    config_yaml,
    displayiters,
    saveiters,
    maxiters,
    max_to_keep=5,
    keepdeconvweights=True,
    allow_growth=True,
    pseudo_labels="",
    init_weights="",
    pseudo_threshold=0.1,
    modelfolder="",
    traintime_resize=False,
    video_path="",
    superanimal=None,
    remove_head=False,
):
    # in case there was already a graph
    tf.compat.v1.reset_default_graph()

    start_path = os.getcwd()
    if modelfolder == "":
        os.chdir(
            str(Path(config_yaml).parents[0])
        )  # switch to folder of config_yaml (for logging)
    else:
        os.chdir(modelfolder)

    setup_logging()

    if isinstance(config_yaml, dict):
        cfg = config_yaml
    else:
        cfg = load_config(config_yaml)

    cfg["pseudo_threshold"] = pseudo_threshold
    cfg["video_path"] = video_path
    cfg["traintime_resize"] = traintime_resize

    if superanimal is not None:
        cfg["superanimal"] = superanimal

    if pseudo_labels != "":
        cfg["pseudo_label"] = pseudo_labels

    if modelfolder != "":
        cfg["log_dir"] = modelfolder
        cfg["project_path"] = modelfolder
        # have to overwrite this
        cfg["snapshot_prefix"] = os.path.join("snapshot")

    if cfg["optimizer"] != "adam":
        print(
            "Setting batchsize to 1! Larger batchsize not supported for this loader:",
            cfg["dataset_type"],
        )
        cfg["batch_size"] = 1

    if (
        cfg["partaffinityfield_predict"] and "multi-animal" in cfg["dataset_type"]
    ):  # the PAF code currently just hijacks the pairwise net stuff (for the batch feeding via Batch.pairwise_targets: 5)
        print("Activating limb prediction...")
        cfg["pairwise_predict"] = True

    dataset = PoseDatasetFactory.create(cfg)

    batch_spec = get_batch_spec(cfg)
    batch, enqueue_op, placeholders = setup_preloading(batch_spec)

    losses = PoseNetFactory.create(cfg).train(batch)
    total_loss = losses["total_loss"]

    for k, t in losses.items():
        tf.compat.v1.summary.scalar(k, t)
    merged_summaries = tf.compat.v1.summary.merge_all()
    net_type = cfg["net_type"]

    if init_weights != "":
        cfg["init_weights"] = init_weights
        cfg["resume_weights_only"] = True
        print("replacing default init weights with: ", init_weights)

    stem = Path(cfg["init_weights"]).stem
    if "snapshot" in stem and keepdeconvweights:
        print("Loading already trained DLC with backbone:", net_type)
        variables_to_restore = slim.get_variables_to_restore()
        if cfg.get("resume_weights_only", False):
            start_iter = 0
        else:
            start_iter = int(stem.split("-")[1])

        if remove_head:
            # removing the decoding layer from the checkpoint
            temp = []
            for variable in variables_to_restore:
                if "pose" not in variable.name:
                    temp.append(variable)
            variables_to_restore = temp

    else:
        print("Loading ImageNet-pretrained", net_type)
        # loading backbone from ResNet, MobileNet etc.
        if "resnet" in net_type:
            variables_to_restore = slim.get_variables_to_restore(include=["resnet_v1"])
        elif "mobilenet" in net_type:
            variables_to_restore = slim.get_variables_to_restore(
                include=["MobilenetV2"]
            )
        elif "efficientnet" in net_type:
            variables_to_restore = slim.get_variables_to_restore(
                include=["efficientnet"]
            )
            variables_to_restore = {
                var.op.name.replace("efficientnet/", "")
                + "/ExponentialMovingAverage": var
                for var in variables_to_restore
            }
        else:
            print("Wait for DLC 2.3.")
        start_iter = 0

    restorer = tf.compat.v1.train.Saver(variables_to_restore)
    saver = tf.compat.v1.train.Saver(
        max_to_keep=max_to_keep
    )  # selects how many snapshots are stored, see https://github.com/DeepLabCut/DeepLabCut/issues/8#issuecomment-387404835

    if allow_growth:
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.compat.v1.Session(config=config)
    else:
        sess = tf.compat.v1.Session()

    coord, thread = start_preloading(sess, enqueue_op, dataset, placeholders)
    train_writer = tf.compat.v1.summary.FileWriter(cfg["log_dir"], sess.graph)
    learning_rate, train_op, tstep = get_optimizer(total_loss, cfg)

    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(tf.compat.v1.local_variables_initializer())

    auxfun_models.smart_restore(restorer, sess, cfg["init_weights"], net_type)

    if maxiters is None:
        max_iter = int(cfg["multi_step"][-1][1])
    else:
        max_iter = min(int(cfg["multi_step"][-1][1]), int(maxiters))
        # display_iters = max(1,int(displayiters))
        print("Max_iters overwritten as", max_iter)

    if displayiters is None:
        display_iters = max(1, int(cfg["display_iters"]))
    else:
        display_iters = max(1, int(displayiters))
        print("Display_iters overwritten as", display_iters)

    if saveiters is None:
        save_iters = max(1, int(cfg["save_iters"]))

    else:
        save_iters = max(1, int(saveiters))
        print("Save_iters overwritten as", save_iters)

    cumloss, partloss, locrefloss, pwloss = 0.0, 0.0, 0.0, 0.0
    lr_gen = LearningRate(cfg)
    lrf = None
    if not isinstance(config_yaml, dict):
        stats_path = Path(config_yaml).with_name("learning_stats.csv")
        lrf = open(str(stats_path), "w")

    print("Training parameters:")
    print(cfg)
    print("Starting multi-animal training....")
    max_iter += start_iter  # max_iter is relative to start_iter
    for it in range(start_iter, max_iter + 1):
        if "efficientnet" in net_type:
            lr_dict = {tstep: it - start_iter}
            current_lr = sess.run(learning_rate, feed_dict=lr_dict)
        else:
            current_lr = lr_gen.get_lr(it - start_iter)
            lr_dict = {learning_rate: current_lr}

        # [_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})
        [_, alllosses, loss_val, summary] = sess.run(
            [train_op, losses, total_loss, merged_summaries], feed_dict=lr_dict
        )

        partloss += alllosses["part_loss"]  # scoremap loss
        if cfg["location_refinement"]:
            locrefloss += alllosses["locref_loss"]
        if cfg["pairwise_predict"]:  # paf loss
            pwloss += alllosses["pairwise_loss"]

        cumloss += loss_val
        train_writer.add_summary(summary, it)
        if it % display_iters == 0 and it > start_iter:
            logging.info(
                "iteration: {} loss: {} scmap loss: {} locref loss: {} limb loss: {} lr: {}".format(
                    it,
                    "{0:.4f}".format(cumloss / display_iters),
                    "{0:.4f}".format(partloss / display_iters),
                    "{0:.4f}".format(locrefloss / display_iters),
                    "{0:.4f}".format(pwloss / display_iters),
                    current_lr,
                )
            )

            if lrf:
                lrf.write(
                    "iteration: {}, loss: {}, scmap loss: {}, locref loss: {}, limb loss: {}, lr: {}\n".format(
                        it,
                        "{0:.4f}".format(cumloss / display_iters),
                        "{0:.4f}".format(partloss / display_iters),
                        "{0:.4f}".format(locrefloss / display_iters),
                        "{0:.4f}".format(pwloss / display_iters),
                        current_lr,
                    )
                )

            cumloss, partloss, locrefloss, pwloss = 0.0, 0.0, 0.0, 0.0
            if lrf:
                lrf.flush()

        # Save snapshot
        if (it % save_iters == 0 and it != start_iter) or it == max_iter:
            model_name = cfg["snapshot_prefix"]
            saver.save(sess, model_name, global_step=it)
    if lrf:
        lrf.close()

    sess.close()
    coord.request_stop()
    coord.join([thread])

    # return to original path.
    os.chdir(str(start_path))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config", help="Path to yaml configuration file.")
    cli_args = parser.parse_args()

    train(Path(cli_args.config).resolve())


--- File: deeplabcut/pose_estimation_tensorflow/core/train.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#

import argparse
import logging
import os
import threading
import warnings
from pathlib import Path

import tensorflow as tf

tf.compat.v1.disable_eager_execution()
import tf_slim as slim

from deeplabcut.pose_estimation_tensorflow.config import load_config
from deeplabcut.pose_estimation_tensorflow.datasets import (
    Batch,
    PoseDatasetFactory,
)
from deeplabcut.pose_estimation_tensorflow.nnets import PoseNetFactory
from deeplabcut.pose_estimation_tensorflow.util.logging import setup_logging
from deeplabcut.utils import auxfun_models


class LearningRate(object):
    def __init__(self, cfg):
        self.steps = cfg["multi_step"]
        self.current_step = 0

    def get_lr(self, iteration):
        lr = self.steps[self.current_step][0]
        if iteration == self.steps[self.current_step][1]:
            self.current_step += 1

        return lr


def get_batch_spec(cfg):
    num_joints = cfg["num_joints"]
    batch_size = cfg["batch_size"]
    return {
        Batch.inputs: [batch_size, None, None, 3],
        Batch.part_score_targets: [batch_size, None, None, num_joints],
        Batch.part_score_weights: [batch_size, None, None, num_joints],
        Batch.locref_targets: [batch_size, None, None, num_joints * 2],
        Batch.locref_mask: [batch_size, None, None, num_joints * 2],
    }


def setup_preloading(batch_spec):
    placeholders = {
        name: tf.compat.v1.placeholder(tf.float32, shape=spec)
        for (name, spec) in batch_spec.items()
    }
    names = placeholders.keys()
    placeholders_list = list(placeholders.values())

    QUEUE_SIZE = 20
    q = tf.queue.FIFOQueue(QUEUE_SIZE, [tf.float32] * len(batch_spec))
    enqueue_op = q.enqueue(placeholders_list)
    batch_list = q.dequeue()

    batch = {}
    for idx, name in enumerate(names):
        batch[name] = batch_list[idx]
        batch[name].set_shape(batch_spec[name])
    return batch, enqueue_op, placeholders


def load_and_enqueue(sess, enqueue_op, coord, dataset, placeholders):
    while not coord.should_stop():
        batch_np = dataset.next_batch()
        food = {pl: batch_np[name] for (name, pl) in placeholders.items()}
        sess.run(enqueue_op, feed_dict=food)


def start_preloading(sess, enqueue_op, dataset, placeholders):
    coord = tf.compat.v1.train.Coordinator()
    t = threading.Thread(
        target=load_and_enqueue,
        args=(sess, enqueue_op, coord, dataset, placeholders),
    )
    t.start()
    return coord, t


def get_optimizer(loss_op, cfg):
    tstep = tf.compat.v1.placeholder(tf.int32, shape=[], name="tstep")
    if "efficientnet" in cfg["net_type"]:
        print("Switching to cosine decay schedule with adam!")
        cfg["optimizer"] = "adam"
        learning_rate = tf.compat.v1.train.cosine_decay(
            cfg["lr_init"], tstep, cfg["decay_steps"], alpha=cfg["alpha_r"]
        )
    else:
        learning_rate = tf.compat.v1.placeholder(tf.float32, shape=[])

    if cfg["optimizer"] == "sgd":
        optimizer = tf.compat.v1.train.MomentumOptimizer(
            learning_rate=learning_rate, momentum=0.9
        )
    elif cfg["optimizer"] == "adam":
        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)
    else:
        raise ValueError("unknown optimizer {}".format(cfg["optimizer"]))
    train_op = slim.learning.create_train_op(loss_op, optimizer)

    return learning_rate, train_op, tstep


def get_optimizer_with_freeze(loss_op, cfg):
    learning_rate = tf.compat.v1.placeholder(tf.float32, shape=[])

    if cfg["optimizer"] == "sgd":
        optimizer = tf.compat.v1.train.MomentumOptimizer(
            learning_rate=learning_rate, momentum=0.9
        )
    elif cfg["optimizer"] == "adam":
        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)
    else:
        raise ValueError("unknown optimizer {}".format(cfg["optimizer"]))

    train_unfrozen_op = slim.learning.create_train_op(loss_op, optimizer)
    variables_unfrozen = tf.compat.v1.get_collection(
        tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, "pose"
    )

    train_frozen_op = slim.learning.create_train_op(
        loss_op, optimizer, variables_to_train=variables_unfrozen
    )

    return learning_rate, train_unfrozen_op, train_frozen_op


def train(
    config_yaml,
    displayiters,
    saveiters,
    maxiters,
    max_to_keep=5,
    keepdeconvweights=True,
    allow_growth=True,
):
    start_path = os.getcwd()
    os.chdir(
        str(Path(config_yaml).parents[0])
    )  # switch to folder of config_yaml (for logging)
    setup_logging()

    cfg = load_config(config_yaml)
    net_type = cfg["net_type"]
    if cfg["dataset_type"] in ("scalecrop", "tensorpack", "deterministic"):
        print(
            "Switching batchsize to 1, as tensorpack/scalecrop/deterministic loaders do not support batches >1. Use imgaug/default loader."
        )
        cfg["batch_size"] = 1  # in case this was edited for analysis.-

    dataset = PoseDatasetFactory.create(cfg)
    batch_spec = get_batch_spec(cfg)
    batch, enqueue_op, placeholders = setup_preloading(batch_spec)

    losses = PoseNetFactory.create(cfg).train(batch)
    total_loss = losses["total_loss"]

    for k, t in losses.items():
        tf.compat.v1.summary.scalar(k, t)
    merged_summaries = tf.compat.v1.summary.merge_all()

    stem = Path(cfg["init_weights"]).stem
    if "snapshot" in stem and keepdeconvweights:
        print("Loading already trained DLC with backbone:", net_type)
        variables_to_restore = slim.get_variables_to_restore()
        start_iter = int(stem.split("-")[1])
    else:
        print("Loading ImageNet-pretrained", net_type)
        # loading backbone from ResNet, MobileNet etc.
        if "resnet" in net_type:
            variables_to_restore = slim.get_variables_to_restore(include=["resnet_v1"])
        elif "mobilenet" in net_type:
            variables_to_restore = slim.get_variables_to_restore(
                include=["MobilenetV2"]
            )
        elif "efficientnet" in net_type:
            variables_to_restore = slim.get_variables_to_restore(
                include=["efficientnet"]
            )
            variables_to_restore = {
                var.op.name.replace("efficientnet/", "")
                + "/ExponentialMovingAverage": var
                for var in variables_to_restore
            }
        else:
            print("Wait for DLC 2.3.")
        start_iter = 0

    restorer = tf.compat.v1.train.Saver(variables_to_restore)
    saver = tf.compat.v1.train.Saver(
        max_to_keep=max_to_keep
    )  # selects how many snapshots are stored, see https://github.com/DeepLabCut/DeepLabCut/issues/8#issuecomment-387404835

    if allow_growth:
        config = tf.compat.v1.ConfigProto()
        config.gpu_options.allow_growth = True
        sess = tf.compat.v1.Session(config=config)
    else:
        sess = tf.compat.v1.Session()

    coord, thread = start_preloading(sess, enqueue_op, dataset, placeholders)
    train_writer = tf.compat.v1.summary.FileWriter(cfg["log_dir"], sess.graph)

    # Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes
    from tensorflow.python.platform import build_info

    info = build_info.build_info
    if not info["is_cuda_build"]:  # Apple Silicon is not built with CUDA
        warnings.warn("Switching to Adam, as SGD crashes on Apple Silicon.")
        cfg["optimizer"] = "adam"
        cfg["lr_init"] = 5e-4
        cfg["multi_step"] = [[1e-4, 7500], [5e-5, 12000], [1e-5, 200000]]

    if cfg.get("freezeencoder", False):
        if "efficientnet" in net_type:
            print("Freezing ONLY supported MobileNet/ResNet currently!!")
            learning_rate, train_op, tstep = get_optimizer(total_loss, cfg)

        print("Freezing encoder...")
        learning_rate, _, train_op = get_optimizer_with_freeze(total_loss, cfg)
    else:
        learning_rate, train_op, tstep = get_optimizer(total_loss, cfg)

    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(tf.compat.v1.local_variables_initializer())

    # Restore variables from disk.
    auxfun_models.smart_restore(restorer, sess, cfg["init_weights"], net_type)

    if maxiters is None:
        max_iter = int(cfg["multi_step"][-1][1])
    else:
        max_iter = min(int(cfg["multi_step"][-1][1]), int(maxiters))
        # display_iters = max(1,int(displayiters))
        print("Max_iters overwritten as", max_iter)

    if displayiters is None:
        display_iters = max(1, int(cfg["display_iters"]))
    else:
        display_iters = max(1, int(displayiters))
        print("Display_iters overwritten as", display_iters)

    if saveiters is None:
        save_iters = max(1, int(cfg["save_iters"]))

    else:
        save_iters = max(1, int(saveiters))
        print("Save_iters overwritten as", save_iters)

    cum_loss = 0.0
    lr_gen = LearningRate(cfg)

    stats_path = Path(config_yaml).with_name("learning_stats.csv")
    lrf = open(str(stats_path), "w")

    print("Training parameter:")
    print(cfg)
    print("Starting training....")
    max_iter += start_iter  # max_iter is relative to start_iter
    for it in range(start_iter, max_iter + 1):
        if "efficientnet" in net_type:
            lr_dict = {tstep: it - start_iter}
            current_lr = sess.run(learning_rate, feed_dict=lr_dict)
        else:
            current_lr = lr_gen.get_lr(it - start_iter)
            lr_dict = {learning_rate: current_lr}

        [_, loss_val, summary] = sess.run(
            [train_op, total_loss, merged_summaries], feed_dict=lr_dict
        )
        cum_loss += loss_val
        train_writer.add_summary(summary, it)

        if it % display_iters == 0 and it > start_iter:
            average_loss = cum_loss / display_iters
            cum_loss = 0.0
            logging.info(
                "iteration: {} loss: {} lr: {}".format(
                    it, "{0:.4f}".format(average_loss), current_lr
                )
            )
            lrf.write("{}, {:.5f}, {}\n".format(it, average_loss, current_lr))
            lrf.flush()

        # Save snapshot
        if (it % save_iters == 0 and it != start_iter) or it == max_iter:
            model_name = cfg["snapshot_prefix"]
            saver.save(sess, model_name, global_step=it)

    lrf.close()
    sess.close()
    coord.request_stop()
    coord.join([thread])
    # return to original path.
    os.chdir(str(start_path))


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config", help="Path to yaml configuration file.")
    cli_args = parser.parse_args()

    train(Path(cli_args.config).resolve())


--- File: deeplabcut/pose_estimation_tensorflow/core/evaluate.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import argparse
import os
from pathlib import Path
from typing import List, Union

import numpy as np
import pandas as pd
from tqdm import tqdm

from deeplabcut.pose_estimation_tensorflow.training import return_train_network_path


def pairwisedistances(DataCombined, scorer1, scorer2, pcutoff=-1, bodyparts=None):
    """Calculates the pairwise Euclidean distance metric over body parts vs. images"""
    mask = DataCombined[scorer2].xs("likelihood", level=1, axis=1) >= pcutoff
    if bodyparts is None:
        Pointwisesquareddistance = (DataCombined[scorer1] - DataCombined[scorer2]) ** 2
        RMSE = np.sqrt(
            Pointwisesquareddistance.xs("x", level=1, axis=1)
            + Pointwisesquareddistance.xs("y", level=1, axis=1)
        )  # Euclidean distance (proportional to RMSE)
        return RMSE, RMSE[mask]
    else:
        Pointwisesquareddistance = (
            DataCombined[scorer1][bodyparts] - DataCombined[scorer2][bodyparts]
        ) ** 2
        RMSE = np.sqrt(
            Pointwisesquareddistance.xs("x", level=1, axis=1)
            + Pointwisesquareddistance.xs("y", level=1, axis=1)
        )  # Euclidean distance (proportional to RMSE)
        return RMSE, RMSE[mask]


def calculatepafdistancebounds(
    config, shuffle=0, trainingsetindex=0, modelprefix="", numdigits=0, onlytrain=False
):
    """
    Returns distances along paf edges in train/test data

    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 0.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to "all".

    numdigits: number of digits to round for distances.

    """
    import os
    from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal
    from deeplabcut.pose_estimation_tensorflow.config import load_config

    # Read file path for pose_config file. >> pass it on
    cfg = auxiliaryfunctions.read_config(config)

    if cfg["multianimalproject"]:
        (
            individuals,
            uniquebodyparts,
            multianimalbodyparts,
        ) = auxfun_multianimal.extractindividualsandbodyparts(cfg)

        # Loading human annotatated data
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
        trainFraction = cfg["TrainingFraction"][trainingsetindex]
        modelfolder = os.path.join(
            cfg["project_path"],
            str(
                auxiliaryfunctions.get_model_folder(
                    trainFraction, shuffle, cfg, modelprefix=modelprefix
                )
            ),
        )

        # Load meta data & annotations
        Data = pd.read_hdf(
            os.path.join(
                cfg["project_path"],
                str(trainingsetfolder),
                "CollectedData_" + cfg["scorer"] + ".h5",
            )
        )[cfg["scorer"]]

        path_train_config, path_test_config, _ = return_train_network_path(
            config=config,
            shuffle=shuffle,
            trainingsetindex=trainingsetindex,
            modelprefix=modelprefix,
        )
        train_pose_cfg = load_config(str(path_train_config))
        test_pose_cfg = load_config(str(path_test_config))

        _, trainIndices, _, _ = auxiliaryfunctions.load_metadata(
            Path(cfg["project_path"]) / train_pose_cfg["metadataset"]
        )

        # get the graph!
        partaffinityfield_graph = test_pose_cfg["partaffinityfield_graph"]
        jointnames = [
            test_pose_cfg["all_joints_names"][i]
            for i in range(len(test_pose_cfg["all_joints"]))
        ]
        path_inferencebounds_config = (
            Path(modelfolder) / "test" / "inferencebounds.yaml"
        )
        inferenceboundscfg = {}
        for pi, edge in enumerate(partaffinityfield_graph):
            j1, j2 = jointnames[edge[0]], jointnames[edge[1]]
            ds_within = []
            ds_across = []
            for ind in individuals:
                for ind2 in individuals:
                    if ind != "single" and ind2 != "single":
                        if (ind, j1, "x") in Data.keys() and (
                            ind2,
                            j2,
                            "y",
                        ) in Data.keys():
                            distances = (
                                np.sqrt(
                                    (Data[ind, j1, "x"] - Data[ind2, j2, "x"]) ** 2
                                    + (Data[ind, j1, "y"] - Data[ind2, j2, "y"]) ** 2
                                )
                                / test_pose_cfg["stride"]
                            )
                        else:
                            distances = None

                        if distances is not None:
                            if onlytrain:
                                distances = distances.iloc[trainIndices]
                            if ind == ind2:
                                ds_within.extend(distances.values.flatten())
                            else:
                                ds_across.extend(distances.values.flatten())

            edgeencoding = str(edge[0]) + "_" + str(edge[1])
            inferenceboundscfg[edgeencoding] = {}
            if len(ds_within) > 0:
                inferenceboundscfg[edgeencoding]["intra_max"] = str(
                    round(np.nanmax(ds_within), numdigits)
                )
                inferenceboundscfg[edgeencoding]["intra_min"] = str(
                    round(np.nanmin(ds_within), numdigits)
                )
            else:
                inferenceboundscfg[edgeencoding]["intra_max"] = str(
                    1e5
                )  # large number (larger than any image diameter)
                inferenceboundscfg[edgeencoding]["intra_min"] = str(0)

            # NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*
            if len(ds_across) > 0:
                inferenceboundscfg[edgeencoding]["inter_max"] = str(
                    round(np.nanmax(ds_across), numdigits)
                )
                inferenceboundscfg[edgeencoding]["inter_min"] = str(
                    round(np.nanmin(ds_across), numdigits)
                )
            else:
                inferenceboundscfg[edgeencoding]["inter_max"] = str(
                    1e5
                )  # large number (larger than image diameters in typical experiments)
                inferenceboundscfg[edgeencoding]["inter_min"] = str(0)

        auxiliaryfunctions.write_plainconfig(
            str(path_inferencebounds_config), dict(inferenceboundscfg)
        )
        return inferenceboundscfg
    else:
        print("You might as well bring owls to Athens.")
        return {}


def Plotting(
    cfg, comparisonbodyparts, DLCscorer, trainIndices, DataCombined, foldername
):
    """Function used for plotting GT and predictions"""
    from deeplabcut.utils import visualization

    colors = visualization.get_cmap(len(comparisonbodyparts), name=cfg["colormap"])
    NumFrames = np.size(DataCombined.index)
    fig, ax = visualization.create_minimal_figure()
    for ind in tqdm(np.arange(NumFrames)):
        ax = visualization.plot_and_save_labeled_frame(
            DataCombined,
            ind,
            trainIndices,
            cfg,
            colors,
            comparisonbodyparts,
            DLCscorer,
            foldername,
            fig,
            ax,
        )
        visualization.erase_artists(ax)


def return_evaluate_network_data(
    config,
    shuffle=0,
    trainingsetindex=0,
    comparisonbodyparts="all",
    Snapindex=None,
    rescale=False,
    fulldata=False,
    show_errors=True,
    modelprefix="",
    returnjustfns=True,
):
    """
    Returns the results for (previously evaluated) network. deeplabcut.evaluate_network(..)
    Returns list of (per model): [trainingsiterations,trainfraction,shuffle,trainerror,testerror,pcutoff,trainerrorpcutoff,testerrorpcutoff,Snapshots[snapindex],scale,net_type]

    If fulldata=True, also returns (the complete annotation and prediction array)
    Returns list of: (DataMachine, Data, data, trainIndices, testIndices, trainFraction, DLCscorer,comparisonbodyparts, cfg, Snapshots[snapindex])
    ----------
    config : string
        Full path of the config.yaml file as a string.

    shuffle: integer
        integers specifying shuffle index of the training dataset. The default is 0.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml). This
        variable can also be set to "all".

    comparisonbodyparts: list of bodyparts, Default is "all".
        The average error will be computed for those body parts only (Has to be a subset of the body parts).

    rescale: bool, default False
        Evaluate the model at the 'global_scale' variable (as set in the test/pose_config.yaml file for a particular project). I.e. every
        image will be resized according to that scale and prediction will be compared to the resized ground truth. The error will be reported
        in pixels at rescaled to the *original* size. I.e. For a [200,200] pixel image evaluated at global_scale=.5, the predictions are calculated
        on [100,100] pixel images, compared to 1/2*ground truth and this error is then multiplied by 2!. The evaluation images are also shown for the
        original size!

    Examples
    --------
    If you do not want to plot
    >>> deeplabcut._evaluate_network_data('/analysis/project/reaching-task/config.yaml', shuffle=[1])
    --------
    If you want to plot
    >>> deeplabcut.evaluate_network('/analysis/project/reaching-task/config.yaml',shuffle=[1],plotting=True)
    """

    import os

    from deeplabcut.pose_estimation_tensorflow.config import load_config
    from deeplabcut.utils import auxiliaryfunctions

    start_path = os.getcwd()
    # Read file path for pose_config file. >> pass it on
    cfg = auxiliaryfunctions.read_config(config)

    # Loading human annotatated data
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    # Data=pd.read_hdf(os.path.join(cfg["project_path"],str(trainingsetfolder),'CollectedData_' + cfg["scorer"] + '.h5'),'df_with_missing')

    # Get list of body parts to evaluate network for
    comparisonbodyparts = (
        auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
            cfg, comparisonbodyparts
        )
    )
    ##################################################
    # Load data...
    ##################################################
    trainFraction = cfg["TrainingFraction"][trainingsetindex]
    modelfolder = os.path.join(
        cfg["project_path"],
        str(
            auxiliaryfunctions.get_model_folder(
                trainFraction, shuffle, cfg, modelprefix=modelprefix
            )
        ),
    )
    path_train_config, path_test_config, _ = return_train_network_path(
        config=config,
        shuffle=shuffle,
        trainingsetindex=trainingsetindex,
        modelprefix=modelprefix,
    )

    try:
        test_pose_cfg = load_config(str(path_test_config))
    except FileNotFoundError:
        raise FileNotFoundError(
            "It seems the model for shuffle %s and trainFraction %s does not exist."
            % (shuffle, trainFraction)
        )

    train_pose_cfg = load_config(str(path_train_config))
    # Load meta data
    data, trainIndices, testIndices, _ = auxiliaryfunctions.load_metadata(
        Path(cfg["project_path"]) / train_pose_cfg["metadataset"],
    )

    ########################### RESCALING (to global scale)
    if rescale == True:
        scale = test_pose_cfg["global_scale"]
        print("Rescaling Data to ", scale)
        Data = (
            pd.read_hdf(
                os.path.join(
                    cfg["project_path"],
                    str(trainingsetfolder),
                    "CollectedData_" + cfg["scorer"] + ".h5",
                )
            )
            * scale
        )
    else:
        scale = 1
        Data = pd.read_hdf(
            os.path.join(
                cfg["project_path"],
                str(trainingsetfolder),
                "CollectedData_" + cfg["scorer"] + ".h5",
            )
        )

    evaluationfolder = os.path.join(
        cfg["project_path"],
        str(
            auxiliaryfunctions.get_evaluation_folder(
                trainFraction, shuffle, cfg, modelprefix=modelprefix
            )
        ),
    )

    Snapshots = auxiliaryfunctions.get_snapshots_from_folder(
        train_folder=Path(modelfolder) / "train",
    )

    if Snapindex is None:
        Snapindex = cfg["snapshotindex"]

    snapshot_names = get_snapshots_by_index(
        idx=Snapindex,
        available_snapshots=Snapshots,
    )

    DATA = []
    results = []
    resultsfns = []
    for snapshot_name in snapshot_names:
        test_pose_cfg["init_weights"] = os.path.join(
            str(modelfolder), "train", snapshot_name
        )  # setting weights to corresponding snapshot.
        trainingsiterations = (test_pose_cfg["init_weights"].split(os.sep)[-1]).split(
            "-"
        )[
            -1
        ]  # read how many training siterations that corresponds to.

        # name for deeplabcut net (based on its parameters)
        DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
            cfg, shuffle, trainFraction, trainingsiterations, modelprefix=modelprefix
        )
        if not returnjustfns:
            print(
                "Retrieving ",
                DLCscorer,
                " with # of trainingiterations:",
                trainingsiterations,
            )

        (
            notanalyzed,
            resultsfilename,
            DLCscorer,
        ) = auxiliaryfunctions.check_if_not_evaluated(
            str(evaluationfolder), DLCscorer, DLCscorerlegacy, snapshot_name
        )
        # resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')
        print(resultsfilename)
        resultsfns.append(resultsfilename)
        if not returnjustfns:
            if not notanalyzed and os.path.isfile(resultsfilename):  # data exists..
                DataMachine = pd.read_hdf(resultsfilename)
                DataCombined = pd.concat([Data.T, DataMachine.T], axis=0).T
                RMSE, RMSEpcutoff = pairwisedistances(
                    DataCombined,
                    cfg["scorer"],
                    DLCscorer,
                    cfg["pcutoff"],
                    comparisonbodyparts,
                )

                testerror = np.nanmean(RMSE.iloc[testIndices].values.flatten())
                trainerror = np.nanmean(RMSE.iloc[trainIndices].values.flatten())
                testerrorpcutoff = np.nanmean(
                    RMSEpcutoff.iloc[testIndices].values.flatten()
                )
                trainerrorpcutoff = np.nanmean(
                    RMSEpcutoff.iloc[trainIndices].values.flatten()
                )
                if show_errors == True:
                    print(
                        "Results for",
                        trainingsiterations,
                        " training iterations:",
                        int(100 * trainFraction),
                        shuffle,
                        "train error:",
                        np.round(trainerror, 2),
                        "pixels. Test error:",
                        np.round(testerror, 2),
                        " pixels.",
                    )
                    print(
                        "With pcutoff of",
                        cfg["pcutoff"],
                        " train error:",
                        np.round(trainerrorpcutoff, 2),
                        "pixels. Test error:",
                        np.round(testerrorpcutoff, 2),
                        "pixels",
                    )
                    print("Snapshot", snapshot_name)

                r = [
                    trainingsiterations,
                    int(100 * trainFraction),
                    shuffle,
                    np.round(trainerror, 2),
                    np.round(testerror, 2),
                    cfg["pcutoff"],
                    np.round(trainerrorpcutoff, 2),
                    np.round(testerrorpcutoff, 2),
                    snapshot_name,
                    scale,
                    test_pose_cfg["net_type"],
                ]
                results.append(r)
            else:
                print("Model not trained/evaluated!")
            if fulldata == True:
                DATA.append(
                    [
                        DataMachine,
                        Data,
                        data,
                        trainIndices,
                        testIndices,
                        trainFraction,
                        DLCscorer,
                        comparisonbodyparts,
                        cfg,
                        evaluationfolder,
                        snapshot_name,
                    ]
                )

    os.chdir(start_path)
    if returnjustfns:
        return resultsfns
    else:
        if fulldata == True:
            return DATA, results
        else:
            return results


def keypoint_error(
    df_error: pd.DataFrame,
    df_error_p_cutoff: pd.DataFrame,
    train_indices: List[int],
    test_indices: List[int],
) -> pd.DataFrame:
    """Computes the RMSE error for each bodypart

    The error dataframes can be in single animal format (non-hierarchical columns, one
    column for each bodypart) or multi-animal format (hierarchical columns with 3
    levels: "scorer", "individuals", "bodyparts").

    Args:
        df_error: dataframe containing the RMSE error for each image, individual and
            bodypart
        df_error_p_cutoff: dataframe containing the RMSE error with p-cutoff for each
            image, individual and bodypart
        train_indices: the indices of rows in the dataframe that are in the train set
        test_indices: the indices of rows in the dataframe that are in the test set

    Returns:
        A dataframe containing 4 rows (train and test error, with and without p-cutoff)
        and one column for each bodypart.
    """
    df_error = df_error.copy()
    df_error_p_cutoff = df_error_p_cutoff.copy()

    error_rows = []
    for row_name, df in [
        ("Train error (px)", df_error.iloc[train_indices, :]),
        ("Test error (px)", df_error.iloc[test_indices, :]),
        ("Train error (px) with p-cutoff", df_error_p_cutoff.iloc[train_indices, :]),
        ("Test error (px) with p-cutoff", df_error_p_cutoff.iloc[test_indices, :]),
    ]:
        df_flat = df.copy()
        if isinstance(df.columns, pd.MultiIndex):
            # MA projects have column indices "scorer", "individuals" and "bodyparts"
            # Drop the scorer level, and put individuals in rows
            df_flat = df.droplevel("scorer", axis=1).stack(level="individuals").copy()

        bodypart_error = df_flat.mean()
        bodypart_error["Error Type"] = row_name
        error_rows.append(bodypart_error)

    # The error rows are series; stack in axis 1 and pivot to get DF
    keypoint_error_df = pd.concat(error_rows, axis=1)
    return keypoint_error_df.T.set_index("Error Type")


def evaluate_network(
    config,
    Shuffles=[1],
    trainingsetindex=0,
    plotting=False,
    show_errors=True,
    comparisonbodyparts="all",
    gputouse=None,
    rescale=False,
    modelprefix="",
    per_keypoint_evaluation: bool = False,
    snapshots_to_evaluate: List[str] = None,
):
    """Evaluates the network.

    Evaluates the network based on the saved models at different stages of the training
    network. The evaluation results are stored in the .h5 and .csv file under the
    subdirectory 'evaluation_results'. Change the snapshotindex parameter in the config
    file to 'all' in order to evaluate all the saved models.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file.

    Shuffles: list, optional, default=[1]
        List of integers specifying the shuffle indices of the training dataset.

    trainingsetindex: int or str, optional, default=0
        Integer specifying which "TrainingsetFraction" to use.
        Note that "TrainingFraction" is a list in config.yaml. This variable can also
        be set to "all".

    plotting: bool or str, optional, default=False
        Plots the predictions on the train and test images.
        If provided it must be either ``True``, ``False``, ``"bodypart"``, or
        ``"individual"``. Setting to ``True`` defaults as ``"bodypart"`` for
        multi-animal projects.

    show_errors: bool, optional, default=True
        Display train and test errors.

    comparisonbodyparts: str or list, optional, default="all"
        The average error will be computed for those body parts only.
        The provided list has to be a subset of the defined body parts.

    gputouse: int or None, optional, default=None
        Indicates the GPU to use (see number in ``nvidia-smi``). If you do not have a
        GPU put `None``.
        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    rescale: bool, optional, default=False
        Evaluate the model at the ``'global_scale'`` variable (as set in the
        ``pose_config.yaml`` file for a particular project). I.e. every image will be
        resized according to that scale and prediction will be compared to the resized
        ground truth. The error will be reported in pixels at rescaled to the
        *original* size. I.e. For a [200,200] pixel image evaluated at
        ``global_scale=.5``, the predictions are calculated on [100,100] pixel images,
        compared to 1/2*ground truth and this error is then multiplied by 2!.
        The evaluation images are also shown for the original size!

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    per_keypoint_evaluation: bool, default=False
        Compute the train and test RMSE for each keypoint, and save the results to
        a {model_name}-keypoint-results.csv in the evalution-results folder

    snapshots_to_evaluate: List[str], optional, default=None
        List of snapshot names to evaluate (e.g. ["snapshot-50000", "snapshot-75000", ...])

    Returns
    -------
    None

    Examples
    --------
    If you do not want to plot and evaluate with shuffle set to 1.

    >>> deeplabcut.evaluate_network(
            '/analysis/project/reaching-task/config.yaml', Shuffles=[1],
        )

    If you want to plot and evaluate with shuffle set to 0 and 1.

    >>> deeplabcut.evaluate_network(
            '/analysis/project/reaching-task/config.yaml',
            Shuffles=[0, 1],
            plotting=True,
        )

    If you want to plot assemblies for a maDLC project

    >>> deeplabcut.evaluate_network(
            '/analysis/project/reaching-task/config.yaml',
            Shuffles=[1],
            plotting="individual",
        )

    Note: This defaults to standard plotting for single-animal projects.
    """
    if plotting not in (True, False, "bodypart", "individual"):
        raise ValueError(f"Unknown value for `plotting`={plotting}")

    import os

    start_path = os.getcwd()
    from deeplabcut.utils import auxiliaryfunctions

    cfg = auxiliaryfunctions.read_config(config)

    if cfg.get("multianimalproject", False):
        from .evaluate_multianimal import evaluate_multianimal_full

        # TODO: Make this code not so redundant!
        evaluate_multianimal_full(
            config=config,
            Shuffles=Shuffles,
            trainingsetindex=trainingsetindex,
            plotting=plotting,
            comparisonbodyparts=comparisonbodyparts,
            gputouse=gputouse,
            modelprefix=modelprefix,
            per_keypoint_evaluation=per_keypoint_evaluation,
            snapshots_to_evaluate=snapshots_to_evaluate,
        )
    else:
        from deeplabcut.utils.auxfun_videos import imread, imresize
        from deeplabcut.pose_estimation_tensorflow.core import predict
        from deeplabcut.pose_estimation_tensorflow.config import load_config
        from deeplabcut.pose_estimation_tensorflow.datasets.utils import data_to_input
        from deeplabcut.utils import auxiliaryfunctions, conversioncode
        import tensorflow as tf

        # If a string was passed in, auto-convert to True for backward compatibility
        plotting = bool(plotting)

        if "TF_CUDNN_USE_AUTOTUNE" in os.environ:
            del os.environ[
                "TF_CUDNN_USE_AUTOTUNE"
            ]  # was potentially set during training

        tf.compat.v1.reset_default_graph()
        os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"  #
        #    tf.logging.set_verbosity(tf.logging.WARN)

        start_path = os.getcwd()
        # Read file path for pose_config file. >> pass it on
        cfg = auxiliaryfunctions.read_config(config)
        if gputouse is not None:  # gpu selectinon
            os.environ["CUDA_VISIBLE_DEVICES"] = str(gputouse)

        if trainingsetindex == "all":
            TrainingFractions = cfg["TrainingFraction"]
        else:
            if 0 <= trainingsetindex < len(cfg["TrainingFraction"]):
                TrainingFractions = [cfg["TrainingFraction"][int(trainingsetindex)]]
            else:
                raise Exception(
                    "Please check the trainingsetindex! ",
                    trainingsetindex,
                    " should be an integer from 0 .. ",
                    int(len(cfg["TrainingFraction"]) - 1),
                )

        # Loading human annotatated data
        trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
        Data = pd.read_hdf(
            os.path.join(
                cfg["project_path"],
                str(trainingsetfolder),
                "CollectedData_" + cfg["scorer"] + ".h5",
            )
        )

        # Get list of body parts to evaluate network for
        comparisonbodyparts = (
            auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
                cfg, comparisonbodyparts
            )
        )
        # Make folder for evaluation
        auxiliaryfunctions.attempt_to_make_folder(
            str(cfg["project_path"] + "/evaluation-results/")
        )
        for shuffle in Shuffles:
            for trainFraction in TrainingFractions:
                ##################################################
                # Load and setup CNN part detector
                ##################################################

                modelfolder_rel_path = auxiliaryfunctions.get_model_folder(
                    trainFraction, shuffle, cfg, modelprefix=modelprefix
                )
                modelfolder = Path(cfg["project_path"]) / modelfolder_rel_path

                # TODO: Unlike using create_training_dataset() If create_training_model_comparison() is used there won't
                #  necessarily be training fractions for every shuffle which will raise the FileNotFoundError..
                #  Not sure if this should throw an exception or just be a warning...
                if not modelfolder.exists():
                    raise FileNotFoundError(
                        f"Model with shuffle {shuffle} and trainFraction {trainFraction} does not exist."
                    )

                if trainingsetindex == "all":
                    train_frac_idx = cfg["TrainingFraction"].index(trainFraction)
                else:
                    train_frac_idx = trainingsetindex

                path_train_config, path_test_config, _ = return_train_network_path(
                    config=config,
                    shuffle=shuffle,
                    trainingsetindex=train_frac_idx,
                    modelprefix=modelprefix,
                )

                test_pose_cfg = load_config(str(path_test_config))
                train_pose_cfg = load_config(str(path_train_config))
                # Load meta data
                _, trainIndices, testIndices, _ = auxiliaryfunctions.load_metadata(
                    Path(cfg["project_path"], train_pose_cfg["metadataset"])
                )

                # change batch size, if it was edited during analysis!
                test_pose_cfg["batch_size"] = 1  # in case this was edited for analysis.

                # Create folder structure to store results.
                evaluationfolder = os.path.join(
                    cfg["project_path"],
                    str(
                        auxiliaryfunctions.get_evaluation_folder(
                            trainFraction, shuffle, cfg, modelprefix=modelprefix
                        )
                    ),
                )
                auxiliaryfunctions.attempt_to_make_folder(
                    evaluationfolder, recursive=True
                )

                Snapshots = auxiliaryfunctions.get_snapshots_from_folder(
                    train_folder=Path(modelfolder) / "train",
                )

                if snapshots_to_evaluate is not None:
                    snapshot_names = get_available_requested_snapshots(
                        requested_snapshots=snapshots_to_evaluate,
                        available_snapshots=Snapshots,
                    )
                else:
                    snapshot_names = get_snapshots_by_index(
                        idx=cfg["snapshotindex"],
                        available_snapshots=Snapshots,
                    )

                final_result = []

                ########################### RESCALING (to global scale)
                if rescale:
                    scale = test_pose_cfg["global_scale"]
                    Data = (
                        pd.read_hdf(
                            os.path.join(
                                cfg["project_path"],
                                str(trainingsetfolder),
                                "CollectedData_" + cfg["scorer"] + ".h5",
                            )
                        )
                        * scale
                    )
                else:
                    scale = 1

                conversioncode.guarantee_multiindex_rows(Data)
                ##################################################
                # Compute predictions over images
                ##################################################
                for snapshot_name in snapshot_names:
                    test_pose_cfg["init_weights"] = os.path.join(
                        str(modelfolder), "train", snapshot_name
                    )  # setting weights to corresponding snapshot.
                    training_iterations = int(snapshot_name.split("-")[-1])

                    # Name for deeplabcut net (based on its parameters)
                    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
                        cfg,
                        shuffle,
                        trainFraction,
                        trainingsiterations=training_iterations,
                        modelprefix=modelprefix,
                    )
                    print(
                        "Running ",
                        DLCscorer,
                        " with # of training iterations:",
                        training_iterations,
                    )
                    (
                        notanalyzed,
                        resultsfilename,
                        DLCscorer,
                    ) = auxiliaryfunctions.check_if_not_evaluated(
                        str(evaluationfolder),
                        DLCscorer,
                        DLCscorerlegacy,
                        snapshot_name,
                    )
                    if notanalyzed:
                        # Specifying state of model (snapshot / training state)
                        sess, inputs, outputs = predict.setup_pose_prediction(
                            test_pose_cfg
                        )
                        Numimages = len(Data.index)
                        PredicteData = np.zeros(
                            (Numimages, 3 * len(test_pose_cfg["all_joints_names"]))
                        )
                        print("Running evaluation ...")
                        for imageindex, imagename in tqdm(enumerate(Data.index)):
                            image = imread(
                                os.path.join(cfg["project_path"], *imagename),
                                mode="skimage",
                            )
                            if scale != 1:
                                image = imresize(image, scale)

                            image_batch = data_to_input(image)
                            # Compute prediction with the CNN
                            outputs_np = sess.run(
                                outputs, feed_dict={inputs: image_batch}
                            )
                            scmap, locref = predict.extract_cnn_output(
                                outputs_np, test_pose_cfg
                            )

                            # Extract maximum scoring location from the heatmap, assume 1 person
                            pose = predict.argmax_pose_predict(
                                scmap, locref, test_pose_cfg["stride"]
                            )
                            PredicteData[imageindex, :] = (
                                pose.flatten()
                            )  # NOTE: thereby     cfg_test['all_joints_names'] should be same order as bodyparts!

                        sess.close()  # closes the current tf session

                        index = pd.MultiIndex.from_product(
                            [
                                [DLCscorer],
                                test_pose_cfg["all_joints_names"],
                                ["x", "y", "likelihood"],
                            ],
                            names=["scorer", "bodyparts", "coords"],
                        )

                        # Saving results
                        DataMachine = pd.DataFrame(
                            PredicteData, columns=index, index=Data.index
                        )
                        DataMachine.to_hdf(resultsfilename, "df_with_missing")

                        print(
                            "Analysis is done and the results are stored (see evaluation-results) for snapshot: ",
                            snapshot_name,
                        )
                        DataCombined = pd.concat(
                            [Data.T, DataMachine.T], axis=0, sort=False
                        ).T

                        RMSE, RMSEpcutoff = pairwisedistances(
                            DataCombined,
                            cfg["scorer"],
                            DLCscorer,
                            cfg["pcutoff"],
                            comparisonbodyparts,
                        )
                        testerror = np.nanmean(RMSE.iloc[testIndices].values.flatten())
                        trainerror = np.nanmean(
                            RMSE.iloc[trainIndices].values.flatten()
                        )
                        testerrorpcutoff = np.nanmean(
                            RMSEpcutoff.iloc[testIndices].values.flatten()
                        )
                        trainerrorpcutoff = np.nanmean(
                            RMSEpcutoff.iloc[trainIndices].values.flatten()
                        )
                        results = [
                            training_iterations,
                            int(100 * trainFraction),
                            shuffle,
                            np.round(trainerror, 2),
                            np.round(testerror, 2),
                            cfg["pcutoff"],
                            np.round(trainerrorpcutoff, 2),
                            np.round(testerrorpcutoff, 2),
                        ]
                        final_result.append(results)

                        if per_keypoint_evaluation:
                            df_keypoint_error = keypoint_error(
                                RMSE, RMSEpcutoff, trainIndices, testIndices
                            )
                            kpt_filename = DLCscorer + "-keypoint-results.csv"
                            df_keypoint_error.to_csv(
                                Path(evaluationfolder) / kpt_filename
                            )

                        if show_errors:
                            print(
                                "Results for",
                                training_iterations,
                                " training iterations:",
                                int(100 * trainFraction),
                                shuffle,
                                "train error:",
                                np.round(trainerror, 2),
                                "pixels. Test error:",
                                np.round(testerror, 2),
                                " pixels.",
                            )
                            print(
                                "With pcutoff of",
                                cfg["pcutoff"],
                                " train error:",
                                np.round(trainerrorpcutoff, 2),
                                "pixels. Test error:",
                                np.round(testerrorpcutoff, 2),
                                "pixels",
                            )
                            if scale != 1:
                                print(
                                    "The predictions have been calculated for rescaled images (and rescaled ground truth). Scale:",
                                    scale,
                                )
                            print(
                                "Thereby, the errors are given by the average distances between the labels by DLC and the scorer."
                            )

                        if plotting:
                            print("Plotting...")
                            foldername = os.path.join(
                                str(evaluationfolder),
                                "LabeledImages_" + DLCscorer + "_" + snapshot_name,
                            )
                            auxiliaryfunctions.attempt_to_make_folder(foldername)
                            Plotting(
                                cfg,
                                comparisonbodyparts,
                                DLCscorer,
                                trainIndices,
                                DataCombined * 1.0 / scale,
                                foldername,
                            )  # Rescaling coordinates to have figure in original size!

                        tf.compat.v1.reset_default_graph()
                        # print(final_result)
                    else:
                        DataMachine = pd.read_hdf(resultsfilename)
                        conversioncode.guarantee_multiindex_rows(DataMachine)
                        if plotting:
                            DataCombined = pd.concat(
                                [Data.T, DataMachine.T], axis=0, sort=False
                            ).T
                            foldername = os.path.join(
                                str(evaluationfolder),
                                "LabeledImages_" + DLCscorer + "_" + snapshot_name,
                            )
                            if not os.path.exists(foldername):
                                print(
                                    "Plotting...(attention scale might be inconsistent in comparison to when data was analyzed; i.e. if you used rescale)"
                                )
                                auxiliaryfunctions.attempt_to_make_folder(foldername)
                                Plotting(
                                    cfg,
                                    comparisonbodyparts,
                                    DLCscorer,
                                    trainIndices,
                                    DataCombined * 1.0 / scale,
                                    foldername,
                                )
                            else:
                                print(
                                    "Plots already exist for this snapshot... Skipping to the next one."
                                )

                if len(final_result) > 0:  # Only append if results were calculated
                    make_results_file(final_result, evaluationfolder, DLCscorer)
                    print(
                        "The network is evaluated and the results are stored in the subdirectory 'evaluation_results'."
                    )
                    print(
                        "Please check the results, then choose the best model (snapshot) for prediction. You can update the config.yaml file with the appropriate index for the 'snapshotindex'.\nUse the function 'analyze_video' to make predictions on new videos."
                    )
                    print(
                        "Otherwise, consider adding more labeled-data and retraining the network (see DeepLabCut workflow Fig 2, Nath 2019)"
                    )

    # returning to initial folder
    os.chdir(str(start_path))


def make_results_file(final_result, evaluationfolder, DLCscorer):
    """
    Makes result file in csv format and saves under evaluation_results directory.
    If the file exists (typically, when the network has already been evaluated),
    newer results are appended to it.
    """
    col_names = [
        "Training iterations:",
        "%Training dataset",
        "Shuffle number",
        " Train error(px)",
        " Test error(px)",
        "p-cutoff used",
        "Train error with p-cutoff",
        "Test error with p-cutoff",
    ]
    df = pd.DataFrame(final_result, columns=col_names)
    output_path = os.path.join(str(evaluationfolder), DLCscorer + "-results.csv")
    if os.path.exists(output_path):
        temp = pd.read_csv(output_path, index_col=0)
        df = pd.concat((temp, df)).reset_index(drop=True)

    df.to_csv(output_path)

    ## Also storing one "large" table with results:
    # note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.
    df = pd.DataFrame(final_result, columns=col_names)
    output_path = os.path.join(
        str(Path(evaluationfolder).parents[0]), "CombinedEvaluation-results.csv"
    )
    if os.path.exists(output_path):
        temp = pd.read_csv(output_path, index_col=0)
        df = pd.concat((temp, df)).reset_index(drop=True)

    df.to_csv(output_path)


def get_available_requested_snapshots(
    requested_snapshots: List[str],
    available_snapshots: List[str],
) -> List[str]:
    """
    Intersects the requested snapshot names with the available snapshots.

    Returns: snapshot names
    """
    snapshot_names = []
    missing_snapshots = []
    for snap in requested_snapshots:
        if snap in available_snapshots:
            snapshot_names.append(snap)
        else:
            missing_snapshots.append(snap)

    if len(snapshot_names) == 0:
        raise ValueError(
            f"None of the requested snapshots were found: \n{missing_snapshots}"
        )
    elif len(missing_snapshots) > 0:
        print(
            f"The following requested snapshots were not found and will be skipped:\n"
            f"{missing_snapshots}"
        )

    return snapshot_names


def get_snapshots_by_index(
    idx: Union[int, str],
    available_snapshots: List[str],
) -> List[str]:
    """
    Assume available_snapshots is ordered in ascending order. Returns snapshot names.
    """
    if isinstance(idx, int) and -len(available_snapshots) <= idx < len(
        available_snapshots
    ):
        return [available_snapshots[idx]]
    elif idx == "all":
        return available_snapshots

    raise IndexError(
        f"Invalid index: {idx}. The index should be an int less than the number of "
        f"available snapshots, negative indexing is supported. The keyword 'all' "
        f"is also a valid option."
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config")
    cli_args = parser.parse_args()


--- File: deeplabcut/pose_estimation_tensorflow/core/evaluate_multianimal.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import imgaug.augmenters as iaa
import os
import pickle
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm import tqdm
from typing import List

from deeplabcut.core import crossvalutils
from deeplabcut.core.crossvalutils import find_closest_neighbors
from deeplabcut.pose_estimation_tensorflow.core.evaluate import (
    make_results_file,
    keypoint_error,
    get_available_requested_snapshots,
    get_snapshots_by_index,
)
from deeplabcut.pose_estimation_tensorflow.training import return_train_network_path
from deeplabcut.pose_estimation_tensorflow.config import load_config
from deeplabcut.utils import visualization


def _percentile(n):
    def percentile_(x):
        return x.quantile(n)

    percentile_.__name__ = f"percentile_{100 * n:.0f}"
    return percentile_


def _compute_stats(df):
    return df.agg(
        [
            "min",
            "max",
            "mean",
            np.std,
            _percentile(0.25),
            _percentile(0.50),
            _percentile(0.75),
        ]
    ).stack(level=1)


def _calc_prediction_error(data):
    _ = data.pop("metadata", None)
    dists = []
    for n, dict_ in enumerate(tqdm(data.values())):
        gt = np.concatenate(dict_["groundtruth"][1])
        xy = np.concatenate(dict_["prediction"]["coordinates"][0])
        p = np.concatenate(dict_["prediction"]["confidence"])
        neighbors = find_closest_neighbors(gt, xy)
        found = neighbors != -1
        gt2 = gt[found]
        xy2 = xy[neighbors[found]]
        dists.append(np.c_[np.linalg.norm(gt2 - xy2, axis=1), p[neighbors[found]]])
    return dists


def _calc_train_test_error(data, metadata, pcutoff=0.3):
    train_inds = set(metadata["data"]["trainIndices"])
    dists = _calc_prediction_error(data)
    dists_train, dists_test = [], []
    for n, dist in enumerate(dists):
        if n in train_inds:
            dists_train.append(dist)
        else:
            dists_test.append(dist)
    dists_train = np.concatenate(dists_train)
    dists_test = np.concatenate(dists_test)
    error_train = np.nanmean(dists_train[:, 0])
    error_train_cut = np.nanmean(dists_train[dists_train[:, 1] >= pcutoff, 0])
    error_test = np.nanmean(dists_test[:, 0])
    error_test_cut = np.nanmean(dists_test[dists_test[:, 1] >= pcutoff, 0])
    return error_train, error_test, error_train_cut, error_test_cut


def evaluate_multianimal_full(
    config,
    Shuffles=[1],
    trainingsetindex=0,
    plotting=False,
    show_errors=True,
    comparisonbodyparts="all",
    gputouse=None,
    modelprefix="",
    per_keypoint_evaluation: bool = False,
    snapshots_to_evaluate: List[str] = None,
):
    from deeplabcut.pose_estimation_tensorflow.core import (
        predict,
        predict_multianimal as predictma,
    )
    from deeplabcut.utils import (
        auxiliaryfunctions,
        auxfun_multianimal,
        auxfun_videos,
        conversioncode,
    )

    import tensorflow as tf

    if "TF_CUDNN_USE_AUTOTUNE" in os.environ:
        del os.environ["TF_CUDNN_USE_AUTOTUNE"]  # was potentially set during training

    tf.compat.v1.reset_default_graph()
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"  #
    if gputouse is not None:  # gpu selectinon
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gputouse)

    start_path = os.getcwd()

    if plotting is True:
        plotting = "bodypart"

    ##################################################
    # Load data...
    ##################################################
    cfg = auxiliaryfunctions.read_config(config)
    if trainingsetindex == "all":
        TrainingFractions = cfg["TrainingFraction"]
    else:
        TrainingFractions = [cfg["TrainingFraction"][trainingsetindex]]

    # Loading human annotatated data
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    Data = pd.read_hdf(
        os.path.join(
            cfg["project_path"],
            str(trainingsetfolder),
            "CollectedData_" + cfg["scorer"] + ".h5",
        )
    )
    conversioncode.guarantee_multiindex_rows(Data)

    # Get list of body parts to evaluate network for
    comparisonbodyparts = (
        auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
            cfg, comparisonbodyparts
        )
    )
    all_bpts = np.asarray(
        len(cfg["individuals"]) * cfg["multianimalbodyparts"] + cfg["uniquebodyparts"]
    )
    colors = visualization.get_cmap(len(comparisonbodyparts), name=cfg["colormap"])
    # Make folder for evaluation
    auxiliaryfunctions.attempt_to_make_folder(
        str(cfg["project_path"] + "/evaluation-results/")
    )
    for shuffle in Shuffles:
        for trainFraction in TrainingFractions:
            ##################################################
            # Load and setup CNN part detector
            ##################################################
            modelfolder_rel_path = auxiliaryfunctions.get_model_folder(
                trainFraction, shuffle, cfg, modelprefix=modelprefix
            )
            modelfolder = Path(cfg["project_path"]) / modelfolder_rel_path

            # TODO: Unlike using create_training_dataset() If create_training_model_comparison() is used there won't
            #  necessarily be training fractions for every shuffle which will raise the FileNotFoundError..
            #  Not sure if this should throw an exception or just be a warning...
            if not modelfolder.exists():
                raise FileNotFoundError(
                    f"Model with shuffle {shuffle} and trainFraction {trainFraction} does not exist."
                )

            if trainingsetindex == "all":
                train_frac_idx = cfg["TrainingFraction"].index(trainFraction)
            else:
                train_frac_idx = trainingsetindex

            path_train_config, path_test_config, _ = return_train_network_path(
                config=config,
                shuffle=shuffle,
                trainingsetindex=train_frac_idx,
                modelprefix=modelprefix,
            )

            test_pose_cfg = load_config(str(path_test_config))
            train_pose_cfg = load_config(str(path_train_config))
            # Load meta data
            _, trainIndices, testIndices, _ = auxiliaryfunctions.load_metadata(
                os.path.join(cfg["project_path"], train_pose_cfg["metadataset"])
            )

            pipeline = iaa.Sequential(random_order=False)
            pre_resize = test_pose_cfg.get("pre_resize")
            if pre_resize:
                width, height = pre_resize
                pipeline.add(iaa.Resize({"height": height, "width": width}))

            # TODO: IMPLEMENT for different batch sizes?
            test_pose_cfg["batch_size"] = 1  # due to differently sized images!!!

            stride = test_pose_cfg["stride"]
            # Ignore best edges possibly defined during a prior evaluation
            _ = test_pose_cfg.pop("paf_best", None)
            joints = test_pose_cfg["all_joints_names"]

            # Create folder structure to store results.
            evaluationfolder = os.path.join(
                cfg["project_path"],
                str(
                    auxiliaryfunctions.get_evaluation_folder(
                        trainFraction, shuffle, cfg, modelprefix=modelprefix
                    )
                ),
            )
            auxiliaryfunctions.attempt_to_make_folder(evaluationfolder, recursive=True)

            try:
                Snapshots = auxiliaryfunctions.get_snapshots_from_folder(
                    train_folder=Path(modelfolder) / "train",
                )
            except FileNotFoundError as e:
                print(e)
                continue

            if snapshots_to_evaluate is not None:
                snapshot_names = get_available_requested_snapshots(
                    requested_snapshots=snapshots_to_evaluate,
                    available_snapshots=Snapshots,
                )
            else:
                try:
                    snapshot_names = get_snapshots_by_index(
                        idx=cfg["snapshotindex"],
                        available_snapshots=Snapshots,
                    )
                except IndexError as err:
                    print(
                        "Failed to get snapshot_names for trainFraction="
                        f"{trainFraction} and shuffle={shuffle}. Error:"
                    )
                    print(err)
                    snapshot_names = []

            final_result = []
            ##################################################
            # Compute predictions over images
            ##################################################
            for snapshot_name in snapshot_names:
                test_pose_cfg["init_weights"] = os.path.join(
                    str(modelfolder), "train", snapshot_name
                )  # setting weights to corresponding snapshot.
                training_iterations = int(snapshot_name.split("-")[-1])

                # name for deeplabcut net (based on its parameters)
                DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
                    cfg,
                    shuffle,
                    trainFraction,
                    trainingsiterations=training_iterations,
                    modelprefix=modelprefix,
                )
                print(
                    "Running ",
                    DLCscorer,
                    " with # of trainingiterations:",
                    training_iterations,
                )
                (
                    notanalyzed,
                    resultsfilename,
                    DLCscorer,
                ) = auxiliaryfunctions.check_if_not_evaluated(
                    str(evaluationfolder),
                    DLCscorer,
                    DLCscorerlegacy,
                    snapshot_name,
                )

                data_path = resultsfilename.split(".h5")[0] + "_full.pickle"

                if plotting:
                    foldername = os.path.join(
                        str(evaluationfolder),
                        "LabeledImages_" + DLCscorer + "_" + snapshot_name,
                    )
                    auxiliaryfunctions.attempt_to_make_folder(foldername)
                    if plotting == "bodypart":
                        fig, ax = visualization.create_minimal_figure()

                if os.path.isfile(data_path):
                    print("Model already evaluated.", resultsfilename)
                else:
                    (
                        sess,
                        inputs,
                        outputs,
                    ) = predict.setup_pose_prediction(test_pose_cfg)

                    PredicteData = {}
                    predicted_poses = np.full((len(Data), len(all_bpts), 2), np.nan)
                    dist = np.full((len(Data), len(all_bpts)), np.nan)
                    conf = np.full_like(dist, np.nan)
                    print("Network Evaluation underway...")
                    for imageindex, imagename in tqdm(enumerate(Data.index)):
                        image_path = os.path.join(cfg["project_path"], *imagename)
                        frame = auxfun_videos.imread(image_path, mode="skimage")

                        GT = Data.iloc[imageindex]
                        if not GT.any():
                            continue

                        # Pass the image and the keypoints through the resizer;
                        # this has no effect if no augmenters were added to it.
                        keypoints = [GT.to_numpy().reshape((-1, 2)).astype(float)]
                        frame_, keypoints = pipeline(
                            images=[frame], keypoints=keypoints
                        )
                        frame = frame_[0]
                        GT[:] = keypoints[0].flatten()

                        df = GT.unstack("coords").reindex(joints, level="bodyparts")

                        # FIXME Is having an empty array vs nan really that necessary?!
                        groundtruthidentity = list(
                            df.index.get_level_values("individuals")
                            .to_numpy()
                            .reshape((-1, 1))
                        )
                        groundtruthcoordinates = list(df.values[:, np.newaxis])
                        for i, coords in enumerate(groundtruthcoordinates):
                            if np.isnan(coords).any():
                                groundtruthcoordinates[i] = np.empty(
                                    (0, 2), dtype=float
                                )
                                groundtruthidentity[i] = np.array([], dtype=str)

                        # Form 2D array of shape (n_rows, 4) where the last dim is
                        # (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.
                        temp = df.reset_index(level="bodyparts").dropna()
                        temp["bodyparts"].replace(
                            dict(zip(joints, range(len(joints)))),
                            inplace=True,
                        )
                        temp["sample"] = 0
                        peaks_gt = temp.loc[
                            :, ["sample", "y", "x", "bodyparts"]
                        ].to_numpy()
                        peaks_gt[:, 1:3] = (peaks_gt[:, 1:3] - stride // 2) / stride

                        pred = predictma.predict_batched_peaks_and_costs(
                            test_pose_cfg,
                            np.expand_dims(frame, axis=0),
                            sess,
                            inputs,
                            outputs,
                            peaks_gt.astype(int),
                        )

                        if not pred:
                            continue
                        else:
                            pred = pred[0]

                        PredicteData[imagename] = {}
                        PredicteData[imagename]["index"] = imageindex
                        PredicteData[imagename]["prediction"] = pred
                        PredicteData[imagename]["groundtruth"] = [
                            groundtruthidentity,
                            groundtruthcoordinates,
                            GT,
                        ]

                        coords_pred = pred["coordinates"][0]
                        probs_pred = pred["confidence"]
                        for bpt, xy_gt in df.groupby(level="bodyparts"):
                            inds_gt = np.flatnonzero(np.all(~np.isnan(xy_gt), axis=1))
                            n_joint = joints.index(bpt)
                            xy = coords_pred[n_joint]
                            if inds_gt.size and xy.size:
                                # Pick the predictions closest to ground truth,
                                # rather than the ones the model has most confident in
                                xy_gt_values = xy_gt.iloc[inds_gt].values
                                neighbors = find_closest_neighbors(
                                    xy_gt_values, xy, k=3
                                )
                                found = neighbors != -1
                                min_dists = np.linalg.norm(
                                    xy_gt_values[found] - xy[neighbors[found]],
                                    axis=1,
                                )
                                inds = np.flatnonzero(all_bpts == bpt)
                                sl = imageindex, inds[inds_gt[found]]
                                dist[sl] = min_dists
                                predicted_poses[sl] = xy[neighbors[found]]
                                conf[sl] = probs_pred[n_joint][
                                    neighbors[found]
                                ].squeeze()

                        if plotting == "bodypart":
                            temp_xy = GT.unstack("bodyparts")[joints].values
                            gt = temp_xy.reshape((-1, 2, temp_xy.shape[1])).T.swapaxes(
                                1, 2
                            )
                            h, w, _ = np.shape(frame)
                            fig.set_size_inches(w / 100, h / 100)
                            ax.set_xlim(0, w)
                            ax.set_ylim(0, h)
                            ax.invert_yaxis()
                            ax = visualization.make_multianimal_labeled_image(
                                frame,
                                gt,
                                coords_pred,
                                probs_pred,
                                colors,
                                cfg["dotsize"],
                                cfg["alphavalue"],
                                cfg["pcutoff"],
                                ax=ax,
                            )
                            visualization.save_labeled_frame(
                                fig,
                                image_path,
                                foldername,
                                imageindex in trainIndices,
                            )
                            visualization.erase_artists(ax)

                    sess.close()  # closes the current tf session

                    # Save predicted poses
                    coordinates = ["x", "y", "conf"]
                    # Create the new MultiIndex by repeating the existing index and adding the new level
                    poses_multi_index = pd.MultiIndex.from_tuples(
                        [(scorer, individual, bodypart, coordinate)
                         for scorer, individual, bodypart in df.index
                         for coordinate in coordinates],
                        names=df.index.names + ['coordinates']
                    )

                    predicted_poses = np.concatenate((predicted_poses, np.expand_dims(conf, axis=-1)), axis=-1)
                    predicted_poses = predicted_poses.reshape(predicted_poses.shape[0], -1)
                    df_predicted_poses = pd.DataFrame(predicted_poses, columns=poses_multi_index)
                    write_poses_path = os.path.join(
                        evaluationfolder, f"predicted_poses_{training_iterations}.h5"
                    )
                    df_predicted_poses.to_hdf(write_poses_path, key="df_with_missing")

                    # Compute all distance statistics
                    df_dist = pd.DataFrame(dist, columns=df.index)
                    df_conf = pd.DataFrame(conf, columns=df.index)
                    df_joint = pd.concat(
                        [df_dist, df_conf],
                        keys=["rmse", "conf"],
                        names=["metrics"],
                        axis=1,
                    )
                    df_joint = df_joint.reorder_levels(
                        list(np.roll(df_joint.columns.names, -1)), axis=1
                    )
                    df_joint.sort_index(
                        axis=1,
                        level=["individuals", "bodyparts"],
                        ascending=[True, True],
                        inplace=True,
                    )
                    write_path = os.path.join(
                        evaluationfolder, f"dist_{training_iterations}.csv"
                    )
                    df_joint.to_csv(write_path)

                    # Calculate overall prediction error
                    error = df_joint.xs("rmse", level="metrics", axis=1)
                    mask = (
                        df_joint.xs("conf", level="metrics", axis=1) >= cfg["pcutoff"]
                    )
                    error_masked = error[mask]
                    error_train = np.nanmean(error.iloc[trainIndices])
                    error_train_cut = np.nanmean(error_masked.iloc[trainIndices])
                    error_test = np.nanmean(error.iloc[testIndices])
                    error_test_cut = np.nanmean(error_masked.iloc[testIndices])
                    results = [
                        training_iterations,
                        int(100 * trainFraction),
                        shuffle,
                        np.round(error_train, 2),
                        np.round(error_test, 2),
                        cfg["pcutoff"],
                        np.round(error_train_cut, 2),
                        np.round(error_test_cut, 2),
                    ]
                    final_result.append(results)

                    if per_keypoint_evaluation:
                        df_keypoint_error = keypoint_error(
                            error,
                            error[mask],
                            trainIndices,
                            testIndices,
                        )
                        kpt_filename = DLCscorer + "-keypoint-results.csv"
                        df_keypoint_error.to_csv(Path(evaluationfolder) / kpt_filename)

                    if show_errors:
                        string = (
                            "Results for {} training iterations, training fraction of {}, and shuffle {}:\n"
                            "Train error: {} pixels. Test error: {} pixels.\n"
                            "With pcutoff of {}:\n"
                            "Train error: {} pixels. Test error: {} pixels."
                        )
                        print(string.format(*results))

                        print("##########################################")
                        print(
                            "Average Euclidean distance to GT per individual (in pixels; test-only)"
                        )
                        print(
                            error_masked.iloc[testIndices]
                            .groupby("individuals", axis=1)
                            .mean()
                            .mean()
                            .to_string()
                        )
                        print(
                            "Average Euclidean distance to GT per bodypart (in pixels; test-only)"
                        )
                        print(
                            error_masked.iloc[testIndices]
                            .groupby("bodyparts", axis=1)
                            .mean()
                            .mean()
                            .to_string()
                        )

                    PredicteData["metadata"] = {
                        "nms radius": test_pose_cfg["nmsradius"],
                        "minimal confidence": test_pose_cfg["minconfidence"],
                        "sigma": test_pose_cfg.get("sigma", 1),
                        "PAFgraph": test_pose_cfg["partaffinityfield_graph"],
                        "PAFinds": np.arange(
                            len(test_pose_cfg["partaffinityfield_graph"])
                        ),
                        "all_joints": [
                            [i] for i in range(len(test_pose_cfg["all_joints"]))
                        ],
                        "all_joints_names": [
                            test_pose_cfg["all_joints_names"][i]
                            for i in range(len(test_pose_cfg["all_joints"]))
                        ],
                        "stride": test_pose_cfg.get("stride", 8),
                    }
                    print(
                        "Done and results stored for snapshot: ",
                        snapshot_name,
                    )

                    dictionary = {
                        "Scorer": DLCscorer,
                        "DLC-model-config file": test_pose_cfg,
                        "trainIndices": trainIndices,
                        "testIndices": testIndices,
                        "trainFraction": trainFraction,
                    }
                    metadata = {"data": dictionary}
                    _ = auxfun_multianimal.SaveFullMultiAnimalData(
                        PredicteData, metadata, resultsfilename
                    )

                    tf.compat.v1.reset_default_graph()

                n_multibpts = len(cfg["multianimalbodyparts"])
                if n_multibpts == 1:
                    continue

                # Skip data-driven skeleton selection unless
                # the model was trained on the full graph.
                max_n_edges = n_multibpts * (n_multibpts - 1) // 2
                n_edges = len(test_pose_cfg["partaffinityfield_graph"])
                if n_edges == max_n_edges:
                    print("Selecting best skeleton...")
                    n_graphs = 10
                    paf_inds = None
                else:
                    n_graphs = 1
                    paf_inds = [list(range(n_edges))]
                (
                    results,
                    paf_scores,
                    best_assemblies,
                ) = crossvalutils.cross_validate_paf_graphs(
                    config,
                    str(path_test_config).replace("pose_", "inference_"),
                    data_path,
                    data_path.replace("_full.", "_meta."),
                    n_graphs=n_graphs,
                    paf_inds=paf_inds,
                    oks_sigma=test_pose_cfg.get("oks_sigma", 0.1),
                    margin=test_pose_cfg.get("bbox_margin", 0),
                    symmetric_kpts=test_pose_cfg.get("symmetric_kpts"),
                )
                if plotting == "individual":
                    assemblies, assemblies_unique, image_paths = best_assemblies
                    fig, ax = visualization.create_minimal_figure()
                    n_animals = len(cfg["individuals"])
                    if cfg["uniquebodyparts"]:
                        n_animals += 1
                    colors = visualization.get_cmap(n_animals, name=cfg["colormap"])
                    for k, v in tqdm(assemblies.items()):
                        imname = image_paths[k]
                        image_path = os.path.join(cfg["project_path"], *imname)
                        frame = auxfun_videos.imread(image_path, mode="skimage")

                        h, w, _ = np.shape(frame)
                        fig.set_size_inches(w / 100, h / 100)
                        ax.set_xlim(0, w)
                        ax.set_ylim(0, h)
                        ax.invert_yaxis()

                        gt = [
                            s.to_numpy().reshape((-1, 2))
                            for _, s in Data.loc[imname].groupby("individuals")
                        ]
                        coords_pred = []
                        coords_pred += [ass.xy for ass in v]
                        probs_pred = []
                        probs_pred += [ass.data[:, 2:3] for ass in v]
                        if assemblies_unique is not None:
                            unique = assemblies_unique.get(k, None)
                            if unique is not None:
                                coords_pred.append(unique[:, :2])
                                probs_pred.append(unique[:, 2:3])
                        while len(coords_pred) < len(gt):
                            coords_pred.append(np.full((1, 2), np.nan))
                            probs_pred.append(np.full((1, 2), np.nan))
                        ax = visualization.make_multianimal_labeled_image(
                            frame,
                            gt,
                            coords_pred,
                            probs_pred,
                            colors,
                            cfg["dotsize"],
                            cfg["alphavalue"],
                            cfg["pcutoff"],
                            ax=ax,
                        )
                        visualization.save_labeled_frame(
                            fig,
                            image_path,
                            foldername,
                            k in trainIndices,
                        )
                        visualization.erase_artists(ax)

                df = results[1].copy()
                df.loc(axis=0)[("mAP_train", "mean")] = [
                    d[0]["mAP"] for d in results[2]
                ]
                df.loc(axis=0)[("mAR_train", "mean")] = [
                    d[0]["mAR"] for d in results[2]
                ]
                df.loc(axis=0)[("mAP_test", "mean")] = [d[1]["mAP"] for d in results[2]]
                df.loc(axis=0)[("mAR_test", "mean")] = [d[1]["mAR"] for d in results[2]]
                with open(data_path.replace("_full.", "_map."), "wb") as file:
                    pickle.dump((df, paf_scores), file)

            if len(final_result) > 0:  # Only append if results were calculated
                make_results_file(final_result, evaluationfolder, DLCscorer)

    os.chdir(str(start_path))


# backwards compatibility
_find_closest_neighbors = find_closest_neighbors


--- File: deeplabcut/pose_estimation_tensorflow/core/openvino/session.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import subprocess

import numpy as np
from tqdm import tqdm
import cv2

try:
    from openvino.runtime import Core, AsyncInferQueue

    is_openvino_available = True
except ImportError:
    is_openvino_available = False


class OpenVINOSession:
    def __init__(self, cfg, device):
        self.core = Core()
        self.xml_path = cfg["init_weights"] + ".xml"
        self.device = device

        # Convert a frozen graph to OpenVINO IR format
        if not os.path.exists(self.xml_path):
            subprocess.run(
                [
                    "mo",
                    "--output_dir",
                    os.path.dirname(cfg["init_weights"]),
                    "--input_model",
                    cfg["init_weights"] + ".pb",
                    "--input_shape",
                    "[1, 747, 832, 3]",
                    "--extensions",
                    os.path.join(os.path.dirname(__file__), "mo_extensions"),
                    "--data_type",
                    "FP16",
                ],
                check=True,
            )

        # Read network into memory
        self.net = self.core.read_model(self.xml_path)
        self.input_name = self.net.inputs[0].get_any_name()
        self.output_name = self.net.outputs[0].get_any_name()
        self.infer_queue = None

    def _init_model(self, inp_h, inp_w):
        # For better efficiency, model is initialized for batch_size 1 and every sample processed independently
        inp_shape = [1, inp_h, inp_w, 3]
        self.net.reshape({self.input_name: inp_shape})

        # Load network to device
        if "CPU" in self.device:
            self.core.set_property(
                "CPU",
                {
                    "CPU_THROUGHPUT_STREAMS": "CPU_THROUGHPUT_AUTO",
                    "CPU_BIND_THREAD": "YES",
                },
            )
        if "GPU" in self.device:
            self.core.set_property(
                "GPU", {"GPU_THROUGHPUT_STREAMS": "GPU_THROUGHPUT_AUTO"}
            )

        compiled_model = self.core.compile_model(self.net, self.device)
        num_requests = compiled_model.get_property("OPTIMAL_NUMBER_OF_INFER_REQUESTS")
        print(f"OpenVINO uses {num_requests} inference requests")
        self.infer_queue = AsyncInferQueue(compiled_model, num_requests)

    def run(self, out_name, feed_dict):
        inp_name, inp = next(iter(feed_dict.items()))

        if self.infer_queue is None:
            self._init_model(inp.shape[1], inp.shape[2])

        batch_size = inp.shape[0]
        batch_output = np.zeros(
            [batch_size] + self.net.outputs[out_name].shape, dtype=np.float32
        )

        def completion_callback(request, inp_id):
            output = next(iter(request.results.values()))
            batch_output[out_id] = output

        self.infer_queue.set_callback(completion_callback)

        for inp_id in range(batch_size):
            self.infer_queue.start_async({inp_name: inp[inp_id : inp_id + 1]}, inp_id)

        self.infer_queue.wait_all()

        return batch_output.reshape(-1, 3)


def GetPoseF_OV(cfg, dlc_cfg, sess, inputs, outputs, cap, nframes, batchsize):
    """Prediction of pose"""
    PredictedData = np.zeros((nframes, 3 * len(dlc_cfg["all_joints_names"])))
    ny, nx = int(cap.get(4)), int(cap.get(3))
    if cfg["cropping"]:
        ny, nx = checkcropping(cfg, cap)

    sess._init_model(ny, nx)

    pbar = tqdm(total=nframes)
    counter = 0
    step = max(10, int(nframes / 100))

    def completion_callback(request, inp_id):
        pose = next(iter(request.results.values()))

        pose[:, [0, 1, 2]] = pose[:, [1, 0, 2]]  # change order to have x,y,confidence
        pose = np.reshape(pose, (1, -1))  # bring into batchsize times x,y,conf etc.
        PredictedData[inp_id] = pose

    sess.infer_queue.set_callback(completion_callback)

    while cap.isOpened():
        if counter % step == 0:
            pbar.update(step)
        ret, frame = cap.read()
        if not ret:
            break

        # Prepare input data
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        if cfg["cropping"]:
            frame = frame[cfg["y1"] : cfg["y2"], cfg["x1"] : cfg["x2"]]

        sess.infer_queue.start_async(
            {sess.input_name: np.expand_dims(frame, axis=0)}, counter
        )

        counter += 1

    sess.infer_queue.wait_all()

    pbar.close()
    return PredictedData, nframes


--- File: deeplabcut/pose_estimation_tensorflow/core/openvino/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_estimation_tensorflow/core/openvino/mo_extensions/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_estimation_tensorflow/core/openvino/mo_extensions/front/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_estimation_tensorflow/core/openvino/mo_extensions/front/tf/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_estimation_tensorflow/core/openvino/mo_extensions/front/tf/unravel_index.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
from openvino.tools.mo.front.common.replacement import FrontReplacementOp
from openvino.tools.mo.graph.graph import Graph, Node
from openvino.tools.mo.ops.const import Const
from openvino.tools.mo.ops.strided_slice import StridedSlice
from openvino.tools.mo.front.common.partial_infer.utils import int64_array
from openvino.tools.mo.ops.elementwise import FloorMod, Div
from openvino.tools.mo.ops.Cast import Cast
from openvino.tools.mo.ops.pack import PackOp


class UnravelIndex(FrontReplacementOp):
    op = "UnravelIndex"
    enabled = True

    def replace_op(self, graph: Graph, node: Node):
        inp0 = node.in_port(0).get_source().node
        inp1 = node.in_port(1).get_source().node

        begin_id = Const(graph, {"value": int64_array([1])}).create_node()
        end_id = Const(graph, {"value": int64_array([2])}).create_node()
        dim1 = StridedSlice(
            graph,
            dict(
                name=inp0.name + "/dim1",
                begin_mask=[1],
                end_mask=[1],
                shrink_axis_mask=[0],
                new_axis_mask=[0],
                ellipsis_mask=[0],
            ),
        ).create_node([inp1, begin_id, end_id])

        rows = Div(graph, dict(name=node.name + "/rows")).create_node([inp0, dim1])

        inp0 = Cast(
            graph, dict(name=inp0.name + "/fp32", dst_type=np.float32)
        ).create_node([inp0])
        dim1 = Cast(
            graph, dict(name=dim1.name + "/fp32", dst_type=np.float32)
        ).create_node([dim1])
        cols = FloorMod(graph, dict(name=node.name + "/cols")).create_node([inp0, dim1])
        cols = Cast(
            graph, dict(name=cols.name + "/i64", dst_type=np.int64)
        ).create_node([cols])

        concat = PackOp(graph, dict(name=node.name + "/merged", axis=0)).create_node(
            [rows, cols]
        )
        return [concat.id]


--- File: deeplabcut/pose_estimation_tensorflow/util/logging.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Adapted from DeeperCut by Eldar Insafutdinov
https://github.com/eldar/pose-tensorflow
"""
import logging
import os


def setup_logging():
    FORMAT = "%(asctime)-15s %(message)s"
    logging.basicConfig(
        filename=os.path.join("log.txt"),
        filemode="a",
        datefmt="%Y-%m-%d %H:%M:%S",
        level=logging.INFO,
        format=FORMAT,
    )

    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    logging.getLogger("").addHandler(console)


--- File: deeplabcut/pose_estimation_tensorflow/util/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Adapted from DeeperCut by Eldar Insafutdinov:
https://github.com/eldar/pose-tensorflow
"""

from deeplabcut.pose_estimation_tensorflow.util.logging import *


--- File: deeplabcut/pose_estimation_tensorflow/util/visualize.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Adapted from DeeperCut by Eldar Insafutdinov
https://github.com/eldar/pose-tensorflow
"""

import math

import matplotlib.pyplot as plt
import numpy as np
import cv2

from deeplabcut.utils.auxfun_videos import imresize


def _npcircle(image, cx, cy, radius, color, transparency=0.0):
    """Draw a circle on an image using only numpy methods."""
    radius = int(radius)
    cx = int(cx)
    cy = int(cy)
    y, x = np.ogrid[-radius:radius, -radius:radius]
    index = x**2 + y**2 <= radius**2
    image[cy - radius : cy + radius, cx - radius : cx + radius][index] = (
        image[cy - radius : cy + radius, cx - radius : cx + radius][index].astype(
            "float32"
        )
        * transparency
        + np.array(color).astype("float32") * (1.0 - transparency)
    ).astype("uint8")


def check_point(cur_x, cur_y, minx, miny, maxx, maxy):
    return minx < cur_x < maxx and miny < cur_y < maxy


def visualize_joints(image, pose):
    marker_size = 8
    minx = 2 * marker_size
    miny = 2 * marker_size
    maxx = image.shape[1] - 2 * marker_size
    maxy = image.shape[0] - 2 * marker_size
    num_joints = pose.shape[0]

    visim = image.copy()
    colors = [
        [255, 0, 0],
        [0, 255, 0],
        [0, 0, 255],
        [0, 245, 255],
        [255, 131, 250],
        [255, 255, 0],
        [255, 0, 0],
        [0, 255, 0],
        [0, 0, 255],
        [0, 245, 255],
        [255, 131, 250],
        [255, 255, 0],
        [0, 0, 0],
        [255, 255, 255],
        [255, 0, 0],
        [0, 255, 0],
        [0, 0, 255],
    ]
    for p_idx in range(num_joints):
        cur_x = pose[p_idx, 0]
        cur_y = pose[p_idx, 1]
        if check_point(cur_x, cur_y, minx, miny, maxx, maxy):
            _npcircle(visim, cur_x, cur_y, marker_size, colors[p_idx], 0.0)
    return visim


def show_heatmaps(cfg, img, scmap, pose, cmap="jet"):
    interp = "bilinear"
    all_joints = cfg["all_joints"]
    all_joints_names = cfg["all_joints_names"]
    subplot_width = 3
    subplot_height = math.ceil((len(all_joints) + 1) / subplot_width)
    f, axarr = plt.subplots(subplot_height, subplot_width)
    for pidx, part in enumerate(all_joints):
        plot_j = (pidx + 1) // subplot_width
        plot_i = (pidx + 1) % subplot_width
        scmap_part = np.sum(scmap[:, :, part], axis=2)
        scmap_part = imresize(scmap_part, 8.0, interpolationmethod=cv2.INTER_CUBIC)
        scmap_part = np.lib.pad(scmap_part, ((4, 0), (4, 0)), "minimum")
        curr_plot = axarr[plot_j, plot_i]
        curr_plot.set_title(all_joints_names[pidx])
        curr_plot.axis("off")
        curr_plot.imshow(img, interpolation=interp)
        curr_plot.imshow(scmap_part, alpha=0.5, cmap=cmap, interpolation=interp)

    curr_plot = axarr[0, 0]
    curr_plot.set_title("Pose")
    curr_plot.axis("off")
    curr_plot.imshow(visualize_joints(img, pose))

    plt.show()


def waitforbuttonpress():
    plt.waitforbuttonpress(timeout=1)


--- File: deeplabcut/pose_estimation_tensorflow/datasets/pose_tensorpack.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""

See pull request:
https://github.com/DeepLabCut/DeepLabCut/pull/409
use tensorpack dataflow to improve augmentation #409 and #426
Written largely by Kate Rupp -- Thanks!

A Neural Net Training Interface on TensorFlow, with focus on speed + flexibility
https://github.com/tensorpack/tensorpack
"""


import multiprocessing
import os

import cv2
import numpy as np
import scipy.io as sio
from deeplabcut.utils.conversioncode import robust_split_path
from numpy import array as arr
from tensorpack.dataflow.base import RNGDataFlow
from tensorpack.dataflow.common import MapData
from tensorpack.dataflow.imgaug import (
    Brightness,
    Contrast,
    RandomResize,
    Rotation,
    Saturation,
    GaussianNoise,
    GaussianBlur,
)
from tensorpack.dataflow.imgaug.crop import RandomCropRandomShape
from tensorpack.dataflow.imgaug.meta import RandomApplyAug
from tensorpack.dataflow.imgaug.transform import CropTransform
from tensorpack.dataflow.parallel import MultiProcessRunnerZMQ, MultiProcessRunner
from tensorpack.utils.utils import get_rng

from .factory import PoseDatasetFactory
from .pose_base import BasePoseDataset
from .utils import Batch, data_to_input


def img_to_bgr(im_path):
    img = cv2.imread(im_path)
    return img


class DataItem:
    def to_dict(self):
        return self.__dict__

    def from_dict(d):
        item = DataItem()
        for k, v in d.items():
            setattr(item, k, v)
        return item


class RandomCropping(RandomCropRandomShape):
    def __init__(self, wmin, hmin, wmax=None, hmax=None):
        self.rng = get_rng()
        super().__init__(wmin, hmin, wmax, hmax)

    def get_transform(self, img):
        hmax = self.hmax or img.shape[0]
        wmax = self.wmax or img.shape[1]
        hmin = min(self.hmin, img.shape[0])
        wmin = min(self.wmin, img.shape[1])
        hmax = min(hmax, img.shape[0])
        wmax = min(wmax, img.shape[1])
        h = self.rng.randint(hmin, hmax + 1)
        w = self.rng.randint(wmin, wmax + 1)
        diffh = img.shape[0] - h
        diffw = img.shape[1] - w
        assert diffh >= 0 and diffw >= 0
        y0 = 0 if diffh == 0 else self.rng.randint(diffh)
        x0 = 0 if diffw == 0 else self.rng.randint(diffw)
        crop_aug = CropTransform(y0, x0, h, w)

        return crop_aug


class Pose(RNGDataFlow):
    def __init__(self, cfg, shuffle=True, dir=None):
        self.shuffle = shuffle
        self.cfg = cfg
        self.data = self.load_dataset()
        self.has_gt = True

    def load_dataset(self):
        cfg = self.cfg
        file_name = os.path.join(self.cfg["project_path"], cfg["dataset"])
        mlab = sio.loadmat(file_name)
        self.raw_data = mlab
        mlab = mlab["dataset"]

        num_images = mlab.shape[1]
        data = []
        has_gt = True

        for i in range(num_images):
            sample = mlab[0, i]

            item = DataItem()
            item.image_id = i
            base = str(self.cfg["project_path"])
            im_path = sample[0][0]
            if isinstance(im_path, str):
                im_path = robust_split_path(im_path)
            else:
                im_path = [s.strip() for s in im_path]
            item.im_path = os.path.join(base, *im_path)
            item.im_size = sample[1][0]
            if len(sample) >= 3:
                joints = sample[2][0][0]
                joint_id = joints[:, 0]
                # make sure joint ids are 0-indexed
                if joint_id.size != 0:
                    assert (joint_id < cfg["num_joints"]).any()
                joints[:, 0] = joint_id
                coords = [joint[1:] for joint in joints]
                coords = arr(coords)
                item.coords = coords
                item.joints = [joints]
                item.joint_id = [arr(joint_id)]
                # print(item.joints)
            else:
                has_gt = False
            # if cfg.crop:
            #    crop = sample[3][0] - 1
            #    item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)
            data.append(item)

        self.has_gt = has_gt
        return data

    def __iter__(self):
        idxs = list(range(len(self.data)))
        while True:
            if self.shuffle:
                self.rng.shuffle(idxs)
            for k in idxs:
                data_item = self.data[k]
                yield data_item


@PoseDatasetFactory.register("tensorpack")
class TensorpackPoseDataset(BasePoseDataset):
    def __init__(self, cfg):
        # First, initializing variables (if they don't exist)
        # what is the fraction of training samples with scaling augmentation?
        cfg["scaleratio"] = cfg.get("scaleratio", 0.6)

        # loading defaults for rotation range!
        # Randomly rotates an image with respect to the image center within the
        # range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data

        if cfg.get("rotation", True):  # i.e. pm 25 degrees
            if type(cfg.get("rotation", False)) == int:
                cfg["rotation"] = cfg.get("rotation", 25)
            else:
                cfg["rotation"] = 25

            # cfg["rotateratio"] = cfg.get(
            #    "rotratio", 0.4
            # )  # what is the fraction of training samples with rotation augmentation?
        else:
            cfg["rotratio"] = 0.0
            cfg["rotation"] = 0

        # Randomly adds brightness within the range [-brightness_dif, brightness_dif]
        # to augment training data
        cfg["brightness_dif"] = cfg.get("brightness_dif", 0.3)
        cfg["brightnessratio"] = cfg.get(
            "brightnessratio", 0.0
        )  # what is the fraction of training samples with brightness augmentation?

        # Randomly applies x = (x - mean) * contrast_factor + mean`` to each
        # color channel within the range [contrast_factor_lo, contrast_factor_up]
        # to augment training data
        cfg["contrast_factor_lo"] = cfg.get("contrast_factor_lo", 0.5)
        cfg["contrast_factor_up"] = cfg.get("contrast_factor_up", 2.0)
        cfg["contrastratio"] = cfg.get(
            "contrastratio", 0.2
        )  # what is the fraction of training samples with contrast augmentation?

        # Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]
        # to augment training data
        cfg["saturation_max_dif"] = cfg.get("saturation_max_dif", 0.5)
        cfg["saturationratio"] = cfg.get(
            "saturationratio", 0.0
        )  # what is the fraction of training samples with saturation augmentation?

        # Randomly applies gaussian noise N(0, noise_sigma^2) to an image
        # to augment training data
        cfg["noise_sigma"] = cfg.get("noise_sigma", 0.1)
        cfg["noiseratio"] = cfg.get(
            "noiseratio", 0.0
        )  # what is the fraction of training samples with noise augmentation?

        # Randomly applies gaussian blur to an image with a random window size
        # within the range [0, 2 * blur_max_window_size + 1] to augment training data
        cfg["blur_max_window_size"] = cfg.get("blur_max_window_size", 10)
        cfg["blurratio"] = cfg.get(
            "blurratio", 0.2
        )  # what is the fraction of training samples with blur augmentation?

        # Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.
        cfg["is_rgb"] = cfg.get("is_rgb", True)

        # Clips image to [0, 255] even when data type is not uint8
        cfg["to_clip"] = cfg.get("to_clip", True)

        # Number of processes to use per core during training
        cfg["processratio"] = cfg.get("processratio", 1)
        # Number of datapoints to prefetch at a time during training
        cfg["num_prefetch"] = cfg.get("num_prefetch", 50)

        # Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)
        # and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.
        # Parameters for augmentation with regard to cropping:

        # what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint
        cfg["minsize"] = cfg.get("minsize", 100)
        cfg["leftwidth"] = cfg.get("leftwidth", 400)
        cfg["rightwidth"] = cfg.get("rightwidth", 400)
        cfg["topheight"] = cfg.get("topheight", 400)
        cfg["bottomheight"] = cfg.get("bottomheight", 400)

        cfg["cropratio"] = cfg.get("cropratio", 0.4)

        super(TensorpackPoseDataset, self).__init__(cfg)
        self.scaling = RandomResize(
            xrange=(
                self.cfg["scale_jitter_lo"] * self.cfg["global_scale"],
                self.cfg["scale_jitter_up"] * self.cfg["global_scale"],
            ),
            aspect_ratio_thres=0.0,
        )
        self.scaling_apply = RandomApplyAug(self.scaling, self.cfg["scaleratio"])
        self.cropping = RandomCropping(
            wmin=self.cfg["minsize"],
            hmin=self.cfg["minsize"],
            wmax=self.cfg["leftwidth"] + self.cfg["rightwidth"] + self.cfg["minsize"],
            hmax=self.cfg["topheight"] + self.cfg["bottomheight"] + self.cfg["minsize"],
        )
        self.rotation = Rotation(max_deg=self.cfg["rotation"])
        self.brightness = Brightness(self.cfg["brightness_dif"])
        self.contrast = Contrast(
            (self.cfg["contrast_factor_lo"], self.cfg["contrast_factor_up"]),
            rgb=self.cfg["is_rgb"],
            clip=self.cfg["to_clip"],
        )
        self.saturation = Saturation(
            self.cfg["saturation_max_dif"], rgb=self.cfg["is_rgb"]
        )
        self.gaussian_noise = GaussianNoise(sigma=self.cfg["noise_sigma"])
        self.gaussian_blur = GaussianBlur(max_size=self.cfg["blur_max_window_size"])
        self.augmentors = [
            RandomApplyAug(self.cropping, self.cfg["cropratio"]),
            RandomApplyAug(self.rotation, self.cfg["rotratio"]),
            RandomApplyAug(self.brightness, self.cfg["brightnessratio"]),
            RandomApplyAug(self.contrast, self.cfg["contrastratio"]),
            RandomApplyAug(self.saturation, self.cfg["saturationratio"]),
            RandomApplyAug(self.gaussian_noise, self.cfg["noiseratio"]),
            RandomApplyAug(self.gaussian_blur, self.cfg["blurratio"]),
            self.scaling_apply,
        ]

        self.has_gt = True
        self.set_shuffle(cfg["shuffle"])
        self.data = self.load_dataset()
        self.num_images = len(self.data)
        df = self.get_dataflow(self.cfg)
        df.reset_state()
        self.aug = iter(df)

    def load_dataset(self):
        p = Pose(cfg=self.cfg, shuffle=self.shuffle)
        return p.load_dataset()

    def augment(self, data):
        img = img_to_bgr(data.im_path)
        coords = data.coords.astype("float64")
        scale = 1
        for aug in self.augmentors:
            tfm = aug.get_transform(img)
            aug_img = tfm.apply_image(img)
            aug_coords = tfm.apply_coords(coords)
            if aug is self.scaling_apply:
                scale = aug_img.shape[0] / img.shape[0]
            img = aug_img
            coords = aug_coords

        aug_img = img
        aug_coords = coords
        size = [aug_img.shape[0], aug_img.shape[1]]
        aug_coords = [
            aug_coords.reshape(int(len(aug_coords[~np.isnan(aug_coords)]) / 2), 2)
        ]
        joint_id = data.joint_id

        return [joint_id, aug_img, aug_coords, data, size, scale]

    def get_dataflow(self, cfg):
        df = Pose(cfg)
        df = MapData(df, self.augment)
        df = MapData(df, self.compute_target_part_scoremap)

        num_cores = multiprocessing.cpu_count()
        num_processes = int(num_cores * self.cfg["processratio"])
        if num_processes <= 1:
            num_processes = 2  # recommended to use more than one process for training
        if os.name == "nt":
            df2 = MultiProcessRunner(
                df, num_proc=num_processes, num_prefetch=self.cfg["num_prefetch"]
            )
        else:
            df2 = MultiProcessRunnerZMQ(
                df, num_proc=num_processes, hwm=self.cfg["num_prefetch"]
            )
        return df2

    def compute_target_part_scoremap(self, components):
        joint_id = components[0]
        aug_img = components[1]
        coords = components[2]
        data_item = components[3]
        img_size = components[4]
        scale = components[5]

        stride = self.cfg["stride"]
        dist_thresh = self.cfg["pos_dist_thresh"] * scale
        num_joints = self.cfg["num_joints"]
        half_stride = stride / 2
        size = np.ceil(arr(img_size) / (stride * 2)).astype(int) * 2
        scmap = np.zeros(np.append(size, num_joints))
        locref_size = np.append(size, num_joints * 2)
        locref_mask = np.zeros(locref_size)
        locref_map = np.zeros(locref_size)

        locref_scale = 1.0 / self.cfg["locref_stdev"]
        dist_thresh_sq = dist_thresh**2

        width = size[1]
        height = size[0]

        for person_id in range(len(coords)):
            for k, j_id in enumerate(joint_id[person_id]):
                joint_pt = coords[person_id][k, :]
                j_x = np.asarray(joint_pt[0]).item()
                j_y = np.asarray(joint_pt[1]).item()

                # don't loop over entire heatmap, but just relevant locations
                j_x_sm = round((j_x - half_stride) / stride)
                j_y_sm = round((j_y - half_stride) / stride)
                min_x = round(max(j_x_sm - dist_thresh - 1, 0))
                max_x = round(min(j_x_sm + dist_thresh + 1, width - 1))
                min_y = round(max(j_y_sm - dist_thresh - 1, 0))
                max_y = round(min(j_y_sm + dist_thresh + 1, height - 1))

                for j in range(min_y, max_y + 1):  # range(height):
                    pt_y = j * stride + half_stride
                    for i in range(min_x, max_x + 1):  # range(width):
                        pt_x = i * stride + half_stride
                        dx = j_x - pt_x
                        dy = j_y - pt_y
                        dist = dx**2 + dy**2
                        # print(la.norm(diff))
                        if dist <= dist_thresh_sq:
                            scmap[j, i, j_id] = 1
                            locref_mask[j, i, j_id * 2 + 0] = 1
                            locref_mask[j, i, j_id * 2 + 1] = 1
                            locref_map[j, i, j_id * 2 + 0] = dx * locref_scale
                            locref_map[j, i, j_id * 2 + 1] = dy * locref_scale

        weights = self.compute_scmap_weights(scmap.shape, joint_id)
        mirror = False
        d = data_item.to_dict()
        d["image"] = aug_img

        return d, scale, mirror, scmap, weights, locref_map, locref_mask

    def set_test_mode(self, test_mode):
        self.has_gt = not test_mode

    def set_shuffle(self, shuffle):
        self.shuffle = shuffle
        if not shuffle:
            assert not self.cfg["mirror"]
            self.image_indices = np.arange(self.num_images)

    def shuffle_images(self):
        num_images = self.num_images
        if self.cfg["mirror"]:
            image_indices = np.random.permutation(num_images * 2)
            self.mirrored = image_indices >= num_images
            image_indices[self.mirrored] = image_indices[self.mirrored] - num_images
            self.image_indices = image_indices
        else:
            self.image_indices = np.random.permutation(num_images)

    def next_batch(self):
        next_batch = next(self.aug)
        return self.make_batch(next_batch)

    def is_valid_size(self, image_size, scale):
        if "min_input_size" in self.cfg and "max_input_size" in self.cfg:
            input_width = image_size[2] * scale
            input_height = image_size[1] * scale
            if (
                input_height < self.cfg["min_input_size"]
                or input_width < self.cfg["min_input_size"]
            ):
                return False
            if input_height * input_width > self.cfg["max_input_size"] ** 2:
                return False

        return True

    def make_batch(self, components):
        data_item = DataItem.from_dict(components[0])
        mirror = components[2]
        part_score_targets = components[3]
        part_score_weights = components[4]
        locref_targets = components[5]
        locref_mask = components[6]

        im_file = data_item.im_path
        # logging.debug('image %s', im_file)
        # print('image: {}'.format(im_file))
        # logging.debug('mirror %r', mirror)

        img = data_item.image  # augmented image

        batch = {Batch.inputs: img}

        if self.has_gt:
            batch.update(
                {
                    Batch.part_score_targets: part_score_targets,
                    Batch.part_score_weights: part_score_weights,
                    Batch.locref_targets: locref_targets,
                    Batch.locref_mask: locref_mask,
                }
            )

        batch = {key: data_to_input(data) for (key, data) in batch.items()}

        batch[Batch.data_item] = data_item

        return batch

    def compute_scmap_weights(self, scmap_shape, joint_id):
        cfg = self.cfg
        if cfg["weigh_only_present_joints"]:
            weights = np.zeros(scmap_shape)
            for person_joint_id in joint_id:
                for j_id in person_joint_id:
                    weights[:, :, j_id] = 1.0
        else:
            weights = np.ones(scmap_shape)
        return weights


--- File: deeplabcut/pose_estimation_tensorflow/datasets/augmentation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import imgaug.augmenters as iaa
import numpy as np
from imgaug import KeypointsOnImage
from scipy.spatial.distance import pdist, squareform
from typing import List, Union, Tuple


class KeypointFliplr(iaa.Fliplr):
    def __init__(
        self,
        keypoints: List[str],
        symmetric_pairs: List[Union[Tuple, List]],
        p: float = 1.0,
    ):
        super().__init__(p=p)
        self.keypoints = keypoints
        self.symmetric_pairs = symmetric_pairs

    @property
    def n_keypoints(self):
        return len(self.keypoints)

    def _augment_batch_(self, batch, random_state, parents, hooks):
        batch = super()._augment_batch_(batch, random_state, parents, hooks)
        keypoints = []
        for kpts in batch.keypoints:
            kpts_ = list(kpts)
            n_kpts = len(kpts_)
            for i1, i2 in self.symmetric_pairs:
                for j in range(0, n_kpts, self.n_keypoints):
                    kpts_[i1 + j], kpts_[i2 + j] = kpts_[i2 + j], kpts_[i1 + j]
            keypoints.append(KeypointsOnImage(kpts_, kpts.shape))
        batch.keypoints = keypoints
        return batch


class KeypointAwareCropToFixedSize(iaa.CropToFixedSize):
    def __init__(
        self,
        width,
        height,
        max_shift=0.4,
        crop_sampling="hybrid",
    ):
        """
        Parameters
        ----------
        width : int
            Crop images down to this maximum width.

        height : int
            Crop images down to this maximum height.

        max_shift : float, optional (default=0.25)
            Maximum allowed shift of the cropping center position
            as a fraction of the crop size.

        crop_sampling : str, optional (default="hybrid")
            Crop centers sampling method. Must be either:
            "uniform" (randomly over the image),
            "keypoints" (randomly over the annotated keypoints),
            "density" (weighing preferentially dense regions of keypoints),
            or "hybrid" (alternating randomly between "uniform" and "density").
        """
        super(KeypointAwareCropToFixedSize, self).__init__(
            width,
            height,
            name="kptscrop",
        )
        # Clamp to 40% of crop size to ensure that at least
        # the center keypoint remains visible after the offset is applied.
        self.max_shift = max(0.0, min(max_shift, 0.4))
        if crop_sampling not in ("uniform", "keypoints", "density", "hybrid"):
            raise ValueError(
                f"Invalid sampling {crop_sampling}. Must be "
                f"either 'uniform', 'keypoints', 'density', or 'hybrid."
            )
        self.crop_sampling = crop_sampling

    @staticmethod
    def calc_n_neighbors(xy, radius):
        d = pdist(xy, "sqeuclidean")
        mat = squareform(d <= radius * radius, checks=False)
        return np.sum(mat, axis=0)

    def _draw_samples(self, batch, random_state):
        n_samples = batch.nb_rows
        offsets = np.empty((n_samples, 2), dtype=np.float32)
        rngs = random_state.duplicate(2)
        shift_x = self.max_shift * self.size[0] * rngs[0].uniform(-1, 1, n_samples)
        shift_y = self.max_shift * self.size[1] * rngs[1].uniform(-1, 1, n_samples)
        sampling = self.crop_sampling
        for n in range(batch.nb_rows):
            if self.crop_sampling == "hybrid":
                sampling = random_state.choice(["uniform", "density"])
            if sampling == "uniform":
                center = random_state.uniform(size=2)
            else:
                h, w = batch.images[n].shape[:2]
                kpts = batch.keypoints[n].to_xy_array()
                kpts = kpts[~np.isnan(kpts).all(axis=1)]
                n_kpts = kpts.shape[0]
                inds = np.arange(n_kpts)
                if sampling == "density":
                    # Points located close to one another are sampled preferentially
                    # in order to augment crowded regions.
                    radius = 0.1 * min(h, w)
                    n_neighbors = self.calc_n_neighbors(kpts, radius)
                    # Include keypoints in the count to avoid null probabilities
                    n_neighbors += 1
                    p = n_neighbors / n_neighbors.sum()
                else:
                    p = np.ones_like(inds) / n_kpts
                center = kpts[random_state.choice(inds, p=p)]
                # Shift the crop center in both dimensions by random amounts
                # and normalize to the original image dimensions.
                center[0] += shift_x[n]
                center[0] /= w
                center[1] += shift_y[n]
                center[1] /= h
            offsets[n] = center
        offsets = np.clip(offsets, 0, 1)
        return [self.size] * n_samples, offsets[:, 0], offsets[:, 1]


def update_crop_size(pipeline, width, height):
    aug = pipeline.find_augmenters_by_name("kptscrop")
    if not aug:
        return
    aug[0].size = width, height


--- File: deeplabcut/pose_estimation_tensorflow/datasets/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from .factory import PoseDatasetFactory
from .pose_deterministic import DeterministicPoseDataset
from .pose_scalecrop import ScalecropPoseDataset
from .pose_imgaug import ImgaugPoseDataset
from .pose_tensorpack import TensorpackPoseDataset
from .pose_multianimal_imgaug import MAImgaugPoseDataset
from .utils import Batch


__all__ = [
    "PoseDatasetFactory",
    "DeterministicPoseDataset",
    "ScalecropPoseDataset",
    "ImgaugPoseDataset",
    "TensorpackPoseDataset",
    "MAImgaugPoseDataset",
    "Batch",
]


--- File: deeplabcut/pose_estimation_tensorflow/datasets/factory.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#


import warnings


class PoseDatasetFactory:
    _datasets = dict()

    @classmethod
    def register(cls, type_):
        def wrapper(dataset):
            if type_ in cls._datasets:
                warnings.warn("Overwriting existing dataset {}.")
            cls._datasets[type_] = dataset
            return dataset

        return wrapper

    @classmethod
    def create(cls, cfg):
        dataset_type = cfg["dataset_type"]
        dataset = cls._datasets.get(dataset_type)
        if dataset is None:
            raise ValueError(f"Unsupported dataset of type {dataset_type}")
        return dataset(cfg)


--- File: deeplabcut/pose_estimation_tensorflow/datasets/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
from enum import Enum


class Batch(Enum):
    inputs = 0
    part_score_targets = 1
    part_score_weights = 2
    locref_targets = 3
    locref_mask = 4
    pairwise_targets = 5
    pairwise_mask = 6
    data_item = 7


class DataItem:
    pass


def data_to_input(data):
    return np.expand_dims(data, axis=0).astype(float)


def mirror_joints_map(all_joints, num_joints):
    res = np.arange(num_joints)
    symmetric_joints = [p for p in all_joints if len(p) == 2]
    for pair in symmetric_joints:
        res[pair[0]] = pair[1]
        res[pair[1]] = pair[0]
    return res


def crop_image(joints, im, Xlabel, Ylabel, cfg):
    """Randomly cropping image around xlabel,ylabel taking into account size of image.
    Introduced in DLC 2.0 (Nature Protocols paper)"""
    widthforward = int(cfg["minsize"] + np.random.randint(cfg["rightwidth"]))
    widthback = int(cfg["minsize"] + np.random.randint(cfg["leftwidth"]))
    hup = int(cfg["minsize"] + np.random.randint(cfg["topheight"]))
    hdown = int(cfg["minsize"] + np.random.randint(cfg["bottomheight"]))
    Xstart = max(0, int(Xlabel - widthback))
    Xstop = min(np.shape(im)[1] - 1, int(Xlabel + widthforward))
    Ystart = max(0, int(Ylabel - hdown))
    Ystop = min(np.shape(im)[0] - 1, int(Ylabel + hup))
    joints[0, :, 1] -= Xstart
    joints[0, :, 2] -= Ystart

    inbounds = np.where(
        (joints[0, :, 1] > 0)
        * (joints[0, :, 1] < np.shape(im)[1])
        * (joints[0, :, 2] > 0)
        * (joints[0, :, 2] < np.shape(im)[0])
    )[0]
    return joints[:, inbounds, :], im[Ystart : Ystop + 1, Xstart : Xstop + 1, :]


--- File: deeplabcut/pose_estimation_tensorflow/datasets/pose_scalecrop.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


from .factory import PoseDatasetFactory
from .pose_deterministic import DeterministicPoseDataset


@PoseDatasetFactory.register("scalecrop")
class ScalecropPoseDataset(DeterministicPoseDataset):
    def __init__(self, cfg):
        super(ScalecropPoseDataset, self).__init__(cfg)
        self.cfg["deterministic"] = False
        self.max_input_sizesquare = cfg.get("max_input_size", 1500) ** 2
        self.min_input_sizesquare = cfg.get("min_input_size", 64) ** 2
        self.locref_scale = 1.0 / cfg["locref_stdev"]
        self.stride = cfg["stride"]
        self.half_stride = self.stride / 2
        self.scale_jitter_lo = cfg.get("scale_jitter_lo", 0.75)
        self.scale_jitter_up = cfg.get("scale_jitter_up", 1.25)

        self.cfg["crop"] = cfg.get("crop", True)
        self.cfg["cropratio"] = cfg.get("cropratio", 0.4)

        # what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint
        self.cfg["minsize"] = cfg.get("minsize", 100)
        self.cfg["leftwidth"] = cfg.get("leftwidth", 400)
        self.cfg["rightwidth"] = cfg.get("rightwidth", 400)
        self.cfg["topheight"] = cfg.get("topheight", 400)
        self.cfg["bottomheight"] = cfg.get("bottomheight", 400)


--- File: deeplabcut/pose_estimation_tensorflow/datasets/pose_deterministic.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import logging
import numpy as np
import os
import scipy.io as sio
from deeplabcut.utils.auxfun_videos import imread, imresize
from deeplabcut.utils.conversioncode import robust_split_path
from .factory import PoseDatasetFactory
from .pose_base import BasePoseDataset
from .utils import (
    DataItem,
    mirror_joints_map,
    crop_image,
    Batch,
    data_to_input,
)


@PoseDatasetFactory.register("deterministic")
class DeterministicPoseDataset(BasePoseDataset):
    def __init__(self, cfg):
        super(DeterministicPoseDataset, self).__init__(cfg)
        self.data = self.load_dataset()
        self.num_images = len(self.data)
        if self.cfg["mirror"]:
            self.symmetric_joints = mirror_joints_map(
                cfg["all_joints"], cfg["num_joints"]
            )
        self.curr_img = 0
        self.scale = cfg["global_scale"]
        self.locref_scale = 1.0 / cfg["locref_stdev"]
        self.stride = cfg["stride"]
        self.half_stride = self.stride / 2
        self.set_shuffle(cfg["shuffle"])

    def load_dataset(self):
        cfg = self.cfg
        file_name = os.path.join(self.cfg["project_path"], cfg["dataset"])
        mlab = sio.loadmat(file_name)
        self.raw_data = mlab
        mlab = mlab["dataset"]

        num_images = mlab.shape[1]
        data = []
        has_gt = True

        for i in range(num_images):
            sample = mlab[0, i]

            item = DataItem()
            item.image_id = i
            im_path = sample[0][0]
            if isinstance(im_path, str):
                im_path = robust_split_path(im_path)
            else:
                im_path = [s.strip() for s in im_path]
            item.im_path = os.path.join(*im_path)
            item.im_size = sample[1][0]
            if len(sample) >= 3:
                joints = sample[2][0][0]
                joint_id = joints[:, 0]
                # make sure joint ids are 0-indexed
                if joint_id.size != 0:
                    assert np.any(joint_id < cfg["num_joints"])
                joints[:, 0] = joint_id
                item.joints = [joints]
            else:
                has_gt = False
            # if cfg.crop:
            #    crop = sample[3][0] - 1
            #    item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)
            data.append(item)

        self.has_gt = has_gt
        return data

    def set_test_mode(self, test_mode):
        self.has_gt = not test_mode

    def set_shuffle(self, shuffle):
        self.shuffle = shuffle
        if not shuffle:
            assert not self.cfg["mirror"]
            self.image_indices = np.arange(self.num_images)

    def mirror_joint_coords(self, joints, image_width):
        # horizontally flip the x-coordinate, keep y unchanged
        joints[:, 1] = image_width - joints[:, 1] - 1
        return joints

    def mirror_joints(self, joints, symmetric_joints, image_width):
        # joint ids are 0 indexed
        res = np.copy(joints)
        res = self.mirror_joint_coords(res, image_width)
        # swap the joint_id for a symmetric one
        joint_id = joints[:, 0].astype(int)
        res[:, 0] = symmetric_joints[joint_id]
        return res

    def shuffle_images(self):
        if self.cfg["deterministic"]:
            np.random.seed(42)
        num_images = self.num_images
        if self.cfg["mirror"]:
            image_indices = np.random.permutation(num_images * 2)
            self.mirrored = image_indices >= num_images
            image_indices[self.mirrored] = image_indices[self.mirrored] - num_images
            self.image_indices = image_indices
        else:
            self.image_indices = np.random.permutation(num_images)

    def num_training_samples(self):
        num = self.num_images
        if self.cfg["mirror"]:
            num *= 2
        return num

    def next_training_sample(self):
        if self.curr_img == 0 and self.shuffle:
            self.shuffle_images()

        curr_img = self.curr_img
        self.curr_img = (self.curr_img + 1) % self.num_training_samples()

        imidx = self.image_indices[curr_img]
        mirror = self.cfg["mirror"] and self.mirrored[curr_img]

        return imidx, mirror

    def get_training_sample(self, imidx):
        return self.data[imidx]

    def next_batch(self):
        while True:
            imidx, mirror = self.next_training_sample()
            data_item = self.get_training_sample(imidx)
            scale = self.sample_scale()

            if not self.is_valid_size(data_item.im_size, scale):
                continue

            return self.make_batch(data_item, scale, mirror)

    def is_valid_size(self, image_size, scale):
        if "min_input_size" in self.cfg and "max_input_size" in self.cfg:
            input_width = image_size[2] * scale
            input_height = image_size[1] * scale
            if (
                input_height < self.cfg["min_input_size"]
                or input_width < self.cfg["min_input_size"]
            ):
                return False
            if input_height * input_width > self.cfg["max_input_size"] ** 2:
                return False

        return True

    def make_batch(self, data_item, scale, mirror):
        im_file = data_item.im_path
        logging.debug("image %s", im_file)
        logging.debug("mirror %r", mirror)
        image = imread(os.path.join(self.cfg["project_path"], im_file), mode="skimage")

        if self.has_gt:
            joints = np.copy(data_item.joints)

        if self.cfg["crop"]:  # adapted cropping for DLC
            if np.random.rand() < self.cfg["cropratio"]:
                j = np.random.randint(np.shape(joints)[1])
                joints, image = crop_image(
                    joints, image, joints[0, j, 1], joints[0, j, 2], self.cfg
                )

        img = imresize(image, scale) if scale != 1 else image
        scaled_img_size = np.array(img.shape[0:2])

        if mirror:
            img = np.fliplr(img)

        batch = {Batch.inputs: img}

        if self.has_gt:
            stride = self.cfg["stride"]
            if mirror:
                joints = [
                    self.mirror_joints(
                        person_joints, self.symmetric_joints, image.shape[1]
                    )
                    for person_joints in joints
                ]
            sm_size = np.ceil(scaled_img_size / (stride * 2)).astype(int) * 2
            scaled_joints = [person_joints[:, 1:3] * scale for person_joints in joints]
            joint_id = [person_joints[:, 0].astype(int) for person_joints in joints]
            (
                part_score_targets,
                part_score_weights,
                locref_targets,
                locref_mask,
            ) = self.compute_target_part_scoremap(
                joint_id, scaled_joints, data_item, sm_size, scale
            )

            batch.update(
                {
                    Batch.part_score_targets: part_score_targets,
                    Batch.part_score_weights: part_score_weights,
                    Batch.locref_targets: locref_targets,
                    Batch.locref_mask: locref_mask,
                }
            )

        batch = {key: data_to_input(data) for (key, data) in batch.items()}

        batch[Batch.data_item] = data_item

        return batch

    def compute_target_part_scoremap(self, joint_id, coords, data_item, size, scale):
        dist_thresh = self.cfg["pos_dist_thresh"] * scale
        dist_thresh_sq = dist_thresh**2
        num_joints = self.cfg["num_joints"]
        scmap = np.zeros(np.concatenate([size, np.array([num_joints])]))
        locref_size = np.concatenate([size, np.array([num_joints * 2])])
        locref_mask = np.zeros(locref_size)
        locref_map = np.zeros(locref_size)
        width = size[1]
        height = size[0]

        for person_id in range(len(coords)):
            for k, j_id in enumerate(joint_id[person_id]):
                joint_pt = coords[person_id][k, :]
                j_x = np.asarray(joint_pt[0]).item()
                j_y = np.asarray(joint_pt[1]).item()

                # don't loop over entire heatmap, but just relevant locations
                j_x_sm = round((j_x - self.half_stride) / self.stride)
                j_y_sm = round((j_y - self.half_stride) / self.stride)
                min_x = round(max(j_x_sm - dist_thresh - 1, 0))
                max_x = round(min(j_x_sm + dist_thresh + 1, width - 1))
                min_y = round(max(j_y_sm - dist_thresh - 1, 0))
                max_y = round(min(j_y_sm + dist_thresh + 1, height - 1))

                for j in range(min_y, max_y + 1):  # range(height):
                    pt_y = j * self.stride + self.half_stride
                    for i in range(min_x, max_x + 1):  # range(width):
                        # pt = arr([i*stride+half_stride, j*stride+half_stride])
                        # diff = joint_pt - pt
                        # The code above is too slow in python
                        pt_x = i * self.stride + self.half_stride
                        dx = j_x - pt_x
                        dy = j_y - pt_y
                        dist = dx**2 + dy**2
                        # print(la.norm(diff))
                        if dist <= dist_thresh_sq:
                            scmap[j, i, j_id] = 1
                            locref_mask[j, i, j_id * 2 + 0] = 1
                            locref_mask[j, i, j_id * 2 + 1] = 1
                            locref_map[j, i, j_id * 2 + 0] = dx * self.locref_scale
                            locref_map[j, i, j_id * 2 + 1] = dy * self.locref_scale

        weights = self.compute_scmap_weights(scmap.shape, joint_id, data_item)

        return scmap, weights, locref_map, locref_mask

    def compute_scmap_weights(self, scmap_shape, joint_id, data_item):
        if self.cfg["weigh_only_present_joints"]:
            weights = np.zeros(scmap_shape)
            for person_joint_id in joint_id:
                for j_id in person_joint_id:
                    weights[:, :, j_id] = 1.0
        else:
            weights = np.ones(scmap_shape)
        return weights


--- File: deeplabcut/pose_estimation_tensorflow/datasets/pose_multianimal_imgaug.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import logging
import os
import pickle
import imageio
import imgaug.augmenters as iaa
import numpy as np
import pandas as pd
from imgaug.augmentables import Keypoint, KeypointsOnImage
from deeplabcut.generate_training_dataset import read_image_shape_fast
from deeplabcut.pose_estimation_tensorflow.datasets import augmentation
from deeplabcut.pose_estimation_tensorflow.datasets.factory import PoseDatasetFactory
from deeplabcut.pose_estimation_tensorflow.datasets.pose_base import BasePoseDataset
from deeplabcut.pose_estimation_tensorflow.datasets.utils import DataItem, Batch
from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal
from deeplabcut.utils.auxfun_videos import imread
from deeplabcut.utils.auxfun_videos import VideoReader
from deeplabcut.utils.conversioncode import robust_split_path
from pathlib import Path
from math import sqrt


@PoseDatasetFactory.register("multi-animal-imgaug")
class MAImgaugPoseDataset(BasePoseDataset):
    def __init__(self, cfg):
        super(MAImgaugPoseDataset, self).__init__(cfg)

        if cfg.get("pseudo_label", ""):
            self._n_kpts = len(cfg["all_joints_names"])
            self._n_animals = 1

        else:
            self.main_cfg = auxiliaryfunctions.read_config(
                os.path.join(self.cfg["project_path"], "config.yaml")
            )
            animals, unique, multi = auxfun_multianimal.extractindividualsandbodyparts(
                self.main_cfg
            )
            self._n_kpts = len(multi) + len(unique)
            self._n_animals = len(animals)

        if cfg.get("pseudo_label", "").endswith(".h5"):
            assert cfg["video_path"]
            print("loading video for image source", cfg["video_path"])
            self.vid = VideoReader(cfg["video_path"])
            self.video_image_size = (3, self.vid.height, self.vid.width)
        else:
            self.vid = None

        self.data = self.load_dataset()
        self.num_images = len(self.data)
        self.batch_size = cfg["batch_size"]
        print("Batch Size is %d" % self.batch_size)
        self._default_size = np.array(self.cfg.get("crop_size", (400, 400)))
        self.pipeline = self.build_augmentation_pipeline(
            apply_prob=cfg.get("apply_prob", 0.5),
        )

    @property
    def default_size(self):
        return self._default_size  # width, height

    @default_size.setter
    def default_size(self, size):
        self._default_size = np.array(size)

    def load_dataset(self):
        cfg = self.cfg

        if cfg.get("pseudo_label", ""):
            if cfg["pseudo_label"].endswith(".h5"):
                pseudo_threshold = cfg.get("pseudo_threshold", 0)
                print(f"Loading pseudo labels with threshold > {pseudo_threshold}")

                # for topview, it's safe to mask keypoints under threshold
                mask_kpts_below_thresh = "topview" in cfg.get("superanimal", "")
                return self._load_pseudo_data_from_h5(
                    cfg,
                    threshold=pseudo_threshold,
                    mask_kpts_below_thresh=mask_kpts_below_thresh,
                )

        file_name = os.path.join(self.cfg["project_path"], cfg["dataset"])
        with open(os.path.join(self.cfg["project_path"], file_name), "rb") as f:
            # Pickle the 'data' dictionary using the highest protocol available.
            pickledata = pickle.load(f)

        self.raw_data = pickledata
        num_images = len(pickledata)
        data = []
        has_gt = True

        for i in range(num_images):
            sample = pickledata[i]  # mlab[0, i]
            item = DataItem()
            item.image_id = i
            im_path = sample["image"]
            if isinstance(im_path, str):
                im_path = robust_split_path(im_path)
            item.im_path = os.path.join(*im_path)
            item.im_size = sample["size"]
            if "joints" in sample.keys():
                Joints = sample["joints"]
                if (
                    np.size(
                        np.concatenate(
                            [Joints[person_id][:, 1:3] for person_id in Joints.keys()]
                        )
                    )
                    > 0
                ):
                    item.joints = Joints
                else:
                    has_gt = False  # no animal has joints!
                # item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal
            else:
                has_gt = False
            data.append(item)

        self.has_gt = has_gt
        return data

    def _load_pseudo_data_from_h5(
        self, cfg, threshold=0.5, mask_kpts_below_thresh=False
    ):
        gt_file = cfg["pseudo_label"]
        assert os.path.exists(gt_file)
        path_ = Path(gt_file)
        print("Using gt file:", path_.name)
        num_kpts = len(cfg["all_joints_names"])
        df = pd.read_hdf(gt_file)
        video_name = path_.name.split("DLC")[0]
        video_root = str(path_.parents[0] / video_name)

        itemlist = []
        for image_id, imagename in enumerate(df.index):
            item = DataItem()
            data = df.loc[imagename]
            # 3 for likelihood
            kpts = data.to_numpy().reshape(-1, 3)
            item.num_joints = kpts.shape[0]
            joint_ids = np.arange(item.num_joints)[..., np.newaxis]
            frame_name = "frame_" + str(int(imagename.split("frame")[1])) + ".png"
            item.im_path = os.path.join(video_root, frame_name)

            if self.vid:
                item.im_size = self.video_image_size
            else:
                item.im_size = read_image_shape_fast(
                    os.path.join(video_root, frame_name)
                )

            item.joints = {}

            if not mask_kpts_below_thresh:
                joints = np.concatenate([joint_ids, kpts], axis=1)
                joints = np.nan_to_num(joints, nan=0)
            else:
                for kpt_id, kpt in enumerate(kpts):
                    if kpt[-1] < threshold:
                        kpts[kpt_id][:-1] = -1
                    if np.isnan(kpt[0]):
                        kpts[kpt_id][:-1] = -1
                        kpts[kpt_id][-1] = 1
                joints = np.concatenate([joint_ids, kpts], axis=1)

            sparse_joints = []

            for coord in joints:
                if coord[1] != 0 and coord[3] > threshold:
                    sparse_joints.append(coord[:3])

            temp = np.array(sparse_joints)
            # we only do single animal here
            item.joints.update({0: temp})
            itemlist.append(item)

        self.has_gt = True
        return itemlist

    def build_augmentation_pipeline(self, apply_prob=0.5):
        cfg = self.cfg

        sometimes = lambda aug: iaa.Sometimes(apply_prob, aug)
        pipeline = iaa.Sequential(random_order=False)

        pre_resize = cfg.get("pre_resize")

        if cfg.get("traintime_resize", False):
            # let's hard code it
            print("using traintime resize")
            pipeline.add(iaa.Resize({"height": 400, "width": "keep-aspect-ratio"}))

        crop_sampling = cfg.get("crop_sampling", "hybrid")
        if pre_resize:
            width, height = pre_resize
            pipeline.add(iaa.Resize({"height": height, "width": width}))
            if crop_sampling == "none":
                self.default_size = width, height

        if crop_sampling != "none":
            # Add smart, keypoint-aware image cropping
            pipeline.add(iaa.PadToFixedSize(*self.default_size))
            pipeline.add(
                augmentation.KeypointAwareCropToFixedSize(
                    *self.default_size,
                    cfg.get("max_shift", 0.4),
                    crop_sampling,
                )
            )

        if cfg.get("fliplr", False) and cfg.get("symmetric_pairs"):
            opt = cfg.get("fliplr", False)
            if type(opt) == int:
                p = opt
            else:
                p = 0.5
            pipeline.add(
                sometimes(
                    augmentation.KeypointFliplr(
                        cfg["all_joints_names"],
                        symmetric_pairs=cfg["symmetric_pairs"],
                        p=p,
                    )
                )
            )
        if cfg.get("rotation", False):
            opt = cfg.get("rotation", False)
            if type(opt) == int:
                pipeline.add(sometimes(iaa.Affine(rotate=(-opt, opt))))
            else:
                pipeline.add(sometimes(iaa.Affine(rotate=(-10, 10))))
        if cfg.get("hist_eq", False):
            pipeline.add(sometimes(iaa.AllChannelsHistogramEqualization()))
        if cfg.get("motion_blur", False):
            opts = cfg.get("motion_blur", False)
            if type(opts) == list:
                opts = dict(opts)
                pipeline.add(sometimes(iaa.MotionBlur(**opts)))
            else:
                pipeline.add(sometimes(iaa.MotionBlur(k=7, angle=(-90, 90))))
        if cfg.get("covering", False):
            pipeline.add(
                sometimes(iaa.CoarseDropout((0, 0.02), size_percent=(0.01, 0.05)))
            )  # , per_channel=0.5)))
        if cfg.get("elastic_transform", False):
            pipeline.add(sometimes(iaa.ElasticTransformation(sigma=5)))
        if cfg.get("gaussian_noise", False):
            opt = cfg.get("gaussian_noise", False)
            if type(opt) == int or type(opt) == float:
                pipeline.add(
                    sometimes(
                        iaa.AdditiveGaussianNoise(
                            loc=0, scale=(0.0, opt), per_channel=0.5
                        )
                    )
                )
            else:
                pipeline.add(
                    sometimes(
                        iaa.AdditiveGaussianNoise(
                            loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5
                        )
                    )
                )
        if cfg.get("grayscale", False):
            pipeline.add(sometimes(iaa.Grayscale(alpha=(0.5, 1.0))))

        def get_aug_param(cfg_value):
            if isinstance(cfg_value, dict):
                opt = cfg_value
            else:
                opt = {}
            return opt

        cfg_cnt = cfg.get("contrast", {})
        cfg_cnv = cfg.get("convolution", {})

        contrast_aug = ["histeq", "clahe", "gamma", "sigmoid", "log", "linear"]
        for aug in contrast_aug:
            aug_val = cfg_cnt.get(aug, False)
            cfg_cnt[aug] = aug_val
            if aug_val:
                cfg_cnt[aug + "ratio"] = cfg_cnt.get(aug + "ratio", 0.1)

        convolution_aug = ["sharpen", "emboss", "edge"]
        for aug in convolution_aug:
            aug_val = cfg_cnv.get(aug, False)
            cfg_cnv[aug] = aug_val
            if aug_val:
                cfg_cnv[aug + "ratio"] = cfg_cnv.get(aug + "ratio", 0.1)

        if cfg_cnt["histeq"]:
            opt = get_aug_param(cfg_cnt["histeq"])
            pipeline.add(
                iaa.Sometimes(
                    cfg_cnt["histeqratio"], iaa.AllChannelsHistogramEqualization(**opt)
                )
            )

        if cfg_cnt["clahe"]:
            opt = get_aug_param(cfg_cnt["clahe"])
            pipeline.add(
                iaa.Sometimes(cfg_cnt["claheratio"], iaa.AllChannelsCLAHE(**opt))
            )

        if cfg_cnt["log"]:
            opt = get_aug_param(cfg_cnt["log"])
            pipeline.add(iaa.Sometimes(cfg_cnt["logratio"], iaa.LogContrast(**opt)))

        if cfg_cnt["linear"]:
            opt = get_aug_param(cfg_cnt["linear"])
            pipeline.add(
                iaa.Sometimes(cfg_cnt["linearratio"], iaa.LinearContrast(**opt))
            )

        if cfg_cnt["sigmoid"]:
            opt = get_aug_param(cfg_cnt["sigmoid"])
            pipeline.add(
                iaa.Sometimes(cfg_cnt["sigmoidratio"], iaa.SigmoidContrast(**opt))
            )

        if cfg_cnt["gamma"]:
            opt = get_aug_param(cfg_cnt["gamma"])
            pipeline.add(iaa.Sometimes(cfg_cnt["gammaratio"], iaa.GammaContrast(**opt)))

        if cfg_cnv["sharpen"]:
            opt = get_aug_param(cfg_cnv["sharpen"])
            pipeline.add(iaa.Sometimes(cfg_cnv["sharpenratio"], iaa.Sharpen(**opt)))

        if cfg_cnv["emboss"]:
            opt = get_aug_param(cfg_cnv["emboss"])
            pipeline.add(iaa.Sometimes(cfg_cnv["embossratio"], iaa.Emboss(**opt)))

        if cfg_cnv["edge"]:
            opt = get_aug_param(cfg_cnv["edge"])
            pipeline.add(iaa.Sometimes(cfg_cnv["edgeratio"], iaa.EdgeDetect(**opt)))

        return pipeline

    def get_batch_from_video(self):
        num_images = len(self.vid)
        batch_images = []
        batch_joints = []
        joint_ids = []
        data_items = []
        trim_ends = self.cfg.get("trim_ends", None)
        if trim_ends is None:
            trim_ends = 0
        # because of the existence of threshold, sampling population is adjusted to len(self.data)
        img_idx = np.random.choice(
            len(self.data) - trim_ends * 2, size=self.batch_size, replace=True
        )
        for i in range(self.batch_size):
            index = img_idx[i]
            offset = trim_ends
            data_item = self.data[index + offset]
            data_items.append(data_item)
            im_file = data_item.im_path

            logging.debug("image %s", im_file)
            self.vid.set_to_frame(index + offset)
            image = self.vid.read_frame()
            if self.has_gt:
                joints = data_item.joints
                if len(joints[0]) == 0:
                    # empty prediction for this frame
                    return None, None, None, None

                kpts = np.full((self._n_kpts * self._n_animals, 2), np.nan)
                for j in range(self._n_animals):
                    for n, x, y in joints.get(j, []):
                        kpts[j * self._n_kpts + int(n)] = x, y

                joint_id = [
                    np.array(list(range(self._n_kpts))) for _ in range(self._n_animals)
                ]
                joint_ids.append(joint_id)
                batch_joints.append(kpts)

            batch_images.append(image)

        return batch_images, joint_ids, batch_joints, data_items

    def get_batch(self):
        img_idx = np.random.choice(self.num_images, size=self.batch_size, replace=True)
        batch_images = []
        batch_joints = []
        joint_ids = []
        data_items = []
        for i in range(self.batch_size):
            data_item = self.data[img_idx[i]]

            data_items.append(data_item)
            im_file = data_item.im_path

            logging.debug("image %s", im_file)
            image = imread(
                os.path.join(self.cfg["project_path"], im_file), mode="skimage"
            )
            if self.has_gt:
                joints = data_item.joints
                kpts = np.full((self._n_kpts * self._n_animals, 2), np.nan)
                for j in range(self._n_animals):
                    for n, x, y in joints.get(j, []):
                        kpts[j * self._n_kpts + int(n)] = x, y
                joint_id = [
                    np.array(list(range(self._n_kpts))) for _ in range(self._n_animals)
                ]
                joint_ids.append(joint_id)
                batch_joints.append(kpts)

            batch_images.append(image)

        return batch_images, joint_ids, batch_joints, data_items

    def get_targetmaps_update(
        self,
        joint_ids,
        joints,
        data_items,
        sm_size,
        scale,
    ):
        part_score_targets = []
        part_score_weights = []
        locref_targets = []
        locref_masks = []
        partaffinityfield_targets = []
        partaffinityfield_masks = []
        for i in range(len(data_items)):
            if self.cfg.get("scmap_type", None) == "gaussian":
                assert 0 == 1  # not implemented for pafs!
                (
                    part_score_target,
                    part_score_weight,
                    locref_target,
                    locref_mask,
                ) = self.gaussian_scmap(
                    joint_ids[i], [joints[i]], data_items[i], sm_size, scale
                )
            else:
                (
                    part_score_target,
                    part_score_weight,
                    locref_target,
                    locref_mask,
                    partaffinityfield_target,
                    partaffinityfield_mask,
                ) = self.compute_target_part_scoremap_numpy(
                    joint_ids[i], joints[i], data_items[i], sm_size, scale
                )

            part_score_targets.append(part_score_target)
            part_score_weights.append(part_score_weight)
            locref_targets.append(locref_target)
            locref_masks.append(locref_mask)
            partaffinityfield_targets.append(partaffinityfield_target)
            partaffinityfield_masks.append(partaffinityfield_mask)

        return {
            Batch.part_score_targets: part_score_targets,
            Batch.part_score_weights: part_score_weights,
            Batch.locref_targets: locref_targets,
            Batch.locref_mask: locref_masks,
            Batch.pairwise_targets: partaffinityfield_targets,
            Batch.pairwise_mask: partaffinityfield_masks,
        }

    def calc_target_and_scoremap_sizes(self):
        target_size = self.default_size * self.sample_scale()
        target_size = np.ceil(target_size).astype(int)
        if not self.is_valid_size(target_size):
            target_size = self.default_size
        stride = self.cfg["stride"]
        sm_size = np.ceil(target_size / (stride * self.cfg.get("smfactor", 2))).astype(
            int
        ) * self.cfg.get("smfactor", 2)
        if stride == 2:
            sm_size = np.ceil(target_size / 16).astype(int)
            sm_size *= 8
        return target_size, sm_size

    def next_batch(self, plotting=False):
        while True:
            if self.vid:
                (
                    batch_images,
                    joint_ids,
                    batch_joints,
                    data_items,
                ) = self.get_batch_from_video()
            else:
                batch_images, joint_ids, batch_joints, data_items = self.get_batch()

            # in case it's empty prediction
            if batch_joints is None or batch_images is None:
                continue

            # Scale is sampled only once (per batch) to transform all of the images into same size.
            target_size, sm_size = self.calc_target_and_scoremap_sizes()
            scale = np.mean(target_size / self.default_size)
            augmentation.update_crop_size(self.pipeline, *target_size)
            batch_images, batch_joints = self.pipeline(
                images=batch_images, keypoints=batch_joints
            )
            batch_images = np.asarray(batch_images)
            image_shape = batch_images.shape[1:3]
            # Discard keypoints whose coordinates lie outside the cropped image
            batch_joints_valid = []
            joint_ids_valid = []

            for joints, ids in zip(batch_joints, joint_ids):
                # Invisible joints are represented by nans
                visible = ~np.isnan(joints[:, 0])
                inside = np.logical_and.reduce(
                    (
                        joints[:, 0] < image_shape[1],
                        joints[:, 0] > 0,
                        joints[:, 1] < image_shape[0],
                        joints[:, 1] > 0,
                    )
                )
                mask = visible & inside
                batch_joints_valid.append(joints[mask])

                temp = []
                start = 0
                for array in ids:
                    end = start + array.size
                    inds = np.arange(start, end)
                    temp.append(array[mask[inds]])
                    start = end
                joint_ids_valid.append(temp)

            # If you would like to check the augmented images, script for saving
            # the images with joints on:
            if plotting:
                for i in range(self.batch_size):
                    joints = batch_joints_valid[i]
                    kps = KeypointsOnImage(
                        [Keypoint(x=joint[0], y=joint[1]) for joint in joints],
                        shape=batch_images[i].shape,
                    )
                    im = kps.draw_on_image(batch_images[i])
                    imageio.imwrite(
                        os.path.join(self.cfg["project_path"], str(i) + ".png"), im
                    )

            batch = {Batch.inputs: batch_images.astype(np.float64)}
            if self.has_gt:
                targetmaps = self.get_targetmaps_update(
                    joint_ids_valid,
                    batch_joints_valid,
                    data_items,
                    (sm_size[1], sm_size[0]),
                    scale,
                )
                batch.update(targetmaps)

            batch = {key: np.asarray(data) for (key, data) in batch.items()}
            batch[Batch.data_item] = data_items
            return batch

    def set_test_mode(self, test_mode):
        self.has_gt = not test_mode

    def num_training_samples(self):
        num = self.num_images
        if self.cfg["mirror"]:
            num *= 2
        return num

    def is_valid_size(self, target_size):
        im_width, im_height = target_size
        min_input_size = self.cfg.get("min_input_size", 100)
        if im_height < min_input_size or im_width < min_input_size:
            return False
        if "max_input_size" in self.cfg:
            max_input_size = self.cfg["max_input_size"]
            if im_width * im_height > max_input_size * max_input_size:
                return False
        return True

    def compute_scmap_weights(self, scmap_shape, joint_id):
        cfg = self.cfg
        if cfg["weigh_only_present_joints"]:
            weights = np.zeros(scmap_shape)
            for k, j_id in enumerate(
                np.concatenate(joint_id)
            ):  # looping over all animals
                weights[:, :, j_id] = 1.0
        else:
            weights = np.ones(scmap_shape)
        return weights

    def compute_target_part_scoremap_numpy(
        self, joint_id, coords, data_item, size, scale
    ):
        stride = self.cfg["stride"]
        half_stride = stride // 2
        dist_thresh = float(self.cfg["pos_dist_thresh"] * scale)
        num_idchannel = self.cfg.get("num_idchannel", 0)

        num_joints = self.cfg["num_joints"]

        scmap = np.zeros((*size, num_joints + num_idchannel))
        locref_size = *size, num_joints * 2
        locref_map = np.zeros(locref_size)
        locref_scale = 1.0 / self.cfg["locref_stdev"]
        dist_thresh_sq = dist_thresh**2

        partaffinityfield_shape = *size, self.cfg["num_limbs"] * 2
        partaffinityfield_map = np.zeros(partaffinityfield_shape)
        if self.cfg["weigh_only_present_joints"]:
            partaffinityfield_mask = np.zeros(partaffinityfield_shape)
            locref_mask = np.zeros(locref_size)
        else:
            partaffinityfield_mask = np.ones(partaffinityfield_shape)
            locref_mask = np.ones(locref_size)

        height, width = size
        grid = np.mgrid[:height, :width].transpose((1, 2, 0))
        xx = np.expand_dims(grid[..., 1], axis=2)
        yy = np.expand_dims(grid[..., 0], axis=2)

        # Produce score maps and location refinement fields
        coords_sm = np.round((coords - half_stride) / stride).astype(int)
        mins = np.round(np.maximum(coords_sm - dist_thresh - 1, 0)).astype(int)
        maxs = np.round(
            np.minimum(coords_sm + dist_thresh + 1, [width - 1, height - 1])
        ).astype(int)
        dx = coords[:, 0] - xx * stride - half_stride
        dx_ = dx * locref_scale
        dy = coords[:, 1] - yy * stride - half_stride
        dy_ = dy * locref_scale
        dist = dx**2 + dy**2
        mask1 = dist <= dist_thresh_sq
        mask2 = (xx >= mins[:, 0]) & (xx <= maxs[:, 0])
        mask3 = (yy >= mins[:, 1]) & (yy <= maxs[:, 1])
        mask = mask1 & mask2 & mask3
        for n, ind in enumerate(np.concatenate(joint_id).tolist()):
            mask_ = mask[..., n]
            scmap[mask_, ind] = 1
            if self.cfg["weigh_only_present_joints"]:
                locref_mask[mask_, [ind * 2 + 0, ind * 2 + 1]] = 1.0
            locref_map[mask_, ind * 2 + 0] = dx_[mask_, n]
            locref_map[mask_, ind * 2 + 1] = dy_[mask_, n]

        if num_idchannel > 0:
            coordinateoffset = 0
            # Find indices of individuals in joint_id
            idx = [
                (i, id_)
                for i, id_ in enumerate(data_item.joints)
                if id_ < num_idchannel
            ]
            for i, person_id in idx:
                joint_ids = joint_id[i]
                n_joints = joint_ids.size
                if n_joints:
                    inds = np.arange(n_joints) + coordinateoffset
                    mask_ = mask[..., inds].any(axis=2)
                    scmap[mask_, person_id + num_joints] = 1
                coordinateoffset += n_joints

        coordinateoffset = 0  # the offset based on
        y, x = np.rollaxis(grid * stride + half_stride, 2)
        if self.cfg["partaffinityfield_graph"]:
            for person_id in range(len(joint_id)):
                joint_ids = joint_id[person_id].tolist()
                if len(joint_ids) >= 2:  # there is a possible edge
                    for l, (bp1, bp2) in enumerate(self.cfg["partaffinityfield_graph"]):
                        try:
                            ind1 = joint_ids.index(bp1)
                        except ValueError:
                            continue
                        try:
                            ind2 = joint_ids.index(bp2)
                        except ValueError:
                            continue
                        j_x, j_y = coords[ind1 + coordinateoffset]
                        linkedj_x, linkedj_y = coords[ind2 + coordinateoffset]
                        dist = sqrt((linkedj_x - j_x) ** 2 + (linkedj_y - j_y) ** 2)
                        if dist > 0:
                            Dx = (linkedj_x - j_x) / dist  # x-axis UNIT VECTOR
                            Dy = (linkedj_y - j_y) / dist
                            d1 = [
                                Dx * j_x + Dy * j_y,
                                Dx * linkedj_x + Dy * linkedj_y,
                            ]  # in-line with direct axis
                            d1lowerboundary = min(d1)
                            d1upperboundary = max(d1)
                            d2mid = j_y * Dx - j_x * Dy  # orthogonal direction

                            distance_along = Dx * x + Dy * y
                            distance_across = (
                                ((y * Dx - x * Dy) - d2mid)
                                * 1.0
                                / self.cfg["pafwidth"]
                                * scale
                            )

                            mask1 = (distance_along >= d1lowerboundary) & (
                                distance_along <= d1upperboundary
                            )
                            distance_across_abs = np.abs(distance_across)
                            mask2 = distance_across_abs <= 1
                            mask = mask1 & mask2
                            temp = 1 - distance_across_abs[mask]
                            if self.cfg["weigh_only_present_joints"]:
                                partaffinityfield_mask[mask, [l * 2 + 0, l * 2 + 1]] = (
                                    1.0
                                )
                            partaffinityfield_map[mask, l * 2 + 0] = Dx * temp
                            partaffinityfield_map[mask, l * 2 + 1] = Dy * temp

                coordinateoffset += len(joint_ids)  # keeping track of the blocks

        weights = self.compute_scmap_weights(scmap.shape, joint_id)
        return (
            scmap,
            weights,
            locref_map,
            locref_mask,
            partaffinityfield_map,
            partaffinityfield_mask,
        )

    def gaussian_scmap(self, joint_id, coords, data_item, size, scale):
        # WIP!
        stride = self.cfg["stride"]
        dist_thresh = float(self.cfg["pos_dist_thresh"] * scale)
        num_idchannel = self.cfg.get("num_idchannel", 0)

        num_joints = self.cfg["num_joints"]
        half_stride = stride / 2
        scmap = np.zeros(np.concatenate([size, np.array([num_joints])]))
        locref_size = np.concatenate([size, np.array([num_joints * 2])])
        locref_mask = np.zeros(locref_size)
        locref_map = np.zeros(locref_size)

        locref_scale = 1.0 / self.cfg["locref_stdev"]
        dist_thresh_sq = dist_thresh**2

        partaffinityfield_shape = np.concatenate(
            [size, np.array([self.cfg["num_limbs"] * 2])]
        )
        partaffinityfield_map = np.zeros(partaffinityfield_shape)
        if self.cfg["weigh_only_present_joints"]:
            partaffinityfield_mask = np.zeros(partaffinityfield_shape)
            locref_mask = np.zeros(locref_size)
        else:
            partaffinityfield_mask = np.ones(partaffinityfield_shape)
            locref_mask = np.ones(locref_size)

        # STD of gaussian is 1/4 of threshold
        std = dist_thresh / 4
        width = size[1]
        height = size[0]
        # Grid of coordinates
        grid = np.mgrid[:height, :width].transpose((1, 2, 0))
        grid = grid * stride + half_stride
        # the animal id plays no role for scoremap + locref!
        # so let's just loop over all bpts.
        for k, j_id in enumerate(np.concatenate(joint_id)):
            joint_pt = coords[0][k, :]
            j_x = joint_pt[0].item()
            j_x_sm = round((j_x - half_stride) / stride)
            j_y = joint_pt[1].item()
            j_y_sm = round((j_y - half_stride) / stride)

            map_j = grid.copy()
            # Distance between the joint point and each coordinate
            dist = np.linalg.norm(grid - (j_y, j_x), axis=2) ** 2
            scmap_j = np.exp(-dist / (2 * (std**2)))
            scmap[..., j_id] = scmap_j
            locref_mask[dist <= dist_thresh_sq, j_id * 2 + 0] = 1
            locref_mask[dist <= dist_thresh_sq, j_id * 2 + 1] = 1
            dx = j_x - grid.copy()[:, :, 1]
            dy = j_y - grid.copy()[:, :, 0]
            locref_map[..., j_id * 2 + 0] = dx * locref_scale
            locref_map[..., j_id * 2 + 1] = dy * locref_scale

        if num_idchannel > 0:
            # NEEDS TO BE DONE!
            assert 0 == 1

        coordinateoffset = 0  # the offset based on
        for person_id in range(len(joint_id)):
            # for k, joint_ids in enumerate(joint_id[person_id]):
            joint_ids = joint_id[person_id].copy()
            if len(joint_ids) > 1:  # otherwise there cannot be a joint!
                # CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?)
                for l in range(self.cfg["num_limbs"]):
                    bp1, bp2 = self.cfg["partaffinityfield_graph"][l]
                    I1 = np.where(np.array(joint_ids) == bp1)[0]
                    I2 = np.where(np.array(joint_ids) == bp2)[0]
                    if (len(I1) > 0) * (len(I2) > 0):
                        indbp1 = I1[0].item()
                        indbp2 = I2[0].item()
                        j_x = (coords[0][indbp1 + coordinateoffset, 0]).item()
                        j_y = (coords[0][indbp1 + coordinateoffset, 1]).item()

                        linkedj_x = (coords[0][indbp2 + coordinateoffset, 0]).item()
                        linkedj_y = (coords[0][indbp2 + coordinateoffset, 1]).item()

                        dist = np.sqrt((linkedj_x - j_x) ** 2 + (linkedj_y - j_y) ** 2)
                        if dist > 0:
                            Dx = (linkedj_x - j_x) * 1.0 / dist  # x-axis UNIT VECTOR
                            Dy = (linkedj_y - j_y) * 1.0 / dist

                            d1 = [
                                Dx * j_x + Dy * j_y,
                                Dx * linkedj_x + Dy * linkedj_y,
                            ]  # in-line with direct axis
                            d1lowerboundary = min(d1)
                            d1upperboundary = max(d1)
                            d2mid = j_y * Dx - j_x * Dy  # orthogonal direction

                            distance_along = Dx * (x * stride + half_stride) + Dy * (
                                y * stride + half_stride
                            )
                            distance_across = (
                                (
                                    (
                                        (y * stride + half_stride) * Dx
                                        - (x * stride + half_stride) * Dy
                                    )
                                    - d2mid
                                )
                                * 1.0
                                / self.cfg["pafwidth"]
                                * scale
                            )
                            mask1 = (distance_along >= d1lowerboundary) & (
                                distance_along <= d1upperboundary
                            )
                            mask2 = np.abs(distance_across) <= 1
                            # mask3 = ((x >= 0) & (x <= width-1))
                            # mask4 = ((y >= 0) & (y <= height-1))
                            mask = mask1 & mask2  # &mask3 &mask4
                            if self.cfg["weigh_only_present_joints"]:
                                partaffinityfield_mask[mask, l * 2 + 0] = 1.0
                                partaffinityfield_mask[mask, l * 2 + 1] = 1.0

                            partaffinityfield_map[mask, l * 2 + 0] = (
                                Dx * (1 - abs(distance_across))
                            )[mask]
                            partaffinityfield_map[mask, l * 2 + 1] = (
                                Dy * (1 - abs(distance_across))
                            )[mask]

            coordinateoffset += len(joint_ids)  # keeping track of the blocks

        weights = self.compute_scmap_weights(scmap.shape, joint_id)
        return scmap, weights, locref_map, locref_mask


--- File: deeplabcut/pose_estimation_tensorflow/datasets/pose_imgaug.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Uses imgaug dataflow for flexible augmentation
Largely written by Mert Yüksekgönül during the summer in the Bethge lab -- Thanks!
https://imgaug.readthedocs.io/en/latest/
"""

import logging
import os
import pickle

import imgaug.augmenters as iaa
import numpy as np
import scipy.io as sio
from deeplabcut.pose_estimation_tensorflow.datasets import augmentation
from deeplabcut.utils.auxfun_videos import imread
from deeplabcut.utils.conversioncode import robust_split_path
from .factory import PoseDatasetFactory
from .pose_base import BasePoseDataset
from .utils import DataItem, Batch


@PoseDatasetFactory.register("default")
@PoseDatasetFactory.register("imgaug")
class ImgaugPoseDataset(BasePoseDataset):
    def __init__(self, cfg):
        super(ImgaugPoseDataset, self).__init__(cfg)
        self._n_kpts = len(cfg["all_joints_names"])
        self.data = self.load_dataset()
        self.batch_size = cfg.get("batch_size", 1)
        self.num_images = len(self.data)
        self.max_input_sizesquare = cfg.get("max_input_size", 1500) ** 2
        self.min_input_sizesquare = cfg.get("min_input_size", 64) ** 2

        self.locref_scale = 1.0 / cfg["locref_stdev"]
        self.stride = cfg["stride"]
        self.half_stride = self.stride / 2
        self.scale = cfg["global_scale"]

        # parameter initialization for augmentation pipeline:
        self.scale_jitter_lo = cfg.get("scale_jitter_lo", 0.75)
        self.scale_jitter_up = cfg.get("scale_jitter_up", 1.25)

        cfg["mirror"] = cfg.get("mirror", False)
        cfg["rotation"] = cfg.get("rotation", True)
        if cfg.get("rotation", True):  # i.e. pm 10 degrees
            opt = cfg.get("rotation", False)
            if type(opt) == int:
                cfg["rotation"] = cfg.get("rotation", 25)
            else:
                cfg["rotation"] = 25
            cfg["rotratio"] = cfg.get(
                "rotateratio", 0.4
            )  # what is the fraction of training samples with rotation augmentation?
        else:
            cfg["rotratio"] = 0.0
            cfg["rotation"] = 0

        cfg["covering"] = cfg.get("covering", True)
        cfg["elastic_transform"] = cfg.get("elastic_transform", True)

        cfg["motion_blur"] = cfg.get("motion_blur", True)
        if cfg["motion_blur"]:
            cfg["motion_blur_params"] = dict(
                cfg.get("motion_blur_params", {"k": 7, "angle": (-90, 90)})
            )

        print("Batch Size is %d" % self.batch_size)

    def load_dataset(self):
        cfg = self.cfg
        file_name = os.path.join(self.cfg["project_path"], cfg["dataset"])
        if ".mat" in file_name:  # legacy loader
            mlab = sio.loadmat(file_name)
            self.raw_data = mlab
            mlab = mlab["dataset"]

            num_images = mlab.shape[1]
            data = []
            has_gt = True

            for i in range(num_images):
                sample = mlab[0, i]

                item = DataItem()
                item.image_id = i
                im_path = sample[0][0]
                if isinstance(im_path, str):
                    im_path = robust_split_path(im_path)
                else:
                    im_path = [s.strip() for s in im_path]
                item.im_path = os.path.join(*im_path)
                item.im_size = sample[1][0]
                if len(sample) >= 3:
                    joints = sample[2][0][0]
                    joint_id = joints[:, 0]
                    # make sure joint ids are 0-indexed
                    if joint_id.size != 0:
                        assert (joint_id < cfg["num_joints"]).any()
                    joints[:, 0] = joint_id
                    item.joints = [joints]
                else:
                    has_gt = False
                data.append(item)

            self.has_gt = has_gt
            return data
        else:
            print("Loading pickle data with float coordinates!")
            file_name = cfg["dataset"].split(".")[0] + ".pickle"
            with open(os.path.join(self.cfg["project_path"], file_name), "rb") as f:
                pickledata = pickle.load(f)

            self.raw_data = pickledata
            num_images = len(pickledata)  # mlab.shape[1]
            data = []
            has_gt = True
            for i in range(num_images):
                sample = pickledata[i]  # mlab[0, i]
                item = DataItem()
                item.image_id = i
                item.im_path = os.path.join(*sample["image"])  # [0][0]
                item.im_size = sample["size"]  # sample[1][0]
                if len(sample) >= 3:
                    item.num_animals = len(sample["joints"])
                    item.joints = [sample["joints"]]

                else:
                    has_gt = False
                data.append(item)
            self.has_gt = has_gt
            return data

    def build_augmentation_pipeline(self, height=None, width=None, apply_prob=0.5):
        sometimes = lambda aug: iaa.Sometimes(apply_prob, aug)
        pipeline = iaa.Sequential(random_order=False)

        cfg = self.cfg
        if cfg["mirror"]:
            opt = cfg["mirror"]  # fliplr
            if type(opt) == int:
                pipeline.add(sometimes(iaa.Fliplr(opt)))
            else:
                pipeline.add(sometimes(iaa.Fliplr(0.5)))

        if cfg.get("fliplr", False) and cfg.get("symmetric_pairs"):
            opt = cfg.get("fliplr", False)
            if type(opt) == int:
                p = opt
            else:
                p = 0.5
            pipeline.add(
                sometimes(
                    augmentation.KeypointFliplr(
                        cfg["all_joints_names"],
                        symmetric_pairs=cfg["symmetric_pairs"],
                        p=p,
                    )
                )
            )

        if cfg["rotation"] > 0:
            pipeline.add(
                iaa.Sometimes(
                    cfg["rotratio"],
                    iaa.Affine(rotate=(-cfg["rotation"], cfg["rotation"])),
                )
            )

        if cfg["motion_blur"]:
            opts = cfg["motion_blur_params"]
            pipeline.add(sometimes(iaa.MotionBlur(**opts)))

        if cfg["covering"]:
            pipeline.add(
                sometimes(iaa.CoarseDropout(0.02, size_percent=0.3, per_channel=0.5))
            )

        if cfg["elastic_transform"]:
            pipeline.add(sometimes(iaa.ElasticTransformation(sigma=5)))

        if cfg.get("gaussian_noise", False):
            opt = cfg.get("gaussian_noise", False)
            if type(opt) == int or type(opt) == float:
                pipeline.add(
                    sometimes(
                        iaa.AdditiveGaussianNoise(
                            loc=0, scale=(0.0, opt), per_channel=0.5
                        )
                    )
                )
            else:
                pipeline.add(
                    sometimes(
                        iaa.AdditiveGaussianNoise(
                            loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5
                        )
                    )
                )
        if cfg.get("grayscale", False):
            pipeline.add(sometimes(iaa.Grayscale(alpha=(0.5, 1.0))))

        def get_aug_param(cfg_value):
            if isinstance(cfg_value, dict):
                opt = cfg_value
            else:
                opt = {}
            return opt

        cfg_cnt = cfg.get("contrast", {})
        cfg_cnv = cfg.get("convolution", {})

        contrast_aug = ["histeq", "clahe", "gamma", "sigmoid", "log", "linear"]
        for aug in contrast_aug:
            aug_val = cfg_cnt.get(aug, False)
            cfg_cnt[aug] = aug_val
            if aug_val:
                cfg_cnt[aug + "ratio"] = cfg_cnt.get(aug + "ratio", 0.1)

        convolution_aug = ["sharpen", "emboss", "edge"]
        for aug in convolution_aug:
            aug_val = cfg_cnv.get(aug, False)
            cfg_cnv[aug] = aug_val
            if aug_val:
                cfg_cnv[aug + "ratio"] = cfg_cnv.get(aug + "ratio", 0.1)

        if cfg_cnt["histeq"]:
            opt = get_aug_param(cfg_cnt["histeq"])
            pipeline.add(
                iaa.Sometimes(
                    cfg_cnt["histeqratio"], iaa.AllChannelsHistogramEqualization(**opt)
                )
            )

        if cfg_cnt["clahe"]:
            opt = get_aug_param(cfg_cnt["clahe"])
            pipeline.add(
                iaa.Sometimes(cfg_cnt["claheratio"], iaa.AllChannelsCLAHE(**opt))
            )

        if cfg_cnt["log"]:
            opt = get_aug_param(cfg_cnt["log"])
            pipeline.add(iaa.Sometimes(cfg_cnt["logratio"], iaa.LogContrast(**opt)))

        if cfg_cnt["linear"]:
            opt = get_aug_param(cfg_cnt["linear"])
            pipeline.add(
                iaa.Sometimes(cfg_cnt["linearratio"], iaa.LinearContrast(**opt))
            )

        if cfg_cnt["sigmoid"]:
            opt = get_aug_param(cfg_cnt["sigmoid"])
            pipeline.add(
                iaa.Sometimes(cfg_cnt["sigmoidratio"], iaa.SigmoidContrast(**opt))
            )

        if cfg_cnt["gamma"]:
            opt = get_aug_param(cfg_cnt["gamma"])
            pipeline.add(iaa.Sometimes(cfg_cnt["gammaratio"], iaa.GammaContrast(**opt)))

        if cfg_cnv["sharpen"]:
            opt = get_aug_param(cfg_cnv["sharpen"])
            pipeline.add(iaa.Sometimes(cfg_cnv["sharpenratio"], iaa.Sharpen(**opt)))

        if cfg_cnv["emboss"]:
            opt = get_aug_param(cfg_cnv["emboss"])
            pipeline.add(iaa.Sometimes(cfg_cnv["embossratio"], iaa.Emboss(**opt)))

        if cfg_cnv["edge"]:
            opt = get_aug_param(cfg_cnv["edge"])
            pipeline.add(iaa.Sometimes(cfg_cnv["edgeratio"], iaa.EdgeDetect(**opt)))

        if height is not None and width is not None:
            if not cfg.get("crop_by", False):
                crop_by = 0.15
            else:
                crop_by = cfg.get("crop_by", False)
            pipeline.add(
                iaa.Sometimes(
                    cfg.get("cropratio", 0.4),
                    iaa.CropAndPad(percent=(-crop_by, crop_by), keep_size=False),
                )
            )
            pipeline.add(iaa.Resize({"height": height, "width": width}))
        return pipeline

    def get_batch(self):
        img_idx = np.random.choice(self.num_images, size=self.batch_size, replace=True)
        batch_images = []
        batch_joints = []
        joint_ids = []
        data_items = []

        # Scale is sampled only once to transform all of the images of a batch into same size.
        scale = self.sample_scale()

        found_valid = False
        n_tries = 10
        while n_tries > 1:
            idx = np.random.choice(self.num_images)
            size = self.data[idx].im_size
            target_size = np.ceil(size[1:3] * scale).astype(int)
            if self.is_valid_size(target_size[1] * target_size[0]):
                found_valid = True
                break
            n_tries -= 1
        if not found_valid:
            if size[1] * size[2] > self.max_input_sizesquare:
                s = "large", "increasing `max_input_size`", "decreasing"
            else:
                s = "small", "decreasing `min_input_size`", "increasing"
            raise ValueError(
                f"Image size {size[1:3]} may be too {s[0]}. "
                f"Consider {s[1]} and/or {s[2]} `global_scale` "
                "in the train/pose_cfg.yaml."
            )

        stride = self.cfg["stride"]
        for i in range(self.batch_size):
            data_item = self.data[img_idx[i]]

            data_items.append(data_item)
            im_file = data_item.im_path

            logging.debug("image %s", im_file)
            image = imread(
                os.path.join(self.cfg["project_path"], im_file), mode="skimage"
            )

            if self.has_gt:
                joints = data_item.joints
                kpts = np.full((self._n_kpts, 2), np.nan)

                for n, x, y in joints[0]:
                    kpts[int(n)] = x, y

                joint_ids.append([np.arange(self._n_kpts)])
                batch_joints.append(kpts)

            batch_images.append(image)
        sm_size = np.ceil(target_size / (stride * 2)).astype(int) * 2
        assert len(batch_images) == self.batch_size
        return batch_images, joint_ids, batch_joints, data_items, sm_size, target_size

    def get_scmap_update(self, joint_ids, joints, data_items, sm_size, target_size):
        part_score_targets, part_score_weights, locref_targets, locref_masks = (
            [],
            [],
            [],
            [],
        )
        for i in range(len(data_items)):
            # Approximating the scale
            scale = min(
                target_size[0] / data_items[i].im_size[1],
                target_size[1] / data_items[i].im_size[2],
            )
            if self.cfg.get("scmap_type", None) == "gaussian":
                (
                    part_score_target,
                    part_score_weight,
                    locref_target,
                    locref_mask,
                ) = self.gaussian_scmap(
                    joint_ids[i], [joints[i]], data_items[i], sm_size, scale
                )
            else:
                (
                    part_score_target,
                    part_score_weight,
                    locref_target,
                    locref_mask,
                ) = self.compute_target_part_scoremap_numpy(
                    joint_ids[i], [joints[i]], data_items[i], sm_size, scale
                )
            part_score_targets.append(part_score_target)
            part_score_weights.append(part_score_weight)
            locref_targets.append(locref_target)
            locref_masks.append(locref_mask)

        return {
            Batch.part_score_targets: part_score_targets,
            Batch.part_score_weights: part_score_weights,
            Batch.locref_targets: locref_targets,
            Batch.locref_mask: locref_masks,
        }

    def next_batch(self):
        cfg = self.cfg
        while True:
            (
                batch_images,
                joint_ids,
                batch_joints,
                data_items,
                sm_size,
                target_size,
            ) = self.get_batch()

            pipeline = self.build_augmentation_pipeline(
                height=target_size[0], width=target_size[1],
                apply_prob=cfg.get("apply_prob", 0.5),
            )

            batch_images, batch_joints = pipeline(
                images=batch_images, keypoints=batch_joints
            )

            image_shape = np.array(batch_images).shape[1:3]

            batch_joints_valid = []
            joint_ids_valid = []
            for joints, ids in zip(batch_joints, joint_ids):
                # invisible joints are represented by nans
                mask = ~np.isnan(joints[:, 0])
                joints = joints[mask, :]
                ids = ids[0][mask]
                inside = np.logical_and.reduce(
                    (
                        joints[:, 0] < image_shape[1],
                        joints[:, 0] > 0,
                        joints[:, 1] < image_shape[0],
                        joints[:, 1] > 0,
                    )
                )

                batch_joints_valid.append(joints[inside])
                joint_ids_valid.append([ids[inside]])

            # If you would like to check the augmented images, script for saving
            # the images with joints on:
            # import imageio
            # for i in range(self.batch_size):
            #    joints = batch_joints[i]
            #    kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)
            #    im = kps.draw_on_image(batch_images[i])
            #    imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)

            batch = {Batch.inputs: np.array(batch_images).astype(np.float64)}
            if self.has_gt:
                scmap_update = self.get_scmap_update(
                    joint_ids_valid,
                    batch_joints_valid,
                    data_items,
                    sm_size,
                    image_shape,
                )
                batch.update(scmap_update)

            batch = {key: np.asarray(data) for (key, data) in batch.items()}
            batch[Batch.data_item] = data_items
            return batch

    def set_test_mode(self, test_mode):
        self.has_gt = not test_mode

    def num_training_samples(self):
        num = self.num_images
        if self.cfg["mirror"]:
            num *= 2
        return num

    def is_valid_size(self, target_size_product):
        if target_size_product > self.max_input_sizesquare:
            return False

        if target_size_product < self.min_input_sizesquare:
            return False

        return True

    def gaussian_scmap(self, joint_id, coords, data_item, size, scale):
        # dist_thresh = float(self.cfg.pos_dist_thresh * scale)
        num_joints = self.cfg["num_joints"]
        scmap = np.zeros(np.concatenate([size, np.array([num_joints])]))
        locref_size = np.concatenate([size, np.array([num_joints * 2])])
        locref_mask = np.zeros(locref_size)
        locref_map = np.zeros(locref_size)

        width = size[1]
        height = size[0]
        dist_thresh = float((width + height) / 6)
        dist_thresh_sq = dist_thresh**2

        std = dist_thresh / 4
        # Grid of coordinates
        grid = np.mgrid[:height, :width].transpose((1, 2, 0))
        grid = grid * self.stride + self.half_stride
        for person_id in range(len(coords)):
            for k, j_id in enumerate(joint_id[person_id]):
                joint_pt = coords[person_id][k, :]
                j_x = np.asarray(joint_pt[0]).item()
                j_x_sm = round((j_x - self.half_stride) / self.stride)
                j_y = np.asarray(joint_pt[1]).item()
                j_y_sm = round((j_y - self.half_stride) / self.stride)
                map_j = grid.copy()
                # Distance between the joint point and each coordinate
                dist = np.linalg.norm(grid - (j_y, j_x), axis=2) ** 2
                scmap_j = np.exp(-dist / (2 * (std**2)))
                scmap[..., j_id] = scmap_j
                locref_mask[dist <= dist_thresh_sq, j_id * 2 + 0] = 1
                locref_mask[dist <= dist_thresh_sq, j_id * 2 + 1] = 1
                dx = j_x - grid.copy()[:, :, 1]
                dy = j_y - grid.copy()[:, :, 0]
                locref_map[..., j_id * 2 + 0] = dx * self.locref_scale
                locref_map[..., j_id * 2 + 1] = dy * self.locref_scale
        weights = self.compute_scmap_weights(scmap.shape, joint_id, data_item)
        return scmap, weights, locref_map, locref_mask

    def compute_scmap_weights(self, scmap_shape, joint_id, data_item):
        if self.cfg["weigh_only_present_joints"]:
            weights = np.zeros(scmap_shape)
            for person_joint_id in joint_id:
                for j_id in person_joint_id:
                    weights[:, :, j_id] = 1.0
        else:
            weights = np.ones(scmap_shape)
        return weights

    def compute_target_part_scoremap_numpy(
        self, joint_id, coords, data_item, size, scale
    ):
        dist_thresh = float(self.cfg["pos_dist_thresh"] * scale)
        dist_thresh_sq = dist_thresh**2
        num_joints = self.cfg["num_joints"]

        scmap = np.zeros(np.concatenate([size, np.array([num_joints])]))
        locref_size = np.concatenate([size, np.array([num_joints * 2])])
        locref_mask = np.zeros(locref_size)
        locref_map = np.zeros(locref_size)

        width = size[1]
        height = size[0]
        grid = np.mgrid[:height, :width].transpose((1, 2, 0))

        for person_id in range(len(coords)):
            for k, j_id in enumerate(joint_id[person_id]):
                joint_pt = coords[person_id][k, :]
                j_x = np.asarray(joint_pt[0]).item()
                j_x_sm = round((j_x - self.half_stride) / self.stride)
                j_y = np.asarray(joint_pt[1]).item()
                j_y_sm = round((j_y - self.half_stride) / self.stride)
                min_x = round(max(j_x_sm - dist_thresh - 1, 0))
                max_x = round(min(j_x_sm + dist_thresh + 1, width - 1))
                min_y = round(max(j_y_sm - dist_thresh - 1, 0))
                max_y = round(min(j_y_sm + dist_thresh + 1, height - 1))
                x = grid.copy()[:, :, 1]
                y = grid.copy()[:, :, 0]
                dx = j_x - x * self.stride - self.half_stride
                dy = j_y - y * self.stride - self.half_stride
                dist = dx**2 + dy**2
                mask1 = dist <= dist_thresh_sq
                mask2 = (x >= min_x) & (x <= max_x)
                mask3 = (y >= min_y) & (y <= max_y)
                mask = mask1 & mask2 & mask3
                scmap[mask, j_id] = 1
                locref_mask[mask, j_id * 2 + 0] = 1
                locref_mask[mask, j_id * 2 + 1] = 1
                locref_map[mask, j_id * 2 + 0] = (dx * self.locref_scale)[mask]
                locref_map[mask, j_id * 2 + 1] = (dy * self.locref_scale)[mask]

        weights = self.compute_scmap_weights(scmap.shape, joint_id, data_item)
        return scmap, weights, locref_map, locref_mask


--- File: deeplabcut/pose_estimation_tensorflow/datasets/pose_base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import abc
import numpy as np


class BasePoseDataset(metaclass=abc.ABCMeta):
    # TODO Finish implementing actual abstract class
    def __init__(self, cfg):
        self.cfg = cfg

    @abc.abstractmethod
    def load_dataset(self): ...

    @abc.abstractmethod
    def next_batch(self): ...

    def sample_scale(self):
        if self.cfg.get("deterministic", False):
            np.random.seed(42)
        scale = self.cfg["global_scale"]
        if "scale_jitter_lo" in self.cfg and "scale_jitter_up" in self.cfg:
            scale_jitter = np.random.uniform(
                self.cfg["scale_jitter_lo"], self.cfg["scale_jitter_up"]
            )
            scale *= scale_jitter
        return scale


--- File: deeplabcut/pose_estimation_tensorflow/models/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_estimation_tensorflow/models/pretrained/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_estimation_tensorflow/models/pretrained/pretrained_model_urls.yaml ---
#Model Zoo from where the Tensor(s) flow
resnet_50: http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz
resnet_101: http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz
resnet_152: http://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz

#Source: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet
mobilenet_v2_1.0: https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz
mobilenet_v2_0.75: https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_224.tgz
mobilenet_v2_0.5: https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_224.tgz
mobilenet_v2_0.35: https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_224.tgz

#Source: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet
efficientnet: https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/ckptsaug/

### DEPRECIATED 2023 -- customized models are banked on HuggingFace and the dlclibrary hosts downloading.
##Harvard server down since Nov 2022
#Previous Model Zoo URLs from where the Charles flow(s)
#full_human: http://deeplabcut.rowland.harvard.edu/models/DLC_human_fullbody_resnet_101.tar.gz
#full_dog: http://deeplabcut.rowland.harvard.edu/models/DLC_Dog_resnet_50_iteration-0_shuffle-0.tar.gz
#full_cat: http://deeplabcut.rowland.harvard.edu/models/DLC_Cat_resnet_50_iteration-0_shuffle-0.tar.gz
#primate_face: http://deeplabcut.rowland.harvard.edu/models/DLC_primate_face_resnet_50_iteration-1_shuffle-1.tar.gz
#mouse_pupil_vclose: http://deeplabcut.rowland.harvard.edu/models/DLC_mouse_pupil_vclose_resnet_50_iteration-0_shuffle-1.tar.gz
#horse_sideview: http://deeplabcut.rowland.harvard.edu/models/DLC_Horses_resnet_50_iteration-1_shuffle-1.tar.gz
#full_macaque: http://deeplabcut.rowland.harvard.edu/models/DLC_macaque_full_resnet50.tar.gz
#full_cheetah: http://deeplabcut.rowland.harvard.edu/models/DLC_full_cheetah_resnet_152.tar.gz

# Current HuggingFace URLs at: https://github.com/DeepLabCut/DLClibrary/blob/main/dlclibrary/modelzoo_urls.yaml


--- File: deeplabcut/pose_estimation_tensorflow/models/pretrained/download.sh ---
# legacy.
#!/bin/sh

curl http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz | tar xvz
curl http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz | tar xvz
curl http://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz | tar xvz

--- File: deeplabcut/pose_estimation_tensorflow/modelzoo/api/superanimal_inference.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import glob
import os
import os.path
import pickle
import time
import warnings
from pathlib import Path

import imgaug.augmenters as iaa
import numpy as np
import pandas as pd
from skimage.util import img_as_ubyte
from tqdm import tqdm

from deeplabcut.pose_estimation_tensorflow.config import load_config
from deeplabcut.pose_estimation_tensorflow.core import predict as single_predict
from deeplabcut.pose_estimation_tensorflow.core import predict_multianimal as predict
from deeplabcut.utils import auxiliaryfunctions
from deeplabcut.utils.auxfun_videos import VideoWriter

warnings.simplefilter("ignore", category=RuntimeWarning)


def get_multi_scale_frames(frame, scale_list):
    augs = []
    shapes = []
    for scale in scale_list:
        aug = iaa.Resize({"width": "keep-aspect-ratio", "height": scale})
        augs.append(aug)

    frames = []
    for i in range(len(scale_list)):
        resized_frame = augs[i](image=frame)
        frames.append(resized_frame)
        shapes.append(frames[-1].shape)

    return frames, shapes


def _project_pred_to_original_size(pred, old_shape, new_shape):
    old_h, old_w, _ = old_shape
    new_h, new_w, _ = new_shape
    ratio_h, ratio_w = old_h / new_h, old_w / new_w

    coordinate = pred["coordinates"][0]
    confidence = pred["confidence"]
    for kpt_id, coord_list in enumerate(coordinate):
        if len(coord_list) == 0:
            continue
        confidence_list = confidence[kpt_id]
        max_idx = np.argmax(confidence_list)
        # ratio_h and ratio_w should match though in reality it does not match exactly
        max_pred = coordinate[kpt_id][max_idx] * ratio_h

        # only keep the max

        confidence[kpt_id] = confidence_list[max_idx]
        coordinate[kpt_id] = max_pred
    return pred


def _average_multiple_scale_preds(
    preds,
    scale_list,
    num_kpts,
    cos_dist_threshold=0.997,
    confidence_threshold=0.1,
):
    if len(scale_list) < 2:
        return preds[0]

    xyp = np.zeros((len(scale_list), num_kpts, 3))
    for scale_id, pred in enumerate(preds):
        # empty prediction if pred is not a dict
        if not isinstance(pred, dict):
            xyp[scale_id] = np.nan
            continue
        coordinates = pred["coordinates"][0]
        confidence = pred["confidence"]
        for i, (coords, conf) in enumerate(zip(coordinates, confidence)):
            if not np.any(coords):
                continue
            xyp[scale_id, i, :2] = coords
            xyp[scale_id, i, 2] = conf
    xy = xyp[..., :2]

    # Compute cosine similarity
    mean_vec = np.nanmedian(xy, axis=0)
    dist_ = np.einsum("ijk,jk->ij", xy, mean_vec)
    n = np.linalg.norm(xy, axis=2) * np.linalg.norm(mean_vec, axis=1)
    dist = np.nan_to_num(dist_ / n)

    mask = np.logical_or(
        xyp[..., 2] < confidence_threshold,
        dist < cos_dist_threshold,
    )
    xyp[mask] = np.nan
    coords = np.nanmedian(xyp[..., :2], axis=0)
    conf = np.nanmedian(xyp[..., 2], axis=0)
    dict_ = {
        "coordinates": [list(coords[:, None])],
        "confidence": list(conf[:, None].astype(np.float32)),
    }
    return dict_


def _video_inference(
    test_cfg,
    sess,
    inputs,
    outputs,
    cap,
    nframes,
    batchsize,
    scale_list=[],
):
    strwidth = int(np.ceil(np.log10(nframes)))  # width for strings
    batch_ind = 0  # keeps track of which image within a batch should be written to

    nx, ny = cap.dimensions

    pbar = tqdm(total=nframes)
    counter = 0
    inds = []
    print("scale list", scale_list)
    PredicteData = {}
    # len(frames) -> (n_scale,)
    # frames[0].shape - > (batchsize, h, w, 3)
    multi_scale_batched_frames = None
    frame_shapes = None

    num_kpts = len(test_cfg["all_joints_names"])

    while cap.video.isOpened():
        # no crop needed
        _frame = cap.read_frame()
        if _frame is not None:
            frame = img_as_ubyte(_frame)

            old_shape = frame.shape
            frames, frame_shapes = get_multi_scale_frames(frame, scale_list)

            if multi_scale_batched_frames is None:
                multi_scale_batched_frames = [
                    np.empty(
                        (batchsize, frame.shape[0], frame.shape[1], 3), dtype="ubyte"
                    )
                    for frame in frames
                ]

            for scale_id, frame in enumerate(frames):
                multi_scale_batched_frames[scale_id][batch_ind] = frame
            inds.append(counter)
            if batch_ind == batchsize - 1:
                preds = []
                for scale_id, batched_frames in enumerate(multi_scale_batched_frames):
                    # batch full, start true inferencing
                    D = predict.predict_batched_peaks_and_costs(
                        test_cfg, batched_frames, sess, inputs, outputs
                    )
                    preds.append(D)
                    # only do this when animal is detected
                ind_start = inds[0]
                for i in range(batchsize):
                    ind = ind_start + i
                    PredicteData["frame" + str(ind).zfill(strwidth)] = []

                    for scale_id in range(len(scale_list)):
                        if i >= len(preds[scale_id]):
                            pred = []
                        else:
                            pred = preds[scale_id][i]
                        if pred != []:
                            pred = _project_pred_to_original_size(
                                pred, old_shape, frame_shapes[scale_id]
                            )

                        PredicteData["frame" + str(ind).zfill(strwidth)].append(pred)

                batch_ind = 0
                inds.clear()
            else:
                batch_ind += 1
        elif counter >= nframes:
            # in case we reach the end of the video
            if batch_ind > 0:
                preds = []
                for scale_id, batched_frames in enumerate(multi_scale_batched_frames):
                    D = predict.predict_batched_peaks_and_costs(
                        test_cfg,
                        batched_frames,
                        sess,
                        inputs,
                        outputs,
                    )

                    preds.append(D)

                ind_start = inds[0]
                for i in range(batchsize):
                    ind = ind_start + i
                    if ind >= nframes:
                        break
                    PredicteData["frame" + str(ind).zfill(strwidth)] = []
                    for scale_id in range(len(scale_list)):
                        if i >= len(preds[scale_id]):
                            pred = []
                        else:
                            pred = preds[scale_id][i]
                        if pred != []:
                            pred = _project_pred_to_original_size(
                                pred, old_shape, frame_shapes[scale_id]
                            )
                        PredicteData["frame" + str(ind).zfill(strwidth)].append(pred)

            break

        counter += 1
        pbar.update(1)

    cap.close()
    pbar.close()

    for k, v in PredicteData.items():
        if v != []:
            PredicteData[k] = _average_multiple_scale_preds(v, scale_list, num_kpts)

    PredicteData["metadata"] = {
        "nms radius": test_cfg.get("nmsradius", None),
        "minimal confidence": test_cfg.get("minconfidence", None),
        "sigma": test_cfg.get("sigma", 1),
        "PAFgraph": test_cfg.get("partaffinityfield_graph", None),
        "PAFinds": test_cfg.get(
            "paf_best", np.arange(len(test_cfg["partaffinityfield_graph"]))
        ),
        "all_joints": [[i] for i in range(len(test_cfg["all_joints"]))],
        "all_joints_names": [
            test_cfg["all_joints_names"][i] for i in range(len(test_cfg["all_joints"]))
        ],
        "nframes": nframes,
    }

    return PredicteData, nframes


def video_inference(
    videos,
    project_name,
    model_name,
    scale_list=[],
    videotype="avi",
    destfolder=None,
    batchsize=1,
    robust_nframes=False,
    allow_growth=False,
    init_weights="",
    customized_test_config="",
):
    dlc_root_path = auxiliaryfunctions.get_deeplabcut_path()

    if customized_test_config == "":
        project_cfg = load_config(
            os.path.join(
                dlc_root_path,
                "modelzoo",
                "project_configs",
                f"{project_name}.yaml",
            )
        )
        model_cfg = load_config(
            os.path.join(
                dlc_root_path,
                "modelzoo",
                "model_configs",
                f"{model_name}.yaml",
            )
        )
        test_cfg = {**project_cfg, **model_cfg}
        test_cfg["all_joints"] = [i for i in range(len(test_cfg["bobyparts"]))]
        test_cfg["all_joints_names"] = test_cfg["bobyparts"]
        num_joints = len(test_cfg["all_joints"])
        test_cfg["num_joints"] = num_joints
        test_cfg["num_limbs"] = int((num_joints * (num_joints - 1)) // 2)

    else:
        test_cfg = customized_test_config

    # add a temp folder for checkpoint
    weight_folder = str(
        Path(dlc_root_path)
        / "modelzoo"
        / "checkpoints"
        / f"{project_name}_{model_name}"
    )
    snapshots = glob.glob(os.path.join(weight_folder, "snapshot-*.index"))
    test_cfg["partaffinityfield_graph"] = []
    test_cfg["partaffinityfield_predict"] = False

    if init_weights != "":
        test_cfg["init_weights"] = init_weights
    else:
        if len(snapshots) == 0:
            raise FileNotFoundError(
                f"Did not find any super animal snapshots in {weight_folder}"
            )

        init_weights = os.path.abspath(snapshots[0]).replace(".index", "")
        test_cfg["init_weights"] = init_weights

    test_cfg["num_outputs"] = 1
    test_cfg["batch_size"] = batchsize

    sess, inputs, outputs = single_predict.setup_pose_prediction(
        test_cfg, allow_growth=allow_growth
    )
    DLCscorer = "DLC_" + Path(test_cfg["init_weights"]).stem
    videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)

    datafiles = []
    for video in videos:
        vname = Path(video).stem

        videofolder = str(Path(video).parents[0])
        if destfolder is None:
            destfolder = videofolder
            auxiliaryfunctions.attempt_to_make_folder(destfolder)

        dataname = os.path.join(destfolder, vname + DLCscorer + ".h5")
        datafiles.append(dataname)

        if os.path.isfile(dataname):
            print("Video already analyzed!", dataname)
        else:
            print("Loading ", video)
            vid = VideoWriter(video)
            if len(scale_list) == 0:
                # spatial pyramid can still be useful for reducing jittering and quantization error
                scale_list = [vid.height - 50, vid.height, vid.height + 50]
            if robust_nframes:
                nframes = vid.get_n_frames(robust=True)
                duration = vid.calc_duration(robust=True)
                fps = nframes / duration
            else:
                nframes = len(vid)
                duration = vid.calc_duration(robust=False)
                fps = vid.fps

            nx, ny = vid.dimensions
            print(
                "Duration of video [s]: ",
                round(duration, 2),
                ", recorded with ",
                round(fps, 2),
                "fps!",
            )
            print(
                "Overall # of frames: ",
                nframes,
                " found with (before cropping) frame dimensions: ",
                nx,
                ny,
            )
            start = time.time()

            print("Starting to extract posture")

            # extra data
            PredicteData, nframes = _video_inference(
                test_cfg,
                sess,
                inputs,
                outputs,
                vid,
                nframes,
                int(test_cfg["batch_size"]),
                scale_list=scale_list,
            )

            stop = time.time()

            coords = [0, nx, 0, ny]

            dictionary = {
                "start": start,
                "stop": stop,
                "run_duration": stop - start,
                "Scorer": DLCscorer,
                "DLC-model-config file": test_cfg,
                "fps": fps,
                "batch_size": test_cfg["batch_size"],
                "frame_dimensions": (ny, nx),
                "nframes": nframes,
                "iteration (active-learning)": 0,
                "cropping": False,
                "training set fraction": 70,
                "cropping_parameters": coords,
            }
            metadata = {"data": dictionary}
            print("Saving results in %s..." % (destfolder))

            metadata_path = dataname.split(".h5")[0] + "_meta.pickle"

            with open(metadata_path, "wb") as f:
                pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)

            xyz_labs = ["x", "y", "likelihood"]
            scorer = DLCscorer
            keypoint_names = test_cfg["all_joints_names"]
            product = [[scorer], keypoint_names, xyz_labs]
            names = ["scorer", "bodyparts", "coords"]
            columnindex = pd.MultiIndex.from_product(product, names=names)
            imagenames = [k for k in PredicteData.keys() if k != "metadata"]

            data = np.full((len(imagenames), len(columnindex)), np.nan)
            for i, imagename in enumerate(imagenames):
                dict_ = PredicteData[imagename]
                if dict_ == [] or dict_ == [[]]:
                    data[i, 2::3] = 0
                else:
                    keypoints = dict_["coordinates"][0]
                    confidence = dict_["confidence"]
                    temp = np.full((len(keypoints), 3), np.nan)
                    for n, (xy, c) in enumerate(zip(keypoints, confidence)):
                        if xy.size and c.size:
                            temp[n, :2] = xy
                            temp[n, 2] = c
                    data[i] = temp.flatten()
            df = pd.DataFrame(data, columns=columnindex, index=imagenames)
            df.to_hdf(dataname, key="df_with_missing")

    return init_weights, datafiles


def _video_inference_superanimal(
    videos,
    project_name,
    model_name,
    scale_list=[],
    videotype=".mp4",
    video_adapt=False,
    plot_trajectories=True,
    pcutoff=0.1,
    adapt_iterations=1000,
    pseudo_threshold=0.1,
):
    """
    WARNING: This function is an internal utility function and should not be
    called directly. It is designed to be used by deeplabcut.modelzoo.api.video_inference.py

    Makes prediction based on a super animal model. Note right now we only support single animal video inference

    The index of the trained network is specified by parameters in the config file (in particular the variable 'snapshotindex')

    Output: The labels are stored as MultiIndex Pandas Array, which contains the name of the network, body part name, (x, y) label position \n
            in pixels, and the likelihood for each frame per body part. These arrays are stored in an efficient Hierarchical Data Format (HDF) \n
            in the same directory, where the video is stored.

    Parameters
    ----------
    videos: list
        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.

    superanimal_name: str
        The name of the superanimal model. We currently only support "superanimal_quadruped" and "superanimal_topviewmouse"
    scale_list: list
        A list of int containing the target height of the multi scale test time augmentation. By default it uses the original size. Users are advised to try a wide range of scale list when the super model does not give reasonable results

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.\n Only videos with this extension are analyzed. The default is ``.avi``

    video_adapt: bool, optional
        Set True if you want to apply video adaptation to make the resulted video less jittering and better. However, adaptation training takes more time than usual video inference

    plot_trajectories: bool, optional (default=True)
        By default, plot the trajectories of various body parts across the video.

    pcutoff: float, optional
        Keypoints confidence that are under pcutoff will not be shown in the resulted video

    adapt_iterations: int, optional:
        Number of iterations for adaptation training

    pseudo_threshold: float, default 0.1
        Video adaptation only uses predictions that are above pseudo_threshold

    Given a list of scales for spatial pyramid, i.e. [600, 700]

    scale_list = range(600,800,100)

    superanimal_name = 'superanimal_topviewmouse'
    videotype = 'mp4'
    scale_list = [200, 300, 400]
    deeplabcut.video_inference_superanimal(
         video,
         superanimal_name,
         videotype = '.avi',
         scale_list = scale_list,
    )
    >>>
    """
    from deeplabcut.pose_estimation_tensorflow.modelzoo.api import (
        SpatiotemporalAdaptation,
    )

    superanimal_name = project_name + "_" + model_name
    for video in videos:
        modelfolder = Path(video).parent / f"{Path(video).stem}_video_adaptation"
        modelfolder.mkdir(exist_ok=True, parents=True)

        adapter = SpatiotemporalAdaptation(
            video,
            superanimal_name,
            modelfolder=str(modelfolder),
            videotype=video.split(".")[-1],
            scale_list=scale_list,
        )
        if not video_adapt:
            adapter.before_adapt_inference(
                make_video=True, pcutoff=pcutoff, plot_trajectories=plot_trajectories
            )
        else:
            adapter.before_adapt_inference(make_video=False)
            adapter.adaptation_training(
                adapt_iterations=adapt_iterations,
                pseudo_threshold=pseudo_threshold,
            )
            adapter.after_adapt_inference(
                pcutoff=pcutoff,
                plot_trajectories=plot_trajectories,
            )


--- File: deeplabcut/pose_estimation_tensorflow/modelzoo/api/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from .spatiotemporal_adapt import SpatiotemporalAdaptation


--- File: deeplabcut/pose_estimation_tensorflow/modelzoo/api/spatiotemporal_adapt.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import glob
import os
import io
from pathlib import Path
import yaml
from deeplabcut.pose_estimation_tensorflow.modelzoo.api.superanimal_inference import (
    video_inference,
)
from deeplabcut.utils.auxiliaryfunctions import (
    get_deeplabcut_path,
    load_analyzed_data,
    read_config,
)
from deeplabcut.utils.make_labeled_video import create_labeled_video
from deeplabcut.utils.plotting import _plot_trajectories


class SpatiotemporalAdaptation:
    def __init__(
        self,
        video_path,
        supermodel_name,
        scale_list=None,
        videotype="mp4",
        adapt_iterations=1000,
        modelfolder="",
        customized_pose_config="",
        init_weights="",
    ):
        """
        This class supports video adaptation to a super model.

        Parameters
        ----------
        video_path: string
           The string to the path of the video
        init_weights: string
           The path to a superanimal model's checkpoint
        supermodel_name: string
           Currently we support supertopview(LabMice) and superquadruped (quadruped side-view animals)
        scale_list: list
           A list of different resolutions for the spatial pyramid
        videotype: string
           Checks for the extension of the video in case the input to the video is a directory.\n Only videos with this extension are analyzed. The default is ``.avi``
        adapt_iterations: int
           Number of iterations for adaptation training. Empirically 1000 is sufficient. Training longer can cause worse performance depending whether there is occlusion in the video
        modelfolder: string, optional
           Because the API does not need a dlc project, the checkpoint and logs go to this temporary model folder, and otherwise model is saved to the current work place
        customized_pose_config: string, optional
           For future support of non modelzoo model

        Examples
        --------

        from  deeplabcut.modelzoo.apis import SpatiotemporalAdaptation
        video_path = '/mnt/md0/shaokai/openfield_video/m3v1mp4.mp4'
        superanimal_name = 'superanimal_topviewmouse'
        videotype = 'mp4'
        >>> adapter = SpatiotemporalAdaptation(video_path,
                                       superanimal_name,
                                       modelfolder = "temp_topview",
                                       videotype = videotype)

        adapter.before_adapt_inference()
        adapter.adaptation_training()
        adapter.after_adapt_inference()


        """
        if scale_list is None:
            scale_list = []

        self.video_path = video_path
        self.supermodel_name = supermodel_name
        self.scale_list = scale_list
        self.videotype = videotype
        vname = str(Path(self.video_path).stem)
        self.adapt_modelprefix = vname + "_video_adaptation"
        self.adapt_iterations = adapt_iterations
        self.modelfolder = modelfolder
        self.init_weights = init_weights

        project_name = "_".join(supermodel_name.split("_")[:-1])
        model_name = supermodel_name.split("_")[-1]
        self.project_name = project_name
        self.model_name = model_name

        if not customized_pose_config:
            dlc_root_path = get_deeplabcut_path()

            project_config = read_config(
                os.path.join(
                    dlc_root_path, "modelzoo", "project_configs", f"{project_name}.yaml"
                )
            )

            model_config = read_config(
                os.path.join(
                    dlc_root_path, "modelzoo", "model_configs", f"{model_name}.yaml"
                )
            )

            joints = [i for i in range(len(project_config["bodyparts"]))]
            num_joints = len(joints)
            model_config["all_joints"] = joints
            model_config["all_joints_names"] = project_config["bodyparts"]
            model_config["num_joints"] = num_joints
            model_config["num_limbs"] = int((num_joints * (num_joints - 1)) // 2)
            self.customized_pose_config = {**project_config, **model_config}
        else:
            self.customized_pose_config = customized_pose_config

    def before_adapt_inference(self, make_video=False, **kwargs):
        if self.init_weights != "":
            print("using customized weights", self.init_weights)
            _, datafiles = video_inference(
                [self.video_path],
                self.project_name,
                self.model_name,
                videotype=self.videotype,
                scale_list=self.scale_list,
                init_weights=self.init_weights,
                customized_test_config=self.customized_pose_config,
            )
        else:
            self.init_weights, datafiles = video_inference(
                [self.video_path],
                self.project_name,
                self.model_name,
                videotype=self.videotype,
                scale_list=self.scale_list,
                customized_test_config=self.customized_pose_config,
            )
        if kwargs.pop("plot_trajectories", True):
            if len(datafiles) == 0:
                print("No data files found for plotting trajectory")
            else:
                _plot_trajectories(datafiles[0])

        if make_video:
            create_labeled_video(
                "",
                [self.video_path],
                videotype=self.videotype,
                filtered=False,
                init_weights=self.init_weights,
                draw_skeleton=True,
                superanimal_name=self.project_name,
                **kwargs,
            )

    def train_without_project(self, pseudo_label_path, **kwargs):
        from deeplabcut.pose_estimation_tensorflow.core.train_multianimal import train

        displayiters = kwargs.pop("displayiters", 500)
        saveiters = kwargs.pop("saveiters", 1000)
        self.adapt_iterations = kwargs.pop("adapt_iterations", self.adapt_iterations)

        train(
            self.customized_pose_config,
            displayiters=displayiters,
            saveiters=saveiters,
            maxiters=self.adapt_iterations,
            modelfolder=self.modelfolder,
            init_weights=self.init_weights,
            pseudo_labels=pseudo_label_path,
            video_path=self.video_path,
            superanimal=self.supermodel_name,
            **kwargs,
        )

    def adaptation_training(self, displayiters=500, saveiters=1000, **kwargs):
        """
        There should be two choices, either taking a config, with is then assuming there is a DLC project.
        Or we make up a fake one, then we use a light way convention to do adaptation
        """

        # looking for the pseudo label path
        DLCscorer = "DLC_" + Path(self.init_weights).stem
        vname = str(Path(self.video_path).stem)
        video_root = Path(self.video_path).parent

        _, pseudo_label_path, _, _ = load_analyzed_data(
            video_root, vname, DLCscorer, False, ""
        )
        if self.modelfolder != "":
            os.makedirs(self.modelfolder, exist_ok=True)

        self.adapt_iterations = kwargs.get("adapt_iterations", self.adapt_iterations)


        self.train_without_project(
            pseudo_label_path,
            displayiters=displayiters,
            saveiters=saveiters,
            **kwargs,
        )

    def after_adapt_inference(self, **kwargs):
        pattern = os.path.join(
            self.modelfolder, f"snapshot-{self.adapt_iterations}.index"
        )
        ref_proj_config_path = ""

        files = glob.glob(pattern)

        if not len(files):
            raise ValueError("Weights were not found.")

        adapt_weights = files[0].replace(".index", "")

        # spatial pyramid is not for adapted model

        scale_list = kwargs.pop("scale_list", [])

        # spatial pyramid can still be useful for reducing jittering and quantization error

        _, datafiles = video_inference(
            [self.video_path],
            self.project_name,
            self.model_name,
            videotype=self.videotype,
            init_weights=adapt_weights,
            scale_list=scale_list,
            customized_test_config=self.customized_pose_config,
        )

        if kwargs.pop("plot_trajectories", True):
            _plot_trajectories(datafiles[0])

        create_labeled_video(
            ref_proj_config_path,
            [self.video_path],
            videotype=self.videotype,
            filtered=False,
            init_weights=adapt_weights,
            draw_skeleton=True,
            superanimal_name=self.project_name,
            **kwargs,
        )


--- File: deeplabcut/pose_estimation_tensorflow/backbones/efficientnet_builder.py ---
#
# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import functools
import os
import re
import tensorflow as tf

import deeplabcut.pose_estimation_tensorflow.backbones.efficientnet_model as efficientnet_model
import deeplabcut.pose_estimation_tensorflow.nnets.utils as utils

MEAN_RGB = [0.485 * 255, 0.456 * 255, 0.406 * 255]
STDDEV_RGB = [0.229 * 255, 0.224 * 255, 0.225 * 255]


def efficientnet_params(model_name):
    """Get efficientnet params based on model name."""
    params_dict = {
        # (width_coefficient, depth_coefficient, resolution, dropout_rate)
        "efficientnet-b0": (1.0, 1.0, 224, 0.2),
        "efficientnet-b1": (1.0, 1.1, 240, 0.2),
        "efficientnet-b2": (1.1, 1.2, 260, 0.3),
        "efficientnet-b3": (1.2, 1.4, 300, 0.3),
        "efficientnet-b4": (1.4, 1.8, 380, 0.4),
        "efficientnet-b5": (1.6, 2.2, 456, 0.4),
        "efficientnet-b6": (1.8, 2.6, 528, 0.5),
        "efficientnet-b7": (2.0, 3.1, 600, 0.5),
    }
    return params_dict[model_name]


class BlockDecoder(object):
    """Block Decoder for readability."""

    def _decode_block_string(self, block_string):
        """Gets a block through a string notation of arguments."""
        assert isinstance(block_string, str)
        ops = block_string.split("_")
        options = {}
        for op in ops:
            splits = re.split(r"(\d.*)", op)
            if len(splits) >= 2:
                key, value = splits[:2]
                options[key] = value

        if "s" not in options or len(options["s"]) != 2:
            raise ValueError("Strides options should be a pair of integers.")

        return efficientnet_model.BlockArgs(
            kernel_size=int(options["k"]),
            num_repeat=int(options["r"]),
            input_filters=int(options["i"]),
            output_filters=int(options["o"]),
            expand_ratio=int(options["e"]),
            id_skip=("noskip" not in block_string),
            se_ratio=float(options["se"]) if "se" in options else None,
            strides=[int(options["s"][0]), int(options["s"][1])],
            conv_type=int(options["c"]) if "c" in options else 0,
        )

    def _encode_block_string(self, block):
        """Encodes a block to a string."""
        args = [
            "r%d" % block.num_repeat,
            "k%d" % block.kernel_size,
            "s%d%d" % (block.strides[0], block.strides[1]),
            "e%s" % block.expand_ratio,
            "i%d" % block.input_filters,
            "o%d" % block.output_filters,
            "c%d" % block.conv_type,
        ]
        if block.se_ratio > 0 and block.se_ratio <= 1:
            args.append("se%s" % block.se_ratio)
        if block.id_skip is False:
            args.append("noskip")
        return "_".join(args)

    def decode(self, string_list):
        """Decodes a list of string notations to specify blocks inside the network.
        Args:
          string_list: a list of strings, each string is a notation of block.
        Returns:
          A list of namedtuples to represent blocks arguments.
        """
        assert isinstance(string_list, list)
        blocks_args = []
        for block_string in string_list:
            blocks_args.append(self._decode_block_string(block_string))
        return blocks_args

    def encode(self, blocks_args):
        """Encodes a list of Blocks to a list of strings.
        Args:
          blocks_args: A list of namedtuples to represent blocks arguments.
        Returns:
          a list of strings, each string is a notation of block.
        """
        block_strings = []
        for block in blocks_args:
            block_strings.append(self._encode_block_string(block))
        return block_strings


def swish(features, use_native=True):
    """Computes the Swish activation function.
    The tf.nn.swish operation uses a custom gradient to reduce memory usage.
    Since saving custom gradients in SavedModel is currently not supported, and
    one would not be able to use an exported TF-Hub module for fine-tuning, we
    provide this wrapper that can allow to select whether to use the native
    TensorFlow swish operation, or whether to use a customized operation that
    has uses default TensorFlow gradient computation.
    Args:
      features: A `Tensor` representing preactivation values.
      use_native: Whether to use the native swish from tf.nn that uses a custom
        gradient to reduce memory usage, or to use customized swish that uses
        default TensorFlow gradient computation.
    Returns:
      The activation value.
    """
    if use_native:
        return tf.nn.swish(features)
    else:
        features = tf.convert_to_tensor(value=features, name="features")
        return features * tf.nn.sigmoid(features)


def efficientnet(
    width_coefficient=None,
    depth_coefficient=None,
    dropout_rate=0.2,
    drop_connect_rate=0.2,
):
    """Creates a efficientnet model."""
    blocks_args = [
        "r1_k3_s11_e1_i32_o16_se0.25",
        "r2_k3_s22_e6_i16_o24_se0.25",
        "r2_k5_s22_e6_i24_o40_se0.25",
        "r3_k3_s22_e6_i40_o80_se0.25",
        "r3_k5_s11_e6_i80_o112_se0.25",
        "r4_k5_s11_e6_i112_o192_se0.25",
        "r1_k3_s11_e6_i192_o320_se0.25",
    ]
    # blocks_args = [
    #     'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',
    #     'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',
    #     'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',
    #     'r1_k3_s11_e6_i192_o320_se0.25',
    # ]
    global_params = efficientnet_model.GlobalParams(
        batch_norm_momentum=0.99,
        batch_norm_epsilon=1e-3,
        dropout_rate=dropout_rate,
        drop_connect_rate=drop_connect_rate,
        data_format="channels_last",
        num_classes=1000,
        width_coefficient=width_coefficient,
        depth_coefficient=depth_coefficient,
        depth_divisor=8,
        min_depth=None,
        relu_fn=tf.nn.swish,
        # The default is TPU-specific batch norm.
        # The alternative is tf.layers.BatchNormalization.
        # batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.
        batch_norm=utils.BatchNormalization,
        use_se=True,
    )
    decoder = BlockDecoder()
    return decoder.decode(blocks_args), global_params


def get_model_params(model_name, override_params):
    """Get the block args and global params for a given model."""
    if model_name.startswith("efficientnet"):
        width_coefficient, depth_coefficient, _, dropout_rate = efficientnet_params(
            model_name
        )
        blocks_args, global_params = efficientnet(
            width_coefficient, depth_coefficient, dropout_rate
        )
    else:
        raise NotImplementedError("model name is not pre-defined: %s" % model_name)

    if override_params:
        # ValueError will be raised here if override_params has fields not included
        # in global_params.
        global_params = global_params._replace(**override_params)

    tf.compat.v1.logging.info("global_params= %s", global_params)
    tf.compat.v1.logging.info("blocks_args= %s", blocks_args)
    return blocks_args, global_params


def build_model(
    images,
    model_name,
    training,
    override_params=None,
    model_dir=None,
    fine_tuning=False,
    features_only=False,
):
    """A helper function to creates a model and returns predicted logits.
    Args:
      images: input images tensor.
      model_name: string, the predefined model name.
      training: boolean, whether the model is constructed for training.
      override_params: A dictionary of params for overriding. Fields must exist in
        efficientnet_model.GlobalParams.
      model_dir: string, optional model dir for saving configs.
      fine_tuning: boolean, whether the model is used for finetuning.
      features_only: build the base feature network only.
    Returns:
      logits: the logits tensor of classes.
      endpoints: the endpoints for each layer.
    Raises:
      When model_name specified an undefined model, raises NotImplementedError.
      When override_params has invalid fields, raises ValueError.
    """
    assert isinstance(images, tf.Tensor)
    if not training or fine_tuning:
        if not override_params:
            override_params = {}
        override_params["batch_norm"] = utils.BatchNormalization
        override_params["relu_fn"] = functools.partial(swish, use_native=False)
    blocks_args, global_params = get_model_params(model_name, override_params)

    if model_dir:
        param_file = os.path.join(model_dir, "model_params.txt")
        if not tf.io.gfile.exists(param_file):
            if not tf.io.gfile.exists(model_dir):
                tf.io.gfile.makedirs(model_dir)
            with tf.io.gfile.GFile(param_file, "w") as f:
                tf.compat.v1.logging.info("writing to %s" % param_file)
                f.write("model_name= %s\n\n" % model_name)
                f.write("global_params= %s\n\n" % str(global_params))
                f.write("blocks_args= %s\n\n" % str(blocks_args))

    with tf.compat.v1.variable_scope(model_name):
        model = efficientnet_model.Model(blocks_args, global_params)
        outputs = model(images, training=training, features_only=features_only)
    outputs = tf.identity(outputs, "features" if features_only else "logits")
    return outputs, model.endpoints


def build_model_base(
    images, model_name, use_batch_norm=False, drop_out=False, override_params=None
):
    """A helper function to create a base model and return global_pool.
    Args:
      images: input images tensor.
      model_name: string, the predefined model name.
      training: boolean, whether the model is constructed for training.
      override_params: A dictionary of params for overriding. Fields must exist in
        efficientnet_model.GlobalParams.
    Returns:
      features: global pool features.
      endpoints: the endpoints for each layer.
    Raises:
      When model_name specified an undefined model, raises NotImplementedError.
      When override_params has invalid fields, raises ValueError.
    """
    assert isinstance(images, tf.Tensor)
    blocks_args, global_params = get_model_params(model_name, override_params)

    with tf.compat.v1.variable_scope(model_name):
        model = efficientnet_model.Model(blocks_args, global_params)
        features = model(
            images, use_batch_norm=use_batch_norm, drop_out=drop_out, features_only=True
        )

    features = tf.identity(features, "features")
    return features, model.endpoints


--- File: deeplabcut/pose_estimation_tensorflow/backbones/mobilenet_v2.py ---
#
# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#


"""Implementation of Mobilenet V2.

Architecture: https://arxiv.org/abs/1801.04381

The base model gives 72.2% accuracy on ImageNet, with 300MMadds,
3.4 M parameters.
"""

import copy
import functools

import tensorflow as tf
import tf_slim as slim

from deeplabcut.pose_estimation_tensorflow.nnets import conv_blocks as ops
from deeplabcut.pose_estimation_tensorflow.backbones import mobilenet as lib

op = lib.op

expand_input = ops.expand_input_by_factor


# pyformat: disable
# Architecture: https://arxiv.org/abs/1801.04381
V2_DEF = dict(
    defaults={
        # Note: these parameters of batch norm affect the architecture
        # that's why they are here and not in training_scope.
        (slim.batch_norm,): {"center": True, "scale": True},
        (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {
            "normalizer_fn": slim.batch_norm,
            "activation_fn": tf.nn.relu6,
        },
        (ops.expanded_conv,): {
            "expansion_size": expand_input(6),
            "split_expansion": 1,
            "normalizer_fn": slim.batch_norm,
            "residual": True,
        },
        (slim.conv2d, slim.separable_conv2d): {"padding": "SAME"},
    },
    spec=[
        op(slim.conv2d, stride=2, num_outputs=32, kernel_size=[3, 3]),
        op(
            ops.expanded_conv,
            expansion_size=expand_input(1, divisible_by=1),
            num_outputs=16,
        ),
        op(ops.expanded_conv, stride=2, num_outputs=24),
        op(ops.expanded_conv, stride=1, num_outputs=24),
        op(ops.expanded_conv, stride=2, num_outputs=32),
        op(ops.expanded_conv, stride=1, num_outputs=32),
        op(ops.expanded_conv, stride=1, num_outputs=32),
        op(ops.expanded_conv, stride=2, num_outputs=64),
        op(ops.expanded_conv, stride=1, num_outputs=64),
        op(ops.expanded_conv, stride=1, num_outputs=64),
        op(ops.expanded_conv, stride=1, num_outputs=64),
        op(ops.expanded_conv, stride=1, num_outputs=96),
        op(ops.expanded_conv, stride=1, num_outputs=96),
        op(ops.expanded_conv, stride=1, num_outputs=96),
        op(
            ops.expanded_conv, stride=1, num_outputs=160
        ),  # NOTE: WE changed this stride to achieve downsampling of 16 rather than 32.
        op(ops.expanded_conv, stride=1, num_outputs=160),
        op(ops.expanded_conv, stride=1, num_outputs=160),
        op(ops.expanded_conv, stride=1, num_outputs=320),
        op(slim.conv2d, stride=1, kernel_size=[1, 1], num_outputs=1280),
    ],
)
# pyformat: enable


@slim.add_arg_scope
def mobilenet(
    input_tensor,
    num_classes=1001,
    depth_multiplier=1.0,
    scope="MobilenetV2",
    conv_defs=None,
    finegrain_classification_mode=False,
    min_depth=None,
    divisible_by=None,
    activation_fn=None,
    **kwargs
):
    """Creates mobilenet V2 network.

    Inference mode is created by default. To create training use training_scope
    below.

    with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):
       logits, endpoints = mobilenet_v2.mobilenet(input_tensor)

    Args:
      input_tensor: The input tensor
      num_classes: number of classes
      depth_multiplier: The multiplier applied to scale number of
      channels in each layer.
      scope: Scope of the operator
      conv_defs: Allows to override default conv def.
      finegrain_classification_mode: When set to True, the model
      will keep the last layer large even for small multipliers. Following
      https://arxiv.org/abs/1801.04381
      suggests that it improves performance for ImageNet-type of problems.
        *Note* ignored if final_endpoint makes the builder exit earlier.
      min_depth: If provided, will ensure that all layers will have that
      many channels after application of depth multiplier.
      divisible_by: If provided will ensure that all layers # channels
      will be divisible by this number.
      activation_fn: Activation function to use, defaults to tf.nn.relu6 if not
        specified.
      **kwargs: passed directly to mobilenet.mobilenet:
        prediction_fn- what prediction function to use.
        reuse-: whether to reuse variables (if reuse set to true, scope
        must be given).
    Returns:
      logits/endpoints pair

    Raises:
      ValueError: On invalid arguments
    """
    if conv_defs is None:
        conv_defs = V2_DEF
    if "multiplier" in kwargs:
        raise ValueError(
            "mobilenetv2 doesn't support generic "
            'multiplier parameter use "depth_multiplier" instead.'
        )
    if finegrain_classification_mode:
        conv_defs = copy.deepcopy(conv_defs)
        if depth_multiplier < 1:
            conv_defs["spec"][-1].params["num_outputs"] /= depth_multiplier
    if activation_fn:
        conv_defs = copy.deepcopy(conv_defs)
        defaults = conv_defs["defaults"]
        conv_defaults = defaults[
            (slim.conv2d, slim.fully_connected, slim.separable_conv2d)
        ]
        conv_defaults["activation_fn"] = activation_fn

    depth_args = {}
    # NB: do not set depth_args unless they are provided to avoid overriding
    # whatever default depth_multiplier might have thanks to arg_scope.
    if min_depth is not None:
        depth_args["min_depth"] = min_depth
    if divisible_by is not None:
        depth_args["divisible_by"] = divisible_by

    with slim.arg_scope((lib.depth_multiplier,), **depth_args):
        return lib.mobilenet(
            input_tensor,
            num_classes=num_classes,
            conv_defs=conv_defs,
            scope=scope,
            multiplier=depth_multiplier,
            **kwargs
        )


mobilenet.default_image_size = 224


def wrapped_partial(func, *args, **kwargs):
    partial_func = functools.partial(func, *args, **kwargs)
    functools.update_wrapper(partial_func, func)
    return partial_func


# Wrappers for mobilenet v2 with depth-multipliers. Be noticed that
# 'finegrain_classification_mode' is set to True, which means the embedding
# layer will not be shrunk when given a depth-multiplier < 1.0.
mobilenet_v2_140 = wrapped_partial(mobilenet, depth_multiplier=1.4)
mobilenet_v2_050 = wrapped_partial(
    mobilenet, depth_multiplier=0.50, finegrain_classification_mode=True
)
mobilenet_v2_035 = wrapped_partial(
    mobilenet, depth_multiplier=0.35, finegrain_classification_mode=True
)


@slim.add_arg_scope
def mobilenet_base(input_tensor, depth_multiplier=1.0, **kwargs):
    """Creates base of the mobilenet (no pooling and no logits) ."""
    return mobilenet(
        input_tensor, depth_multiplier=depth_multiplier, base_only=True, **kwargs
    )


def training_scope(**kwargs):
    """Defines MobilenetV2 training scope.

    Usage:
       with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):
         logits, endpoints = mobilenet_v2.mobilenet(input_tensor)

    with slim.

    Args:
      **kwargs: Passed to mobilenet.training_scope. The following parameters
      are supported:
        weight_decay- The weight decay to use for regularizing the model.
        stddev-  Standard deviation for initialization, if negative uses xavier.
        dropout_keep_prob- dropout keep probability
        bn_decay- decay for the batch norm moving averages.

    Returns:
      An `arg_scope` to use for the mobilenet v2 model.
    """
    return lib.training_scope(**kwargs)


__all__ = ["training_scope", "mobilenet_base", "mobilenet", "V2_DEF"]


--- File: deeplabcut/pose_estimation_tensorflow/backbones/efficientnet_model.py ---
#
# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""Contains definitions for EfficientNet model.
[1] Mingxing Tan, Quoc V. Le
  EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.
  ICML'19, https://arxiv.org/abs/1905.11946
"""
import collections
import math
import numpy as np
import tensorflow as tf

import deeplabcut.pose_estimation_tensorflow.nnets.utils as utils

GlobalParams = collections.namedtuple(
    "GlobalParams",
    [
        "batch_norm_momentum",
        "batch_norm_epsilon",
        "dropout_rate",
        "data_format",
        "num_classes",
        "width_coefficient",
        "depth_coefficient",
        "depth_divisor",
        "min_depth",
        "drop_connect_rate",
        "relu_fn",
        "batch_norm",
        "use_se",
    ],
)
GlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)

BlockArgs = collections.namedtuple(
    "BlockArgs",
    [
        "kernel_size",
        "num_repeat",
        "input_filters",
        "output_filters",
        "expand_ratio",
        "id_skip",
        "strides",
        "se_ratio",
        "conv_type",
    ],
)
# defaults will be a public argument for namedtuple in Python 3.7
# https://docs.python.org/3/library/collections.html#collections.namedtuple
BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)


def conv_kernel_initializer(shape, dtype=None, partition_info=None):
    """Initialization for convolutional kernels.
    The main difference with tf.variance_scaling_initializer is that
    tf.variance_scaling_initializer uses a truncated normal with an uncorrected
    standard deviation, whereas here we use a normal distribution. Similarly,
    tf.contrib.layers.variance_scaling_initializer uses a truncated normal with
    a corrected standard deviation.
    Args:
      shape: shape of variable
      dtype: dtype of variable
      partition_info: unused
    Returns:
      an initialization for the variable
    """
    del partition_info
    kernel_height, kernel_width, _, out_filters = shape
    fan_out = int(kernel_height * kernel_width * out_filters)
    return tf.random.normal(shape, mean=0.0, stddev=np.sqrt(2.0 / fan_out), dtype=dtype)


def dense_kernel_initializer(shape, dtype=None, partition_info=None):
    """Initialization for dense kernels.
    This initialization is equal to
      tf.variance_scaling_initializer(scale=1.0/3.0, mode='fan_out',
                                      distribution='uniform').
    It is written out explicitly here for clarity.
    Args:
      shape: shape of variable
      dtype: dtype of variable
      partition_info: unused
    Returns:
      an initialization for the variable
    """
    del partition_info
    init_range = 1.0 / np.sqrt(shape[1])
    return tf.random.uniform(shape, -init_range, init_range, dtype=dtype)


def round_filters(filters, global_params):
    """Round number of filters based on depth multiplier."""
    orig_f = filters
    multiplier = global_params.width_coefficient
    divisor = global_params.depth_divisor
    min_depth = global_params.min_depth
    if not multiplier:
        return filters

    filters *= multiplier
    min_depth = min_depth or divisor
    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_filters < 0.9 * filters:
        new_filters += divisor
    tf.compat.v1.logging.info(
        "round_filter input={} output={}".format(orig_f, new_filters)
    )
    return int(new_filters)


def round_repeats(repeats, global_params):
    """Round number of filters based on depth multiplier."""
    multiplier = global_params.depth_coefficient
    if not multiplier:
        return repeats
    return int(math.ceil(multiplier * repeats))


class MBConvBlock(tf.keras.layers.Layer):
    """A class of MBConv: Mobile Inverted Residual Bottleneck.
    Attributes:
      endpoints: dict. A list of internal tensors.
    """

    def __init__(self, block_args, global_params):
        """Initializes a MBConv block.
        Args:
          block_args: BlockArgs, arguments to create a Block.
          global_params: GlobalParams, a set of global parameters.
        """
        super(MBConvBlock, self).__init__()
        self._block_args = block_args
        self._batch_norm_momentum = global_params.batch_norm_momentum
        self._batch_norm_epsilon = global_params.batch_norm_epsilon
        self._batch_norm = global_params.batch_norm
        self._data_format = global_params.data_format
        if self._data_format == "channels_first":
            self._channel_axis = 1
            self._spatial_dims = [2, 3]
        else:
            self._channel_axis = -1
            self._spatial_dims = [1, 2]

        self._relu_fn = global_params.relu_fn or tf.nn.swish
        self._has_se = (
            global_params.use_se
            and self._block_args.se_ratio is not None
            and 0 < self._block_args.se_ratio <= 1
        )

        self.endpoints = None

        # Builds the block accordings to arguments.
        self._build()

    def block_args(self):
        return self._block_args

    def _build(self):
        """Builds block according to the arguments."""
        filters = self._block_args.input_filters * self._block_args.expand_ratio
        if self._block_args.expand_ratio != 1:
            # Expansion phase:
            self._expand_conv = tf.compat.v1.layers.Conv2D(
                filters,
                kernel_size=[1, 1],
                strides=[1, 1],
                kernel_initializer=conv_kernel_initializer,
                padding="same",
                data_format=self._data_format,
                use_bias=False,
            )
            self._bn0 = self._batch_norm(
                axis=self._channel_axis,
                momentum=self._batch_norm_momentum,
                epsilon=self._batch_norm_epsilon,
            )

        kernel_size = self._block_args.kernel_size
        # Depth-wise convolution phase:
        self._depthwise_conv = utils.DepthwiseConv2D(
            [kernel_size, kernel_size],
            strides=self._block_args.strides,
            depthwise_initializer=conv_kernel_initializer,
            padding="same",
            data_format=self._data_format,
            use_bias=False,
        )
        self._bn1 = self._batch_norm(
            axis=self._channel_axis,
            momentum=self._batch_norm_momentum,
            epsilon=self._batch_norm_epsilon,
        )

        if self._has_se:
            num_reduced_filters = max(
                1, int(self._block_args.input_filters * self._block_args.se_ratio)
            )
            # Squeeze and Excitation layer.
            self._se_reduce = tf.compat.v1.layers.Conv2D(
                num_reduced_filters,
                kernel_size=[1, 1],
                strides=[1, 1],
                kernel_initializer=conv_kernel_initializer,
                padding="same",
                data_format=self._data_format,
                use_bias=True,
            )
            self._se_expand = tf.compat.v1.layers.Conv2D(
                filters,
                kernel_size=[1, 1],
                strides=[1, 1],
                kernel_initializer=conv_kernel_initializer,
                padding="same",
                data_format=self._data_format,
                use_bias=True,
            )

        # Output phase:
        filters = self._block_args.output_filters
        self._project_conv = tf.compat.v1.layers.Conv2D(
            filters,
            kernel_size=[1, 1],
            strides=[1, 1],
            kernel_initializer=conv_kernel_initializer,
            padding="same",
            data_format=self._data_format,
            use_bias=False,
        )
        self._bn2 = self._batch_norm(
            axis=self._channel_axis,
            momentum=self._batch_norm_momentum,
            epsilon=self._batch_norm_epsilon,
        )

    def _call_se(self, input_tensor):
        """Call Squeeze and Excitation layer.
        Args:
          input_tensor: Tensor, a single input tensor for Squeeze/Excitation layer.
        Returns:
          A output tensor, which should have the same shape as input.
        """
        se_tensor = tf.reduce_mean(
            input_tensor=input_tensor, axis=self._spatial_dims, keepdims=True
        )
        se_tensor = self._se_expand(self._relu_fn(self._se_reduce(se_tensor)))
        tf.compat.v1.logging.info(
            "Built Squeeze and Excitation with tensor shape: %s" % (se_tensor.shape)
        )
        return tf.sigmoid(se_tensor) * input_tensor

    def call(
        self, inputs, use_batch_norm=False, drop_out=False, drop_connect_rate=None
    ):
        """Implementation of call().
        Args:
          inputs: the inputs tensor.
          training: boolean, whether the model is constructed for training.
          drop_connect_rate: float, between 0 to 1, drop connect rate.
        Returns:
          A output tensor.
        """
        tf.compat.v1.logging.info(
            "Block input: %s shape: %s" % (inputs.name, inputs.shape)
        )
        if self._block_args.expand_ratio != 1:
            x = self._relu_fn(
                self._bn0(self._expand_conv(inputs), training=use_batch_norm)
            )
        else:
            x = inputs
        tf.compat.v1.logging.info("Expand: %s shape: %s" % (x.name, x.shape))

        x = self._relu_fn(self._bn1(self._depthwise_conv(x), training=use_batch_norm))
        tf.compat.v1.logging.info("DWConv: %s shape: %s" % (x.name, x.shape))

        if self._has_se:
            with tf.compat.v1.variable_scope("se"):
                x = self._call_se(x)

        self.endpoints = {"expansion_output": x}

        x = self._bn2(self._project_conv(x), training=use_batch_norm)
        if self._block_args.id_skip:
            if (
                all(s == 1 for s in self._block_args.strides)
                and self._block_args.input_filters == self._block_args.output_filters
            ):
                # only apply drop_connect if skip presents.
                if drop_connect_rate:
                    x = utils.drop_connect(x, drop_out, drop_connect_rate)
                x = tf.add(x, inputs)
        tf.compat.v1.logging.info("Project: %s shape: %s" % (x.name, x.shape))
        return x


class MBConvBlockWithoutDepthwise(MBConvBlock):
    """MBConv-like block without depthwise convolution and squeeze-and-excite."""

    def _build(self):
        """Builds block according to the arguments."""
        filters = self._block_args.input_filters * self._block_args.expand_ratio
        if self._block_args.expand_ratio != 1:
            # Expansion phase:
            self._expand_conv = tf.compat.v1.layers.Conv2D(
                filters,
                kernel_size=[3, 3],
                strides=[1, 1],
                kernel_initializer=conv_kernel_initializer,
                padding="same",
                use_bias=False,
            )
            self._bn0 = self._batch_norm(
                axis=self._channel_axis,
                momentum=self._batch_norm_momentum,
                epsilon=self._batch_norm_epsilon,
            )

        # Output phase:
        filters = self._block_args.output_filters
        self._project_conv = tf.compat.v1.layers.Conv2D(
            filters,
            kernel_size=[1, 1],
            strides=self._block_args.strides,
            kernel_initializer=conv_kernel_initializer,
            padding="same",
            use_bias=False,
        )
        self._bn1 = self._batch_norm(
            axis=self._channel_axis,
            momentum=self._batch_norm_momentum,
            epsilon=self._batch_norm_epsilon,
        )

    def call(
        self, inputs, use_batch_norm=False, drop_out=False, drop_connect_rate=None
    ):
        """Implementation of call().
        Args:
          inputs: the inputs tensor.
          training: boolean, whether the model is constructed for training.
          drop_connect_rate: float, between 0 to 1, drop connect rate.
        Returns:
          A output tensor.
        """
        tf.compat.v1.logging.info(
            "Block input: %s shape: %s" % (inputs.name, inputs.shape)
        )
        if self._block_args.expand_ratio != 1:
            x = self._relu_fn(
                self._bn0(self._expand_conv(inputs), training=use_batch_norm)
            )
        else:
            x = inputs
        tf.compat.v1.logging.info("Expand: %s shape: %s" % (x.name, x.shape))

        self.endpoints = {"expansion_output": x}

        x = self._bn1(self._project_conv(x), training=use_batch_norm)
        if self._block_args.id_skip:
            if (
                all(s == 1 for s in self._block_args.strides)
                and self._block_args.input_filters == self._block_args.output_filters
            ):
                # only apply drop_connect if skip presents.
                if drop_connect_rate:
                    x = utils.drop_connect(x, drop_out, drop_connect_rate)
                x = tf.add(x, inputs)
        tf.compat.v1.logging.info("Project: %s shape: %s" % (x.name, x.shape))
        return x


class Model(tf.keras.Model):
    """A class implements tf.keras.Model for MNAS-like model.
    Reference: https://arxiv.org/abs/1807.11626
    """

    def __init__(self, blocks_args=None, global_params=None):
        """Initializes an `Model` instance.
        Args:
          blocks_args: A list of BlockArgs to construct block modules.
          global_params: GlobalParams, a set of global parameters.
        Raises:
          ValueError: when blocks_args is not specified as a list.
        """
        super(Model, self).__init__()
        if not isinstance(blocks_args, list):
            raise ValueError("blocks_args should be a list.")
        self._global_params = global_params
        self._blocks_args = blocks_args
        self._relu_fn = global_params.relu_fn or tf.nn.swish
        self._batch_norm = global_params.batch_norm

        self.endpoints = None

        self._build()

    def _get_conv_block(self, conv_type):
        conv_block_map = {0: MBConvBlock, 1: MBConvBlockWithoutDepthwise}
        return conv_block_map[conv_type]

    def _build(self):
        """Builds a model."""
        self._blocks = []
        # Builds blocks.
        for block_args in self._blocks_args:
            assert block_args.num_repeat > 0
            # Update block input and output filters based on depth multiplier.
            block_args = block_args._replace(
                input_filters=round_filters(
                    block_args.input_filters, self._global_params
                ),
                output_filters=round_filters(
                    block_args.output_filters, self._global_params
                ),
                num_repeat=round_repeats(block_args.num_repeat, self._global_params),
            )

            # The first block needs to take care of stride and filter size increase.
            conv_block = self._get_conv_block(block_args.conv_type)
            self._blocks.append(conv_block(block_args, self._global_params))
            if block_args.num_repeat > 1:
                # pylint: disable=protected-access
                block_args = block_args._replace(
                    input_filters=block_args.output_filters, strides=[1, 1]
                )
                # pylint: enable=protected-access
            for _ in range(block_args.num_repeat - 1):
                self._blocks.append(conv_block(block_args, self._global_params))

        batch_norm_momentum = self._global_params.batch_norm_momentum
        batch_norm_epsilon = self._global_params.batch_norm_epsilon
        if self._global_params.data_format == "channels_first":
            channel_axis = 1
        else:
            channel_axis = -1

        # Stem part.
        self._conv_stem = tf.compat.v1.layers.Conv2D(
            filters=round_filters(32, self._global_params),
            kernel_size=[3, 3],
            strides=[2, 2],
            kernel_initializer=conv_kernel_initializer,
            padding="same",
            data_format=self._global_params.data_format,
            use_bias=False,
        )
        self._bn0 = self._batch_norm(
            axis=channel_axis, momentum=batch_norm_momentum, epsilon=batch_norm_epsilon
        )

        # Head part.
        self._conv_head = tf.compat.v1.layers.Conv2D(
            filters=round_filters(1280, self._global_params),
            kernel_size=[1, 1],
            strides=[1, 1],
            kernel_initializer=conv_kernel_initializer,
            padding="same",
            use_bias=False,
        )
        self._bn1 = self._batch_norm(
            axis=channel_axis, momentum=batch_norm_momentum, epsilon=batch_norm_epsilon
        )

        self._avg_pooling = tf.keras.layers.GlobalAveragePooling2D(
            data_format=self._global_params.data_format
        )
        if self._global_params.num_classes:
            self._fc = tf.compat.v1.layers.Dense(
                self._global_params.num_classes,
                kernel_initializer=dense_kernel_initializer,
            )
        else:
            self._fc = None

        if self._global_params.dropout_rate > 0:
            self._dropout = tf.keras.layers.Dropout(self._global_params.dropout_rate)
        else:
            self._dropout = None

    def call(self, inputs, use_batch_norm=False, drop_out=False, features_only=None):
        """Implementation of call().
        Args:
          inputs: input tensors.
          training: boolean, whether the model is constructed for training.
          features_only: build the base feature network only.
        Returns:
          output tensors.
        """
        outputs = None
        self.endpoints = {}
        # Calls Stem layers
        with tf.compat.v1.variable_scope("stem"):
            outputs = self._relu_fn(
                self._bn0(self._conv_stem(inputs), training=use_batch_norm)
            )
        tf.compat.v1.logging.info(
            "Built stem layers with output shape: %s" % outputs.shape
        )
        self.endpoints["stem"] = outputs

        # Calls blocks.
        reduction_idx = 0
        for idx, block in enumerate(self._blocks):
            is_reduction = False
            if (idx == len(self._blocks) - 1) or self._blocks[
                idx + 1
            ].block_args().strides[0] > 1:
                is_reduction = True
                reduction_idx += 1

            with tf.compat.v1.variable_scope("blocks_%s" % idx):
                drop_rate = self._global_params.drop_connect_rate
                if drop_rate:
                    drop_rate *= float(idx) / len(self._blocks)
                    tf.compat.v1.logging.info(
                        "block_%s drop_connect_rate: %s" % (idx, drop_rate)
                    )
                outputs = block.call(
                    outputs,
                    use_batch_norm=use_batch_norm,
                    drop_out=drop_out,
                    drop_connect_rate=drop_rate,
                )
                self.endpoints["block_%s" % idx] = outputs
                if is_reduction:
                    self.endpoints["reduction_%s" % reduction_idx] = outputs
                if block.endpoints:
                    for k, v in block.endpoints.items():
                        self.endpoints["block_%s/%s" % (idx, k)] = v
                        if is_reduction:
                            self.endpoints["reduction_%s/%s" % (reduction_idx, k)] = v
        self.endpoints["features"] = outputs

        if not features_only:
            # Calls final layers and returns logits.
            with tf.compat.v1.variable_scope("head"):
                outputs = self._relu_fn(
                    self._bn1(self._conv_head(outputs), training=use_batch_norm)
                )
                outputs = self._avg_pooling(outputs)
                if self._dropout:
                    outputs = self._dropout(outputs, training=drop_out)
                self.endpoints["global_pool"] = outputs
                if self._fc:
                    outputs = self._fc(outputs)
                self.endpoints["head"] = outputs
        return outputs


--- File: deeplabcut/pose_estimation_tensorflow/backbones/__init__.py ---
#
# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#


--- File: deeplabcut/pose_estimation_tensorflow/backbones/mobilenet.py ---
#
# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
import collections
import contextlib
import copy
import os

import tensorflow as tf
import tf_slim as slim

from deeplabcut.pose_estimation_tensorflow.nnets.conv_blocks import (
    _fixed_padding,
    _make_divisible,
)


@slim.add_arg_scope
def apply_activation(x, name=None, activation_fn=None):
    return activation_fn(x, name=name) if activation_fn else x


@contextlib.contextmanager
def _set_arg_scope_defaults(defaults):
    """Sets arg scope defaults for all items present in defaults.

    Args:
      defaults: dictionary/list of pairs, containing a mapping from
      function to a dictionary of default args.

    Yields:
      context manager where all defaults are set.
    """
    if hasattr(defaults, "items"):
        items = list(defaults.items())
    else:
        items = defaults
    if not items:
        yield
    else:
        func, default_arg = items[0]
        with slim.arg_scope(func, **default_arg):
            with _set_arg_scope_defaults(items[1:]):
                yield


@slim.add_arg_scope
def depth_multiplier(
    output_params, multiplier, divisible_by=8, min_depth=8, **unused_kwargs
):
    if "num_outputs" not in output_params:
        return
    d = output_params["num_outputs"]
    output_params["num_outputs"] = _make_divisible(
        d * multiplier, divisible_by, min_depth
    )


_Op = collections.namedtuple("Op", ["op", "params", "multiplier_func"])


def op(opfunc, multiplier_func=depth_multiplier, **params):
    multiplier = params.pop("multiplier_transform", multiplier_func)
    return _Op(opfunc, params=params, multiplier_func=multiplier)


class NoOpScope(object):
    """No-op context manager."""

    def __enter__(self):
        return None

    def __exit__(self, exc_type, exc_value, traceback):
        return False


def safe_arg_scope(funcs, **kwargs):
    """Returns `slim.arg_scope` with all None arguments removed.

    Arguments:
      funcs: Functions to pass to `arg_scope`.
      **kwargs: Arguments to pass to `arg_scope`.

    Returns:
      arg_scope or No-op context manager.

    Note: can be useful if None value should be interpreted as "do not overwrite
      this parameter value".
    """
    filtered_args = {name: value for name, value in kwargs.items() if value is not None}
    if filtered_args:
        return slim.arg_scope(funcs, **filtered_args)
    else:
        return NoOpScope()


@slim.add_arg_scope
def mobilenet_base(  # pylint: disable=invalid-name
    inputs,
    conv_defs,
    multiplier=1.0,
    final_endpoint=None,
    output_stride=None,
    use_explicit_padding=False,
    scope=None,
    is_training=False,
):
    """Mobilenet base network.

    Constructs a network from inputs to the given final endpoint. By default
    the network is constructed in inference mode. To create network
    in training mode use:

    with slim.arg_scope(mobilenet.training_scope()):
       logits, endpoints = mobilenet_base(...)

    Args:
      inputs: a tensor of shape [batch_size, height, width, channels].
      conv_defs: A list of op(...) layers specifying the net architecture.
      multiplier: Float multiplier for the depth (number of channels)
        for all convolution ops. The value must be greater than zero. Typical
        usage will be to set this value in (0, 1) to reduce the number of
        parameters or computation cost of the model.
      final_endpoint: The name of last layer, for early termination for
      for V1-based networks: last layer is "layer_14", for V2: "layer_20"
      output_stride: An integer that specifies the requested ratio of input to
        output spatial resolution. If not None, then we invoke atrous convolution
        if necessary to prevent the network from reducing the spatial resolution
        of the activation maps. Allowed values are 1 or any even number, excluding
        zero. Typical values are 8 (accurate fully convolutional mode), 16
        (fast fully convolutional mode), and 32 (classification mode).

        NOTE- output_stride relies on all consequent operators to support dilated
        operators via "rate" parameter. This might require wrapping non-conv
        operators to operate properly.

      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
        inputs so that the output dimensions are the same as if 'SAME' padding
        were used.
      scope: optional variable scope.
      is_training: How to setup batch_norm and other ops. Note: most of the time
        this does not need be set directly. Use mobilenet.training_scope() to set
        up training instead. This parameter is here for backward compatibility
        only. It is safe to set it to the value matching
        training_scope(is_training=...). It is also safe to explicitly set
        it to False, even if there is outer training_scope set to to training.
        (The network will be built in inference mode). If this is set to None,
        no arg_scope is added for slim.batch_norm's is_training parameter.

    Returns:
      tensor_out: output tensor.
      end_points: a set of activations for external use, for example summaries or
                  losses.

    Raises:
      ValueError: depth_multiplier <= 0, or the target output_stride is not
                  allowed.
    """
    if multiplier <= 0:
        raise ValueError("multiplier is not greater than zero.")

    # Set conv defs defaults and overrides.
    conv_defs_defaults = conv_defs.get("defaults", {})
    conv_defs_overrides = conv_defs.get("overrides", {})
    if use_explicit_padding:
        conv_defs_overrides = copy.deepcopy(conv_defs_overrides)
        conv_defs_overrides[(slim.conv2d, slim.separable_conv2d)] = {"padding": "VALID"}

    if output_stride is not None:
        if output_stride == 0 or (output_stride > 1 and output_stride % 2):
            raise ValueError("Output stride must be None, 1 or a multiple of 2.")

    # a) Set the tensorflow scope
    # b) set padding to default: note we might consider removing this
    # since it is also set by mobilenet_scope
    # c) set all defaults
    # d) set all extra overrides.
    # pylint: disable=g-backslash-continuation
    with _scope_all(scope, default_scope="Mobilenet"), safe_arg_scope(
        [slim.batch_norm], is_training=is_training
    ), _set_arg_scope_defaults(conv_defs_defaults), _set_arg_scope_defaults(
        conv_defs_overrides
    ):
        # The current_stride variable keeps track of the output stride of the
        # activations, i.e., the running product of convolution strides up to the
        # current network layer. This allows us to invoke atrous convolution
        # whenever applying the next convolution would result in the activations
        # having output stride larger than the target output_stride.
        current_stride = 1

        # The atrous convolution rate parameter.
        rate = 1

        net = inputs
        # Insert default parameters before the base scope which includes
        # any custom overrides set in mobilenet.
        end_points = {}
        scopes = {}
        for i, opdef in enumerate(conv_defs["spec"]):
            params = dict(opdef.params)
            opdef.multiplier_func(params, multiplier)
            stride = params.get("stride", 1)
            if output_stride is not None and current_stride == output_stride:
                # If we have reached the target output_stride, then we need to employ
                # atrous convolution with stride=1 and multiply the atrous rate by the
                # current unit's stride for use in subsequent layers.
                layer_stride = 1
                layer_rate = rate
                rate *= stride
            else:
                layer_stride = stride
                layer_rate = 1
                current_stride *= stride
            # Update params.
            params["stride"] = layer_stride
            # Only insert rate to params if rate > 1 and kernel size is not [1, 1].
            if layer_rate > 1:
                if tuple(params.get("kernel_size", [])) != (1, 1):
                    # We will apply atrous rate in the following cases:
                    # 1) When kernel_size is not in params, the operation then uses
                    #   default kernel size 3x3.
                    # 2) When kernel_size is in params, and if the kernel_size is not
                    #   equal to (1, 1) (there is no need to apply atrous convolution to
                    #   any 1x1 convolution).
                    params["rate"] = layer_rate
            # Set padding
            if use_explicit_padding:
                if "kernel_size" in params:
                    net = _fixed_padding(net, params["kernel_size"], layer_rate)
                else:
                    params["use_explicit_padding"] = True

            end_point = "layer_%d" % (i + 1)
            try:
                net = opdef.op(net, **params)
            except Exception:
                print("Failed to create op %i: %r params: %r" % (i, opdef, params))
                raise
            end_points[end_point] = net
            scope = os.path.dirname(net.name)
            scopes[scope] = end_point
            if final_endpoint is not None and end_point == final_endpoint:
                break

        # Add all tensors that end with 'output' to
        # endpoints
        for t in net.graph.get_operations():
            scope = os.path.dirname(t.name)
            bn = os.path.basename(t.name)
            if scope in scopes and t.name.endswith("output"):
                end_points[scopes[scope] + "/" + bn] = t.outputs[0]
        return net, end_points


@contextlib.contextmanager
def _scope_all(scope, default_scope=None):
    with tf.compat.v1.variable_scope(
        scope, default_name=default_scope
    ) as s, tf.compat.v1.name_scope(s.original_name_scope):
        yield s


@slim.add_arg_scope
def mobilenet(
    inputs,
    num_classes=1001,
    prediction_fn=slim.softmax,
    reuse=None,
    scope="Mobilenet",
    base_only=False,
    **mobilenet_args
):
    """Mobilenet model for classification, supports both V1 and V2.

    Note: default mode is inference, use mobilenet.training_scope to create
    training network.


    Args:
      inputs: a tensor of shape [batch_size, height, width, channels].
      num_classes: number of predicted classes. If 0 or None, the logits layer
        is omitted and the input features to the logits layer (before dropout)
        are returned instead.
      prediction_fn: a function to get predictions out of logits
        (default softmax).
      reuse: whether or not the network and its variables should be reused. To be
        able to reuse 'scope' must be given.
      scope: Optional variable_scope.
      base_only: if True will only create the base of the network (no pooling
      and no logits).
      **mobilenet_args: passed to mobilenet_base verbatim.
        - conv_defs: list of conv defs
        - multiplier: Float multiplier for the depth (number of channels)
        for all convolution ops. The value must be greater than zero. Typical
        usage will be to set this value in (0, 1) to reduce the number of
        parameters or computation cost of the model.
        - output_stride: will ensure that the last layer has at most total stride.
        If the architecture calls for more stride than that provided
        (e.g. output_stride=16, but the architecture has 5 stride=2 operators),
        it will replace output_stride with fractional convolutions using Atrous
        Convolutions.

    Returns:
      logits: the pre-softmax activations, a tensor of size
        [batch_size, num_classes]
      end_points: a dictionary from components of the network to the corresponding
        activation tensor.

    Raises:
      ValueError: Input rank is invalid.
    """
    is_training = mobilenet_args.get("is_training", False)
    input_shape = inputs.get_shape().as_list()
    if len(input_shape) != 4:
        raise ValueError("Expected rank 4 input, was: %d" % len(input_shape))

    with tf.compat.v1.variable_scope(scope, "Mobilenet", reuse=reuse) as scope:
        inputs = tf.identity(inputs, "input")
        net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)
        if base_only:
            return net, end_points

        net = tf.identity(net, name="embedding")

        with tf.compat.v1.variable_scope("Logits"):
            net = global_pool(net)
            end_points["global_pool"] = net
            if not num_classes:
                return net, end_points
            net = slim.dropout(net, scope="Dropout", is_training=is_training)
            # 1 x 1 x num_classes
            # Note: legacy scope name.
            logits = slim.conv2d(
                net,
                num_classes,
                [1, 1],
                activation_fn=None,
                normalizer_fn=None,
                biases_initializer=tf.compat.v1.zeros_initializer(),
                scope="Conv2d_1c_1x1",
            )

            logits = tf.squeeze(logits, [1, 2])

            logits = tf.identity(logits, name="output")
        end_points["Logits"] = logits
        if prediction_fn:
            end_points["Predictions"] = prediction_fn(logits, "Predictions")
    return logits, end_points


def global_pool(input_tensor, pool_op=tf.nn.avg_pool2d):
    """Applies avg pool to produce 1x1 output.

    NOTE: This function is functionally equivalent to reduce_mean, but it has
    baked in average pool which has better support across hardware.

    Args:
      input_tensor: input tensor
      pool_op: pooling op (avg pool is default)
    Returns:
      a tensor batch_size x 1 x 1 x depth.
    """
    shape = input_tensor.get_shape().as_list()
    if shape[1] is None or shape[2] is None:
        kernel_size = tf.convert_to_tensor(
            value=[
                1,
                tf.shape(input=input_tensor)[1],
                tf.shape(input=input_tensor)[2],
                1,
            ]
        )
    else:
        kernel_size = [1, shape[1], shape[2], 1]
    output = pool_op(
        input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding="VALID"
    )
    # Recover output shape, for unknown shape.
    output.set_shape([None, 1, 1, None])
    return output


def training_scope(
    is_training=True,
    weight_decay=0.00004,
    stddev=0.09,
    dropout_keep_prob=0.8,
    bn_decay=0.997,
):
    """Defines Mobilenet training scope.

    Usage:
       with tf.contrib.slim.arg_scope(mobilenet.training_scope()):
         logits, endpoints = mobilenet_v2.mobilenet(input_tensor)

       # the network created will be trainble with dropout/batch norm
       # initialized appropriately.
    Args:
      is_training: if set to False this will ensure that all customizations are
        set to non-training mode. This might be helpful for code that is reused
        across both training/evaluation, but most of the time training_scope with
        value False is not needed. If this is set to None, the parameters is not
        added to the batch_norm arg_scope.

      weight_decay: The weight decay to use for regularizing the model.
      stddev: Standard deviation for initialization, if negative uses xavier.
      dropout_keep_prob: dropout keep probability (not set if equals to None).
      bn_decay: decay for the batch norm moving averages (not set if equals to
        None).

    Returns:
      An argument scope to use via arg_scope.
    """
    # Note: do not introduce parameters that would change the inference
    # model here (for example whether to use bias), modify conv_def instead.
    batch_norm_params = {"decay": bn_decay, "is_training": is_training}
    if stddev < 0:
        weight_intitializer = slim.initializers.xavier_initializer()
    else:
        weight_intitializer = tf.compat.v1.truncated_normal_initializer(stddev=stddev)

    # Set weight_decay for weights in Conv and FC layers.
    with slim.arg_scope(
        [slim.conv2d, slim.fully_connected, slim.separable_conv2d],
        weights_initializer=weight_intitializer,
        normalizer_fn=slim.batch_norm,
    ), slim.arg_scope(
        [mobilenet_base, mobilenet], is_training=is_training
    ), safe_arg_scope(
        [slim.batch_norm], **batch_norm_params
    ), safe_arg_scope(
        [slim.dropout], is_training=is_training, keep_prob=dropout_keep_prob
    ), slim.arg_scope(
        [slim.conv2d],
        weights_regularizer=tf.keras.regularizers.l2(0.5 * (weight_decay)),
    ), slim.arg_scope(
        [slim.separable_conv2d], weights_regularizer=None
    ) as s:
        return s


--- File: deeplabcut/pose_estimation_tensorflow/lib/inferenceutils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Backwards compatibility"""
from deeplabcut.core.inferenceutils import *


--- File: deeplabcut/pose_estimation_tensorflow/lib/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

# imports for backwards compatibility
import deeplabcut.core.crossvalutils
import deeplabcut.core.inferenceutils
import deeplabcut.core.trackingutils


--- File: deeplabcut/pose_estimation_tensorflow/lib/crossvalutils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Backwards compatibility"""
from deeplabcut.core.crossvalutils import *


--- File: deeplabcut/pose_estimation_tensorflow/lib/trackingutils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Backwards compatibility"""
from deeplabcut.core.trackingutils import *


--- File: deeplabcut/pose_estimation_tensorflow/nnets/multi.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import re
import tensorflow as tf
import tf_slim as slim
from tf_slim.nets import resnet_v1

import deeplabcut.pose_estimation_tensorflow.backbones.efficientnet_builder as eff
from deeplabcut.pose_estimation_tensorflow.nnets import conv_blocks
from deeplabcut.pose_estimation_tensorflow.backbones import mobilenet_v2, mobilenet
from .base import BasePoseNet
from .factory import PoseNetFactory
from .layers import prediction_layer_stage
from .utils import wrapper


# Change the stride from 2 to 1 to get 16x downscaling instead of 32x.
mobilenet_v2.V2_DEF["spec"][14] = mobilenet.op(
    conv_blocks.expanded_conv, stride=1, num_outputs=160
)


net_funcs = {
    "resnet_50": resnet_v1.resnet_v1_50,
    "resnet_101": resnet_v1.resnet_v1_101,
    "resnet_152": resnet_v1.resnet_v1_152,
    "mobilenet_v2_1.0": mobilenet_v2.mobilenet_base,
    "mobilenet_v2_0.75": wrapper(
        mobilenet_v2.mobilenet_base,
        depth_multiplier=0.75,
        final_endpoint="layer_19",
        finegrain_classification_mode=True,
    ),
    "mobilenet_v2_0.5": wrapper(
        mobilenet_v2.mobilenet_base,
        depth_multiplier=0.5,
        final_endpoint="layer_19",
        finegrain_classification_mode=True,
    ),
    "mobilenet_v2_0.35": wrapper(
        mobilenet_v2.mobilenet_base,
        depth_multiplier=0.35,
        final_endpoint="layer_19",
        finegrain_classification_mode=True,
    ),
    "mobilenet_v2_0.1": wrapper(
        mobilenet_v2.mobilenet_base,
        depth_multiplier=0.1,
        final_endpoint="layer_19",
        finegrain_classification_mode=True,
    ),
    "mobilenet_v2_0.35_10": wrapper(
        mobilenet_v2.mobilenet_base,
        depth_multiplier=0.35,
        final_endpoint="layer_10",
        finegrain_classification_mode=True,
    ),
    "mobilenet_v2_0.1_10": wrapper(
        mobilenet_v2.mobilenet_base,
        depth_multiplier=0.1,
        final_endpoint="layer_10",
        finegrain_classification_mode=True,
    ),
}

# https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142
parallel_layers = {
    "b0": "4",
    "b1": "7",
    "b2": "7",
    "b3": "7",
    "b4": "9",
    "b5": "12",
    "b6": "14",
    "b7": "17",
}


def prediction_layer(cfg, input, name, num_outputs):
    with slim.arg_scope(
        [slim.conv2d, slim.conv2d_transpose],
        padding="SAME",
        activation_fn=None,
        normalizer_fn=None,
        weights_regularizer=slim.l2_regularizer(cfg["weight_decay"]),
    ):
        with tf.compat.v1.variable_scope(name):
            pred = slim.conv2d_transpose(
                input,
                num_outputs,
                kernel_size=[3, 3],
                stride=2,
            )
            return pred


@PoseNetFactory.register("multi")
class PoseMultiNet(BasePoseNet):
    def __init__(self, cfg):
        super(PoseMultiNet, self).__init__(cfg)
        multi_stage = self.cfg.get("multi_stage", False)
        # Multi stage is currently only implemented for resnets
        self.cfg["multi_stage"] = multi_stage and "resnet" in self.cfg["net_type"]

    def extract_features(self, inputs):
        im_centered = self.center_inputs(inputs)
        net_type = self.cfg["net_type"]
        if "resnet" in net_type:
            net_fun = net_funcs[net_type]
            with slim.arg_scope(resnet_v1.resnet_arg_scope()):
                net, end_points = net_fun(
                    im_centered, global_pool=False, output_stride=16, is_training=False
                )
        elif "mobilenet" in net_type:
            net_fun = net_funcs[net_type]
            with slim.arg_scope(mobilenet_v2.training_scope()):
                net, end_points = net_fun(im_centered)
        elif "efficientnet" in net_type:
            if "use_batch_norm" not in self.cfg.keys():
                self.cfg["use_batch_norm"] = False
            if "use_drop_out" not in self.cfg.keys():
                self.cfg["use_drop_out"] = False

            im_centered /= tf.constant(eff.STDDEV_RGB, shape=[1, 1, 3])
            net, end_points = eff.build_model_base(
                im_centered,
                net_type,
                use_batch_norm=self.cfg["use_batch_norm"],
                drop_out=self.cfg["use_drop_out"],
            )
        else:
            raise ValueError(f"Unknown network of type {net_type}")
        return net, end_points

    def prediction_layers(
        self,
        features,
        end_points,
        input_shape,
        scope="pose",
        reuse=None,
    ):
        net_type = self.cfg["net_type"]
        if self.cfg["multi_stage"]:  # MuNet! (multi_stage decoder + multi_fusion)
            # Defining multi_fusion backbone
            num_layers = re.findall("resnet_([0-9]*)", net_type)[0]
            layer_name = (
                "resnet_v1_{}".format(num_layers) + "/block{}/unit_{}/bottleneck_v1"
            )
            mid_pt_block1 = layer_name.format(1, 3)
            mid_pt_block2 = layer_name.format(2, 3)

            final_dims = tf.math.ceil(
                tf.divide(input_shape[1:3], tf.convert_to_tensor(16))
            )

            interim_dims_s8 = tf.scalar_mul(2, final_dims)
            interim_dims_s8 = tf.cast(interim_dims_s8, tf.int32)
            interim_dims_s4 = tf.scalar_mul(2, interim_dims_s8)
            interim_dims_s4 = tf.cast(interim_dims_s4, tf.int32)

            bank_1 = end_points[mid_pt_block1]
            bank_2 = end_points[mid_pt_block2]

            bank_2_s8 = tf.compat.v1.image.resize_images(bank_2, interim_dims_s8)
            bank_1_s4 = tf.compat.v1.image.resize_images(bank_1, interim_dims_s4)

            with slim.arg_scope(
                [slim.conv2d],
                padding="SAME",
                normalizer_fn=slim.layers.batch_norm,
                activation_fn=tf.nn.relu,
                weights_regularizer=slim.l2_regularizer(self.cfg["weight_decay"]),
            ):
                with tf.compat.v1.variable_scope("decoder_filters"):
                    bank_2_s16 = slim.conv2d(
                        bank_2_s8,
                        512,
                        kernel_size=[3, 3],
                        stride=2,
                        scope="decoder_parallel_1",
                    )
                    bank_2_s16 = slim.conv2d(
                        bank_2_s16,
                        128,
                        kernel_size=[1, 1],
                        stride=1,
                        scope="decoder_parallel_2",
                    )

                    bank_1_s8 = slim.conv2d(
                        bank_1_s4,
                        256,
                        kernel_size=[3, 3],
                        stride=2,
                        scope="decoder_parallel_3",
                    )
                    bank_1_s16 = slim.conv2d(
                        bank_1_s8,
                        256,
                        kernel_size=[3, 3],
                        stride=2,
                        scope="decoder_parallel_4",
                    )
                    bank_1_s16 = slim.conv2d(
                        bank_1_s16,
                        128,
                        kernel_size=[1, 1],
                        stride=1,
                        scope="decoder_parallel_5",
                    )

            with slim.arg_scope(
                [slim.conv2d_transpose],
                padding="SAME",
                normalizer_fn=None,
                weights_regularizer=slim.l2_regularizer(self.cfg["weight_decay"]),
            ):
                with tf.compat.v1.variable_scope("upsampled_features"):
                    concat_3_s16 = tf.concat([bank_1_s16, bank_2_s16, features], 3)

                    if self.cfg["stride"] == 8:
                        net = concat_3_s16

                    elif self.cfg["stride"] == 4:
                        upsampled_features_2x = slim.conv2d_transpose(
                            concat_3_s16,
                            self.cfg.get("bank3", 128),
                            kernel_size=[3, 3],
                            stride=2,
                            scope="block3",
                        )
                        net = upsampled_features_2x

                    elif self.cfg["stride"] == 2:
                        upsampled_features_2x = slim.conv2d_transpose(
                            concat_3_s16,
                            self.cfg.get("bank3", 128),
                            kernel_size=[3, 3],
                            stride=2,
                            scope="block3",
                        )
                        upsampled_features_4x = slim.conv2d_transpose(
                            upsampled_features_2x,
                            self.cfg.get("bank5", 128),
                            kernel_size=[3, 3],
                            stride=2,
                            scope="block4",
                        )
                        net = upsampled_features_4x

            out = {}
            # Attaching multi-stage decoder
            with tf.compat.v1.variable_scope(scope, reuse=reuse):
                stage1_hm_out = prediction_layer(
                    self.cfg,
                    net,
                    "part_pred_s1",
                    self.cfg["num_joints"] + self.cfg.get("num_idchannel", 0),
                )

                if self.cfg["location_refinement"]:
                    out["locref"] = prediction_layer(
                        self.cfg, net, "locref_pred", self.cfg["num_joints"] * 2
                    )
                if (
                    self.cfg["pairwise_predict"]
                    and "multi-animal" not in self.cfg["dataset_type"]
                ):
                    out["pairwise_pred"] = prediction_layer(
                        self.cfg,
                        net,
                        "pairwise_pred",
                        self.cfg["num_joints"] * (self.cfg["num_joints"] - 1) * 2,
                    )
                if (
                    self.cfg["partaffinityfield_predict"]
                    and "multi-animal" in self.cfg["dataset_type"]
                ):
                    feature = slim.conv2d_transpose(
                        net, self.cfg.get("bank3", 128), kernel_size=[3, 3], stride=2
                    )

                    stage1_paf_out = prediction_layer(
                        self.cfg, net, "pairwise_pred_s1", self.cfg["num_limbs"] * 2
                    )

                    stage2_in = tf.concat([stage1_hm_out, stage1_paf_out, feature], 3)
                    stage_input = stage2_in
                    stage_paf_output = stage1_paf_out
                    stage_hm_output = stage1_hm_out

                    for i in range(2, 5):
                        pre_stage_paf_output = stage_paf_output
                        pre_stage_hm_output = stage_hm_output

                        stage_paf_output = prediction_layer_stage(
                            self.cfg,
                            stage_input,
                            f"pairwise_pred_s{i}",
                            self.cfg["num_limbs"] * 2,
                        )

                        stage_hm_output = prediction_layer_stage(
                            self.cfg,
                            stage_input,
                            f"part_pred_s{i}",
                            self.cfg["num_joints"] + self.cfg.get("num_idchannel", 0),
                        )

                        if i > 2:
                            # stage_paf_output = stage_paf_output + pre_stage_paf_output
                            stage_hm_output = stage_hm_output + pre_stage_hm_output

                        stage_input = tf.concat(
                            [stage_hm_output, stage_paf_output, feature], 3
                        )

                    out["part_pred"] = prediction_layer_stage(
                        self.cfg,
                        stage_input,
                        "part_pred",
                        self.cfg["num_joints"] + self.cfg.get("num_idchannel", 0),
                    )

                    out["pairwise_pred"] = prediction_layer_stage(
                        self.cfg,
                        stage_input,
                        "pairwise_pred",
                        self.cfg["num_limbs"] * 2,
                    )

                if self.cfg["intermediate_supervision"]:
                    interm_name = layer_name.format(
                        3, self.cfg["intermediate_supervision_layer"]
                    )
                    block_interm_out = end_points[interm_name]
                    out["part_pred_interm"] = prediction_layer(
                        self.cfg,
                        block_interm_out,
                        "intermediate_supervision",
                        self.cfg["num_joints"] + self.cfg.get("num_idchannel", 0),
                    )

        else:  # dual fusion net (for stride 4 experiments)
            if "resnet" in net_type:
                num_layers = re.findall("resnet_([0-9]*)", net_type)[0]
                layer_name = "resnet_v1_{}/block{}/unit_{}/bottleneck_v1"
                mid_pt = layer_name.format(num_layers, 2, 3)
            elif "mobilenet" in net_type:
                mid_pt = "layer_7"
            elif "efficientnet" in net_type:
                mid_pt = f"block_{parallel_layers[net_type.split('-')[1]]}"
            else:
                raise ValueError(f"Unknown network of type {net_type}")

            final_dims = tf.math.ceil(
                tf.divide(input_shape[1:3], tf.convert_to_tensor(value=16))
            )
            interim_dims = tf.scalar_mul(2, final_dims)
            interim_dims = tf.cast(interim_dims, tf.int32)
            bank_3 = end_points[mid_pt]
            bank_3 = tf.image.resize(bank_3, interim_dims)

            with slim.arg_scope(
                [slim.conv2d],
                padding="SAME",
                normalizer_fn=None,
                weights_regularizer=tf.keras.regularizers.l2(
                    0.5 * (self.cfg["weight_decay"])
                ),
            ):
                with tf.compat.v1.variable_scope("decoder_filters"):
                    bank_3 = slim.conv2d(
                        bank_3,
                        self.cfg.get("bank3", 128),
                        1,
                        scope="decoder_parallel_1",
                    )

            with slim.arg_scope(
                [slim.conv2d_transpose],
                padding="SAME",
                normalizer_fn=None,
                weights_regularizer=tf.keras.regularizers.l2(
                    0.5 * (self.cfg["weight_decay"])
                ),
            ):
                with tf.compat.v1.variable_scope("upsampled_features"):
                    upsampled_features = slim.conv2d_transpose(
                        features,
                        self.cfg.get("bank5", 128),
                        kernel_size=[3, 3],
                        stride=2,
                        scope="block4",
                    )
            net = tf.concat([bank_3, upsampled_features], 3)
            out = super(PoseMultiNet, self).prediction_layers(
                net,
                scope,
                reuse,
            )
            with tf.compat.v1.variable_scope(scope, reuse=reuse):
                if (
                    self.cfg["intermediate_supervision"]
                    and "efficientnet" not in net_type
                ):
                    if "mobilenet" in net_type:
                        feat = end_points[
                            f"layer_{self.cfg['intermediate_supervision_layer']}"
                        ]
                    elif "resnet" in net_type:
                        layer_name = "resnet_v1_{}/block{}/unit_{}/bottleneck_v1"
                        num_layers = re.findall("resnet_([0-9]*)", net_type)[0]
                        interm_name = layer_name.format(
                            num_layers, 3, self.cfg["intermediate_supervision_layer"]
                        )
                        feat = end_points[interm_name]
                    else:
                        return out
                    pred_layer = out["part_pred_interm"] = prediction_layer(
                        self.cfg,
                        feat,
                        "intermediate_supervision",
                        self.cfg["num_joints"] + self.cfg.get("num_idchannel", 0),
                    )
                    out["part_pred_interm"] = pred_layer
        out["features"] = features
        return out

    def get_net(self, inputs):
        net, end_points = self.extract_features(inputs)
        return self.prediction_layers(net, end_points, tf.shape(input=inputs))


--- File: deeplabcut/pose_estimation_tensorflow/nnets/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from .factory import PoseNetFactory
from .efficientnet import PoseEfficientNet
from .mobilenet import PoseMobileNet
from .multi import PoseMultiNet
from .resnet import PoseResnet


__all__ = [
    "PoseNetFactory",
    "PoseEfficientNet",
    "PoseMobileNet",
    "PoseMultiNet",
    "PoseResnet",
]


--- File: deeplabcut/pose_estimation_tensorflow/nnets/factory.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import warnings


class PoseNetFactory:
    _nets = dict()

    @classmethod
    def register(cls, type_):
        def wrapper(net):
            if type_ in cls._nets:
                warnings.warn("Overwriting existing network {}.")
            cls._nets[type_] = net
            return net

        return wrapper

    @classmethod
    def create(cls, cfg):
        if cfg.get("stride", 8) < 8:
            net_type = "multi"
        else:
            net_type = cfg["net_type"]
        key = cls._find_matching_key(cls._nets, net_type)
        if key is None:
            raise ValueError(f"Unsupported network of type {net_type}")
        net = cls._nets.get(key)
        return net(cfg)

    @staticmethod
    def _find_matching_key(dict_, key):
        try:
            match = next(k for k in dict_ if k in key)
        except StopIteration:
            match = None
        return match


--- File: deeplabcut/pose_estimation_tensorflow/nnets/efficientnet.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""

Effnet added by T. Biasi & AM
Efficient Nets added by T. Biasi & AM
See https://openaccess.thecvf.com/content/WACV2021/html/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html

"""

import tensorflow as tf
import deeplabcut.pose_estimation_tensorflow.backbones.efficientnet_builder as eff
from .base import BasePoseNet
from .factory import PoseNetFactory


@PoseNetFactory.register("efficientnet")
class PoseEfficientNet(BasePoseNet):
    def __init__(self, cfg):
        super(PoseEfficientNet, self).__init__(cfg)
        if "use_batch_norm" not in self.cfg:
            self.cfg["use_batch_norm"] = False
        if "use_drop_out" not in self.cfg:
            self.cfg["use_drop_out"] = False

    def extract_features(self, inputs, use_batch_norm=False, use_drop_out=False):
        im_centered = self.center_inputs(inputs)
        im_centered /= tf.constant(eff.STDDEV_RGB, shape=[1, 1, 3])
        with tf.compat.v1.variable_scope("efficientnet"):
            eff_net_type = self.cfg["net_type"].replace("_", "-")
            net, end_points = eff.build_model_base(
                im_centered,
                eff_net_type,
                use_batch_norm=use_batch_norm,
                drop_out=use_drop_out,
            )
        return net, end_points

    def get_net(self, inputs, use_batch_norm=False, use_drop_out=False):
        net, _ = self.extract_features(inputs, use_batch_norm, use_drop_out)
        return self.prediction_layers(net)

    def test(self, inputs):
        heads = self.get_net(
            inputs, self.cfg["use_batch_norm"], self.cfg["use_drop_out"]
        )
        return self.add_inference_layers(heads)


--- File: deeplabcut/pose_estimation_tensorflow/nnets/mobilenet.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

"""
Also see our paper:
Pretraining boosts out-of-domain robustness for pose estimation
by Alexander Mathis, Mert Yüksekgönül, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis
https://arxiv.org/abs/1909.11229

Based on Slim implementation of mobilenets:
https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
"""

import tensorflow as tf
import tf_slim as slim

from deeplabcut.pose_estimation_tensorflow.backbones import mobilenet_v2
from .base import BasePoseNet
from .factory import PoseNetFactory
from .layers import prediction_layer
from .utils import wrapper


networks = {
    "mobilenet_v2_1.0": (mobilenet_v2.mobilenet_base, mobilenet_v2.training_scope),
    "mobilenet_v2_0.75": (
        wrapper(
            mobilenet_v2.mobilenet_base,
            depth_multiplier=0.75,
            finegrain_classification_mode=True,
        ),
        mobilenet_v2.training_scope,
    ),
    "mobilenet_v2_0.5": (
        wrapper(
            mobilenet_v2.mobilenet_base,
            depth_multiplier=0.5,
            finegrain_classification_mode=True,
        ),
        mobilenet_v2.training_scope,
    ),
    "mobilenet_v2_0.35": (
        wrapper(
            mobilenet_v2.mobilenet_base,
            depth_multiplier=0.35,
            finegrain_classification_mode=True,
        ),
        mobilenet_v2.training_scope,
    ),
}


@PoseNetFactory.register("mobilenet")
class PoseMobileNet(BasePoseNet):
    def __init__(self, cfg):
        super(PoseMobileNet, self).__init__(cfg)

    def extract_features(self, inputs):
        net_fun, net_arg_scope = networks[self.cfg["net_type"]]
        im_centered = self.center_inputs(inputs)
        with slim.arg_scope(net_arg_scope()):
            net, end_points = net_fun(im_centered)

        return net, end_points

    def prediction_layers(
        self,
        features,
        end_points,
        scope="pose",
        reuse=None,
    ):
        out = super(PoseMobileNet, self).prediction_layers(
            features,
            scope,
            reuse,
        )
        with tf.compat.v1.variable_scope(scope, reuse=reuse):
            if self.cfg["intermediate_supervision"]:
                out["part_pred_interm"] = prediction_layer(
                    self.cfg,
                    end_points[f"layer_{self.cfg['intermediate_supervision_layer']}"],
                    "intermediate_supervision",
                    self.cfg["num_joints"] + self.cfg.get("num_idchannel", 0),
                )
        return out

    def get_net(self, inputs):
        net, end_points = self.extract_features(inputs)
        return self.prediction_layers(net, end_points)


--- File: deeplabcut/pose_estimation_tensorflow/nnets/resnet.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Adapted from DeeperCut by Eldar Insafutdinov
# https://github.com/eldar/pose-tensorflow
#
# Licensed under GNU Lesser General Public License v3.0
#

import re
import tensorflow as tf
import tf_slim as slim
from tf_slim.nets import resnet_v1

from .base import BasePoseNet
from .factory import PoseNetFactory
from .layers import prediction_layer


net_funcs = {
    "resnet_50": resnet_v1.resnet_v1_50,
    "resnet_101": resnet_v1.resnet_v1_101,
    "resnet_152": resnet_v1.resnet_v1_152,
}


@PoseNetFactory.register("resnet")
class PoseResnet(BasePoseNet):
    def __init__(self, cfg):
        super(PoseResnet, self).__init__(cfg)

    def extract_features(self, inputs):
        net_fun = net_funcs[self.cfg["net_type"]]
        im_centered = self.center_inputs(inputs)
        with slim.arg_scope(resnet_v1.resnet_arg_scope()):
            net, end_points = net_fun(
                im_centered,
                global_pool=False,
                output_stride=16,
                is_training=False,
            )
        return net, end_points

    def prediction_layers(
        self,
        features,
        end_points,
        scope="pose",
        reuse=None,
    ):
        out = super(PoseResnet, self).prediction_layers(
            features,
            scope,
            reuse,
        )
        out["features"] = features
        with tf.compat.v1.variable_scope(scope, reuse=reuse):
            if self.cfg["intermediate_supervision"]:
                layer_name = "resnet_v1_{}/block{}/unit_{}/bottleneck_v1"
                num_layers = re.findall("resnet_([0-9]*)", self.cfg["net_type"])[0]
                interm_name = layer_name.format(
                    num_layers, 3, self.cfg["intermediate_supervision_layer"]
                )
                block_interm_out = end_points[interm_name]
                out["part_pred_interm"] = prediction_layer(
                    self.cfg,
                    block_interm_out,
                    "intermediate_supervision",
                    self.cfg["num_joints"] + self.cfg.get("num_idchannel", 0),
                )
        return out

    def get_net(self, inputs):
        net, end_points = self.extract_features(inputs)
        return self.prediction_layers(net, end_points)


--- File: deeplabcut/pose_estimation_tensorflow/nnets/utils.py ---
#
# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

import functools
import numpy as np
import tensorflow as tf
from deeplabcut.pose_estimation_tensorflow.datasets import Batch
from tensorflow.python.tpu.ops import tpu_ops
from tensorflow.python.tpu import tpu_function


def wrapper(func, *args, **kwargs):
    partial_func = functools.partial(func, *args, **kwargs)
    functools.update_wrapper(partial_func, func)
    return partial_func


def get_batch_spec(cfg):
    num_joints = cfg["num_joints"]
    num_limbs = cfg["num_limbs"]
    batch_size = cfg["batch_size"]
    batch_spec = {
        Batch.inputs: [batch_size, None, None, 3],
        Batch.part_score_targets: [
            batch_size,
            None,
            None,
            num_joints + cfg.get("num_idchannel", 0),
        ],
        Batch.part_score_weights: [
            batch_size,
            None,
            None,
            num_joints + cfg.get("num_idchannel", 0),
        ],
    }
    if cfg["location_refinement"]:
        batch_spec[Batch.locref_targets] = [batch_size, None, None, num_joints * 2]
        batch_spec[Batch.locref_mask] = [batch_size, None, None, num_joints * 2]
    if cfg["pairwise_predict"]:
        print("Getting specs", cfg["dataset_type"], num_limbs, num_joints)
        if (
            "multi-animal" not in cfg["dataset_type"]
        ):  # this can be used for pairwise conditional
            batch_spec[Batch.pairwise_targets] = [
                batch_size,
                None,
                None,
                num_joints * (num_joints - 1) * 2,
            ]
            batch_spec[Batch.pairwise_mask] = [
                batch_size,
                None,
                None,
                num_joints * (num_joints - 1) * 2,
            ]
        else:  # train partaffinity fields
            batch_spec[Batch.pairwise_targets] = [
                batch_size,
                None,
                None,
                num_limbs * 2,
            ]
            batch_spec[Batch.pairwise_mask] = [
                batch_size,
                None,
                None,
                num_limbs * 2,
            ]
    return batch_spec


def make_2d_gaussian_kernel(sigma, size):
    sigma = tf.convert_to_tensor(sigma, dtype=tf.float32)
    k = tf.range(-size // 2 + 1, size // 2 + 1)
    k = tf.cast(k**2, sigma.dtype)
    k = tf.nn.softmax(-k / (2 * (sigma**2)))
    return tf.einsum("i,j->ij", k, k)


def build_learning_rate(
    initial_lr,
    global_step,
    steps_per_epoch=None,
    lr_decay_type="exponential",
    decay_factor=0.97,
    decay_epochs=2.4,
    total_steps=None,
    warmup_epochs=5,
):
    """Build learning rate."""
    if lr_decay_type == "exponential":
        assert steps_per_epoch is not None
        decay_steps = steps_per_epoch * decay_epochs
        lr = tf.compat.v1.train.exponential_decay(
            initial_lr, global_step, decay_steps, decay_factor, staircase=True
        )
    elif lr_decay_type == "cosine":
        assert total_steps is not None
        lr = (
            0.5
            * initial_lr
            * (1 + tf.cos(np.pi * tf.cast(global_step, tf.float32) / total_steps))
        )
    elif lr_decay_type == "constant":
        lr = initial_lr
    else:
        assert False, "Unknown lr_decay_type : %s" % lr_decay_type

    if warmup_epochs:
        tf.compat.v1.logging.info("Learning rate warmup_epochs: %d" % warmup_epochs)
        warmup_steps = int(warmup_epochs * steps_per_epoch)
        warmup_lr = (
            initial_lr
            * tf.cast(global_step, tf.float32)
            / tf.cast(warmup_steps, tf.float32)
        )
        lr = tf.cond(
            pred=global_step < warmup_steps,
            true_fn=lambda: warmup_lr,
            false_fn=lambda: lr,
        )

    return lr


def build_optimizer(
    learning_rate, optimizer_name="rmsprop", decay=0.9, epsilon=0.001, momentum=0.9
):
    """Build optimizer."""
    if optimizer_name == "sgd":
        tf.compat.v1.logging.info("Using SGD optimizer")
        optimizer = tf.compat.v1.train.GradientDescentOptimizer(
            learning_rate=learning_rate
        )
    elif optimizer_name == "momentum":
        tf.compat.v1.logging.info("Using Momentum optimizer")
        optimizer = tf.compat.v1.train.MomentumOptimizer(
            learning_rate=learning_rate, momentum=momentum
        )
    elif optimizer_name == "rmsprop":
        tf.compat.v1.logging.info("Using RMSProp optimizer")
        optimizer = tf.compat.v1.train.RMSPropOptimizer(
            learning_rate, decay, momentum, epsilon
        )
    else:
        tf.compat.v1.logging.fatal("Unknown optimizer:", optimizer_name)
    return optimizer


class TpuBatchNormalization(tf.compat.v1.layers.BatchNormalization):
    """Cross replica batch normalization."""

    def __init__(self, fused=False, **kwargs):
        if fused in (True, None):
            raise ValueError("TpuBatchNormalization does not support fused=True.")
        super(TpuBatchNormalization, self).__init__(fused=fused, **kwargs)

    @staticmethod
    def _cross_replica_average(t, num_shards_per_group):
        """Calculates the average value of input tensor across TPU replicas."""
        num_shards = tpu_function.get_tpu_context().number_of_shards
        group_assignment = None
        if num_shards_per_group > 1:
            if num_shards % num_shards_per_group != 0:
                raise ValueError(
                    "num_shards: %d mod shards_per_group: %d, should be 0"
                    % (num_shards, num_shards_per_group)
                )
            num_groups = num_shards // num_shards_per_group
            group_assignment = [
                [x for x in range(num_shards) if x // num_shards_per_group == y]
                for y in range(num_groups)
            ]
        return tpu_ops.cross_replica_sum(t, group_assignment) / tf.cast(
            num_shards_per_group, t.dtype
        )

    def _moments(self, inputs, reduction_axes, keep_dims):
        """Compute the mean and variance: it overrides the original _moments."""
        shard_mean, shard_variance = super(TpuBatchNormalization, self)._moments(
            inputs, reduction_axes, keep_dims=keep_dims
        )

        num_shards = tpu_function.get_tpu_context().number_of_shards or 1
        if num_shards <= 8:  # Skip cross_replica for 2x2 or smaller slices.
            num_shards_per_group = 1
        else:
            num_shards_per_group = max(8, num_shards // 8)
        tf.compat.v1.logging.info(
            "TpuBatchNormalization with num_shards_per_group %s", num_shards_per_group
        )
        if num_shards_per_group > 1:
            # Compute variance using: Var[X]= E[X^2] - E[X]^2.
            shard_square_of_mean = tf.math.square(shard_mean)
            shard_mean_of_square = shard_variance + shard_square_of_mean
            group_mean = self._cross_replica_average(shard_mean, num_shards_per_group)
            group_mean_of_square = self._cross_replica_average(
                shard_mean_of_square, num_shards_per_group
            )
            group_variance = group_mean_of_square - tf.math.square(group_mean)
            return group_mean, group_variance
        return shard_mean, shard_variance


class BatchNormalization(tf.compat.v1.layers.BatchNormalization):
    """Fixed default name of BatchNormalization to match TpuBatchNormalization."""

    def __init__(self, name="tpu_batch_normalization", **kwargs):
        super(BatchNormalization, self).__init__(name=name, **kwargs)


def drop_connect(inputs, is_training, drop_connect_rate):
    """Apply drop connect."""
    if not is_training:
        return inputs

    # Compute keep_prob
    # TODO(tanmingxing): add support for training progress.
    keep_prob = 1.0 - drop_connect_rate

    # Compute drop_connect tensor
    batch_size = tf.shape(input=inputs)[0]
    random_tensor = keep_prob
    random_tensor += tf.random.uniform([batch_size, 1, 1, 1], dtype=inputs.dtype)
    binary_tensor = tf.floor(random_tensor)
    output = tf.compat.v1.div(inputs, keep_prob) * binary_tensor
    return output


class DepthwiseConv2D(tf.keras.layers.DepthwiseConv2D, tf.compat.v1.layers.Layer):
    """Wrap keras DepthwiseConv2D to tf.layers."""

    pass


--- File: deeplabcut/pose_estimation_tensorflow/nnets/layers.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import tensorflow as tf
import tf_slim as slim


# FIXME Fix wrong scope with Keras layers
# def prediction_layer(cfg, input, name, num_outputs):
#     with tf.compat.v1.variable_scope(name):
#         layer = tf.keras.layers.Conv2DTranspose(
#             filters=num_outputs,
#             kernel_size=(3, 3),
#             strides=2,
#             padding="same",
#             kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),
#             name=name,
#             dtype=input.dtype.base_dtype,
#         )
#         return layer(input)


def prediction_layer(cfg, input, name, num_outputs):
    with slim.arg_scope(
        [slim.conv2d, slim.conv2d_transpose],
        padding="SAME",
        activation_fn=None,
        normalizer_fn=None,
        weights_regularizer=slim.l2_regularizer(cfg["weight_decay"]),
    ):
        with tf.compat.v1.variable_scope(name):
            pred = slim.conv2d_transpose(
                input, num_outputs, kernel_size=[3, 3], stride=2, scope="block4"
            )
            return pred


### New DLCNet Addition: multi-stage decoder
# def prediction_layer_stage(cfg, input, name, num_outputs):
#     with tf.compat.v1.variable_scope(name):
#         layer = tf.keras.layers.Conv2D(
#             filters=num_outputs,
#             kernel_size=(3, 3),
#             strides=1,
#             padding="same",
#             kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),
#             name=name,
#             dtype=input.dtype.base_dtype,
#         )
#         return layer(input)


def prediction_layer_stage(cfg, input, name, num_outputs):
    with slim.arg_scope(
        [slim.conv2d, slim.conv2d_transpose],
        padding="SAME",
        activation_fn=None,
        normalizer_fn=None,
        weights_regularizer=slim.l2_regularizer(cfg["weight_decay"]),
    ):
        with tf.compat.v1.variable_scope(name):
            pred = slim.conv2d(
                input,
                num_outputs,
                kernel_size=[3, 3],
                stride=1,
            )
            return pred


--- File: deeplabcut/pose_estimation_tensorflow/nnets/conv_blocks.py ---
#
# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""Convolution blocks for mobilenet."""
import contextlib
import functools

import tensorflow as tf
import tf_slim as slim


def _fixed_padding(inputs, kernel_size, rate=1):
    """Pads the input along the spatial dimensions independently of input size.

    Pads the input such that if it was used in a convolution with 'VALID' padding,
    the output would have the same dimensions as if the unpadded input was used
    in a convolution with 'SAME' padding.

    Args:
      inputs: A tensor of size [batch, height_in, width_in, channels].
      kernel_size: The kernel to be used in the conv2d or max_pool2d operation.
      rate: An integer, rate for atrous convolution.

    Returns:
      output: A tensor of size [batch, height_out, width_out, channels] with the
        input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).
    """
    size = kernel_size[0] + (kernel_size[0] - 1) * (rate - 1)
    kernel_size_effective = [size, size]
    pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]
    pad_beg = [pad_total[0] // 2, pad_total[1] // 2]
    pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]
    padded_inputs = tf.pad(
        tensor=inputs,
        paddings=[[0, 0], [pad_beg[0], pad_end[0]], [pad_beg[1], pad_end[1]], [0, 0]],
    )
    return padded_inputs


def _make_divisible(v, divisor, min_value=None):
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    # Make sure that round down does not go down by more than 10%.
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


def _split_divisible(num, num_ways, divisible_by=8):
    """Evenly splits num, num_ways so each piece is a multiple of divisible_by."""
    assert num % divisible_by == 0
    assert num / num_ways >= divisible_by
    # Note: want to round down, we adjust each split to match the total.
    base = num // num_ways // divisible_by * divisible_by
    result = []
    accumulated = 0
    for i in range(num_ways):
        r = base
        while accumulated + r < num * (i + 1) / num_ways:
            r += divisible_by
        result.append(r)
        accumulated += r
    assert accumulated == num
    return result


@contextlib.contextmanager
def _v1_compatible_scope_naming(scope):
    if scope is None:  # Create uniqified separable blocks.
        with tf.compat.v1.variable_scope(
            None, default_name="separable"
        ) as s, tf.compat.v1.name_scope(s.original_name_scope):
            yield ""
    else:
        # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.
        # which provide numbered scopes.
        scope += "_"
        yield scope


@slim.add_arg_scope
def split_separable_conv2d(
    input_tensor,
    num_outputs,
    scope=None,
    normalizer_fn=None,
    stride=1,
    rate=1,
    endpoints=None,
    use_explicit_padding=False,
):
    """Separable mobilenet V1 style convolution.

    Depthwise convolution, with default non-linearity,
    followed by 1x1 depthwise convolution.  This is similar to
    slim.separable_conv2d, but differs in that it applies batch
    normalization and non-linearity to depthwise. This  matches
    the basic building of Mobilenet Paper
    (https://arxiv.org/abs/1704.04861)

    Args:
      input_tensor: input
      num_outputs: number of outputs
      scope: optional name of the scope. Note if provided it will use
      scope_depthwise for deptwhise, and scope_pointwise for pointwise.
      normalizer_fn: which normalizer function to use for depthwise/pointwise
      stride: stride
      rate: output rate (also known as dilation rate)
      endpoints: optional, if provided, will export additional tensors to it.
      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
        inputs so that the output dimensions are the same as if 'SAME' padding
        were used.

    Returns:
      output tesnor
    """

    with _v1_compatible_scope_naming(scope) as scope:
        dw_scope = scope + "depthwise"
        endpoints = endpoints if endpoints is not None else {}
        kernel_size = [3, 3]
        padding = "SAME"
        if use_explicit_padding:
            padding = "VALID"
            input_tensor = _fixed_padding(input_tensor, kernel_size, rate)
        net = slim.separable_conv2d(
            input_tensor,
            None,
            kernel_size,
            depth_multiplier=1,
            stride=stride,
            rate=rate,
            normalizer_fn=normalizer_fn,
            padding=padding,
            scope=dw_scope,
        )

        endpoints[dw_scope] = net

        pw_scope = scope + "pointwise"
        net = slim.conv2d(
            net,
            num_outputs,
            [1, 1],
            stride=1,
            normalizer_fn=normalizer_fn,
            scope=pw_scope,
        )
        endpoints[pw_scope] = net
    return net


def expand_input_by_factor(n, divisible_by=8):
    return lambda num_inputs, **_: _make_divisible(num_inputs * n, divisible_by)


@slim.add_arg_scope
def expanded_conv(
    input_tensor,
    num_outputs,
    expansion_size=expand_input_by_factor(6),
    stride=1,
    rate=1,
    kernel_size=(3, 3),
    residual=True,
    normalizer_fn=None,
    project_activation_fn=tf.identity,
    split_projection=1,
    split_expansion=1,
    split_divisible_by=8,
    expansion_transform=None,
    depthwise_location="expansion",
    depthwise_channel_multiplier=1,
    endpoints=None,
    use_explicit_padding=False,
    padding="SAME",
    scope=None,
):
    """Depthwise Convolution Block with expansion.

    Builds a composite convolution that has the following structure
    expansion (1x1) -> depthwise (kernel_size) -> projection (1x1)

    Args:
      input_tensor: input
      num_outputs: number of outputs in the final layer.
      expansion_size: the size of expansion, could be a constant or a callable.
        If latter it will be provided 'num_inputs' as an input. For forward
        compatibility it should accept arbitrary keyword arguments.
        Default will expand the input by factor of 6.
      stride: depthwise stride
      rate: depthwise rate
      kernel_size: depthwise kernel
      residual: whether to include residual connection between input
        and output.
      normalizer_fn: batchnorm or otherwise
      project_activation_fn: activation function for the project layer
      split_projection: how many ways to split projection operator
        (that is conv expansion->bottleneck)
      split_expansion: how many ways to split expansion op
        (that is conv bottleneck->expansion) ops will keep depth divisible
        by this value.
      split_divisible_by: make sure every split group is divisible by this number.
      expansion_transform: Optional function that takes expansion
        as a single input and returns output.
      depthwise_location: where to put depthwise covnvolutions supported
        values None, 'input', 'output', 'expansion'
      depthwise_channel_multiplier: depthwise channel multiplier:
      each input will replicated (with different filters)
      that many times. So if input had c channels,
      output will have c x depthwise_channel_multpilier.
      endpoints: An optional dictionary into which intermediate endpoints are
        placed. The keys "expansion_output", "depthwise_output",
        "projection_output" and "expansion_transform" are always populated, even
        if the corresponding functions are not invoked.
      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
        inputs so that the output dimensions are the same as if 'SAME' padding
        were used.
      padding: Padding type to use if `use_explicit_padding` is not set.
      scope: optional scope.

    Returns:
      Tensor of depth num_outputs

    Raises:
      TypeError: on inval
    """
    with tf.compat.v1.variable_scope(
        scope, default_name="expanded_conv"
    ) as s, tf.compat.v1.name_scope(s.original_name_scope):
        prev_depth = input_tensor.get_shape().as_list()[3]
        if depthwise_location not in [None, "input", "output", "expansion"]:
            raise TypeError(
                "%r is unknown value for depthwise_location" % depthwise_location
            )
        if use_explicit_padding:
            if padding != "SAME":
                raise TypeError(
                    "`use_explicit_padding` should only be used with " '"SAME" padding.'
                )
            padding = "VALID"
        depthwise_func = functools.partial(
            slim.separable_conv2d,
            num_outputs=None,
            kernel_size=kernel_size,
            depth_multiplier=depthwise_channel_multiplier,
            stride=stride,
            rate=rate,
            normalizer_fn=normalizer_fn,
            padding=padding,
            scope="depthwise",
        )
        # b1 -> b2 * r -> b2
        #   i -> (o * r) (bottleneck) -> o
        input_tensor = tf.identity(input_tensor, "input")
        net = input_tensor

        if depthwise_location == "input":
            if use_explicit_padding:
                net = _fixed_padding(net, kernel_size, rate)
            net = depthwise_func(net, activation_fn=None)

        if callable(expansion_size):
            inner_size = expansion_size(num_inputs=prev_depth)
        else:
            inner_size = expansion_size

        if inner_size > net.shape[3]:
            net = split_conv(
                net,
                inner_size,
                num_ways=split_expansion,
                scope="expand",
                divisible_by=split_divisible_by,
                stride=1,
                normalizer_fn=normalizer_fn,
            )
            net = tf.identity(net, "expansion_output")
        if endpoints is not None:
            endpoints["expansion_output"] = net

        if depthwise_location == "expansion":
            if use_explicit_padding:
                net = _fixed_padding(net, kernel_size, rate)
            net = depthwise_func(net)

        net = tf.identity(net, name="depthwise_output")
        if endpoints is not None:
            endpoints["depthwise_output"] = net
        if expansion_transform:
            net = expansion_transform(expansion_tensor=net, input_tensor=input_tensor)
        # Note in contrast with expansion, we always have
        # projection to produce the desired output size.
        net = split_conv(
            net,
            num_outputs,
            num_ways=split_projection,
            stride=1,
            scope="project",
            divisible_by=split_divisible_by,
            normalizer_fn=normalizer_fn,
            activation_fn=project_activation_fn,
        )
        if endpoints is not None:
            endpoints["projection_output"] = net
        if depthwise_location == "output":
            if use_explicit_padding:
                net = _fixed_padding(net, kernel_size, rate)
            net = depthwise_func(net, activation_fn=None)

        if callable(residual):  # custom residual
            net = residual(input_tensor=input_tensor, output_tensor=net)
        elif (
            residual
            and
            # stride check enforces that we don't add residuals when spatial
            # dimensions are None
            stride == 1
            and
            # Depth matches
            net.get_shape().as_list()[3] == input_tensor.get_shape().as_list()[3]
        ):
            net += input_tensor
        return tf.identity(net, name="output")


def split_conv(input_tensor, num_outputs, num_ways, scope, divisible_by=8, **kwargs):
    """Creates a split convolution.

    Split convolution splits the input and output into
    'num_blocks' blocks of approximately the same size each,
    and only connects $i$-th input to $i$ output.

    Args:
      input_tensor: input tensor
      num_outputs: number of output filters
      num_ways: num blocks to split by.
      scope: scope for all the operators.
      divisible_by: make sure that every part is divisiable by this.
      **kwargs: will be passed directly into conv2d operator
    Returns:
      tensor
    """
    b = input_tensor.get_shape().as_list()[3]

    if num_ways == 1 or min(b // num_ways, num_outputs // num_ways) < divisible_by:
        # Don't do any splitting if we end up with less than 8 filters
        # on either side.
        return slim.conv2d(input_tensor, num_outputs, [1, 1], scope=scope, **kwargs)

    outs = []
    input_splits = _split_divisible(b, num_ways, divisible_by=divisible_by)
    output_splits = _split_divisible(num_outputs, num_ways, divisible_by=divisible_by)
    inputs = tf.split(input_tensor, input_splits, axis=3, name="split_" + scope)
    base = scope
    for i, (input_tensor, out_size) in enumerate(zip(inputs, output_splits)):
        scope = base + "_part_%d" % (i,)
        n = slim.conv2d(input_tensor, out_size, [1, 1], scope=scope, **kwargs)
        n = tf.identity(n, scope + "_output")
        outs.append(n)
    return tf.concat(outs, 3, name=scope + "_concat")


--- File: deeplabcut/pose_estimation_tensorflow/nnets/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import abc
import tensorflow as tf
from deeplabcut.pose_estimation_tensorflow.datasets import Batch
from deeplabcut.pose_estimation_tensorflow.core import predict_multianimal
from .layers import prediction_layer
from .utils import make_2d_gaussian_kernel


class BasePoseNet(metaclass=abc.ABCMeta):
    def __init__(self, cfg):
        self.cfg = cfg

    @abc.abstractmethod
    def extract_features(self, inputs): ...

    @abc.abstractmethod
    def get_net(self, inputs): ...

    def train(self, batch):
        heads = self.get_net(batch[Batch.inputs])
        if self.cfg["weigh_part_predictions"]:
            part_score_weights = batch[Batch.part_score_weights]
        else:
            part_score_weights = 1.0

        def add_part_loss(pred_layer):
            return tf.compat.v1.losses.sigmoid_cross_entropy(
                batch[Batch.part_score_targets], heads[pred_layer], part_score_weights
            )

        loss = {"part_loss": add_part_loss("part_pred")}
        total_loss = loss["part_loss"]

        if (
            self.cfg["intermediate_supervision"]
            and "efficientnet" not in self.cfg["net_type"]
        ):
            loss["part_loss_interm"] = add_part_loss("part_pred_interm")
            total_loss += loss["part_loss_interm"]

        if self.cfg["location_refinement"]:
            locref_pred = heads["locref"]
            locref_targets = batch[Batch.locref_targets]
            locref_weights = batch[Batch.locref_mask]
            loss_func = (
                tf.compat.v1.losses.huber_loss
                if self.cfg["locref_huber_loss"]
                else tf.compat.v1.losses.mean_squared_error
            )
            loss["locref_loss"] = self.cfg["locref_loss_weight"] * loss_func(
                locref_targets, locref_pred, locref_weights
            )
            total_loss += loss["locref_loss"]

        if self.cfg["pairwise_predict"] or self.cfg["partaffinityfield_predict"]:
            pairwise_pred = heads["pairwise_pred"]
            pairwise_targets = batch[Batch.pairwise_targets]
            pairwise_weights = batch[Batch.pairwise_mask]
            loss_func = (
                tf.compat.v1.losses.huber_loss
                if self.cfg["pairwise_huber_loss"]
                else tf.compat.v1.losses.mean_squared_error
            )
            loss["pairwise_loss"] = self.cfg["pairwise_loss_weight"] * loss_func(
                pairwise_targets, pairwise_pred, pairwise_weights
            )
            total_loss += loss["pairwise_loss"]

        loss["total_loss"] = total_loss
        return loss

    def test(self, inputs):
        heads = self.get_net(inputs)
        return self.add_inference_layers(heads)

    def prediction_layers(
        self,
        features,
        scope="pose",
        reuse=None,
    ):
        out = {}
        n_joints = self.cfg["num_joints"]
        with tf.compat.v1.variable_scope(scope, reuse=reuse):
            out["part_pred"] = prediction_layer(
                self.cfg,
                features,
                "part_pred",
                n_joints + self.cfg.get("num_idchannel", 0),
            )
            if self.cfg["location_refinement"]:
                out["locref"] = prediction_layer(
                    self.cfg,
                    features,
                    "locref_pred",
                    n_joints * 2,
                )
            if (
                self.cfg["pairwise_predict"]
                and "multi-animal" not in self.cfg["dataset_type"]
            ):
                out["pairwise_pred"] = prediction_layer(
                    self.cfg,
                    features,
                    "pairwise_pred",
                    n_joints * (n_joints - 1) * 2,
                )
            if (
                self.cfg["partaffinityfield_predict"]
                and "multi-animal" in self.cfg["dataset_type"]
            ):
                out["pairwise_pred"] = prediction_layer(
                    self.cfg,
                    features,
                    "pairwise_pred",
                    self.cfg["num_limbs"] * 2,
                )
        out["features"] = features
        return out

    def inference(self, inputs):
        """Direct TF inference on GPU.
        Added with: https://arxiv.org/abs/1909.11229
        """
        heads = self.get_net(inputs)
        locref = heads["locref"]
        probs = tf.sigmoid(heads["part_pred"])

        if self.cfg["batch_size"] == 1:
            probs = tf.squeeze(probs, axis=0)
            locref = tf.squeeze(locref, axis=0)
            l_shape = tf.shape(input=probs)
            locref = tf.reshape(locref, (l_shape[0] * l_shape[1], -1, 2))
            probs = tf.reshape(probs, (l_shape[0] * l_shape[1], -1))
            maxloc = tf.argmax(input=probs, axis=0)
            loc = tf.unravel_index(
                maxloc, (tf.cast(l_shape[0], tf.int64), tf.cast(l_shape[1], tf.int64))
            )
            maxloc = tf.reshape(maxloc, (1, -1))

            joints = tf.reshape(
                tf.range(0, tf.cast(l_shape[2], dtype=tf.int64)), (1, -1)
            )
        else:
            l_shape = tf.shape(
                input=probs
            )  # batchsize times x times y times body parts
            locref = tf.reshape(
                locref, (l_shape[0], l_shape[1], l_shape[2], l_shape[3], 2)
            )
            # turn into x times y time bs * bpts
            locref = tf.transpose(a=locref, perm=[1, 2, 0, 3, 4])
            probs = tf.transpose(a=probs, perm=[1, 2, 0, 3])

            l_shape = tf.shape(input=probs)  # x times y times batch times body parts

            locref = tf.reshape(locref, (l_shape[0] * l_shape[1], -1, 2))
            probs = tf.reshape(probs, (l_shape[0] * l_shape[1], -1))
            maxloc = tf.argmax(input=probs, axis=0)
            loc = tf.unravel_index(
                maxloc, (tf.cast(l_shape[0], tf.int64), tf.cast(l_shape[1], tf.int64))
            )  # tuple of max indices
            maxloc = tf.reshape(maxloc, (1, -1))
            joints = tf.reshape(
                tf.range(0, tf.cast(l_shape[2] * l_shape[3], dtype=tf.int64)), (1, -1)
            )

        # extract corresponding locref x and y as well as probability
        indices = tf.transpose(a=tf.concat([maxloc, joints], axis=0))
        offset = tf.gather_nd(locref, indices)
        offset = tf.gather(offset, [1, 0], axis=1)
        likelihood = tf.reshape(tf.gather_nd(probs, indices), (-1, 1))

        pose = (
            self.cfg["stride"] * tf.cast(tf.transpose(a=loc), dtype=tf.float32)
            + self.cfg["stride"] * 0.5
            + offset * self.cfg["locref_stdev"]
        )
        pose = tf.concat([pose, likelihood], axis=1)
        return {"pose": pose}

    def add_inference_layers(self, heads):
        """initialized during inference"""
        prob = tf.sigmoid(heads["part_pred"])
        nms_radius = int(self.cfg.get("nmsradius", 5))

        # Filter predicted heatmaps with a 2D Gaussian kernel as in:
        # https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf
        scmaps = tf.gather(prob, tf.range(self.cfg["num_joints"]), axis=3)
        kernel = make_2d_gaussian_kernel(
            sigma=self.cfg.get("sigma", 1),
            size=nms_radius * 2 + 1,
        )
        kernel = kernel[:, :, tf.newaxis, tf.newaxis]

        kernel_sc = tf.tile(kernel, [1, 1, tf.shape(scmaps)[3], 1])
        scmaps = tf.nn.depthwise_conv2d(
            scmaps,
            kernel_sc,
            strides=[1, 1, 1, 1],
            padding="SAME",
        )
        peak_inds = predict_multianimal.find_local_peak_indices_maxpool_nms(
            scmaps,
            nms_radius,
            self.cfg.get("minconfidence", 0.01),
        )
        outputs = {"part_prob": prob, "peak_inds": peak_inds}
        if self.cfg["location_refinement"]:
            locref = heads["locref"]
            if self.cfg.get("locref_smooth", False):
                kernel_loc = tf.tile(kernel, [1, 1, tf.shape(locref)[3], 1])
                locref = tf.nn.depthwise_conv2d(
                    locref,
                    kernel_loc,
                    strides=[1, 1, 1, 1],
                    padding="SAME",
                )
            outputs["locref"] = locref

        if self.cfg["pairwise_predict"] or self.cfg["partaffinityfield_predict"]:
            outputs["pairwise_pred"] = heads["pairwise_pred"]

        if "features" in heads:
            outputs["features"] = heads["features"]

        return outputs

    def center_inputs(self, inputs):
        mean = tf.constant(
            self.cfg["mean_pixel"],
            dtype=tf.float32,
            shape=[1, 1, 1, 3],
            name="img_mean",
        )
        return inputs - mean


--- File: deeplabcut/utils/auxiliaryfunctions_3d.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""

import glob
import os
import pickle
from pathlib import Path

import cv2
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


def Foldernames3Dproject(cfg_3d):
    """Definitions of subfolders in 3D projects"""

    img_path = os.path.join(cfg_3d["project_path"], "calibration_images")
    path_corners = os.path.join(cfg_3d["project_path"], "corners")
    path_camera_matrix = os.path.join(cfg_3d["project_path"], "camera_matrix")
    path_undistort = os.path.join(cfg_3d["project_path"], "undistortion")
    path_removed_images = os.path.join(
        cfg_3d["project_path"], "removed_calibration_images"
    )

    return (
        img_path,
        path_corners,
        path_camera_matrix,
        path_undistort,
        path_removed_images,
    )


def create_empty_df(dataframe, scorer, flag):
    # Creates an empty dataFrame of same shape as df_side_view
    # flag = 2d or 3d

    df = dataframe
    bodyparts = df.columns.get_level_values("bodyparts").unique()
    a = np.full((df.shape[0], 3), np.nan)
    dataFrame = None
    for bodypart in bodyparts:
        if flag == "2d":
            pdindex = pd.MultiIndex.from_product(
                [[scorer], [bodypart], ["x", "y", "likelihood"]],
                names=["scorer", "bodyparts", "coords"],
            )
        elif flag == "3d":
            pdindex = pd.MultiIndex.from_product(
                [[scorer], [bodypart], ["x", "y", "z"]],
                names=["scorer", "bodyparts", "coords"],
            )
        frame = pd.DataFrame(a, columns=pdindex, index=range(0, df.shape[0]))
        dataFrame = pd.concat([frame, dataFrame], axis=1)
    return (dataFrame, scorer, bodyparts)


def compute_triangulation_calibration_images(
    stereo_matrix, projectedPoints1, projectedPoints2, path_undistort, cfg_3d, plot=True
):
    """
    Performs triangulation of the calibration images.
    """
    triangulate = []
    P1 = stereo_matrix["P1"]
    P2 = stereo_matrix["P2"]
    cmap = cfg_3d["colormap"]
    colormap = plt.get_cmap(cmap)
    markerSize = cfg_3d["dotsize"]
    markerType = cfg_3d["markerType"]

    for i in range(projectedPoints1.shape[0]):
        X_l = triangulatePoints(P1, P2, projectedPoints1[i], projectedPoints2[i])
        triangulate.append(X_l)
    triangulate = np.asanyarray(triangulate)

    # Plotting
    if plot == True:
        col = colormap(np.linspace(0, 1, triangulate.shape[0]))
        fig = plt.figure()
        ax = fig.add_subplot(111, projection="3d")

        for i in range(triangulate.shape[0]):
            xs = triangulate[i, 0, :]
            ys = triangulate[i, 1, :]
            zs = triangulate[i, 2, :]
            ax.scatter(xs, ys, zs, c=col[i], marker=markerType, s=markerSize)
            ax.set_xlabel("X")
            ax.set_ylabel("Y")
            ax.set_zlabel("Z")
        plt.savefig(os.path.join(str(path_undistort), "checkerboard_3d.png"))
    return triangulate


def triangulatePoints(P1, P2, x1, x2):
    X = cv2.triangulatePoints(P1[:3], P2[:3], x1, x2)
    return X / X[3]


def get_camerawise_videos(path, cam_names, videotype):
    """
    This function returns the list of videos corresponding to the camera names specified in the cam_names.
    e.g. if cam_names = ['camera-1','camera-2']

    then it will return [['somename-camera-1-othername.avi', 'somename-camera-2-othername.avi']]
    """
    import glob
    from pathlib import Path

    vid = []

    # Find videos only specific to the cam names
    videos = [
        glob.glob(os.path.join(path, str("*" + cam_names[i] + "*" + videotype)))
        for i in range(len(cam_names))
    ]
    videos = [y for x in videos for y in x]

    # Exclude the labeled video files
    if "." in videotype:
        file_to_exclude = str("labeled" + videotype)
    else:
        file_to_exclude = str("labeled." + videotype)
    videos = [v for v in videos if os.path.isfile(v) and not (file_to_exclude in v)]
    video_list = []
    cam = cam_names[0]  # camera1
    vid.append(
        [
            name
            for name in glob.glob(os.path.join(path, str("*" + cam + "*" + videotype)))
        ]
    )  # all videos with cam
    # print("here is what I found",vid)
    for k in range(len(vid[0])):
        if cam in str(Path(vid[0][k]).stem):
            ending = Path(vid[0][k]).suffix
            pref = str(Path(vid[0][k]).stem).split(cam)[0]
            suf = str(Path(vid[0][k]).stem).split(cam)[1]
            if pref == "":
                if suf == "":
                    print("Strange naming convention on your part. Respect.")
                else:
                    putativecam2name = os.path.join(path, cam_names[1] + suf + ending)
            else:
                if suf == "":
                    putativecam2name = os.path.join(path, pref + cam_names[1] + ending)
                else:
                    putativecam2name = os.path.join(
                        path, pref + cam_names[1] + suf + ending
                    )
            # print([os.path.join(path,pref+cam+suf+ending),putativecam2name])
            if os.path.isfile(putativecam2name):
                # found a pair!!!
                video_list.append(
                    [os.path.join(path, pref + cam + suf + ending), putativecam2name]
                )
    return video_list


def Get_list_of_triangulated_and_videoFiles(
    filepath, videotype, scorer_3d, cam_names, videofolder
):
    """
    Returns the list of triangulated h5 and the corresponding video files.
    """

    prefix = []
    suffix = []
    file_list = []
    string_to_search = scorer_3d + ".h5"

    # Checks if filepath is a directory
    if [os.path.isdir(i) for i in filepath] == [True]:
        """
        Analyzes all the videos in the directory.
        """
        print("Analyzing all the videos in the directory")
        videofolder = filepath[0]
        cwd = os.getcwd()
        os.chdir(videofolder)
        triangulated_file_list = [
            fn for fn in os.listdir(os.curdir) if (string_to_search in fn)
        ]
        video_list = get_camerawise_videos(videofolder, cam_names, videotype)
        os.chdir(cwd)
        triangulated_folder = videofolder
    else:
        triangulated_file_list = [
            str(Path(fn).name) for fn in filepath if (string_to_search in fn)
        ]
        triangulated_folder = [
            str(Path(fn).parents[0]) for fn in filepath if (string_to_search in fn)
        ]
        triangulated_folder = triangulated_folder[0]

        if videofolder is None:
            videofolder = str(Path(filepath[0]).parents[0])
        video_list = get_camerawise_videos(videofolder, cam_names, videotype)

    # Get the filename of the triangulated file excluding the scorer name and remove any '-' or _ from it
    filename = [i.split(string_to_search)[0] for i in triangulated_file_list]
    for i in range(len(filename)):
        if filename[i][-1] == "_" or filename[i][-1] == "-":
            filename[i] = filename[i][:-1]
        if filename[i][0] == "_" or filename[i][0] == "-":
            filename[i] = filename[i][1:]

    # Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.
    for i in range(len(video_list)):
        pre = [
            str(Path(video_list[i][0]).stem).split(cam_names[0])[0],
            str(Path(video_list[i][1]).stem).split(cam_names[1])[0],
        ]
        suf = [
            str(Path(video_list[i][0]).stem).split(cam_names[0])[-1],
            str(Path(video_list[i][1]).stem).split(cam_names[1])[-1],
        ]
        for i in range(len(cam_names)):
            if pre[i] == "":
                pass
            elif pre[i][-1] == "_" or pre[i][-1] == "-":
                pre[i] = pre[i][:-1]
            if suf[i] == "":
                pass
            elif suf[i][0] == "_" or suf[i][0] == "-":
                suf[i] = suf[i][1:]
        suffix.append(suf)
        prefix.append(pre)

    # Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.
    for k in range(len(filename)):
        for j in range(len(prefix)):
            if (prefix[j][0] in filename[k] and prefix[j][1] in filename[k]) and (
                suffix[j][0] in filename[k] and suffix[j][1] in filename[k]
            ):
                triangulated_file = glob.glob(
                    os.path.join(
                        triangulated_folder,
                        str("*" + filename[k] + "*" + string_to_search),
                    )
                )
                vfiles = get_camerawise_videos(videofolder, cam_names, videotype)
                vfiles = [
                    z for z in vfiles if prefix[j][0] in z[0] and suffix[j][0] in z[1]
                ][0]
                file_list.append(triangulated_file + vfiles)

    return file_list


def SaveMetadata3d(metadatafilename, metadata):
    with open(metadatafilename, "wb") as f:
        pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)


def LoadMetadata3d(metadatafilename):
    with open(metadatafilename, "rb") as f:
        metadata = pickle.load(f)
        return metadata


def _reconstruct_tracks_as_tracklets(df):
    """
    Parameters:
    -----------
    df: DataFrame
        loaded from an .h5 tracks file (obtained from `stitch_tracklets()`)
    """
    from deeplabcut.refine_training_dataset.stitch import Tracklet

    tracklets = []
    for _, group in df.groupby("individuals", axis=1):
        temp = group.dropna(how="all")
        inds = temp.index.to_numpy()
        track = Tracklet(temp.to_numpy().reshape((len(temp), -1, 3)), inds)
        track = track.interpolate(max_gap=len(group))
        tracklets.append(track)
    return tracklets


def _associate_paired_view_tracks(tracklets1, tracklets2, F):
    """
    Computes the optimal matching between tracks in two cameras
    using the xFx'=0 epipolar constraint equation.

    Parameters:
    -----------
    tracklets1/2: Tracklet() object (defined in stitch.py)
    F: numpy.ndarray
        Fundamental matrix between cam1 and cam2
    """
    from scipy.optimize import linear_sum_assignment

    # Initialize costs matrix
    costs = np.zeros([len(tracklets1), len(tracklets2)])

    for i, t1 in enumerate(tracklets1):
        for j, t2 in enumerate(tracklets2):
            # get common bodypart detections in track pair
            _t1 = t1.xy[np.isin(t1.inds, t2.inds)]
            _t2 = t2.xy[np.isin(t2.inds, t1.inds)]

            # add 3rd dim to the points
            _t1 = np.c_[_t1, np.ones((*_t1.shape[:2], 1))]
            _t2 = np.c_[_t2, np.ones((*_t2.shape[:2], 1))]

            try:
                # cost for any point in time of t1 being the same
                # any point in time of t2
                cost = np.abs(np.nansum(np.matmul(_t1, F) * _t2, axis=2))

                # Get average cost of the entire track
                cost = cost.mean()
            except:
                # typically when dim 2 differs, with uniquebodyparts
                cost = 100000.0

            costs[i, j] = cost

    match_inds = linear_sum_assignment(np.abs(costs))
    voting = dict(zip(*match_inds))

    return costs, voting


def cross_view_match_dataframes(df1, df2, F):
    """
    Computes the costs and matched voting for tracks between
    a camera pair

    df: Data read from .h5 track file
    F: fundamental matrix from OpenCV
    """

    tracks1 = _reconstruct_tracks_as_tracklets(df1)
    tracks2 = _reconstruct_tracks_as_tracklets(df2)
    costs, voting = _associate_paired_view_tracks(tracks1, tracks2, F)

    return costs, voting


--- File: deeplabcut/utils/auxfun_models.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""

import os
from pathlib import Path
from deeplabcut.utils import auxiliaryfunctions


# This dictionary maps the model types to the file locations where the models exist.
MODEL_BASE_PATH = Path("pose_estimation_tensorflow") / "models" / "pretrained"
MODELTYPE_FILEPATH_MAP = {
    "resnet_50": MODEL_BASE_PATH / "resnet_v1_50.ckpt",
    "resnet_101": MODEL_BASE_PATH / "resnet_v1_101.ckpt",
    "resnet_152": MODEL_BASE_PATH / "resnet_v1_152.ckpt",
    "mobilenet_v2_1.0": MODEL_BASE_PATH / "mobilenet_v2_1.0_224.ckpt",
    "mobilenet_v2_0.75": MODEL_BASE_PATH / "mobilenet_v2_0.75_224.ckpt",
    "mobilenet_v2_0.5": MODEL_BASE_PATH / "mobilenet_v2_0.5_224.ckpt",
    "mobilenet_v2_0.35": MODEL_BASE_PATH / "mobilenet_v2_0.35_224.ckpt",
    "efficientnet-b0": MODEL_BASE_PATH / "efficientnet-b0" / "model.ckpt",
    "efficientnet-b1": MODEL_BASE_PATH / "efficientnet-b1" / "model.ckpt",
    "efficientnet-b2": MODEL_BASE_PATH / "efficientnet-b2" / "model.ckpt",
    "efficientnet-b3": MODEL_BASE_PATH / "efficientnet-b3" / "model.ckpt",
    "efficientnet-b4": MODEL_BASE_PATH / "efficientnet-b4" / "model.ckpt",
    "efficientnet-b5": MODEL_BASE_PATH / "efficientnet-b5" / "model.ckpt",
    "efficientnet-b6": MODEL_BASE_PATH / "efficientnet-b6" / "model.ckpt",
}


def check_for_weights(modeltype, parent_path):
    """gets local path to network weights and checks if they are present. If not, downloads them from tensorflow.org"""
    if modeltype not in MODELTYPE_FILEPATH_MAP.keys():
        print(
            "Currently ResNet (50, 101, 152), MobilenetV2 (1, 0.75, 0.5 and 0.35) and EfficientNet (b0-b6) are supported, please change 'resnet' entry in config.yaml!"
        )
        # Exit the function early if an unknown modeltype is provided.
        return parent_path

    exists = False
    model_path = parent_path / MODELTYPE_FILEPATH_MAP[modeltype]
    try:
        for file in os.listdir(model_path.parent):
            if model_path.name in file:
                exists = True
                break
    except FileNotFoundError:
        pass

    if not exists:
        if "efficientnet" in modeltype:
            download_weights(modeltype, model_path.parent)
        else:
            download_weights(modeltype, model_path)

    return str(model_path)


def download_weights(modeltype, model_path):
    """
    Downloads the ImageNet pretrained weights for ResNets, MobileNets et al. from TensorFlow...
    """
    import urllib
    import tarfile
    from io import BytesIO

    target_dir = model_path.parents[0]
    neturls = auxiliaryfunctions.read_plainconfig(
        target_dir / "pretrained_model_urls.yaml"
    )
    try:
        if "efficientnet" in modeltype:
            url = neturls["efficientnet"]
            url = url + modeltype.replace("_", "-") + ".tar.gz"
        else:
            url = neturls[modeltype]
        print("Downloading a ImageNet-pretrained model from {}....".format(url))
        response = urllib.request.urlopen(url)
        with tarfile.open(fileobj=BytesIO(response.read()), mode="r:gz") as tar:
            tar.extractall(path=target_dir)
    except KeyError:
        print("Model does not exist: ", modeltype)
        print("Pick one of the following: ", neturls.keys())


def download_model(modelname, target_dir):
    """
    Downloads a DeepLabCut Model Zoo Project
    """
    import urllib.request
    import tarfile
    from tqdm import tqdm

    def show_progress(count, block_size, total_size):
        pbar.update(block_size)

    def tarfilenamecutting(tarf):
        """' auxfun to extract folder path
        ie. /xyz-trainsetxyshufflez/
        """
        for memberid, member in enumerate(tarf.getmembers()):
            if memberid == 0:
                parent = str(member.path)
                l = len(parent) + 1
            if member.path.startswith(parent):
                member.path = member.path[l:]
                yield member

    dlc_path = auxiliaryfunctions.get_deeplabcut_path()
    neturls = auxiliaryfunctions.read_plainconfig(
        os.path.join(
            dlc_path,
            "pose_estimation_tensorflow",
            "models",
            "pretrained",
            "pretrained_model_urls.yaml",
        )
    )
    if modelname in neturls.keys():
        url = neturls[modelname]
        response = urllib.request.urlopen(url)
        print(
            "Downloading the model from the DeepLabCut server @Harvard -> Go Crimson!!! {}....".format(
                url
            )
        )
        total_size = int(response.getheader("Content-Length"))
        pbar = tqdm(unit="B", total=total_size, position=0)
        filename, _ = urllib.request.urlretrieve(url, reporthook=show_progress)
        with tarfile.open(filename, mode="r:gz") as tar:
            tar.extractall(target_dir, members=tarfilenamecutting(tar))
    else:
        models = [
            fn
            for fn in neturls.keys()
            if "resnet_" not in fn
            and "efficientnet" not in fn
            and "mobilenet_" not in fn
        ]
        print("Model does not exist: ", modelname)
        print("Pick one of the following: ", models)


def set_visible_devices(gputouse: int):
    import tensorflow as tf

    physical_devices = tf.config.list_physical_devices("GPU")
    n_devices = len(physical_devices)
    if gputouse >= n_devices:
        raise ValueError(
            f"There are {n_devices} available GPUs: {physical_devices}\nPlease choose `gputouse` in {list(range(n_devices))}."
        )
    tf.config.set_visible_devices(physical_devices[gputouse], "GPU")


def smart_restore(restorer, sess, checkpoint_path, net_type):
    "Restore pretrained weights, smartly redownloading them if missing."
    try:
        restorer.restore(sess, checkpoint_path)
    except ValueError as e:  # The path may be wrong, or the weights no longer exist
        dlcparent_path = auxiliaryfunctions.get_deeplabcut_path()
        correct_model_path = os.path.join(
            dlcparent_path,
            MODELTYPE_FILEPATH_MAP[net_type],
        )
        if checkpoint_path == correct_model_path:
            # The path is right, hence the weights are missing; we'll download them again.
            _ = check_for_weights(net_type, Path(dlcparent_path))
            restorer.restore(sess, checkpoint_path)
        else:
            raise ValueError(e)


# Aliases for backwards-compatibility
Check4Weights = check_for_weights
Downloadweights = download_weights
DownloadModel = download_model


--- File: deeplabcut/utils/skeleton.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.2 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""

import os
import warnings

import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib.collections import LineCollection
from matplotlib.path import Path
from matplotlib.widgets import Button, LassoSelector
from ruamel.yaml import YAML
from scipy.spatial import cKDTree as KDTree
from skimage import io


def read_config(configname):
    if not os.path.exists(configname):
        raise FileNotFoundError(
            f"Config {configname} is not found. Please make sure that the file exists."
        )
    with open(configname) as file:
        return YAML().load(file)


def write_config(configname, cfg):
    with open(configname, "w") as file:
        YAML().dump(cfg, file)


class SkeletonBuilder:
    def __init__(self, config_path):
        self.config_path = config_path
        self.cfg = read_config(config_path)
        # Find uncropped labeled data
        self.df = None
        found = False
        root = os.path.join(self.cfg["project_path"], "labeled-data")
        for dir_ in os.listdir(root):
            folder = os.path.join(root, dir_)
            if os.path.isdir(folder) and not any(
                folder.endswith(s) for s in ("cropped", "labeled")
            ):
                self.df = pd.read_hdf(
                    os.path.join(folder, f'CollectedData_{self.cfg["scorer"]}.h5')
                )
                row, col = self.pick_labeled_frame()
                if "individuals" in self.df.columns.names:
                    self.df = self.df.xs(col, axis=1, level="individuals")
                self.xy = self.df.loc[row].values.reshape((-1, 2))
                missing = np.flatnonzero(np.isnan(self.xy).all(axis=1))
                if not missing.size:
                    found = True
                    break
        if self.df is None:
            raise IOError("No labeled data were found.")

        self.bpts = self.df.columns.get_level_values("bodyparts").unique()
        if not found:
            warnings.warn(
                f"A fully labeled animal could not be found. "
                f"{', '.join(self.bpts[missing])} will need to be manually connected in the config.yaml."
            )
        self.tree = KDTree(self.xy)
        # Handle image previously annotated on a different platform
        if isinstance(row, str):
            sep = "/" if "/" in row else "\\"
            row = row.split(sep)
        self.image = io.imread(os.path.join(self.cfg["project_path"], *row))
        self.inds = set()
        self.segs = set()
        # Draw the skeleton if already existent
        if self.cfg["skeleton"]:
            for bone in self.cfg["skeleton"]:
                pair = np.flatnonzero(self.bpts.isin(bone))
                if len(pair) != 2:
                    continue
                pair_sorted = tuple(sorted(pair))
                self.inds.add(pair_sorted)
                self.segs.add(tuple(map(tuple, self.xy[pair_sorted, :])))
        self.lines = LineCollection(
            self.segs, colors=mcolors.to_rgba(self.cfg["skeleton_color"])
        )
        self.lines.set_picker(True)
        self.show()

    def pick_labeled_frame(self):
        # Find the most 'complete' animal
        try:
            count = self.df.groupby(level="individuals", axis=1).count()
            if "single" in count:
                count.drop("single", axis=1, inplace=True)
        except KeyError:
            count = self.df.count(axis=1).to_frame()
        mask = count.where(count == count.values.max())
        kept = mask.stack().index.to_list()
        np.random.shuffle(kept)
        picked = kept.pop()
        row = picked[:-1]
        col = picked[-1]
        return row, col

    def show(self):
        self.fig = plt.figure()
        ax = self.fig.add_subplot(111)
        ax.axis("off")
        lo = np.nanmin(self.xy, axis=0)
        hi = np.nanmax(self.xy, axis=0)
        center = (hi + lo) / 2
        w, h = hi - lo
        ampl = 1.3
        w *= ampl
        h *= ampl
        ax.set_xlim(center[0] - w / 2, center[0] + w / 2)
        ax.set_ylim(center[1] - h / 2, center[1] + h / 2)
        ax.imshow(self.image)
        ax.scatter(*self.xy.T, s=self.cfg["dotsize"] ** 2)
        ax.add_collection(self.lines)
        ax.invert_yaxis()

        self.lasso = LassoSelector(ax, onselect=self.on_select)
        ax_clear = self.fig.add_axes([0.85, 0.55, 0.1, 0.1])
        ax_export = self.fig.add_axes([0.85, 0.45, 0.1, 0.1])
        self.clear_button = Button(ax_clear, "Clear")
        self.clear_button.on_clicked(self.clear)
        self.export_button = Button(ax_export, "Export")
        self.export_button.on_clicked(self.export)
        self.fig.canvas.mpl_connect("pick_event", self.on_pick)
        plt.show()

    def clear(self, *args):
        self.inds.clear()
        self.segs.clear()
        self.lines.set_segments(self.segs)

    def export(self, *args):
        inds_flat = set(ind for pair in self.inds for ind in pair)
        unconnected = [i for i in range(len(self.xy)) if i not in inds_flat]
        if len(unconnected):
            warnings.warn(
                f"You didn't connect all the bodyparts (which is fine!). This is just a note to let you know."
            )
        self.cfg["skeleton"] = [tuple(self.bpts[list(pair)]) for pair in self.inds]
        write_config(self.config_path, self.cfg)

    def on_pick(self, event):
        if event.mouseevent.button == 3:
            removed = event.artist.get_segments().pop(event.ind[0])
            self.segs.remove(tuple(map(tuple, removed)))
            self.inds.remove(tuple(self.tree.query(removed)[1]))

    def on_select(self, verts):
        self.path = Path(verts)
        self.verts = verts
        inds = self.tree.query_ball_point(verts, 5)
        inds_unique = []
        for lst in inds:
            if len(lst) and lst[0] not in inds_unique:
                inds_unique.append(lst[0])
        for pair in zip(inds_unique, inds_unique[1:]):
            pair_sorted = tuple(sorted(pair))
            self.inds.add(pair_sorted)
            self.segs.add(tuple(map(tuple, self.xy[pair_sorted, :])))
        self.lines.set_segments(self.segs)
        self.fig.canvas.draw_idle()


--- File: deeplabcut/utils/video_processor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Author: Hao Wu
hwu01@g.harvard.edu
You can find the directory for your ffmpeg bindings by: "find / | grep ffmpeg" and then setting it.

This is the helper class for video reading and saving in DeepLabCut.
Updated by AM

You can set various codecs below,
fourcc = cv2.VideoWriter_fourcc(*'MJPG')
i.e. 'XVID'
"""

import cv2
import numpy as np


class VideoProcessor(object):
    """
    Base class for a video processing unit, implementation is required for video loading and saving

    sh and sw are the output height and width respectively.
    """

    def __init__(
        self, fname="", sname="", nframes=-1, fps=None, codec="X264", sh="", sw=""
    ):
        self.fname = fname
        self.sname = sname
        self.nframes = nframes
        self.codec = codec
        self.h = 0
        self.w = 0
        self.nc = 3
        self.i = 0

        try:
            if self.fname != "":
                self.vid = self.get_video()
                self.get_info()
                self.sh = 0
                self.sw = 0
            if self.sname != "":
                if sh == "" and sw == "":
                    self.sh = self.h
                    self.sw = self.w
                else:
                    self.sw = sw
                    self.sh = sh
                self.svid = self.create_video()

        except Exception as ex:
            print("Error: %s", ex)

        if fps is not None:  # Overwrite the video's FPS
            self.FPS = fps

    def load_frame(self):
        frame = self._read_frame()
        if frame is not None:
            self.i += 1
        return frame

    def height(self):
        return self.h

    def width(self):
        return self.w

    def fps(self):
        return self.FPS

    def counter(self):
        return self.i

    def frame_count(self):
        return self.nframes

    def get_video(self):
        """
        implement your own
        """
        pass

    def get_info(self):
        """
        implement your own
        """
        pass

    def create_video(self):
        """
        implement your own
        """
        pass

    def _read_frame(self):
        """
        implement your own
        """
        pass

    def save_frame(self, frame):
        """
        implement your own
        """
        pass

    def close(self):
        """
        implement your own
        """
        pass


class VideoProcessorCV(VideoProcessor):
    """
    OpenCV implementation of VideoProcessor
    requires opencv-python==3.4.0.12
    """

    def __init__(self, *args, **kwargs):
        super(VideoProcessorCV, self).__init__(*args, **kwargs)

    def get_video(self):
        return cv2.VideoCapture(self.fname)

    def get_info(self):
        self.w = int(self.vid.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.h = int(self.vid.get(cv2.CAP_PROP_FRAME_HEIGHT))
        all_frames = int(self.vid.get(cv2.CAP_PROP_FRAME_COUNT))
        self.FPS = self.vid.get(cv2.CAP_PROP_FPS)
        self.nc = 3
        if self.nframes == -1 or self.nframes > all_frames:
            self.nframes = all_frames

    def create_video(self):
        fourcc = cv2.VideoWriter_fourcc(*self.codec)
        return cv2.VideoWriter(self.sname, fourcc, self.FPS, (self.sw, self.sh), True)

    def _read_frame(self):  # return RGB (rather than BGR)!
        # return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)
        success, frame = self.vid.read()
        if not success:
            return frame
        return np.flip(frame, 2)

    def save_frame(self, frame):
        if frame is not None:
            self.svid.write(np.flip(frame, 2))

    def close(self):
        if hasattr(self, "svid") and self.svid is not None:
            self.svid.release()
        if hasattr(self, "vid") and self.vid is not None:
            self.vid.release()


--- File: deeplabcut/utils/plotting.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut

Please see AUTHORS for contributors.
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""
from __future__ import annotations

import argparse
import os
import pickle
import pandas as pd

####################################################
# Dependencies
####################################################
import os.path
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np

from deeplabcut.core import crossvalutils
from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal, visualization


def Histogram(vector, color, bins, ax=None, linewidth=1.0):
    dvector = np.diff(vector)
    dvector = dvector[np.isfinite(dvector)]
    if ax is None:
        fig = plt.figure()
        ax = fig.add_subplot(111)
    ax.hist(dvector, color=color, histtype="step", bins=bins, linewidth=linewidth)


def PlottingResults(
    tmpfolder,
    Dataframe,
    cfg,
    bodyparts2plot,
    individuals2plot,
    showfigures=False,
    suffix=".png",
    resolution=100,
    linewidth=1.0,
):
    """Plots poses vs time; pose x vs pose y; histogram of differences and likelihoods."""
    pcutoff = cfg["pcutoff"]
    colors = visualization.get_cmap(len(bodyparts2plot), name=cfg["colormap"])
    alphavalue = cfg["alphavalue"]
    if individuals2plot:
        Dataframe = Dataframe.loc(axis=1)[:, individuals2plot]
    animal_bpts = Dataframe.columns.get_level_values("bodyparts")
    # Pose X vs pose Y
    fig1 = plt.figure(figsize=(8, 6))
    ax1 = fig1.add_subplot(111)
    ax1.set_xlabel("X position in pixels")
    ax1.set_ylabel("Y position in pixels")
    ax1.invert_yaxis()

    # Poses vs time
    fig2 = plt.figure(figsize=(10, 3))
    ax2 = fig2.add_subplot(111)
    ax2.set_xlabel("Frame Index")
    ax2.set_ylabel("X-(dashed) and Y- (solid) position in pixels")

    # Likelihoods
    fig3 = plt.figure(figsize=(10, 3))
    ax3 = fig3.add_subplot(111)
    ax3.set_xlabel("Frame Index")
    ax3.set_ylabel("Likelihood (use to set pcutoff)")

    # Histograms
    fig4 = plt.figure()
    ax4 = fig4.add_subplot(111)
    ax4.set_ylabel("Count")
    ax4.set_xlabel("DeltaX and DeltaY")
    bins = np.linspace(0, np.amax(Dataframe.max()), 100)

    with np.errstate(invalid="ignore"):
        for bpindex, bp in enumerate(bodyparts2plot):
            if (
                bp in animal_bpts
            ):  # Avoid 'unique' bodyparts only present in the 'single' animal
                prob = Dataframe.xs(
                    (bp, "likelihood"), level=(-2, -1), axis=1
                ).values.squeeze()
                mask = prob < pcutoff
                temp_x = np.ma.array(
                    Dataframe.xs((bp, "x"), level=(-2, -1), axis=1).values.squeeze(),
                    mask=mask,
                )
                temp_y = np.ma.array(
                    Dataframe.xs((bp, "y"), level=(-2, -1), axis=1).values.squeeze(),
                    mask=mask,
                )
                ax1.plot(temp_x, temp_y, ".", color=colors(bpindex), alpha=alphavalue)

                ax2.plot(
                    temp_x,
                    "--",
                    color=colors(bpindex),
                    linewidth=linewidth,
                    alpha=alphavalue,
                )
                ax2.plot(
                    temp_y,
                    "-",
                    color=colors(bpindex),
                    linewidth=linewidth,
                    alpha=alphavalue,
                )

                ax3.plot(
                    prob,
                    "-",
                    color=colors(bpindex),
                    linewidth=linewidth,
                    alpha=alphavalue,
                )

                Histogram(temp_x, colors(bpindex), bins, ax4, linewidth=linewidth)
                Histogram(temp_y, colors(bpindex), bins, ax4, linewidth=linewidth)

    sm = plt.cm.ScalarMappable(
        cmap=plt.get_cmap(cfg["colormap"]),
        norm=plt.Normalize(vmin=0, vmax=len(bodyparts2plot) - 1),
    )
    sm._A = []
    for ax in ax1, ax2, ax3, ax4:
        cbar = plt.colorbar(sm, ax=ax, ticks=range(len(bodyparts2plot)))
        cbar.set_ticklabels(bodyparts2plot)

    fig1.savefig(
        os.path.join(tmpfolder, "trajectory" + suffix),
        bbox_inches="tight",
        dpi=resolution,
    )
    fig2.savefig(
        os.path.join(tmpfolder, "plot" + suffix), bbox_inches="tight", dpi=resolution
    )
    fig3.savefig(
        os.path.join(tmpfolder, "plot-likelihood" + suffix),
        bbox_inches="tight",
        dpi=resolution,
    )
    fig4.savefig(
        os.path.join(tmpfolder, "hist" + suffix), bbox_inches="tight", dpi=resolution
    )

    if not showfigures:
        plt.close("all")
    else:
        plt.show()


##################################################
# Looping analysis over video
##################################################


def plot_trajectories(
    config,
    videos,
    videotype="",
    shuffle=1,
    trainingsetindex=0,
    filtered=False,
    displayedbodyparts="all",
    displayedindividuals="all",
    showfigures=False,
    destfolder=None,
    modelprefix="",
    imagetype=".png",
    resolution=100,
    linewidth=1.0,
    track_method="",
    pcutoff: float | None = None,
):
    """Plots the trajectories of various bodyparts across the video.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos: list[str]
        Full paths to videos for analysis or a path to the directory, where all the
        videos with same extension are stored.

    videotype: str, optional, default=""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions
        ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional, default=1
        Integer specifying the shuffle index of the training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    filtered: bool, optional, default=False
        Boolean variable indicating if filtered output should be plotted rather than
        frame-by-frame predictions. Filtered version can be calculated with
        ``deeplabcut.filterpredictions``.

    displayedbodyparts: list[str] or str, optional, default="all"
        This select the body parts that are plotted in the video.
        Either ``all``, then all body parts from config.yaml are used,
        or a list of strings that are a subset of the full list.
        E.g. ['hand','Joystick'] for the demo Reaching-Mackenzie-2018-08-30/config.yaml
        to select only these two body parts.

    showfigures: bool, optional, default=False
        If ``True`` then plots are also displayed.

    destfolder: string or None, optional, default=None
        Specifies the destination folder that was used for storing analysis data. If
        ``None``, the path of the video is used.

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    imagetype: string, optional, default=".png"
        Specifies the output image format - '.tif', '.jpg', '.svg' and ".png".

    resolution: int, optional, default=100
        Specifies the resolution (in dpi) of saved figures.
        Note higher resolution figures take longer to generate.

    linewidth: float, optional, default=1.0
        Specifies width of line for line and histogram plots.

    track_method: string, optional, default=""
         Specifies the tracker used to generate the data.
         Empty by default (corresponding to a single animal project).
         For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
         be taken from the config.yaml file if none is given.

    pcutoff: string, optional, default=None
        Overrides the pcutoff set in the project configuration to plot the trajectories.

    Returns
    -------
    None

    Examples
    --------

    To label the frames

    >>> deeplabcut.plot_trajectories(
            'home/alex/analysis/project/reaching-task/config.yaml',
            ['/home/alex/analysis/project/videos/reachingvideo1.avi'],
        )
    """
    cfg = auxiliaryfunctions.read_config(config)

    if pcutoff is None:
        pcutoff = cfg["pcutoff"]

    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)

    trainFraction = cfg["TrainingFraction"][trainingsetindex]
    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
        cfg, shuffle, trainFraction, modelprefix=modelprefix
    )  # automatically loads corresponding model (even training iteration based on snapshot index)
    bodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
        cfg, displayedbodyparts
    )
    individuals = auxfun_multianimal.IntersectionofIndividualsandOnesGivenbyUser(
        cfg, displayedindividuals
    )
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if not len(Videos):
        print(
            "No videos found. Make sure you passed a list of videos and that *videotype* is right."
        )
        return

    failures, multianimal_errors = [], []
    for video in Videos:
        if destfolder is None:
            videofolder = str(Path(video).parents[0])
        else:
            videofolder = destfolder

        vname = str(Path(video).stem)
        print("Loading ", video, "and data.")
        try:
            df, filepath, _, suffix = auxiliaryfunctions.load_analyzed_data(
                videofolder, vname, DLCscorer, filtered, track_method
            )
            tmpfolder = os.path.join(videofolder, "plot-poses", vname)
            _plot_trajectories(
                filepath,
                bodyparts,
                individuals,
                showfigures,
                resolution,
                linewidth,
                cfg["colormap"],
                cfg["alphavalue"],
                pcutoff,
                suffix,
                imagetype,
                tmpfolder,
            )
        except FileNotFoundError as e:
            print(e)
            failures.append(video)
            if track_method != "":
                # In a multi animal scenario, show more verbose errors.
                try:
                    _ = auxiliaryfunctions.load_detection_data(
                        video, DLCscorer, track_method
                    )
                    error_message = 'Call "deeplabcut.stitch_tracklets() prior to plotting the trajectories.'
                except FileNotFoundError as e:
                    print(e)
                    error_message = (
                        f"Make sure {video} was previously analyzed, and that "
                        "detections were successively converted to tracklets using "
                        '"deeplabcut.convert_detections2tracklets()" and "deeplabcut.stitch_tracklets()".'
                    )
                multianimal_errors.append(error_message)

    if len(failures) > 0:
        # Some vidoes were not evaluated.
        failed_videos = ",".join(failures)
        if len(multianimal_errors) > 0:
            verbose_error = ": " + " ".join(multianimal_errors)
        else:
            verbose_error = "."
        print(
            f"Plots could not be created for {failed_videos}. "
            f"Videos were not evaluated with the current scorer {DLCscorer}"
            + verbose_error
        )
    else:
        print(
            'Plots created! Please check the directory "plot-poses" within the video directory'
        )


def _plot_trajectories(
    h5file,
    bodyparts=None,
    individuals=None,
    show=False,
    resolution=100,
    linewidth=1.0,
    colormap="viridis",
    alpha=1.0,
    pcutoff=0.01,
    suffix="",
    image_type=".png",
    dest_folder=None,
):
    df = pd.read_hdf(h5file)
    if bodyparts is None:
        bodyparts = list(df.columns.get_level_values("bodyparts").unique())
    if individuals is None:
        try:
            individuals = set(df.columns.get_level_values("individuals"))
        except KeyError:
            individuals = [""]
    if dest_folder is None:
        vname = os.path.basename(h5file).split("DLC")[0]
        vid_folder = os.path.dirname(h5file)
        dest_folder = os.path.join(vid_folder, "plot-poses", vname)
    auxiliaryfunctions.attempt_to_make_folder(dest_folder, recursive=True)
    # Keep only the individuals and bodyparts that were labeled
    labeled_bpts = [
        bp
        for bp in df.columns.get_level_values("bodyparts").unique()
        if bp in bodyparts
    ]
    # Either display the animals defined in the config if they are found
    # in the dataframe, or all the trajectories regardless of their names
    try:
        animals = set(df.columns.get_level_values("individuals"))
    except KeyError:
        animals = {""}
    cfg = {
        "colormap": colormap,
        "alphavalue": alpha,
        "pcutoff": pcutoff,
    }
    for animal in animals.intersection(individuals) or animals:
        PlottingResults(
            dest_folder,
            df,
            cfg,
            labeled_bpts,
            animal,
            show,
            suffix + animal + image_type,
            resolution=resolution,
            linewidth=linewidth,
        )


def _plot_paf_performance(
    within,
    between,
    nbins=51,
    kde=True,
    colors=None,
    ax=None,
):
    import seaborn as sns

    bins = np.linspace(0, 1, nbins)
    if colors is None:
        colors = "#EFC9AF", "#1F8AC0"
    if ax is None:
        fig, ax = plt.subplots(tight_layout=True, figsize=(3, 3))
    sns.histplot(within, kde=kde, ax=ax, stat="probability", color=colors[0], bins=bins)
    sns.histplot(
        between, kde=kde, ax=ax, stat="probability", color=colors[1], bins=bins
    )
    return ax


def plot_edge_affinity_distributions(
    eval_pickle_file,
    include_bodyparts="all",
    output_name="",
    figsize=(10, 7),
):
    """
    Display the distribution of affinity costs of within- and between-animal edges.

    Parameters
    ----------
    eval_pickle_file : string
        Path to a *_full.pickle from the evaluation-results folder.

    include_bodyparts : list of strings, optional
        A list of body part names whose edges are to be shown.
        By default, all body parts and their corresponding edges are analyzed.
        We recommend only passing a subset of body parts for projects with large graphs.

    output_name: string, optional
        Path where the plot is saved. By default, it is stored as costdist.png.

    figsize: tuple
        Figure size in inches.

    """

    with open(eval_pickle_file, "rb") as file:
        data = pickle.load(file)
    meta_pickle_file = eval_pickle_file.replace("_full.", "_meta.")
    with open(meta_pickle_file, "rb") as file:
        metadata = pickle.load(file)
    (w_train, _), (b_train, _) = crossvalutils._calc_within_between_pafs(
        data,
        metadata,
        train_set_only=True,
    )
    data.pop("metadata", None)
    nonempty = set(i for i, vals in w_train.items() if vals)
    meta = metadata["data"]["DLC-model-config file"]
    bpts = list(map(str.lower, meta["all_joints_names"]))
    inds_multi = set(b for edge in meta["partaffinityfield_graph"] for b in edge)
    if include_bodyparts == "all":
        include_bodyparts = inds_multi
    else:
        include_bodyparts = set(bpts.index(bpt) for bpt in include_bodyparts)
    edges_to_keep = set()
    graph = meta["partaffinityfield_graph"]
    for n, edge in enumerate(graph):
        if not any(i in include_bodyparts for i in edge):
            continue
        edges_to_keep.add(n)
    edge_inds = edges_to_keep.intersection(nonempty)
    nrows = int(np.ceil(np.sqrt(len(edge_inds))))
    ncols = int(np.ceil(len(edge_inds) / nrows))
    fig, axes_ = plt.subplots(
        nrows,
        ncols,
        figsize=figsize,
        tight_layout=True,
        squeeze=False,
    )
    axes = axes_.flatten()
    for ax in axes:
        ax.axis("off")
    for n, ind in enumerate(edge_inds):
        i1, i2 = graph[ind]
        w_tr = w_train[ind]
        b_tr = b_train[ind]
        sep, _ = crossvalutils._calc_separability(b_tr, w_tr, metric="auc")
        axes[n].text(
            0.5,
            0.8,
            f"{bpts[i1]}–{bpts[i2]}\n{sep:.2f}",
            size=8,
            ha="center",
            transform=axes[n].transAxes,
        )
        _plot_paf_performance(w_tr, b_tr, ax=axes[n], kde=False)
    axes[0].set_xticks([])
    axes[0].set_yticks([])
    if not output_name:
        output_name = "costdist.jpg"
    fig.savefig(output_name, dpi=600)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config")
    parser.add_argument("video")
    cli_args = parser.parse_args()


--- File: deeplabcut/utils/conversioncode.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""
import os
import pandas as pd
from deeplabcut.utils import auxiliaryfunctions
from itertools import islice
from pathlib import Path


SUPPORTED_FILETYPES = "csv", "nwb"


def convertcsv2h5(config, userfeedback=True, scorer=None):
    """
    Convert (image) annotation files in folder labeled-data from csv to h5.
    This function allows the user to manually edit the csv (e.g. to correct the scorer name and then convert it into hdf format).
    WARNING: conversion might corrupt the data.

    config : string
        Full path of the config.yaml file as a string.

    userfeedback: bool, optional
        If true the user will be asked specifically for each folder in labeled-data if the containing csv shall be converted to hdf format.

    scorer: string, optional
        If a string is given, then the scorer/annotator in all csv and hdf files that are changed, will be overwritten with this name.

    Examples
    --------
    Convert csv annotation files for reaching-task project into hdf.
    >>> deeplabcut.convertcsv2h5('/analysis/project/reaching-task/config.yaml')

    --------
    Convert csv annotation files for reaching-task project into hdf while changing the scorer/annotator in all annotation files to Albert!
    >>> deeplabcut.convertcsv2h5('/analysis/project/reaching-task/config.yaml',scorer='Albert')
    --------
    """
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / "labeled-data" / Path(i) for i in video_names]
    if not scorer:
        scorer = cfg["scorer"]

    for folder in folders:
        try:
            if userfeedback:
                print("Do you want to convert the csv file in folder:", folder, "?")
                askuser = input("yes/no")
            else:
                askuser = "yes"

            if askuser in ("y", "yes", "Ja", "ha", "oui"):  # multilanguage support :)
                fn = os.path.join(
                    str(folder), "CollectedData_" + cfg["scorer"] + ".csv"
                )
                # Determine whether the data are single- or multi-animal without loading into memory
                # simply by checking whether 'individuals' is in the second line of the CSV.
                with open(fn) as datafile:
                    head = list(islice(datafile, 0, 5))
                if "individuals" in head[1]:
                    header = list(range(4))
                else:
                    header = list(range(3))
                if head[-1].split(",")[0] == "labeled-data":
                    index_col = [0, 1, 2]
                else:
                    index_col = 0
                data = pd.read_csv(fn, index_col=index_col, header=header)
                data.columns = data.columns.set_levels([scorer], level="scorer")
                guarantee_multiindex_rows(data)
                data.to_hdf(fn.replace(".csv", ".h5"), key="df_with_missing", mode="w")
                data.to_csv(fn)
        except FileNotFoundError:
            print("Attention:", folder, "does not appear to have labeled data!")


def adapt_labeled_data_to_new_project(
    config_path, remove_old_bodyparts=False, other_scorer=False, userfeedback=False
):
    """Given the config.yaml file, this function will convert the labels of an ancient project to a new project.
        For this, the labeled data must be in the project folder, under the labeled-data folder and with the same configuration as all deeplabcut projects.

    Parameters
    ----------
    config_path : str
        The path to the config.yaml file.
    remove_old_bodyparts : bool (default = False)
        If True, the old bodyparts that are not in the new project will be removed from the dataframe.
    other_scorer : bool (default = False)
        If True, the labels will be converted to the new scorer.
    userfeedback : bool (default = True)
        If true the user will be asked specifically for each folder in labeled-data if the containing csv shall be converted to hdf format.
    """

    # Load the config file
    cfg = dlc.auxiliaryfunctions.read_config(config_path)

    # Get the Project path
    project_path = cfg["project_path"]

    # Get the bodyparts
    bodyparts = cfg["multianimalbodyparts"]
    print("New Bodyparts:", bodyparts)

    # Iterate over each labeled data video

    # Use tqdm for a progress bar
    for video in tqdm.tqdm(cfg["video_sets"]):
        print("Video:", video)

        video_name = video.split("\\")[-1]
        # discard the file extension
        video_name = video_name.split(".")[0]
        # Load the csv file
        label_path = os.path.join(project_path, "labeled-data", video_name)
        csv_files = [file for file in os.listdir(label_path) if file.endswith(".csv")]
        if not csv_files:
            print("No csv file in the folder:", label_path)
        else:
            csv_path = os.path.join(label_path, csv_files[0])
            df = pd.read_csv(csv_path, header=None)

            # get the scorer
            if other_scorer:
                scorer = cfg["scorer"]
                # Change the scorer in the dataframe
                df.iloc[0, 3:] = pd.Series([scorer] * len(df.columns[3:]))

            else:
                scorer = df.iloc[0, 3]

            # Get the individuals
            individuals = np.unique(df.iloc[1, 3:])

            # Get the old bodyparts
            old_bodyparts = np.unique(df.iloc[2, 3:])
            print("Old bodyparts:", old_bodyparts)

            # Get the unmber of old bodyparts
            num_of_old_bodyparts = len(old_bodyparts)

            # Bodyparts to add
            print("Bodyparts to add:", set(bodyparts) - set(old_bodyparts))

            # If a bodypart is missing, add it to the dataframe
            for index, bodypart in enumerate(bodyparts):
                if bodypart not in old_bodyparts:
                    num_of_old_bodyparts += 1
                    for i, individual in enumerate(individuals):
                        # create the columns for the bodypart, concatenate, the individual, the bodypart, and nan values
                        x_column = pd.concat(
                            [
                                pd.Series(scorer),
                                pd.Series(individual),
                                pd.Series(bodypart),
                                pd.Series("x"),
                                pd.Series(np.nan, index=df.index),
                            ],
                            axis=0,
                            ignore_index=True,
                        )
                        y_column = pd.concat(
                            [
                                pd.Series(scorer),
                                pd.Series(individual),
                                pd.Series(bodypart),
                                pd.Series("y"),
                                pd.Series(np.nan, index=df.index),
                            ],
                            axis=0,
                            ignore_index=True,
                        )
                        # Insert the columns in the dataframe
                        df.insert(
                            i * 2 * num_of_old_bodyparts + index * 2 + 3,
                            "insert_" + bodypart + "_x" + individual,
                            x_column,
                        )
                        df.insert(
                            i * 2 * num_of_old_bodyparts + index * 2 + 4,
                            "insert" + bodypart + "_y" + individual,
                            y_column,
                        )

            # If the old bodyparts are not in the new project, remove them
            if remove_old_bodyparts:
                for bodypart in old_bodyparts:
                    if bodypart not in bodyparts:
                        df = df.drop(df.columns[df.iloc[2, :] == bodypart], axis=1)

            # Save the dataframe
            df.to_csv(csv_path, index=False, header=False)

    # Create/Update the h5 file
    convertcsv2h5(config_path, userfeedback=userfeedback)


def analyze_videos_converth5_to_csv(video_folder, videotype=".mp4", listofvideos=False):
    """
    By default the output poses (when running analyze_videos) are stored as MultiIndex Pandas Array, which contains the name of the network, body part name, (x, y) label position \n
    in pixels, and the likelihood for each frame per body part. These arrays are stored in an efficient Hierarchical Data Format (HDF) \n
    in the same directory, where the video is stored. This functions converts hdf (h5) files to the comma-separated values format (.csv),
    which in turn can be imported in many programs, such as MATLAB, R, Prism, etc.

    Parameters
    ----------

    video_folder : string
        Absolute path of a folder containing videos and the corresponding h5 data files.

    videotype: string, optional (default=.mp4)
        Only videos with this extension are screened.

    Examples
    --------

    Converts all pose-output files belonging to mp4 videos in the folder '/media/alex/experimentaldata/cheetahvideos' to csv files.
    deeplabcut.analyze_videos_converth5_to_csv('/media/alex/experimentaldata/cheetahvideos','.mp4')

    """

    if listofvideos:  # can also be called with a list of videos (from GUI)
        videos = video_folder  # GUI gives a list of videos
        if len(videos) > 0:
            h5_files = list(
                auxiliaryfunctions.grab_files_in_folder(
                    Path(videos[0]).parent, "h5", relative=False
                )
            )
        else:
            h5_files = []
    else:
        h5_files = list(
            auxiliaryfunctions.grab_files_in_folder(video_folder, "h5", relative=False)
        )
        videos = auxiliaryfunctions.grab_files_in_folder(
            video_folder, videotype, relative=False
        )

    _convert_h5_files_to("csv", None, h5_files, videos)


def analyze_videos_converth5_to_nwb(
    config,
    video_folder,
    videotype=".mp4",
    listofvideos=False,
):
    """
    Convert all h5 output data files in `video_folder` to NWB format.

    Parameters
    ----------
    config : string
        Absolute path to the project YAML config file.

    video_folder : string
        Absolute path of a folder containing videos and the corresponding h5 data files.

    videotype: string, optional (default=.mp4)
        Only videos with this extension are screened.

    Examples
    --------

    Converts all pose-output files belonging to mp4 videos in the folder '/media/alex/experimentaldata/cheetahvideos' to csv files.
    deeplabcut.analyze_videos_converth5_to_csv('/media/alex/experimentaldata/cheetahvideos','.mp4')

    """
    if listofvideos:  # can also be called with a list of videos (from GUI)
        videos = video_folder  # GUI gives a list of videos
        if len(videos) > 0:
            h5_files = list(
                auxiliaryfunctions.grab_files_in_folder(
                    Path(videos[0]).parent, "h5", relative=False
                )
            )
        else:
            h5_files = []
    else:
        h5_files = list(
            auxiliaryfunctions.grab_files_in_folder(video_folder, "h5", relative=False)
        )
        videos = auxiliaryfunctions.grab_files_in_folder(
            video_folder, videotype, relative=False
        )

    _convert_h5_files_to("nwb", config, h5_files, videos)


def _convert_h5_files_to(filetype, config, h5_files, videos):
    filetype = filetype.lower()
    if filetype not in SUPPORTED_FILETYPES:
        raise ValueError(
            f"""Unsupported destination format {filetype}.
            Must be one of {SUPPORTED_FILETYPES}."""
        )

    if filetype == "nwb":
        try:
            from dlc2nwb.utils import convert_h5_to_nwb
        except ImportError:
            raise ImportError(
                "The package `dlc2nwb` is missing. Please run `pip install dlc2nwb`."
            )

    for video in videos:
        if "_labeled" in video:
            continue
        vname = Path(video).stem
        for file in h5_files:
            if vname in file:
                scorer = file.split(vname)[1].split(".h5")[0]
                if "DLC" in scorer or "DeepCut" in scorer:
                    print("Found output file for scorer:", scorer)
                    print(f"Converting {file}...")
                    if filetype == "csv":
                        df = pd.read_hdf(file)
                        df.to_csv(file.replace(".h5", ".csv"))
                    else:
                        convert_h5_to_nwb(config, file)

    print(f"All H5 files were converted to {filetype.upper()}.")


def merge_windowsannotationdataONlinuxsystem(cfg):
    """If a project was created on Windows (and labeled there,) but ran on unix then the data folders
    corresponding in the keys in cfg['video_sets'] are not found. This function gets them directly by
    looping over all folders in labeled-data"""

    AnnotationData = []
    data_path = Path(cfg["project_path"], "labeled-data")
    annotationfolders = []
    for elem in auxiliaryfunctions.grab_files_in_folder(data_path, relative=False):
        if os.path.isdir(elem):
            annotationfolders.append(elem)
    print("The following folders were found:", annotationfolders)
    for folder in annotationfolders:
        filename = os.path.join(folder, "CollectedData_" + cfg["scorer"] + ".h5")
        try:
            data = pd.read_hdf(filename)
            guarantee_multiindex_rows(data)
            AnnotationData.append(data)
        except FileNotFoundError:
            print(filename, " not found (perhaps not annotated)")

    return AnnotationData


def guarantee_multiindex_rows(df):
    # Make paths platform-agnostic if they are not already
    if not isinstance(df.index, pd.MultiIndex):  # Backwards compatibility
        path = df.index[0]
        try:
            sep = "/" if "/" in path else "\\"
            splits = tuple(df.index.str.split(sep))
            df.index = pd.MultiIndex.from_tuples(splits)
        except TypeError:  #  Ignore numerical index of frame indices
            pass

    # Ensure folder names are strings
    try:
        df.index = df.index.set_levels(df.index.levels[1].astype(str), level=1)
    except AttributeError:
        pass


def robust_split_path(s):
    sep = "/" if "/" in s else "\\"
    return tuple(s.split(sep))


--- File: deeplabcut/utils/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.utils.auxfun_multianimal import *
from deeplabcut.utils.auxfun_videos import *
from deeplabcut.utils.auxiliaryfunctions import *
from deeplabcut.utils.conversioncode import *
from deeplabcut.utils.frameselectiontools import *
from deeplabcut.utils.make_labeled_video import *
from deeplabcut.utils.plotting import *
from deeplabcut.utils.video_processor import *


--- File: deeplabcut/utils/visualization.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""
from __future__ import annotations

import os
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib.collections import LineCollection
from matplotlib.colors import Colormap
import matplotlib.patches as patches
from skimage import io, color
from tqdm import trange

from deeplabcut.utils import auxiliaryfunctions, auxfun_videos


def get_cmap(n: int, name: str = "hsv") -> Colormap:
    """
    Args:
        n: number of distinct colors
        name: name of matplotlib colormap

    Returns:
         A function that maps each index in 0, 1, ..., n-1 to a distinct
         RGB color; the keyword argument name must be a standard mpl colormap name.
    """
    return plt.cm.get_cmap(name, n)


def make_labeled_image(
    frame,
    DataCombined,
    imagenr,
    pcutoff,
    Scorers,
    bodyparts,
    colors,
    cfg,
    labels=["+", ".", "x"],
    scaling=1,
    ax=None,
):
    """Creating a labeled image with the original human labels, as well as the DeepLabCut's!"""

    alphavalue = cfg["alphavalue"]  # .5
    dotsize = cfg["dotsize"]  # =15

    if ax is None:
        if np.ndim(frame) > 2:  # color image!
            h, w, numcolors = np.shape(frame)
        else:
            h, w = np.shape(frame)
        _, ax = prepare_figure_axes(w, h, scaling)
    ax.imshow(frame, "gray")
    for scorerindex, loopscorer in enumerate(Scorers):
        for bpindex, bp in enumerate(bodyparts):
            if np.isfinite(
                DataCombined[loopscorer][bp]["y"][imagenr]
                + DataCombined[loopscorer][bp]["x"][imagenr]
            ):
                y, x = (
                    int(DataCombined[loopscorer][bp]["y"][imagenr]),
                    int(DataCombined[loopscorer][bp]["x"][imagenr]),
                )
                if cfg["scorer"] not in loopscorer:
                    p = DataCombined[loopscorer][bp]["likelihood"][imagenr]
                    if p > pcutoff:
                        ax.plot(
                            x,
                            y,
                            labels[1],
                            ms=dotsize,
                            alpha=alphavalue,
                            color=colors(int(bpindex)),
                        )
                    else:
                        ax.plot(
                            x,
                            y,
                            labels[2],
                            ms=dotsize,
                            alpha=alphavalue,
                            color=colors(int(bpindex)),
                        )
                else:  # this is the human labeler
                    ax.plot(
                        x,
                        y,
                        labels[0],
                        ms=dotsize,
                        alpha=alphavalue,
                        color=colors(int(bpindex)),
                    )
    return ax


def make_multianimal_labeled_image(
    frame: np.ndarray,
    coords_truth: np.ndarray | list,
    coords_pred: np.ndarray | list,
    probs_pred: np.ndarray | list,
    colors: Colormap,
    dotsize: float | int = 12,
    alphavalue: float = 0.7,
    pcutoff: float = 0.6,
    labels: list = ["+", ".", "x"],
    ax: plt.Axes | None = None,
    bounding_boxes: tuple[np.ndarray, np.ndarray] | None = None,
    bboxes_cutoff: float = 0.6,
    bboxes_color: Colormap | str | None = None,
) -> plt.Axes:
    """
    Plots groundtruth labels and predictions onto the matplotlib's axes, with the specified graphical parameters.

    Args:
        frame: image
        coords_truth: groundtruth labels
        coords_pred: predictions
        probs_pred: prediction probabilities
        colors: colors for poses
        dotsize: size of dot
        alphavalue: transparency for the keypoints
        pcutoff: cut-off confidence value
        labels: labels to use for ground truth, reliable predictions, and not reliable predictions (confidence below cut-off value)
        ax: matplotlib plot's axes object
        bounding_boxes: bounding boxes (top-left corner, size) and their respective confidence levels,
        bboxes_cutoff: bounding boxes confidence cutoff threshold.
        bboxes_color: color(s) for the bounding boxes.
            If Colormap is passed -> each bounding box will be colored into its own color from the colormap.
            If string is passed -> all bboxes will be of string's defined color.
            If None -> all bboxes will be colored into a default color.

    Returns:
        matplotlib Axes object with plotted labels and predictions.
    """

    if ax is None:
        h, w, _ = np.shape(frame)
        _, ax = prepare_figure_axes(w, h)
    ax.imshow(frame, "gray")

    if bounding_boxes is not None:
        for i, (bbox, bbox_score) in enumerate(
            zip(bounding_boxes[0], bounding_boxes[1])
        ):
            bbox_origin = (bbox[0], bbox[1])
            (bbox_width, bbox_height) = (bbox[2], bbox[3])
            if isinstance(bboxes_color, Colormap):
                bbox_color = bboxes_color(i)
            elif bboxes_color is None:
                bbox_color = "red"
            else:
                bbox_color = bboxes_color
            rectangle = patches.Rectangle(
                bbox_origin,
                bbox_width,
                bbox_height,
                linewidth=1,
                edgecolor=bbox_color,
                facecolor="none",
                linestyle="--" if bbox_score < bboxes_cutoff else "-",
            )
            ax.add_patch(rectangle)

    for n, data in enumerate(zip(coords_truth, coords_pred, probs_pred)):
        color = colors(n)
        coord_gt, coord_pred, prob_pred = data

        ax.plot(*coord_gt.T, labels[0], ms=dotsize, alpha=alphavalue, color=color)
        if not coord_pred.shape[0]:
            continue

        reliable = np.repeat(prob_pred >= pcutoff, coord_pred.shape[1], axis=1)
        ax.plot(
            *coord_pred[reliable[:, 0]].T,
            labels[1],
            ms=dotsize,
            alpha=alphavalue,
            color=color,
        )
        if not np.all(reliable):
            ax.plot(
                *coord_pred[~reliable[:, 0]].T,
                labels[2],
                ms=dotsize,
                alpha=alphavalue,
                color=color,
            )
    return ax


def plot_and_save_labeled_frame(
    DataCombined,
    ind,
    trainIndices,
    cfg,
    colors,
    comparisonbodyparts,
    DLCscorer,
    foldername,
    fig,
    ax,
    scaling=1,
):
    if isinstance(DataCombined.index[ind], tuple):
        image_path = os.path.join(cfg["project_path"], *DataCombined.index[ind])
    else:
        image_path = os.path.join(cfg["project_path"], DataCombined.index[ind])
    frame = io.imread(image_path)
    if np.ndim(frame) > 2:  # color image!
        h, w, numcolors = np.shape(frame)
    else:
        h, w = np.shape(frame)
    fig.set_size_inches(w / 100, h / 100)
    ax.set_xlim(0, w)
    ax.set_ylim(0, h)
    ax.invert_yaxis()
    ax = make_labeled_image(
        frame,
        DataCombined,
        ind,
        cfg["pcutoff"],
        [cfg["scorer"], DLCscorer],
        comparisonbodyparts,
        colors,
        cfg,
        scaling=scaling,
        ax=ax,
    )
    save_labeled_frame(fig, image_path, foldername, ind in trainIndices)
    return ax


def save_labeled_frame(fig, image_path, dest_folder, belongs_to_train):
    path = Path(image_path)
    imagename = path.parts[-1]
    imfoldername = path.parts[-2]
    if belongs_to_train:
        dest = "-".join(("Training", imfoldername, imagename))
    else:
        dest = "-".join(("Test", imfoldername, imagename))
    full_path = os.path.join(dest_folder, dest)

    # Windows throws error if file path is > 260 characters, can fix with prefix.
    # See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation
    if len(full_path) >= 260 and os.name == "nt":
        full_path = "\\\\?\\" + full_path
    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
    fig.savefig(full_path)


def create_minimal_figure(dpi=100):
    fig, ax = plt.subplots(frameon=False, dpi=dpi)
    ax.axis("off")
    ax.invert_yaxis()
    return fig, ax


def erase_artists(ax):
    for artist in ax.lines + ax.collections + ax.artists + ax.patches + ax.images:
        artist.remove()
    ax.figure.canvas.draw_idle()


def prepare_figure_axes(width, height, scale=1.0, dpi=100):
    fig = plt.figure(
        frameon=False, figsize=(width * scale / dpi, height * scale / dpi), dpi=dpi
    )
    ax = fig.add_subplot(111)
    ax.axis("off")
    ax.set_xlim(0, width)
    ax.set_ylim(0, height)
    ax.invert_yaxis()
    return fig, ax


def make_labeled_images_from_dataframe(
    df,
    cfg,
    destfolder="",
    scale=1.0,
    dpi=100,
    keypoint="+",
    draw_skeleton=True,
    color_by="bodypart",
):
    """
    Write labeled frames to disk from a DataFrame.
    Parameters
    ----------
    df : pd.DataFrame
        DataFrame containing the labeled data. Typically, the DataFrame is obtained
        through pandas.read_csv() or pandas.read_hdf().
    cfg : dict
        Project configuration.
    destfolder : string, optional
        Destination folder into which images will be stored. By default, same location as the labeled data.
        Note that the folder will be created if it does not exist.
    scale : float, optional
        Up/downscale the output dimensions.
        By default, outputs are of the same dimensions as the original images.
    dpi : int, optional
        Output resolution. 100 dpi by default.
    keypoint : str, optional
        Keypoint appearance. By default, keypoints are marked by a + sign.
        Refer to https://matplotlib.org/3.2.1/api/markers_api.html for a list of all possible options.
    draw_skeleton : bool, optional
        Whether to draw the animal skeleton as defined in *cfg*. True by default.
    color_by : str, optional
        Color scheme of the keypoints. Must be either 'bodypart' or 'individual'.
        By default, keypoints are colored relative to the bodypart they represent.
    """

    bodyparts = df.columns.get_level_values("bodyparts")
    bodypart_names = bodyparts.unique()
    nbodyparts = len(bodypart_names)
    bodyparts = bodyparts[::2]
    draw_skeleton = (
        draw_skeleton and cfg["skeleton"]
    )  # Only draw if a skeleton is defined

    if color_by == "bodypart":
        map_ = bodyparts.map(dict(zip(bodypart_names, range(nbodyparts))))
        cmap = get_cmap(nbodyparts, cfg["colormap"])
        colors = cmap(map_)
    elif color_by == "individual":
        try:
            individuals = df.columns.get_level_values("individuals")
            individual_names = individuals.unique().to_list()
            nindividuals = len(individual_names)
            individuals = individuals[::2]
            map_ = individuals.map(dict(zip(individual_names, range(nindividuals))))
            cmap = get_cmap(nindividuals, cfg["colormap"])
            colors = cmap(map_)
        except KeyError as e:
            raise Exception(
                "Coloring by individuals is only valid for multi-animal data"
            ) from e
    else:
        raise ValueError("`color_by` must be either `bodypart` or `individual`.")

    bones = []
    if draw_skeleton:
        for bp1, bp2 in cfg["skeleton"]:
            match1, match2 = [], []
            for j, bp in enumerate(bodyparts):
                if bp == bp1:
                    match1.append(j)
                elif bp == bp2:
                    match2.append(j)
            bones.extend(zip(match1, match2))
    ind_bones = tuple(zip(*bones))

    images_list = [
        os.path.join(cfg["project_path"], *tuple_) for tuple_ in df.index.tolist()
    ]
    if not destfolder:
        destfolder = os.path.dirname(images_list[0])
    tmpfolder = destfolder + "_labeled"
    auxiliaryfunctions.attempt_to_make_folder(tmpfolder)
    ic = io.imread_collection(images_list)

    h, w = ic[0].shape[:2]
    all_same_shape = True
    for array in ic[1:]:
        if array.shape[:2] != (h, w):
            all_same_shape = False
            break

    xy = df.values.reshape((df.shape[0], -1, 2))
    segs = xy[:, ind_bones].swapaxes(1, 2)

    s = cfg["dotsize"]
    alpha = cfg["alphavalue"]
    if all_same_shape:  # Very efficient, avoid re-drawing the whole plot
        fig, ax = prepare_figure_axes(w, h, scale, dpi)
        im = ax.imshow(np.zeros((h, w)), "gray")
        pts = [ax.plot([], [], keypoint, ms=s, alpha=alpha, color=c)[0] for c in colors]
        coll = LineCollection([], colors=cfg["skeleton_color"], alpha=alpha)
        ax.add_collection(coll)
        for i in trange(len(ic)):
            filename = ic.files[i]
            ind = images_list.index(filename)
            coords = xy[ind]
            img = ic[i]
            if img.ndim == 2 or img.shape[-1] == 1:
                img = color.gray2rgb(ic[i])
            im.set_data(img)
            for pt, coord in zip(pts, coords):
                pt.set_data(*np.expand_dims(coord, axis=1))
            if ind_bones:
                coll.set_segments(segs[ind])
            imagename = os.path.basename(filename)
            fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            fig.savefig(
                os.path.join(tmpfolder, imagename.replace(".png", f"_{color_by}.png")),
                dpi=dpi,
            )
        plt.close(fig)

    else:  # Good old inelegant way
        for i in trange(len(ic)):
            filename = ic.files[i]
            ind = images_list.index(filename)
            coords = xy[ind]
            image = ic[i]
            h, w = image.shape[:2]
            fig, ax = prepare_figure_axes(w, h, scale, dpi)
            ax.imshow(image)
            for coord, c in zip(coords, colors):
                ax.plot(*coord, keypoint, ms=s, alpha=alpha, color=c)
            if ind_bones:
                coll = LineCollection(
                    segs[ind], colors=cfg["skeleton_color"], alpha=alpha
                )
                ax.add_collection(coll)
            imagename = os.path.basename(filename)
            fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            fig.savefig(
                os.path.join(tmpfolder, imagename.replace(".png", f"_{color_by}.png")),
                dpi=dpi,
            )
            plt.close(fig)


def plot_evaluation_results(
    df_combined: pd.DataFrame,
    project_root: str,
    scorer: str,
    model_name: str,
    output_folder: str,
    in_train_set: bool,
    plot_unique_bodyparts: bool = False,
    mode: str = "bodypart",
    colormap: str = "rainbow",
    dot_size: int = 12,
    alpha_value: float = 0.7,
    p_cutoff: float = 0.6,
    bounding_boxes: dict | None = None,
    bboxes_cutoff: float = 0.6,
    bounding_boxes_color: str = "auto",
) -> None:
    """
    Creates labeled images using the results of inference, and saves them to an output
    folder.

    Args:
        df_combined: dataframe with multiindex rows ("labeled-data", video_name,
            image_name) and columns ("scorer", "individuals", "bodyparts", "coords").
            There should be two scorers: scorer (for ground truth data) and model_name
            (for prediction data)
        project_root: the project root path
        scorer: the name of the scorer for ground truth data in df_combined
        model_name: the name of the model for predictions in df_combined
        output_folder: the name of the folder where images should be saved
        in_train_set: whether df_combined is for train set images
        plot_unique_bodyparts: whether we should plot unique bodyparts
        mode: one of {"bodypart", "individual"}. Determines the keypoint color grouping
        colormap: the colormap to use for keypoints
        dot_size: the dot size to use for keypoints
        alpha_value: the alpha value to use for keypoints
        p_cutoff: the p-cutoff for "confident" keypoints
        bounding_boxes: dictionary with df_combined rows as keys and bounding boxes
            (np array for coordinates and np array for confidence).
            None corresponds to no bounding boxes.
        bboxes_cutoff: bounding boxes confidence cutoff threshold.
        bounding_boxes_color: If plotting bounding boxes, this is the color that will be used for bounding boxes.
            If set to "auto" (default value):
                - if mode is "bodypart", the bbox color will be a default color
                - if mode is "individual", each individual's color will be used for its bounding box

    """
    if bounding_boxes is None:
        bounding_boxes = {}

    for row_index, row in df_combined.iterrows():
        if isinstance(row_index, str):
            image_rel_path = Path(row_index)
            data_folder = image_rel_path.parent.parent.name
            video = image_rel_path.parent.name
            image = image_rel_path.name
        else:
            data_folder, video, image = row_index

        image_path = Path(project_root) / data_folder / video / image
        frame = auxfun_videos.imread(str(image_path), mode="skimage")

        row_multi = row.loc[
            (slice(None), row.index.get_level_values("individuals") != "single")
        ]
        individuals = len(row_multi.index.get_level_values("individuals").unique())
        bodyparts = len(row_multi.index.get_level_values("bodyparts").unique())
        df_gt = row_multi[scorer]
        df_predictions = row_multi[model_name]

        # Shape (num_individuals, num_bodyparts, xy)
        ground_truth = df_gt.to_numpy().reshape((individuals, bodyparts, 2))
        predictions = df_predictions.to_numpy().reshape((individuals, bodyparts, 3))

        bboxes = bounding_boxes.get(row_index)

        if plot_unique_bodyparts:
            row_unique = row.loc[
                (slice(None), row.index.get_level_values("individuals") == "single")
            ]
            unique_individuals = 1
            unique_bodyparts = len(
                row_unique.index.get_level_values("bodyparts").unique()
            )
            unique_ground_truth = (
                row_unique[scorer]
                .to_numpy()
                .reshape((unique_individuals, unique_bodyparts, 2))
            )
            unique_predictions = (
                row_unique[model_name]
                .to_numpy()
                .reshape((unique_individuals, unique_bodyparts, 3))
            )

        fig, ax = create_minimal_figure()
        h, w, _ = np.shape(frame)
        fig.set_size_inches(w / 100, h / 100)
        ax.set_xlim(0, w)
        ax.set_ylim(0, h)
        ax.invert_yaxis()

        if mode == "bodypart":
            num_colors = bodyparts
            if plot_unique_bodyparts:
                num_colors += unique_bodyparts

            colors = get_cmap(num_colors, name=colormap)
            predictions = predictions.swapaxes(0, 1)
            ground_truth = ground_truth.swapaxes(0, 1)
        elif mode == "individual":
            colors = get_cmap(individuals + 1, name=colormap)
        else:
            colors = []

        if bounding_boxes_color == "auto":
            if mode == "bodypart":
                bboxes_color = None
            elif mode == "individual":
                bboxes_color = get_cmap(individuals + 1, name=colormap)
            else:
                raise ValueError(f"Invalid mode: {mode}")
        else:
            bboxes_color = bounding_boxes_color

        ax = make_multianimal_labeled_image(
            frame=frame,
            coords_truth=ground_truth,
            coords_pred=predictions[:, :, :2],
            probs_pred=predictions[:, :, 2:],
            colors=colors,
            dotsize=dot_size,
            alphavalue=alpha_value,
            pcutoff=p_cutoff,
            ax=ax,
            bounding_boxes=bboxes,
            bboxes_cutoff=bboxes_cutoff,
            bboxes_color=bboxes_color,
        )
        if plot_unique_bodyparts:
            unique_predictions = unique_predictions.swapaxes(0, 1)
            unique_ground_truth = unique_ground_truth.swapaxes(0, 1)
            ax = make_multianimal_labeled_image(
                frame=frame,
                coords_truth=unique_ground_truth,
                coords_pred=unique_predictions[:, :, :2],
                probs_pred=unique_predictions[:, :, 2:],
                colors=colors,
                dotsize=dot_size,
                alphavalue=alpha_value,
                pcutoff=p_cutoff,
                ax=ax,
            )

        save_labeled_frame(
            fig,
            str(image_path),
            output_folder,
            belongs_to_train=in_train_set,
        )
        erase_artists(ax)
        plt.close()


--- File: deeplabcut/utils/make_labeled_video.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0

Hao Wu, hwu01@g.harvard.edu contributed the original OpenCV class. Thanks!
You can find the directory for your ffmpeg bindings by: "find / | grep ffmpeg" and then setting it.
"""
from __future__ import annotations

import argparse
import os

####################################################
# Dependencies
####################################################
import os.path
from functools import partial
from multiprocessing import get_start_method, Pool
from pathlib import Path
from typing import Callable, Iterable, List, Optional, Union

import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import patches
from matplotlib.animation import FFMpegWriter
from matplotlib.collections import LineCollection
from skimage.draw import disk, line_aa, set_color, rectangle_perimeter
from skimage.util import img_as_ubyte
from tqdm import trange

from deeplabcut.core.engine import Engine
from deeplabcut.utils import auxfun_multianimal, auxiliaryfunctions, visualization
from deeplabcut.utils.auxfun_videos import VideoWriter
from deeplabcut.utils.video_processor import (
    VideoProcessorCV as vp,
)  # used to CreateVideo


def get_segment_indices(bodyparts2connect, all_bpts):
    bpts2connect = []
    for bpt1, bpt2 in bodyparts2connect:
        if bpt1 in all_bpts and bpt2 in all_bpts:
            bpts2connect.extend(
                zip(
                    *(
                        np.flatnonzero(all_bpts == bpt1),
                        np.flatnonzero(all_bpts == bpt2),
                    )
                )
            )
    return bpts2connect


def CreateVideo(
    clip,
    Dataframe,
    pcutoff,
    dotsize,
    colormap,
    bodyparts2plot,
    trailpoints,
    cropping,
    x1,
    x2,
    y1,
    y2,
    bodyparts2connect,
    skeleton_color,
    draw_skeleton,
    displaycropped,
    color_by,
    confidence_to_alpha=None,
    plot_bboxes=True,
    bboxes_list=None,
    bboxes_pcutoff=0.6,
    bboxes_color: tuple | None = None,
):
    """Creating individual frames with labeled body parts and making a video"""
    bpts = Dataframe.columns.get_level_values("bodyparts")
    all_bpts = bpts.values[::3]
    if draw_skeleton:
        color_for_skeleton = (
            np.array(mcolors.to_rgba(skeleton_color))[:3] * 255
        ).astype(np.uint8)
        # recode the bodyparts2connect into indices for df_x and df_y for speed
        bpts2connect = get_segment_indices(bodyparts2connect, all_bpts)

    if displaycropped:
        ny, nx = y2 - y1, x2 - x1
    else:
        ny, nx = clip.height(), clip.width()

    fps = clip.fps()
    if isinstance(fps, float):
        if fps * 1000 > 65535:
            fps = round(fps)
    nframes = clip.nframes
    duration = nframes / fps

    print(
        "Duration of video [s]: {}, recorded with {} fps!".format(
            round(duration, 2), round(fps, 2)
        )
    )
    print(
        "Overall # of frames: {} with cropped frame dimensions: {} {}".format(
            nframes, nx, ny
        )
    )
    print("Generating frames and creating video.")

    df_x, df_y, df_likelihood = Dataframe.values.reshape((len(Dataframe), -1, 3)).T

    if cropping and not displaycropped:
        df_x += x1
        df_y += y1
    colorclass = plt.cm.ScalarMappable(cmap=colormap)

    bplist = bpts.unique().to_list()
    nbodyparts = len(bplist)
    if Dataframe.columns.nlevels == 3:
        nindividuals = int(len(all_bpts) / len(set(all_bpts)))
        map2bp = list(np.repeat(list(range(len(set(all_bpts)))), nindividuals))
        map2id = list(range(nindividuals)) * len(set(all_bpts))
    else:
        nindividuals = len(Dataframe.columns.get_level_values("individuals").unique())
        map2bp = [bplist.index(bp) for bp in all_bpts]
        nbpts_per_ind = (
            Dataframe.groupby(level="individuals", axis=1).size().values // 3
        )
        map2id = []
        for i, j in enumerate(nbpts_per_ind):
            map2id.extend([i] * j)
    keep = np.flatnonzero(np.isin(all_bpts, bodyparts2plot))
    bpts2color = [(ind, map2bp[ind], map2id[ind]) for ind in keep]

    if color_by == "bodypart":
        C = colorclass.to_rgba(np.linspace(0, 1, nbodyparts))
    else:
        C = colorclass.to_rgba(np.linspace(0, 1, nindividuals))
    colors = (C[:, :3] * 255).astype(np.uint8)

    if bboxes_color is None:
        bboxes_color = (255, 0, 0)

    with np.errstate(invalid="ignore"):
        for index in trange(min(nframes, len(Dataframe))):
            image = clip.load_frame()
            if displaycropped:
                image = image[y1:y2, x1:x2]

            # Draw bounding boxes if required and present
            if plot_bboxes and bboxes_list:
                bboxes = bboxes_list[index]["bboxes"]
                bbox_scores = bboxes_list[index]["bbox_scores"]
                n_bboxes = bboxes.shape[0]
                for i in range(n_bboxes):
                    bbox = bboxes[i, :]
                    x, y = bbox[0], bbox[1]
                    x += x1
                    y += y1
                    w, h = bbox[2], bbox[3]
                    confidence = bbox_scores[i]
                    if confidence < bboxes_pcutoff:
                        continue
                    rect_coords = rectangle_perimeter(start=(y, x), extent=(h, w))

                    set_color(
                        image,
                        rect_coords,
                        bboxes_color,
                    )

            # Draw the skeleton for specific bodyparts to be connected as
            # specified in the config file
            if draw_skeleton:
                for bpt1, bpt2 in bpts2connect:
                    if np.all(df_likelihood[[bpt1, bpt2], index] > pcutoff) and not (
                        np.any(np.isnan(df_x[[bpt1, bpt2], index]))
                        or np.any(np.isnan(df_y[[bpt1, bpt2], index]))
                    ):
                        rr, cc, val = line_aa(
                            int(np.clip(df_y[bpt1, index], 0, ny - 1)),
                            int(np.clip(df_x[bpt1, index], 0, nx - 1)),
                            int(np.clip(df_y[bpt2, index], 1, ny - 1)),
                            int(np.clip(df_x[bpt2, index], 1, nx - 1)),
                        )
                        image[rr, cc] = color_for_skeleton

            for ind, num_bp, num_ind in bpts2color:
                if df_likelihood[ind, index] > pcutoff:
                    if color_by == "bodypart":
                        color = colors[num_bp]
                    else:
                        color = colors[num_ind]
                    if trailpoints > 0:
                        for k in range(1, min(trailpoints, index + 1)):
                            rr, cc = disk(
                                (df_y[ind, index - k], df_x[ind, index - k]),
                                dotsize,
                                shape=(ny, nx),
                            )
                            image[rr, cc] = color
                    rr, cc = disk(
                        (df_y[ind, index], df_x[ind, index]), dotsize, shape=(ny, nx)
                    )
                    alpha = 1
                    if confidence_to_alpha is not None:
                        alpha = confidence_to_alpha(df_likelihood[ind, index])

                    set_color(image, (rr, cc), color, alpha)

            clip.save_frame(image)
    clip.close()


def CreateVideoSlow(
    videooutname,
    clip,
    Dataframe,
    tmpfolder,
    dotsize,
    colormap,
    alphavalue,
    pcutoff,
    trailpoints,
    cropping,
    x1,
    x2,
    y1,
    y2,
    save_frames,
    bodyparts2plot,
    outputframerate,
    Frames2plot,
    bodyparts2connect,
    skeleton_color,
    draw_skeleton,
    displaycropped,
    color_by,
    plot_bboxes=True,
    bboxes_list=None,
    bboxes_pcutoff=0.6,
    bboxes_color: str | None = None,
):
    """Creating individual frames with labeled body parts and making a video"""

    if displaycropped:
        ny, nx = y2 - y1, x2 - x1
    else:
        ny, nx = clip.height(), clip.width()

    fps = clip.fps()
    if outputframerate is None:  # by def. same as input rate.
        outputframerate = fps

    nframes = clip.nframes
    duration = nframes / fps

    print(
        "Duration of video [s]: {}, recorded with {} fps!".format(
            round(duration, 2), round(fps, 2)
        )
    )
    print(
        "Overall # of frames: {} with cropped frame dimensions: {} {}".format(
            nframes, nx, ny
        )
    )
    print("Generating frames and creating video.")
    df_x, df_y, df_likelihood = Dataframe.values.reshape((len(Dataframe), -1, 3)).T
    if cropping and not displaycropped:
        df_x += x1
        df_y += y1

    bpts = Dataframe.columns.get_level_values("bodyparts")
    all_bpts = bpts.values[::3]
    if draw_skeleton:
        bpts2connect = get_segment_indices(bodyparts2connect, all_bpts)

    bplist = bpts.unique().to_list()
    nbodyparts = len(bplist)
    if Dataframe.columns.nlevels == 3:
        nindividuals = int(len(all_bpts) / len(set(all_bpts)))
        map2bp = list(np.repeat(list(range(len(set(all_bpts)))), nindividuals))
        map2id = list(range(nindividuals)) * len(set(all_bpts))
    else:
        nindividuals = len(Dataframe.columns.get_level_values("individuals").unique())
        map2bp = [bplist.index(bp) for bp in all_bpts]
        nbpts_per_ind = (
            Dataframe.groupby(level="individuals", axis=1).size().values // 3
        )
        map2id = []
        for i, j in enumerate(nbpts_per_ind):
            map2id.extend([i] * j)
    keep = np.flatnonzero(np.isin(all_bpts, bodyparts2plot))
    bpts2color = [(ind, map2bp[ind], map2id[ind]) for ind in keep]
    if color_by == "individual":
        colors = visualization.get_cmap(nindividuals, name=colormap)
    else:
        colors = visualization.get_cmap(nbodyparts, name=colormap)

    if bboxes_color is None:
        bboxes_color = "red"

    nframes_digits = int(np.ceil(np.log10(nframes)))
    if nframes_digits > 9:
        raise Exception(
            "Your video has more than 10**9 frames, we recommend chopping it up."
        )

    if Frames2plot is None:
        Index = set(range(nframes))
    else:
        Index = {int(k) for k in Frames2plot if 0 <= k < nframes}

    # Prepare figure
    prev_backend = plt.get_backend()
    plt.switch_backend("agg")
    dpi = 100
    fig = plt.figure(frameon=False, figsize=(nx / dpi, ny / dpi))
    ax = fig.add_subplot(111)

    writer = FFMpegWriter(fps=outputframerate, codec="h264")
    with writer.saving(fig, videooutname, dpi=dpi), np.errstate(invalid="ignore"):
        for index in trange(min(nframes, len(Dataframe))):
            imagename = tmpfolder + "/file" + str(index).zfill(nframes_digits) + ".png"
            image = img_as_ubyte(clip.load_frame())
            if index in Index:  # then extract the frame!
                if cropping and displaycropped:
                    image = image[y1:y2, x1:x2]
                ax.imshow(image)

                # Draw bounding boxes of required and present
                if plot_bboxes and bboxes_list:
                    bboxes = bboxes_list[index]["bboxes"]
                    bbox_scores = bboxes_list[index]["bbox_scores"]
                    n_bboxes = bboxes.shape[0]
                    for i in range(n_bboxes):
                        bbox = bboxes[i, :]
                        bbox_origin = (bbox[0], bbox[1])
                        (bbox_width, bbox_height) = (bbox[2], bbox[3])
                        bbox_confidence = bbox_scores[i]
                        if bbox_confidence < bboxes_pcutoff:
                            continue
                        rectangle = patches.Rectangle(
                            bbox_origin,
                            bbox_width,
                            bbox_height,
                            linewidth=1,
                            edgecolor=bboxes_color,
                            facecolor="none",
                        )
                        ax.add_patch(rectangle)

                # Draw skeleton
                if draw_skeleton:
                    for bpt1, bpt2 in bpts2connect:
                        if np.all(df_likelihood[[bpt1, bpt2], index] > pcutoff):
                            ax.plot(
                                [df_x[bpt1, index], df_x[bpt2, index]],
                                [df_y[bpt1, index], df_y[bpt2, index]],
                                color=skeleton_color,
                                alpha=alphavalue,
                            )

                # Draw bodyparts
                for ind, num_bp, num_ind in bpts2color:
                    if df_likelihood[ind, index] > pcutoff:
                        if color_by == "bodypart":
                            color = colors(num_bp)
                        else:
                            color = colors(num_ind)
                        if trailpoints > 0:
                            ax.scatter(
                                df_x[ind][max(0, index - trailpoints) : index],
                                df_y[ind][max(0, index - trailpoints) : index],
                                s=dotsize**2,
                                color=color,
                                alpha=alphavalue * 0.75,
                            )
                        ax.scatter(
                            df_x[ind, index],
                            df_y[ind, index],
                            s=dotsize**2,
                            color=color,
                            alpha=alphavalue,
                        )
                ax.set_xlim(0, nx)
                ax.set_ylim(0, ny)
                ax.axis("off")
                ax.invert_yaxis()
                fig.subplots_adjust(
                    left=0, bottom=0, right=1, top=1, wspace=0, hspace=0
                )
                if save_frames:
                    fig.savefig(imagename)
                writer.grab_frame()
                ax.clear()

    print("Labeled video {} successfully created.".format(videooutname))
    plt.switch_backend(prev_backend)


def create_labeled_video(
    config: str,
    videos: list[str],
    videotype: str = "",
    shuffle: int = 1,
    trainingsetindex: int = 0,
    filtered: bool = False,
    fastmode: bool = True,
    save_frames: bool = False,
    keypoints_only: bool = False,
    Frames2plot: list[int] | None = None,
    displayedbodyparts: list[str] | str = "all",
    displayedindividuals: list[str] | str = "all",
    codec: str = "mp4v",
    outputframerate: int | None = None,
    destfolder: Path | str | None = None,
    draw_skeleton: bool = False,
    trailpoints: int = 0,
    displaycropped: bool = False,
    color_by: str = "bodypart",
    modelprefix: str = "",
    init_weights: str = "",
    track_method: str = "",
    superanimal_name: str = "",
    pcutoff: float | None = None,
    skeleton: list = [],
    skeleton_color: str = "white",
    dotsize: int = 8,
    colormap: str = "rainbow",
    alphavalue: float = 0.5,
    overwrite: bool = False,
    confidence_to_alpha: Union[bool, Callable[[float], float]] = False,
    plot_bboxes: bool = True,
    bboxes_pcutoff: float | None = None,
):
    """Labels the bodyparts in a video.

    Make sure the video is already analyzed by the function
    ``deeplabcut.analyze_videos``.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file.

    videos : list[str]
        A list of strings containing the full paths to videos for analysis or a path
        to the directory, where all the videos with same extension are stored.

    videotype: str, optional, default=""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions
        ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle : int, optional, default=1
        Number of shuffles of training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    filtered: bool, optional, default=False
        Boolean variable indicating if filtered output should be plotted rather than
        frame-by-frame predictions. Filtered version can be calculated with
        ``deeplabcut.filterpredictions``.

    fastmode: bool, optional, default=True
        If ``True``, uses openCV (much faster but less customization of video) instead
        of matplotlib if ``False``. You can also "save_frames" individually or not in
        the matplotlib mode (if you set the "save_frames" variable accordingly).
        However, using matplotlib to create the frames it therefore allows much more
        flexible (one can set transparency of markers, crop, and easily customize).

    save_frames: bool, optional, default=False
        If ``True``, creates each frame individual and then combines into a video.
        Setting this to ``True`` is relatively slow as it stores all individual frames.

    keypoints_only: bool, optional, default=False
        By default, both video frames and keypoints are visible. If ``True``, only the
        keypoints are shown. These clips are an hommage to Johansson movies,
        see https://www.youtube.com/watch?v=1F5ICP9SYLU and of course his seminal
        paper: "Visual perception of biological motion and a model for its analysis"
        by Gunnar Johansson in Perception & Psychophysics 1973.

    Frames2plot: List[int] or None, optional, default=None
        If not ``None`` and ``save_frames=True`` then the frames corresponding to the
        index will be plotted. For example, ``Frames2plot=[0,11]`` will plot the first
        and the 12th frame.

    displayedbodyparts: list[str] or str, optional, default="all"
        This selects the body parts that are plotted in the video. If ``all``, then all
        body parts from config.yaml are used. If a list of strings that are a subset of
        the full list. E.g. ['hand','Joystick'] for the demo
        Reaching-Mackenzie-2018-08-30/config.yaml to select only these body parts.

    displayedindividuals: list[str] or str, optional, default="all"
        Individuals plotted in the video.
        By default, all individuals present in the config will be shown.

    codec: str, optional, default="mp4v"
        Codec for labeled video. For available options, see
        http://www.fourcc.org/codecs.php. Note that this depends on your ffmpeg
        installation.

    outputframerate: int or None, optional, default=None
        Positive number, output frame rate for labeled video (only available for the
        mode with saving frames.) If ``None``, which results in the original video
        rate.

    destfolder: Path, string or None, optional, default=None
        Specifies the destination folder that was used for storing analysis data. If
        ``None``, the path of the video file is used.

    draw_skeleton: bool, optional, default=False
        If ``True`` adds a line connecting the body parts making a skeleton on each
        frame. The body parts to be connected and the color of these connecting lines
        are specified in the config file.

    trailpoints: int, optional, default=0
        Number of previous frames whose body parts are plotted in a frame
        (for displaying history).

    displaycropped: bool, optional, default=False
        Specifies whether only cropped frame is displayed (with labels analyzed
        therein), or the original frame with the labels analyzed in the cropped subset.

    color_by : string, optional, default='bodypart'
        Coloring rule. By default, each bodypart is colored differently.
        If set to 'individual', points belonging to a single individual are colored the
        same.

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    init_weights: str,
        Checkpoint path to the super model

    track_method: string, optional, default=""
        Specifies the tracker used to generate the data.
        Empty by default (corresponding to a single animal project).
        For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
        be taken from the config.yaml file if none is given.

    superanimal_name: str, optional, default=""
        Name of the superanimal model.

    pcutoff: float, optional, default=None
        Overrides the pcutoff set in the project configuration to plot the trajectories.

    skeleton: list, optional, default=[],

    skeleton_color: string, optional, default="white",
        Color for the skeleton

    dotsize, int, optional, default=8,
        Size of label dots tu use

    colormap: str, optional, default="rainbow",
        Colormap to use for the labels

    alphavalue: float, optional, default=0.5,

    overwrite: bool, optional, default=False
        If ``True`` overwrites existing labeled videos.

    confidence_to_alpha: Union[bool, Callable[[float], float], default=False
        If False, all keypoints will be plot with alpha=1. Otherwise, this can be
        defined as a function f: [0, 1] -> [0, 1] such that the alpha value for a
        keypoint will be set as a function of its score: alpha = f(score). The default
        function used when True is f(x) = max(0, (x - pcutoff)/(1 - pcutoff)).

    plot_bboxes: bool, optional, default=True
        If using Pytorch and in Top-Down mode, setting this to true will also plot the bounding boxes

    bboxes_pcutoff, float, optional, default=None:
        If plotting bounding boxes, this overrides the bboxes_pcutoff set in the model configuration.

    Returns
    -------
        results : list[bool]
        ``True`` if the video is successfully created for each item in ``videos``.

    Examples
    --------

    Create the labeled video for a single video

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/reachingvideo1.avi'],
        )

    Create the labeled video for a single video and store the individual frames

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/reachingvideo1.avi'],
            fastmode=True,
            save_frames=True,
        )

    Create the labeled video for multiple videos

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            [
                '/analysis/project/videos/reachingvideo1.avi',
                '/analysis/project/videos/reachingvideo2.avi',
            ],
        )

    Create the labeled video for all the videos with an .avi extension in a directory.

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/'],
        )

    Create the labeled video for all the videos with an .mp4 extension in a directory.

    >>> deeplabcut.create_labeled_video(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/videos/'],
            videotype='mp4',
        )
    """
    if config == "":
        if pcutoff is None:
            pcutoff = 0.6
        if bboxes_pcutoff is None:
            bboxes_pcutoff = 0.6

        individuals = [""]
        uniquebodyparts = []
    else:
        cfg = auxiliaryfunctions.read_config(config)
        train_fraction = cfg["TrainingFraction"][trainingsetindex]
        track_method = auxfun_multianimal.get_track_method(
            cfg, track_method=track_method
        )
        if pcutoff is None:
            pcutoff = cfg["pcutoff"]

        # Get individuals from the config
        individuals = cfg.get("individuals", [""])
        uniquebodyparts = cfg.get("uniquebodyparts", [])

        # Only for PyTorch engine - check if the shuffle was fine-tuned from a
        #  SuperAnimal model with memory replay -> SuperAnimal bodyparts must be used
        model_folder = auxiliaryfunctions.get_model_folder(
            train_fraction,
            shuffle,
            cfg,
            modelprefix,
            engine=Engine.PYTORCH,
        )
        model_config_path = (
            Path(config).parent / model_folder / "train" / Engine.PYTORCH.pose_cfg_name
        )
        if model_config_path.exists():
            model_config = auxiliaryfunctions.read_plainconfig(str(model_config_path))
            if (
                model_config["train_settings"]
                .get("weight_init", {})
                .get("memory_replay", False)
            ):
                superanimal_name = model_config["train_settings"]["weight_init"][
                    "dataset"
                ]
            if bboxes_pcutoff is None:
                bboxes_pcutoff = (
                    model_config.get("detector", {})
                    .get("model", {})
                    .get("box_score_thresh", 0.6)
                )
        else:
            if bboxes_pcutoff is None:
                bboxes_pcutoff = 0.6

    if init_weights == "":
        DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
            cfg, shuffle, train_fraction, modelprefix=modelprefix
        )  # automatically loads corresponding model (even training iteration based on snapshot index)
    else:
        DLCscorer = "DLC_" + Path(init_weights).stem
        DLCscorerlegacy = "DLC_" + Path(init_weights).stem

    if save_frames:
        fastmode = False  # otherwise one cannot save frames
        keypoints_only = False

    # parse the alpha selection function
    if isinstance(confidence_to_alpha, bool):
        confidence_to_alpha = _get_default_conf_to_alpha(confidence_to_alpha, pcutoff)

    if superanimal_name != "":
        dlc_root_path = auxiliaryfunctions.get_deeplabcut_path()
        test_cfg = auxiliaryfunctions.read_plainconfig(
            os.path.join(
                dlc_root_path,
                "modelzoo",
                "project_configs",
                f"{superanimal_name}.yaml",
            )
        )

        bodyparts = test_cfg["bodyparts"]
        cfg = {
            "skeleton": skeleton,
            "skeleton_color": skeleton_color,
            "pcutoff": pcutoff,
            "dotsize": dotsize,
            "alphavalue": alphavalue,
            "colormap": colormap,
            "bodyparts": bodyparts,
            "multianimalbodyparts": bodyparts,
            "individuals": individuals,
            "uniquebodyparts": uniquebodyparts,
        }
    else:
        bodyparts = (
            auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
                cfg, displayedbodyparts
            )
        )

    if draw_skeleton:
        bodyparts2connect = cfg["skeleton"]
        if displayedbodyparts != "all":
            bodyparts2connect = [
                pair
                for pair in bodyparts2connect
                if all(element in displayedbodyparts for element in pair)
            ]
        skeleton_color = cfg["skeleton_color"]
    else:
        bodyparts2connect = None
        skeleton_color = None

    start_path = os.getcwd()
    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)

    if not Videos:
        return []

    func = partial(
        proc_video,
        videos,
        destfolder,
        filtered,
        DLCscorer,
        DLCscorerlegacy,
        track_method,
        cfg,
        displayedindividuals,
        color_by,
        bodyparts,
        codec,
        bodyparts2connect,
        trailpoints,
        save_frames,
        outputframerate,
        Frames2plot,
        draw_skeleton,
        skeleton_color,
        displaycropped,
        fastmode,
        keypoints_only,
        overwrite,
        init_weights=init_weights,
        pcutoff=pcutoff,
        confidence_to_alpha=confidence_to_alpha,
        plot_bboxes=plot_bboxes,
        bboxes_pcutoff=bboxes_pcutoff,
    )

    if get_start_method() == "fork":
        with Pool(min(os.cpu_count(), len(Videos))) as pool:
            results = pool.map(func, Videos)
    else:
        results = []
        for video in Videos:
            results.append(func(video))

    os.chdir(start_path)
    return results


def proc_video(
    videos,
    destfolder,
    filtered,
    DLCscorer,
    DLCscorerlegacy,
    track_method,
    cfg,
    individuals,
    color_by,
    bodyparts,
    codec,
    bodyparts2connect,
    trailpoints,
    save_frames,
    outputframerate,
    Frames2plot,
    draw_skeleton,
    skeleton_color,
    displaycropped,
    fastmode,
    keypoints_only,
    overwrite,
    video,
    init_weights="",
    pcutoff: float | None = None,
    confidence_to_alpha: Optional[Callable[[float], float]] = None,
    plot_bboxes: bool = True,
    bboxes_pcutoff: float = 0.6,
):
    """Helper function for create_videos

    Parameters
    ----------


    Returns
    -------
        result : bool
        ``True`` if a video is successfully created.
    """
    videofolder = Path(video).parent
    if destfolder is None:
        destfolder = videofolder  # where your folder with videos is.

    if pcutoff is None:
        pcutoff = cfg["pcutoff"]

    auxiliaryfunctions.attempt_to_make_folder(destfolder)

    os.chdir(destfolder)  # THE VIDEO IS STILL IN THE VIDEO FOLDER
    print("Starting to process video: {}".format(video))
    vname = str(Path(video).stem)

    if init_weights != "":
        DLCscorer = "DLC_" + Path(init_weights).stem
        DLCscorerlegacy = "DLC_" + Path(init_weights).stem

    if filtered:
        videooutname1 = os.path.join(vname + DLCscorer + "filtered_labeled.mp4")
        videooutname2 = os.path.join(vname + DLCscorerlegacy + "filtered_labeled.mp4")
    else:
        videooutname1 = os.path.join(vname + DLCscorer + "_labeled.mp4")
        videooutname2 = os.path.join(vname + DLCscorerlegacy + "_labeled.mp4")

    if (
        os.path.isfile(videooutname1) or os.path.isfile(videooutname2)
    ) and not overwrite:
        print("Labeled video {} already created.".format(vname))
        return True
    else:
        print("Loading {} and data.".format(video))
        try:
            df, filepath, _, _ = auxiliaryfunctions.load_analyzed_data(
                destfolder, vname, DLCscorer, filtered, track_method
            )
            metadata = auxiliaryfunctions.load_video_metadata(
                destfolder, vname, DLCscorer
            )
            if cfg.get("multianimalproject", False):
                s = "_id" if color_by == "individual" else "_bp"
            else:
                s = ""

            videooutname = filepath.replace(
                ".h5", f"{s}_p{int(100 * pcutoff)}_labeled.mp4"
            )
            if os.path.isfile(videooutname) and not overwrite:
                print("Labeled video already created. Skipping...")
                return

            if individuals != "all":
                if isinstance(individuals, str):
                    individuals = [individuals]

                if all(individuals) and "individuals" in df.columns.names:
                    mask = df.columns.get_level_values("individuals").isin(individuals)
                    df = df.loc[:, mask]

            cropping = metadata["data"]["cropping"]
            [x1, x2, y1, y2] = metadata["data"]["cropping_parameters"]
            labeled_bpts = [
                bp
                for bp in df.columns.get_level_values("bodyparts").unique()
                if bp in bodyparts
            ]

            # The full data file is not created for single-animal TensorFlow models
            try:
                full_data = auxiliaryfunctions.load_video_full_data(
                    destfolder, vname, DLCscorer
                )
                frames_dict = {
                    int(key.replace("frame", "")): value
                    for key, value in full_data.items()
                    if key.startswith("frame") and key[5:].isdigit()
                }
                bboxes_list = None
                if "bboxes" in frames_dict.get(min(frames_dict.keys()), {}):
                    bboxes_list = [
                        frames_dict[key] for key in sorted(frames_dict.keys())
                    ]
            except FileNotFoundError:
                bboxes_list = None

            if keypoints_only:
                # Mask rather than drop unwanted bodyparts to ensure consistent coloring
                mask = df.columns.get_level_values("bodyparts").isin(bodyparts)
                df.loc[:, ~mask] = np.nan
                inds = None
                if bodyparts2connect:
                    all_bpts = df.columns.get_level_values("bodyparts")[::3]
                    inds = get_segment_indices(bodyparts2connect, all_bpts)
                clip = vp(fname=video, fps=outputframerate)
                create_video_with_keypoints_only(
                    df,
                    videooutname,
                    inds,
                    pcutoff,
                    cfg["dotsize"],
                    cfg["alphavalue"],
                    skeleton_color=skeleton_color,
                    color_by=color_by,
                    colormap=cfg["colormap"],
                    fps=clip.fps(),
                )
                clip.close()
            elif not fastmode:
                tmpfolder = os.path.join(str(videofolder), "temp-" + vname)
                if save_frames:
                    auxiliaryfunctions.attempt_to_make_folder(tmpfolder)
                clip = vp(video)
                CreateVideoSlow(
                    videooutname,
                    clip,
                    df,
                    tmpfolder,
                    cfg["dotsize"],
                    cfg["colormap"],
                    cfg["alphavalue"],
                    pcutoff,
                    trailpoints,
                    cropping,
                    x1,
                    x2,
                    y1,
                    y2,
                    save_frames,
                    labeled_bpts,
                    outputframerate,
                    Frames2plot,
                    bodyparts2connect,
                    skeleton_color,
                    draw_skeleton,
                    displaycropped,
                    color_by,
                    plot_bboxes=plot_bboxes,
                    bboxes_list=bboxes_list,
                    bboxes_pcutoff=bboxes_pcutoff,
                )
                clip.close()
            else:
                create_video(
                    video,
                    filepath,
                    keypoints2show=labeled_bpts,
                    animals2show=individuals,
                    bbox=(x1, x2, y1, y2),
                    codec=codec,
                    output_path=videooutname,
                    pcutoff=pcutoff,
                    dotsize=cfg["dotsize"],
                    cmap=cfg["colormap"],
                    color_by=color_by,
                    skeleton_edges=bodyparts2connect,
                    skeleton_color=skeleton_color,
                    trailpoints=trailpoints,
                    fps=outputframerate,
                    display_cropped=displaycropped,
                    confidence_to_alpha=confidence_to_alpha,
                    plot_bboxes=plot_bboxes,
                    bboxes_list=bboxes_list,
                    bboxes_pcutoff=bboxes_pcutoff,
                )

            return True

        except FileNotFoundError as e:
            print(e)
            return False


def create_video(
    video,
    h5file,
    keypoints2show="all",
    animals2show="all",
    skeleton_edges=None,
    pcutoff=0.6,
    dotsize=6,
    cmap="rainbow",
    color_by="bodypart",
    skeleton_color="k",
    trailpoints=0,
    bbox=None,
    display_cropped=False,
    codec="mp4v",
    fps=None,
    output_path="",
    confidence_to_alpha=None,
    plot_bboxes=True,
    bboxes_list=None,
    bboxes_pcutoff=0.6,
    bboxes_color: tuple | None = None,
):
    if color_by not in ("bodypart", "individual"):
        raise ValueError("`color_by` should be either 'bodypart' or 'individual'.")

    if not output_path:
        s = "_id" if color_by == "individual" else "_bp"
        output_path = h5file.replace(".h5", f"{s}_labeled.mp4")

    clip = vp(
        fname=video,
        sname=output_path,
        codec=codec,
        sw=bbox[1]-bbox[0] if display_cropped else "",
        sh=bbox[3]-bbox[2] if display_cropped else "",
        fps=fps,
    )

    cropping = bbox != (0, clip.w, 0, clip.h)

    x1, x2, y1, y2 = bbox if bbox is not None else (0, clip.w, 0, clip.h)

    df = pd.read_hdf(h5file)

    try:
        animals = df.columns.get_level_values("individuals").unique().to_list()
        if animals2show != "all" and isinstance(animals, Iterable):
            animals = [a for a in animals if a in animals2show]
        df = df.loc(axis=1)[:, animals]
    except KeyError:
        pass

    kpts = df.columns.get_level_values("bodyparts").unique().to_list()

    if keypoints2show != "all" and isinstance(keypoints2show, Iterable):
        kpts = [kpt for kpt in kpts if kpt in keypoints2show]

    CreateVideo(
        clip,
        df,
        pcutoff,
        dotsize,
        cmap,
        kpts,
        trailpoints,
        cropping,
        x1,
        x2,
        y1,
        y2,
        skeleton_edges,
        skeleton_color,
        bool(skeleton_edges),
        display_cropped,
        color_by,
        confidence_to_alpha=confidence_to_alpha,
        plot_bboxes=plot_bboxes,
        bboxes_list=bboxes_list,
        bboxes_pcutoff=bboxes_pcutoff,
        bboxes_color=bboxes_color,
    )


# for backwards compatibility
_create_labeled_video = create_video


def create_video_with_keypoints_only(
    df,
    output_name,
    ind_links=None,
    pcutoff=0.6,
    dotsize=8,
    alpha=0.7,
    background_color="k",
    skeleton_color="navy",
    color_by="bodypart",
    colormap="viridis",
    fps=25,
    dpi=200,
    codec="h264",
):
    bodyparts = df.columns.get_level_values("bodyparts")[::3]
    bodypart_names = bodyparts.unique()
    n_bodyparts = len(bodypart_names)
    nx = int(np.nanmax(df.xs("x", axis=1, level="coords")))
    ny = int(np.nanmax(df.xs("y", axis=1, level="coords")))

    n_frames = df.shape[0]
    xyp = df.values.reshape((n_frames, -1, 3))

    if color_by == "bodypart":
        map_ = bodyparts.map(dict(zip(bodypart_names, range(n_bodyparts))))
        cmap = plt.get_cmap(colormap, n_bodyparts)
    elif color_by == "individual":
        try:
            individuals = df.columns.get_level_values("individuals")[::3]
            individual_names = individuals.unique().to_list()
            n_individuals = len(individual_names)
            map_ = individuals.map(dict(zip(individual_names, range(n_individuals))))
            cmap = plt.get_cmap(colormap, n_individuals)
        except KeyError as e:
            raise Exception(
                "Coloring by individuals is only valid for multi-animal data"
            ) from e
    else:
        raise ValueError(f"Invalid color_by={color_by}")

    prev_backend = plt.get_backend()
    plt.switch_backend("agg")
    fig = plt.figure(frameon=False, figsize=(nx / dpi, ny / dpi))
    ax = fig.add_subplot(111)
    scat = ax.scatter([], [], s=dotsize**2, alpha=alpha)
    coords = xyp[0, :, :2]
    coords[xyp[0, :, 2] < pcutoff] = np.nan
    scat.set_offsets(coords)
    colors = cmap(map_)
    scat.set_color(colors)
    segs = coords[tuple(zip(*tuple(ind_links))), :].swapaxes(0, 1) if ind_links else []
    coll = LineCollection(segs, colors=skeleton_color, alpha=alpha)
    ax.add_collection(coll)
    ax.set_xlim(0, nx)
    ax.set_ylim(0, ny)
    ax.axis("off")
    ax.add_patch(
        plt.Rectangle(
            (0, 0), 1, 1, facecolor=background_color, transform=ax.transAxes, zorder=-1
        )
    )
    ax.invert_yaxis()
    plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)

    writer = FFMpegWriter(fps=fps, codec=codec)
    with writer.saving(fig, output_name, dpi=dpi):
        writer.grab_frame()
        for index, _ in enumerate(trange(n_frames - 1), start=1):
            coords = xyp[index, :, :2]
            coords[xyp[index, :, 2] < pcutoff] = np.nan
            scat.set_offsets(coords)
            if ind_links:
                segs = coords[tuple(zip(*tuple(ind_links))), :].swapaxes(0, 1)
            coll.set_segments(segs)
            writer.grab_frame()
    plt.close(fig)
    plt.switch_backend(prev_backend)


def create_video_with_all_detections(
    config,
    videos,
    videotype="",
    shuffle=1,
    trainingsetindex=0,
    displayedbodyparts="all",
    cropping: Optional[List[int]] = None,
    destfolder=None,
    modelprefix="",
    confidence_to_alpha: Union[bool, Callable[[float], float]] = False,
    plot_bboxes: bool = True,
):
    """
    Create a video labeled with all the detections stored in a '*_full.pickle' file.

    Parameters
    ----------
    config : str
        Absolute path to the config.yaml file

    videos : list of str
        A list of strings containing the full paths to videos for analysis or a path to the directory,
        where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.\n Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle : int, optional
        Number of shuffles of training dataset. Default is set to 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    displayedbodyparts: list of strings, optional
        This selects the body parts that are plotted in the video. Either ``all``, then all body parts
        from config.yaml are used orr a list of strings that are a subset of the full list.
        E.g. ['hand','Joystick'] for the demo Reaching-Mackenzie-2018-08-30/config.yaml to select only these two body parts.

    cropping: list[int], optional (default=None)
        If passed in, the [x1, x2, y1, y2] crop coordinates are used to shift detections appropriately.

    destfolder: string, optional
        Specifies the destination folder that was used for storing analysis data (default is the path of the video).

    confidence_to_alpha: Union[bool, Callable[[float], float], default=False
        If False, all keypoints will be plot with alpha=1. Otherwise, this can be
        defined as a function f: [0, 1] -> [0, 1] such that the alpha value for a
        keypoint will be set as a function of its score: alpha = f(score). The default
        function used when True is f(x) = x.

    plot_bboxes: bool, optional (default=True)
        If detections were produced using a Pytorch Top-Down model, setting this parameter to True will also plot
        the bounding boxes generated by the detector.
    """
    import re

    from deeplabcut.core.inferenceutils import Assembler

    cfg = auxiliaryfunctions.read_config(config)
    trainFraction = cfg["TrainingFraction"][trainingsetindex]
    DLCscorername, _ = auxiliaryfunctions.get_scorer_name(
        cfg, shuffle, trainFraction, modelprefix=modelprefix
    )

    videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if not videos:
        return

    if isinstance(confidence_to_alpha, bool):
        confidence_to_alpha = _get_default_conf_to_alpha(confidence_to_alpha, 0)

    for video in videos:
        videofolder = os.path.splitext(video)[0]

        if destfolder is None:
            outputname = "{}_full.mp4".format(videofolder + DLCscorername)
            full_pickle = os.path.join(videofolder + DLCscorername + "_full.pickle")
        else:
            auxiliaryfunctions.attempt_to_make_folder(destfolder)
            outputname = os.path.join(
                destfolder, str(Path(video).stem) + DLCscorername + "_full.mp4"
            )
            full_pickle = os.path.join(
                destfolder, str(Path(video).stem) + DLCscorername + "_full.pickle"
            )

        if not (os.path.isfile(outputname)):
            video_name = str(Path(video).stem)
            print("Creating labeled video for ", video_name)
            h5file = full_pickle.replace("_full.pickle", ".h5")
            data, metadata = auxfun_multianimal.LoadFullMultiAnimalData(h5file)
            data = dict(
                data
            )  # Cast to dict (making a copy) so items can safely be popped

            x1, y1 = 0, 0
            if cropping is not None:
                x1, _, y1, _ = cropping
            elif metadata.get("data", {}).get("cropping"):
                x1, _, y1, _ = metadata["data"]["cropping_parameters"]

            header = data.pop("metadata")
            all_jointnames = header["all_joints_names"]

            if displayedbodyparts == "all":
                numjoints = len(all_jointnames)
                bpts = range(numjoints)
            else:  # select only "displayedbodyparts"
                bpts = []
                for bptindex, bp in enumerate(all_jointnames):
                    if bp in displayedbodyparts:
                        bpts.append(bptindex)
                numjoints = len(bpts)
            frame_names = list(data)
            frames = [int(re.findall(r"\d+", name)[0]) for name in frame_names]
            colorclass = plt.cm.ScalarMappable(cmap=cfg["colormap"])
            C = colorclass.to_rgba(np.linspace(0, 1, numjoints))
            colors = (C[:, :3] * 255).astype(np.uint8)

            pcutoff = cfg["pcutoff"]
            dotsize = cfg["dotsize"]
            clip = vp(fname=video, sname=outputname, codec="mp4v")
            ny, nx = clip.height(), clip.width()

            bboxes_pcutoff = (
                metadata.get("data", {})
                .get("pytorch-config", {})
                .get("detector", {})
                .get("model", {})
                .get("box_score_thresh", 0.6)
            )
            bboxes_color = (255, 0, 0)

            for n in trange(clip.nframes):
                frame = clip.load_frame()
                if frame is None:
                    continue
                try:
                    ind = frames.index(n)

                    # Draw bounding boxes of required and present
                    if plot_bboxes and "bboxes" in data[frame_names[ind]]:
                        bboxes = data[frame_names[ind]]["bboxes"]
                        bbox_scores = data[frame_names[ind]]["bbox_scores"]
                        n_bboxes = bboxes.shape[0]
                        for i in range(n_bboxes):
                            bbox = bboxes[i, :]
                            x, y = bbox[0], bbox[1]
                            x += x1
                            y += y1
                            w, h = bbox[2], bbox[3]
                            confidence = bbox_scores[i]
                            if confidence < bboxes_pcutoff:
                                continue
                            rect_coords = rectangle_perimeter(
                                start=(y, x), extent=(h, w)
                            )

                            set_color(
                                frame,
                                rect_coords,
                                bboxes_color,
                            )

                    # Draw detected bodyparts
                    dets = Assembler._flatten_detections(data[frame_names[ind]])
                    for det in dets:
                        if det.label not in bpts or det.confidence < pcutoff:
                            continue
                        x, y = det.pos
                        x += x1
                        y += y1
                        rr, cc = disk((y, x), dotsize, shape=(ny, nx))
                        alpha = 1
                        if confidence_to_alpha is not None:
                            alpha = confidence_to_alpha(det.confidence)

                        set_color(
                            frame,
                            (rr, cc),
                            colors[bpts.index(det.label)],
                            alpha,
                        )
                except ValueError as err:  # No data stored for that particular frame
                    print(n, f"no data: {err}")
                    pass
                try:
                    clip.save_frame(frame)
                except:
                    print(n, "frame writing error.")
                    pass
            clip.close()
        else:
            print("Detections already plotted, ", outputname)


def _create_video_from_tracks(video, tracks, destfolder, output_name, pcutoff, scale=1):
    import subprocess

    from tqdm import tqdm

    if not os.path.isdir(destfolder):
        os.mkdir(destfolder)

    vid = VideoWriter(video)
    nframes = len(vid)
    strwidth = int(np.ceil(np.log10(nframes)))  # width for strings
    nx, ny = vid.dimensions
    # cropping!
    X2 = nx  # 1600
    X1 = 0
    # nx=X2-X1
    numtracks = len(tracks.keys()) - 1
    trackids = [t for t in tracks.keys() if t != "header"]
    cc = np.random.rand(numtracks + 1, 3)
    fig, ax = visualization.prepare_figure_axes(nx, ny, scale)
    im = ax.imshow(np.zeros((ny, nx)))
    markers = sum([ax.plot([], [], ".", c=c) for c in cc], [])
    for index in tqdm(range(nframes)):
        vid.set_to_frame(index)
        imname = "frame" + str(index).zfill(strwidth)
        image_output = os.path.join(destfolder, imname + ".png")
        frame = vid.read_frame()
        if frame is not None and not os.path.isfile(image_output):
            im.set_data(frame[:, X1:X2])
            for n, trackid in enumerate(trackids):
                if imname in tracks[trackid]:
                    x, y, p = tracks[trackid][imname][:, :3].reshape((-1, 3)).T
                    markers[n].set_data(x[p > pcutoff], y[p > pcutoff])
                else:
                    markers[n].set_data([], [])
            fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.savefig(image_output)

    outputframerate = 30
    os.chdir(destfolder)

    subprocess.call(
        [
            "ffmpeg",
            "-framerate",
            str(vid.fps),
            "-i",
            f"frame%0{strwidth}d.png",
            "-r",
            str(outputframerate),
            output_name,
        ]
    )
    # remove frames used for video creation
    [os.remove(image) for image in os.listdir(destfolder) if "frame" in image]


def create_video_from_pickled_tracks(
    video, pickle_file, destfolder="", output_name="", pcutoff=0.6
):
    if not destfolder:
        destfolder = os.path.splitext(video)[0]
    if not output_name:
        video_name, ext = os.path.splitext(os.path.split(video)[1])
        output_name = video_name + "DLClabeled" + ext
    tracks = auxiliaryfunctions.read_pickle(pickle_file)
    _create_video_from_tracks(video, tracks, destfolder, output_name, pcutoff)


def _get_default_conf_to_alpha(
    confidence_to_alpha: bool,
    pcutoff: float,
) -> Optional[Callable[[float], float]]:
    """Creates the default confidence_to_alpha function"""
    if not confidence_to_alpha:
        return None

    def default_confidence_to_alpha(x):
        if pcutoff == 0:
            return x
        return np.clip((x - pcutoff) / (1 - pcutoff), 0, 1)

    return default_confidence_to_alpha


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config")
    parser.add_argument("videos")
    cli_args = parser.parse_args()


--- File: deeplabcut/utils/auxiliaryfunctions.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""
from __future__ import annotations

import os
import typing
import pickle
import warnings
from pathlib import Path
from typing import List

import numpy as np
import pandas as pd
import ruamel.yaml.representer
import yaml
from ruamel.yaml import YAML

from deeplabcut.core.engine import Engine
from deeplabcut.core.trackingutils import TRACK_METHODS
from deeplabcut.utils import auxfun_videos, auxfun_multianimal


def create_config_template(multianimal=False):
    """
    Creates a template for config.yaml file. This specific order is preserved while saving as yaml file.
    """
    if multianimal:
        yaml_str = """\
# Project definitions (do not edit)
Task:
scorer:
date:
multianimalproject:
identity:
\n
# Project path (change when moving around)
project_path:
\n
# Default DeepLabCut engine to use for shuffle creation (either pytorch or tensorflow)
engine: pytorch
\n
# Annotation data set configuration (and individual video cropping parameters)
video_sets:
individuals:
uniquebodyparts:
multianimalbodyparts:
bodyparts:
\n
# Fraction of video to start/stop when extracting frames for labeling/refinement
start:
stop:
numframes2pick:
\n
# Plotting configuration
skeleton:
skeleton_color:
pcutoff:
dotsize:
alphavalue:
colormap:
\n
# Training,Evaluation and Analysis configuration
TrainingFraction:
iteration:
default_net_type:
default_augmenter:
default_track_method:
snapshotindex:
detector_snapshotindex:
batch_size:
\n
# Cropping Parameters (for analysis and outlier frame detection)
cropping:
#if cropping is true for analysis, then set the values here:
x1:
x2:
y1:
y2:
\n
# Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
move2corner:
\n
# Conversion tables to fine-tune SuperAnimal weights
SuperAnimalConversionTables:
        """
    else:
        yaml_str = """\
# Project definitions (do not edit)
Task:
scorer:
date:
multianimalproject:
identity:
\n
# Project path (change when moving around)
project_path:
\n
# Default DeepLabCut engine to use for shuffle creation (either pytorch or tensorflow)
engine: pytorch
\n
# Annotation data set configuration (and individual video cropping parameters)
video_sets:
bodyparts:
\n
# Fraction of video to start/stop when extracting frames for labeling/refinement
start:
stop:
numframes2pick:
\n
# Plotting configuration
skeleton:
skeleton_color:
pcutoff:
dotsize:
alphavalue:
colormap:
\n
# Training,Evaluation and Analysis configuration
TrainingFraction:
iteration:
default_net_type:
default_augmenter:
snapshotindex:
detector_snapshotindex:
batch_size:
detector_batch_size:
\n
# Cropping Parameters (for analysis and outlier frame detection)
cropping:
#if cropping is true for analysis, then set the values here:
x1:
x2:
y1:
y2:
\n
# Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
move2corner:
\n
# Conversion tables to fine-tune SuperAnimal weights
SuperAnimalConversionTables:
        """

    ruamelFile = YAML()
    cfg_file = ruamelFile.load(yaml_str)
    return cfg_file, ruamelFile


def create_config_template_3d():
    """
    Creates a template for config.yaml file for 3d project. This specific order is preserved while saving as yaml file.
    """
    yaml_str = """\
# Project definitions (do not edit)
Task:
scorer:
date:
\n
# Project path (change when moving around)
project_path:
\n
# Plotting configuration
skeleton: # Note that the pairs must be defined, as you want them linked!
skeleton_color:
pcutoff:
colormap:
dotsize:
alphaValue:
markerType:
markerColor:
\n
# Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:
num_cameras:
camera_names:
scorername_3d: # Enter the scorer name for the 3D output
    """
    ruamelFile_3d = YAML()
    cfg_file_3d = ruamelFile_3d.load(yaml_str)
    return cfg_file_3d, ruamelFile_3d


def read_config(configname):
    """
    Reads structured config file defining a project.
    """
    ruamelFile = YAML()
    path = Path(configname)
    if os.path.exists(path):
        try:
            with open(path, "r") as f:
                cfg = ruamelFile.load(f)
                curr_dir = str(Path(configname).parent.resolve())

                if cfg.get("engine") is None:
                    cfg["engine"] = Engine.TF.aliases[0]
                    write_config(configname, cfg)

                if cfg.get("detector_snapshotindex") is None:
                    cfg["detector_snapshotindex"] = -1

                if cfg.get("detector_batch_size") is None:
                    cfg["detector_batch_size"] = 1

                if cfg["project_path"] != curr_dir:
                    cfg["project_path"] = curr_dir
                    write_config(configname, cfg)
        except Exception as err:
            if len(err.args) > 2:
                if (
                    err.args[2]
                    == "could not determine a constructor for the tag '!!python/tuple'"
                ):
                    with open(path, "r") as ymlfile:
                        cfg = yaml.load(ymlfile, Loader=yaml.SafeLoader)
                        write_config(configname, cfg)
                else:
                    raise

    else:
        raise FileNotFoundError(
            f"Config file at {path} not found. Please make sure that the file exists and/or that you passed the path of the config file correctly!"
        )
    return cfg


def write_config(configname, cfg):
    """
    Write structured config file.
    """
    with open(configname, "w") as cf:
        cfg_file, ruamelFile = create_config_template(
            cfg.get("multianimalproject", False)
        )
        for key in cfg.keys():
            cfg_file[key] = cfg[key]

        # Adding default value for variable skeleton and skeleton_color for backward compatibility.
        if not "skeleton" in cfg.keys():
            cfg_file["skeleton"] = []
            cfg_file["skeleton_color"] = "black"
        ruamelFile.dump(cfg_file, cf)


def edit_config(configname, edits, output_name=""):
    """
    Convenience function to edit and save a config file from a dictionary.

    Parameters
    ----------
    configname : string
        String containing the full path of the config file in the project.
    edits : dict
        Key–value pairs to edit in config
    output_name : string, optional (default='')
        Overwrite the original config.yaml by default.
        If passed in though, new filename of the edited config.

    Examples
    --------
    config_path = 'my_stellar_lab/dlc/config.yaml'

    edits = {'numframes2pick': 5,
             'trainingFraction': [0.5, 0.8],
             'skeleton': [['a', 'b'], ['b', 'c']]}

    deeplabcut.auxiliaryfunctions.edit_config(config_path, edits)
    """
    cfg = read_plainconfig(configname)
    for key, value in edits.items():
        cfg[key] = value
    if not output_name:
        output_name = configname
    try:
        write_plainconfig(output_name, cfg)
    except ruamel.yaml.representer.RepresenterError:
        warnings.warn(
            "Some edits could not be written. "
            "The configuration file will be left unchanged."
        )
        for key in edits:
            cfg.pop(key)
        write_plainconfig(output_name, cfg)
    return cfg


def get_bodyparts(cfg: dict) -> typing.List[str]:
    """
    Args:
        cfg: a project configuration file

    Returns: bodyparts listed in the project (does not include the unique_bodyparts entry)
    """
    if cfg.get("multianimalproject", False):
        (
            _,
            _,
            multianimal_bodyparts,
        ) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
        return multianimal_bodyparts

    return cfg["bodyparts"]


def get_unique_bodyparts(cfg: dict) -> typing.List[str]:
    """
    Args:
        cfg: a project configuration file

    Returns: all unique bodyparts listed in the project
    """
    if cfg.get("multianimalproject", False):
        (
            _,
            unique_bodyparts,
            _,
        ) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
        return unique_bodyparts

    return []


def write_config_3d(configname, cfg):
    """
    Write structured 3D config file.
    """
    with open(configname, "w") as cf:
        cfg_file, ruamelFile = create_config_template_3d()
        for key in cfg.keys():
            cfg_file[key] = cfg[key]
        ruamelFile.dump(cfg_file, cf)


def write_config_3d_template(projconfigfile, cfg_file_3d, ruamelFile_3d):
    with open(projconfigfile, "w") as cf:
        ruamelFile_3d.dump(cfg_file_3d, cf)


def read_plainconfig(configname):
    if not os.path.exists(configname):
        raise FileNotFoundError(
            f"Config {configname} is not found. Please make sure that the file exists."
        )
    with open(configname) as file:
        return YAML().load(file)


def write_plainconfig(configname, cfg):
    with open(configname, "w") as file:
        YAML().dump(cfg, file)


def attempt_to_make_folder(foldername, recursive=False):
    """Attempts to create a folder with specified name. Does nothing if it already exists."""
    try:
        os.path.isdir(foldername)
    except TypeError:  # https://www.python.org/dev/peps/pep-0519/
        foldername = os.fspath(
            foldername
        )  # https://github.com/DeepLabCut/DeepLabCut/issues/105 (windows)

    if os.path.isdir(foldername):
        pass
    else:
        if recursive:
            os.makedirs(foldername)
        else:
            os.mkdir(foldername)


def read_pickle(filename):
    """Read the pickle file"""
    with open(filename, "rb") as handle:
        return pickle.load(handle)


def write_pickle(filename, data):
    """Write the pickle file"""
    with open(filename, "wb") as handle:
        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)


def get_list_of_videos(
    videos: typing.Union[typing.List[str], str],
    videotype: typing.Union[typing.List[str], str] = "",
    in_random_order: bool = True,
) -> typing.List[str]:
    """Returns list of videos of videotype "videotype" in
    folder videos or for list of videos.

    NOTE: excludes keyword videos of the form:

    *_labeled.videotype
    *_full.videotype

    Args:
        videos (list[str], str): List of video paths or a single path string. If string (or len() == 1 list of strings) is a directory,
            finds all videos whose extension matches  ``videotype`` in the directory

        videotype (list[str], str): File extension used to filter videos. Optional if ``videos`` is a list of video files,
            and filters with common video extensions if a directory is passed in.

        in_random_order (bool): Whether or not to return a shuffled list of videos.
    """
    if isinstance(videos, str):
        videos = [videos]

    if [os.path.isdir(i) for i in videos] == [True]:  # checks if input is a directory
        """
        Returns all the videos in the directory.
        """
        if not videotype:
            videotype = auxfun_videos.SUPPORTED_VIDEOS

        print("Analyzing all the videos in the directory...")
        videofolder = videos[0]

        # make list of full paths
        videos = [os.path.join(videofolder, fn) for fn in os.listdir(videofolder)]

        if in_random_order:
            from random import shuffle

            shuffle(
                videos
            )  # this is useful so multiple nets can be used to analyze simultaneously
        else:
            videos.sort()

    if isinstance(videotype, str):
        videotype = [videotype]
    if not videotype:
        videotype = auxfun_videos.SUPPORTED_VIDEOS
    # filter list of videos
    videos = [
        v
        for v in videos
        if os.path.isfile(v)
        and any(v.endswith(ext) for ext in videotype)
        and "_labeled." not in v
        and "_full." not in v
    ]

    return videos


def save_data(PredicteData, metadata, dataname, pdindex, imagenames, save_as_csv):
    """Save predicted data as h5 file and metadata as pickle file; created by predict_videos.py"""
    DataMachine = pd.DataFrame(PredicteData, columns=pdindex, index=imagenames)
    if save_as_csv:
        print("Saving csv poses!")
        DataMachine.to_csv(dataname.split(".h5")[0] + ".csv")
    DataMachine.to_hdf(dataname, "df_with_missing", format="table", mode="w")
    with open(dataname.split(".h5")[0] + "_meta.pickle", "wb") as f:
        # Pickle the 'data' dictionary using the highest protocol available.
        pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)


def save_metadata(metadatafilename, data, trainIndices, testIndices, trainFraction):
    with open(metadatafilename, "wb") as f:
        # Pickle the 'labeled-data' dictionary using the highest protocol available.
        pickle.dump(
            [data, trainIndices, testIndices, trainFraction], f, pickle.HIGHEST_PROTOCOL
        )


def load_metadata(metadatafile):
    with open(metadatafile, "rb") as f:
        [
            trainingdata_details,
            trainIndices,
            testIndices,
            testFraction_data,
        ] = pickle.load(f)
        return trainingdata_details, trainIndices, testIndices, testFraction_data


def get_immediate_subdirectories(a_dir):
    """Get list of immediate subdirectories"""
    return [
        name for name in os.listdir(a_dir) if os.path.isdir(os.path.join(a_dir, name))
    ]


def grab_files_in_folder(folder, ext="", relative=True):
    """Return the paths of files with extension *ext* present in *folder*."""
    for file in os.listdir(folder):
        if file.endswith(ext):
            yield file if relative else os.path.join(folder, file)


def filter_files_by_patterns(
    folder: str | Path,
    start_patterns: set[str] | None = None,
    contain_patterns: set[str] | None = None,
    end_patterns: set[str] | None = None,
) -> List[Path]:
    """
    Filters files in a folder based on start, contain, and end patterns.

    Args:
        folder (str | Path): The folder to search for files.

        start_patterns (Set[str] | None): Patterns the filenames should start with.
            If None or empty, this pattern is not taken into account.

        contain_patterns (set[str]): Patterns the filenames should contain.
            If None or empty, this pattern is not taken into account.

        end_patterns (set[str]): Patterns the filenames should end with.
            If None or empty, this pattern is not taken into account.

    Returns:
        List[Path]: List of files that match the criteria.
    """
    folder = Path(folder)  # Ensure the folder is a Path object
    if not folder.is_dir():
        raise ValueError(f"{folder} is not a valid directory.")

    # Filter files based on the given patterns
    matching_files = [
        file
        for file in folder.iterdir()
        if file.is_file()
        and (
            not start_patterns
            or any(file.name.startswith(start) for start in start_patterns)
        )
        and (
            not contain_patterns
            or any(contain in file.name for contain in contain_patterns)
        )
        and (not end_patterns or any(file.name.endswith(end) for end in end_patterns))
    ]

    return matching_files


def get_video_list(filename, videopath, videtype):
    """Get list of videos in a path (if filetype == all), otherwise just a specific file."""
    videos = list(grab_files_in_folder(videopath, videtype))
    if filename == "all":
        return videos
    else:
        if filename in videos:
            videos = [filename]
        else:
            videos = []
            print("Video not found!", filename)
    return videos


## Various functions to get filenames, foldernames etc. based on configuration parameters.
def get_training_set_folder(cfg: dict) -> Path:
    """Training Set folder for config file based on parameters"""
    Task = cfg["Task"]
    date = cfg["date"]
    iterate = "iteration-" + str(cfg["iteration"])
    return Path(
        os.path.join("training-datasets", iterate, "UnaugmentedDataSet_" + Task + date)
    )


def get_data_and_metadata_filenames(trainingsetfolder, trainFraction, shuffle, cfg):
    # Filename for metadata and data relative to project path for corresponding parameters
    metadatafn = os.path.join(
        str(trainingsetfolder),
        "Documentation_data-"
        + cfg["Task"]
        + "_"
        + str(int(trainFraction * 100))
        + "shuffle"
        + str(shuffle)
        + ".pickle",
    )
    datafn = os.path.join(
        str(trainingsetfolder),
        cfg["Task"]
        + "_"
        + cfg["scorer"]
        + str(int(100 * trainFraction))
        + "shuffle"
        + str(shuffle)
        + ".mat",
    )

    return datafn, metadatafn


def get_model_folder(
    trainFraction: float,
    shuffle: int,
    cfg: dict,
    modelprefix: str = "",
    engine: Engine = Engine.TF,
) -> Path:
    """
    Args:
        trainFraction: the training fraction (as defined in the project configuration)
            for which to get the model folder
        shuffle: the index of the shuffle for which to get the model folder
        cfg: the project configuration
        modelprefix: The name of the folder
        engine: The engine for which we want the model folder. Defaults to `tensorflow`
            for backwards compatibility with DeepLabCut 2.X

    Returns:
        the relative path from the project root to the folder containing the model files
        for a shuffle (configuration files, snapshots, training logs, ...)
    """
    proj_id = f"{cfg['Task']}{cfg['date']}"
    return Path(
        modelprefix,
        engine.model_folder_name,
        f"iteration-{cfg['iteration']}",
        f"{proj_id}-trainset{int(trainFraction * 100)}shuffle{shuffle}",
    )


def get_evaluation_folder(
    trainFraction: float,
    shuffle: int,
    cfg: dict,
    engine: Engine | None = None,
    modelprefix: str = "",
) -> Path:
    """
    Args:
        trainFraction: the training fraction (as defined in the project configuration)
            for which to get the evaluation folder
        shuffle: the index of the shuffle for which to get the evaluation folder
        cfg: the project configuration
        engine: The engine for which we want the model folder. Defaults to None,
            which automatically gets the engine for the shuffle from the training
            dataset metadata file.
        modelprefix: The name of the folder

    Returns:
        the relative path from the project root to the folder containing the model files
        for a shuffle (configuration files, snapshots, training logs, ...)
    """
    if engine is None:
        from deeplabcut.generate_training_dataset.metadata import get_shuffle_engine

        engine = get_shuffle_engine(
            cfg=cfg,
            trainingsetindex=cfg["TrainingFraction"].index(trainFraction),
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    Task = cfg["Task"]
    date = cfg["date"]
    iterate = "iteration-" + str(cfg["iteration"])
    if "eval_prefix" in cfg:
        eval_prefix = cfg["eval_prefix"]
    else:
        eval_prefix = engine.results_folder_name
    return Path(
        modelprefix,
        eval_prefix,
        iterate,
        Task
        + date
        + "-trainset"
        + str(int(trainFraction * 100))
        + "shuffle"
        + str(shuffle),
    )


def get_snapshots_from_folder(train_folder: Path) -> List[str]:
    """
    Returns an ordered list of existing snapshot names in the train folder, sorted by
    increasing training iterations.

    Raises:
        FileNotFoundError: if no snapshot_names are found in the train_folder.
    """
    snapshot_names = [
        file.stem for file in train_folder.iterdir() if "index" in file.name
    ]

    if len(snapshot_names) == 0:
        raise FileNotFoundError(
            f"No snapshots were found in {train_folder}! Please ensure the network has "
            f"been trained and verify the iteration, shuffle and trainFraction are "
            f"correct."
        )

    # sort in ascending order of iteration number
    return sorted(snapshot_names, key=lambda name: int(name.split("-")[1]))


def get_deeplabcut_path():
    """Get path of where deeplabcut is currently running"""
    import importlib.util

    return os.path.split(importlib.util.find_spec("deeplabcut").origin)[0]


def intersection_of_body_parts_and_ones_given_by_user(cfg, comparisonbodyparts):
    """Returns all body parts when comparisonbodyparts=='all', otherwise all bpts that are in the intersection of comparisonbodyparts and the actual bodyparts"""
    # if "MULTI!" in allbpts:
    if cfg["multianimalproject"]:
        allbpts = cfg["multianimalbodyparts"] + cfg["uniquebodyparts"]
    else:
        allbpts = cfg["bodyparts"]

    if comparisonbodyparts == "all":
        return list(allbpts)
    else:  # take only items in list that are actually bodyparts...
        cpbpts = [bp for bp in allbpts if bp in comparisonbodyparts]
        # Ensure same order as in config.yaml
        return cpbpts


def get_labeled_data_folder(cfg, video):
    videoname = os.path.splitext(os.path.basename(video))[0]
    return os.path.join(cfg["project_path"], "labeled-data", videoname)


def form_data_containers(df, bodyparts):
    mask = df.columns.get_level_values("bodyparts").isin(bodyparts)
    df_masked = df.loc[:, mask]
    df_likelihood = df_masked.xs("likelihood", level=-1, axis=1).values.T
    df_x = df_masked.xs("x", level=-1, axis=1).values.T
    df_y = df_masked.xs("y", level=-1, axis=1).values.T
    return df_x, df_y, df_likelihood


def get_scorer_name(
    cfg: dict,
    shuffle: int,
    trainFraction: float,
    trainingsiterations: str | int = "unknown",
    modelprefix: str = "",
    engine: Engine | None = None,
):
    """Extract the scorer/network name for a particular shuffle, training fraction, etc.
    If the engine is not specified, determines which to use from
    Returns tuple of DLCscorer, DLCscorerlegacy (old naming convention)
    """
    if engine is None:
        from deeplabcut.generate_training_dataset.metadata import get_shuffle_engine

        engine = get_shuffle_engine(
            cfg=cfg,
            trainingsetindex=cfg["TrainingFraction"].index(trainFraction),
            shuffle=shuffle,
            modelprefix=modelprefix,
        )

    if engine == Engine.PYTORCH:
        from deeplabcut.pose_estimation_pytorch.apis.utils import get_scorer_name

        snapshot_index = None
        if isinstance(trainingsiterations, int):
            snapshot_index = trainingsiterations

        dlc3_scorer = get_scorer_name(
            cfg=cfg,
            shuffle=shuffle,
            train_fraction=trainFraction,
            snapshot_index=snapshot_index,
            detector_index=None,
            modelprefix=modelprefix,
        )
        return dlc3_scorer, dlc3_scorer

    Task = cfg["Task"]
    date = cfg["date"]

    if trainingsiterations == "unknown":
        snapshotindex = get_snapshot_index_for_scorer(
            "snapshotindex", cfg["snapshotindex"]
        )
        model_folder = get_model_folder(
            trainFraction, shuffle, cfg, engine=engine, modelprefix=modelprefix
        )
        train_folder = Path(cfg["project_path"]) / model_folder / "train"
        snapshot_names = get_snapshots_from_folder(train_folder)
        snapshot_name = snapshot_names[snapshotindex]
        trainingsiterations = (snapshot_name.split(os.sep)[-1]).split("-")[-1]

    dlc_cfg = read_plainconfig(
        os.path.join(
            cfg["project_path"],
            str(
                get_model_folder(
                    trainFraction, shuffle, cfg, engine=engine, modelprefix=modelprefix
                )
            ),
            "train",
            engine.pose_cfg_name,
        )
    )
    # ABBREVIATE NETWORK NAMES -- esp. for mobilenet!
    if "resnet" in dlc_cfg["net_type"]:
        if dlc_cfg.get("multi_stage", False):
            netname = "dlcrnetms5"
        else:
            netname = dlc_cfg["net_type"].replace("_", "")
    elif "mobilenet" in dlc_cfg["net_type"]:  # mobilenet >> mobnet_100; mobnet_35 etc.
        netname = "mobnet_" + str(int(float(dlc_cfg["net_type"].split("_")[-1]) * 100))
    elif "efficientnet" in dlc_cfg["net_type"]:
        netname = "effnet_" + dlc_cfg["net_type"].split("-")[1]
    else:
        raise ValueError(f"Failed to abbreviate network name: {dlc_cfg['net_type']}")

    scorer = (
        "DLC_"
        + netname
        + "_"
        + Task
        + str(date)
        + "shuffle"
        + str(shuffle)
        + "_"
        + str(trainingsiterations)
    )
    # legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!
    # scorer_legacy = 'DeepCut' + "_resnet" + str(cfg['resnet']) + "_" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)
    scorer_legacy = scorer.replace("DLC", "DeepCut")
    return scorer, scorer_legacy


def check_if_post_processing(
    folder, vname, DLCscorer, DLCscorerlegacy, suffix="filtered"
):
    """Checks if filtered/bone lengths were already calculated. If not, figures
    out if data was already analyzed (either with legacy scorer name or new one!)"""
    outdataname = os.path.join(folder, vname + DLCscorer + suffix + ".h5")
    sourcedataname = os.path.join(folder, vname + DLCscorer + ".h5")
    if os.path.isfile(outdataname):  # was data already processed?
        if suffix == "filtered":
            print("Video already filtered...", outdataname)
        elif suffix == "_skeleton":
            print("Skeleton in video already processed...", outdataname)

        return False, outdataname, sourcedataname, DLCscorer
    else:
        odn = os.path.join(folder, vname + DLCscorerlegacy + suffix + ".h5")
        if os.path.isfile(odn):  # was it processed by DLC <2.1 project?
            if suffix == "filtered":
                print("Video already filtered...(with DLC<2.1)!", odn)
            elif suffix == "_skeleton":
                print("Skeleton in video already processed... (with DLC<2.1)!", odn)
            return False, odn, odn, DLCscorerlegacy
        else:
            sdn = os.path.join(folder, vname + DLCscorerlegacy + ".h5")
            tracks = sourcedataname.replace(".h5", "tracks.h5")
            if os.path.isfile(sourcedataname):  # Was the video already analyzed?
                return True, outdataname, sourcedataname, DLCscorer
            elif os.path.isfile(sdn):  # was it analyzed with DLC<2.1?
                return True, odn, sdn, DLCscorerlegacy
            elif os.path.isfile(tracks):  # May be a MA project with tracklets
                return True, tracks.replace(".h5", f"{suffix}.h5"), tracks, DLCscorer
            else:
                print("Video not analyzed -- Run analyze_videos first.")
                return False, outdataname, sourcedataname, DLCscorer


def check_if_not_analyzed(destfolder, vname, DLCscorer, DLCscorerlegacy, flag="video"):
    h5files = list(grab_files_in_folder(destfolder, "h5", relative=False))
    if not len(h5files):
        dataname = os.path.join(destfolder, vname + DLCscorer + ".h5")
        return True, dataname, DLCscorer

    # Iterate over data files and stop as soon as one matching the scorer is found
    for h5file in h5files:
        if vname + DLCscorer in Path(h5file).stem:
            if flag == "video":
                print("Video already analyzed!", h5file)
            elif flag == "framestack":
                print("Frames already analyzed!", h5file)
            return False, h5file, DLCscorer
        elif vname + DLCscorerlegacy in Path(h5file).stem:
            if flag == "video":
                print("Video already analyzed!", h5file)
            elif flag == "framestack":
                print("Frames already analyzed!", h5file)
            return False, h5file, DLCscorerlegacy

    # If there was no match...
    dataname = os.path.join(destfolder, vname + DLCscorer + ".h5")
    return True, dataname, DLCscorer


def check_if_not_evaluated(folder, DLCscorer, DLCscorerlegacy, snapshot):
    dataname = os.path.join(folder, DLCscorer + "-" + str(snapshot) + ".h5")
    if os.path.isfile(dataname):
        print("This net has already been evaluated!")
        return False, dataname, DLCscorer
    else:
        dn = os.path.join(folder, DLCscorerlegacy + "-" + str(snapshot) + ".h5")
        if os.path.isfile(dn):
            print("This net has already been evaluated (with DLC<2.1)!")
            return False, dn, DLCscorerlegacy
        else:
            return True, dataname, DLCscorer


def find_video_full_data(folder, videoname, scorer):
    scorer_legacy = scorer.replace("DLC", "DeepCut")
    full_files = filter_files_by_patterns(
        folder=folder,
        start_patterns={videoname + scorer, videoname + scorer_legacy},
        contain_patterns={"full"},
        end_patterns={"pickle"},
    )
    if not full_files:
        raise FileNotFoundError(
            f"No full data found in {folder} "
            f"for video {videoname} and scorer {scorer}."
        )
    return full_files[0]


def find_video_metadata(folder, videoname, scorer):
    """For backward compatibility, let us search the substring 'meta'"""
    scorer_legacy = scorer.replace("DLC", "DeepCut")
    meta_files = filter_files_by_patterns(
        folder=folder,
        start_patterns={videoname + scorer, videoname + scorer_legacy},
        contain_patterns={"meta"},
        end_patterns={"pickle"},
    )
    if not meta_files:
        raise FileNotFoundError(
            f"No metadata found in {folder} "
            f"for video {videoname} and scorer {scorer}."
        )
    return meta_files[0]


def load_video_metadata(folder, videoname, scorer):
    return read_pickle(find_video_metadata(folder, videoname, scorer))


def load_video_full_data(folder, videoname, scorer):
    return read_pickle(find_video_full_data(folder, videoname, scorer))


def find_analyzed_data(folder, videoname, scorer, filtered=False, track_method=""):
    """Find potential data files from the hints given to the function."""
    scorer_legacy = scorer.replace("DLC", "DeepCut")
    suffix = "_filtered" if filtered else ""
    tracker = TRACK_METHODS.get(track_method, "")

    candidates = []
    for file in grab_files_in_folder(folder, "h5"):
        stem = Path(file).stem.replace("_filtered", "")
        starts_by_scorer = file.startswith(videoname + scorer) or file.startswith(
            videoname + scorer_legacy
        )
        if tracker:
            matches_tracker = stem.endswith(tracker)
        else:
            matches_tracker = not any(stem.endswith(s) for s in TRACK_METHODS.values())
        if all(
            (
                starts_by_scorer,
                "skeleton" not in file,
                matches_tracker,
                (filtered and "filtered" in file)
                or (not filtered and "filtered" not in file),
            )
        ):
            candidates.append(file)

    if not len(candidates):
        msg = (
            f'No {"un" if not filtered else ""}filtered data file found in {folder} '
            f"for video {videoname} and scorer {scorer}"
        )
        if track_method:
            msg += f" and {track_method} tracker"
        msg += "."
        raise FileNotFoundError(msg)

    n_candidates = len(candidates)
    if n_candidates > 1:  # This should not be happening anyway...
        print(
            f"{n_candidates} possible data files were found: {candidates}.\n"
            f"Picking the first by default..."
        )
    filepath = os.path.join(folder, candidates[0])
    scorer = scorer if scorer in filepath else scorer_legacy
    return filepath, scorer, suffix


def load_analyzed_data(folder, videoname, scorer, filtered=False, track_method=""):
    filepath, scorer, suffix = find_analyzed_data(
        folder, videoname, scorer, filtered, track_method
    )
    df = pd.read_hdf(filepath)
    return df, filepath, scorer, suffix


def load_detection_data(video, scorer, track_method):
    folder = os.path.dirname(video)
    videoname = os.path.splitext(os.path.basename(video))[0]
    if track_method == "skeleton":
        tracker = "sk"
    elif track_method == "box":
        tracker = "bx"
    elif track_method == "ellipse":
        tracker = "el"
    else:
        raise ValueError(f"Unrecognized track_method={track_method}")

    filepath = os.path.splitext(video)[0] + scorer + f"_{tracker}.pickle"
    if not os.path.isfile(filepath):
        raise FileNotFoundError(
            f"No detection data found in {folder} for video {videoname}, "
            f"scorer {scorer}, and tracker {track_method}"
        )
    return read_pickle(filepath)


def find_next_unlabeled_folder(config_path, verbose=False):
    cfg = read_config(config_path)
    base_folder = Path(os.path.join(cfg["project_path"], "labeled-data"))
    h5files = sorted(
        base_folder.rglob("*.h5"),
        key=lambda p: p.lstat().st_mtime,
        reverse=True,
    )
    folders = sorted(f for f in base_folder.iterdir() if f.is_dir())
    most_recent_folder = h5files[0].parent
    ind = folders.index(most_recent_folder)
    next_folder = folders[min(ind + 1, len(folders) - 1)]
    if verbose:  # Print some stats about data completeness
        print("Data completeness\n-----------------")
        for folder in folders:
            dfs = []
            for file in folder.rglob("*.h5"):
                dfs.append(pd.read_hdf(file))
            if dfs:
                df = pd.concat(dfs)
                frac = (~df.isna()).sum().sum() / df.size
                print(f"{folder.name} | {int(100 * frac)} %")
    return next_folder


def get_snapshot_index_for_scorer(name: str, index: int | str) -> int:
    if index == "all":
        print(
            f"Changing {name} to the last one -- plotting, videomaking, etc. should "
            "not be performed for all indices. For more selectivity enter the ordinal "
            "number of the snapshot you want (ie. 4 for the fifth) in the config file."
        )
        return -1

    return index


# aliases for backwards-compatibility.
SaveData = save_data
SaveMetadata = save_metadata
LoadMetadata = load_metadata
GetVideoList = get_video_list
GetTrainingSetFolder = get_training_set_folder
GetDataandMetaDataFilenames = get_data_and_metadata_filenames
IntersectionofBodyPartsandOnesGivenbyUser = (
    intersection_of_body_parts_and_ones_given_by_user
)
GetScorerName = get_scorer_name
CheckifPostProcessing = check_if_post_processing
CheckifNotAnalyzed = check_if_not_analyzed
CheckifNotEvaluated = check_if_not_evaluated
GetEvaluationFolder = get_evaluation_folder
GetModelFolder = get_model_folder


--- File: deeplabcut/utils/auxfun_multianimal.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""

import math
import os
import pickle
import random
import shelve
import warnings
from itertools import combinations
from pathlib import Path

import networkx as nx
import numpy as np
import pandas as pd

from deeplabcut.utils import auxiliaryfunctions, conversioncode
from deeplabcut.generate_training_dataset import trainingsetmanipulation
from deeplabcut.core.trackingutils import TRACK_METHODS


def reorder_individuals_in_df(df: pd.DataFrame, order: list) -> pd.DataFrame:
    """
    Reorders data of df to match the order given in a list

    Parameters:
    ----------
    df: pd.DataFrame
        Data from tracked .h5 file
    order: list of str
        Desired order of individuals

    Return:
    -------
        df: pd.DataFrame
            Reordered DataFrame
    """
    columns = df.columns
    inds = df.index

    data = df.loc(axis=1)[:, order].to_numpy()
    df = pd.DataFrame(data, columns=columns, index=inds)

    return df


def extractindividualsandbodyparts(cfg):
    individuals = cfg["individuals"].copy()
    if len(cfg["uniquebodyparts"]) > 0:
        individuals.append("single")
    return individuals, cfg["uniquebodyparts"], cfg["multianimalbodyparts"]


def get_track_method(cfg, track_method=""):
    if cfg.get("multianimalproject", False):
        if track_method != "":
            # check if it exists:
            if track_method not in TRACK_METHODS:
                raise ValueError(
                    f"Invalid tracking method. Only {', '.join(TRACK_METHODS)} are currently supported."
                )
            return track_method
        else:  # default
            track_method = cfg.get("default_track_method", "")
            if not track_method:
                warnings.warn(
                    "default_track_method` is undefined in the config.yaml file and will be set to `ellipse`."
                )
                track_method = "ellipse"
                cfg["default_track_method"] = track_method
                auxiliaryfunctions.write_config(
                    str(Path(cfg["project_path"]) / "config.yaml"), cfg
                )
            return track_method

    else:  # no tracker for single-animal projects
        return ""


def IntersectionofIndividualsandOnesGivenbyUser(cfg, individuals):
    """Returns all individuals when set to 'all', otherwise all bpts that are in the intersection of comparisonbodyparts and the actual bodyparts"""
    if "individuals" not in cfg:  # Not a multi-animal project...
        return [""]
    all_indivs = extractindividualsandbodyparts(cfg)[0]
    if individuals == "all":
        return all_indivs
    else:  # take only items in list that are actually bodyparts...
        return [ind for ind in individuals if ind in all_indivs]


def filter_unwanted_paf_connections(cfg, paf_graph):
    """Get rid of skeleton connections between multi and unique body parts."""
    multi = extractindividualsandbodyparts(cfg)[2]
    desired = list(combinations(range(len(multi)), 2))
    return [i for i, edge in enumerate(paf_graph) if tuple(edge) not in desired]


def validate_paf_graph(cfg, paf_graph):
    multianimalbodyparts = extractindividualsandbodyparts(cfg)[2]
    connected = set()
    for bpt1, bpt2 in paf_graph:
        connected.add(bpt1)
        connected.add(bpt2)
    unconnected = set(range(len(multianimalbodyparts))).difference(connected)
    if unconnected and len(multianimalbodyparts) > 1:  # for single bpt not important!
        raise ValueError(
            f'Unconnected {", ".join(multianimalbodyparts[i] for i in unconnected)}. '
            f"For multi-animal projects, all multianimalbodyparts should be connected. "
            f"Ideally there should be at least one (multinode) path from each multianimalbodyparts to each other multianimalbodyparts. "
        )


def prune_paf_graph(list_of_edges, desired_n_edges=None, average_degree=None):
    if not (desired_n_edges or average_degree):
        raise ValueError(
            "Either `desired_n_edges` or `average_degree` must be specified."
        )

    G = nx.Graph(list_of_edges)
    n_edges = len(G.edges)
    n_nodes = len(G.nodes)
    if average_degree is not None:
        # (average_degree / 2) as many edges as there are nodes is required
        # for undirected graphs to reach the target degree.
        desired_n_edges = math.ceil(n_nodes * average_degree / 2)
    if not n_nodes - 1 <= desired_n_edges < n_edges:
        raise ValueError(
            f"""`desired_n_edges` should be greater than or equal to {n_nodes - 1},
            but smaller than {n_edges}."""
        )

    while True:
        g = nx.Graph(random.sample(list(G.edges), desired_n_edges))
        if len(g.nodes) == n_nodes and nx.is_connected(g):
            print("Valid subgraph found...")
            break
    return [sorted(edge) for edge in g.edges]


def getpafgraph(cfg, printnames=True):
    """Auxiliary function that turns skeleton (list of connected bodypart pairs)
    into a list of corresponding indices (with regard to the stacked multianimal/uniquebodyparts)

    Convention: multianimalbodyparts go first!
    """
    individuals, uniquebodyparts, multianimalbodyparts = extractindividualsandbodyparts(
        cfg
    )
    # Attention this order has to be consistent (for training set creation, training, inference etc.)

    bodypartnames = multianimalbodyparts + uniquebodyparts
    lookupdict = {bodypartnames[j]: j for j in range(len(bodypartnames))}

    if cfg["skeleton"] is None:
        cfg["skeleton"] = []

    connected = set()
    partaffinityfield_graph = []
    for link in cfg["skeleton"]:
        if link[0] in bodypartnames and link[1] in bodypartnames:
            bp1 = int(lookupdict[link[0]])
            bp2 = int(lookupdict[link[1]])
            connected.add(bp1)
            connected.add(bp2)
            partaffinityfield_graph.append([bp1, bp2])
        else:
            print("Attention, parts do not exist!", link)

    if printnames:
        graph2names(cfg, partaffinityfield_graph)

    return partaffinityfield_graph


def graph2names(cfg, partaffinityfield_graph):
    individuals, uniquebodyparts, multianimalbodyparts = extractindividualsandbodyparts(
        cfg
    )
    bodypartnames = multianimalbodyparts + uniquebodyparts
    for pair in partaffinityfield_graph:
        print(pair, bodypartnames[pair[0]], bodypartnames[pair[1]])


def SaveFullMultiAnimalData(data, metadata, dataname, suffix="_full"):
    """Save predicted data as h5 file and metadata as pickle file; created by predict_videos.py"""
    data_path = dataname.split(".h5")[0] + suffix + ".pickle"
    metadata_path = dataname.split(".h5")[0] + "_meta.pickle"

    with open(data_path, "wb") as f:
        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)
    with open(metadata_path, "wb") as f:
        pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)
    return data_path, metadata_path


def LoadFullMultiAnimalData(dataname):
    """Save predicted data as h5 file and metadata as pickle file; created by predict_videos.py"""
    data_file = dataname.split(".h5")[0] + "_full.pickle"
    try:
        with open(data_file, "rb") as handle:
            data = pickle.load(handle)
    except (pickle.UnpicklingError, FileNotFoundError):
        data = shelve.open(data_file, flag="r")
    with open(data_file.replace("_full.", "_meta."), "rb") as handle:
        metadata = pickle.load(handle)
    return data, metadata


def returnlabelingdata(config):
    """Returns a specific labeleing data set -- the user will be asked which one."""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / "labeled-data" / Path(i) for i in video_names]
    for folder in folders:
        print("Do you want to get the data for folder:", folder, "?")
        askuser = input("yes/no")
        if (
            askuser == "y" or askuser == "yes" or askuser == "Ja" or askuser == "ha"
        ):  # multilanguage support :)
            fn = os.path.join(str(folder), "CollectedData_" + cfg["scorer"] + ".h5")
            Data = pd.read_hdf(fn)
            return Data


def convert2_maDLC(config, userfeedback=True, forceindividual=None):
    """
    Converts single animal annotation file into a multianimal annotation file,
    by introducing an individuals column with either the first individual
    in individuals list in config.yaml or whatever is passed via "forceindividual".

    ----------
    config : string
        Full path of the config.yaml file as a string.

    userfeedback: bool, optional
            If this is set to false during automatic mode then frames for all videos are extracted. The user can set this to true, which will result in a dialog,
            where the user is asked for each video if (additional/any) frames from this video should be extracted. Use this, e.g. if you have already labeled
            some folders and want to extract data for new videos.

    forceindividual: None default
            If a string is given that is used in the individuals column.

    Examples
    --------
    Converts mulianimalbodyparts under the 'first individual' in individuals list in config.yaml
    and uniquebodyparts under 'single'
    >>> deeplabcut.convert2_maDLC('/socialrearing-task/config.yaml')

    --------
    Converts mulianimalbodyparts under the individual label mus17 and uniquebodyparts under 'single'
    >>> deeplabcut.convert2_maDLC('/socialrearing-task/config.yaml', forceindividual='mus17')
    """

    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [trainingsetmanipulation._robust_path_split(i)[1] for i in videos]
    folders = [Path(config).parent / "labeled-data" / Path(i) for i in video_names]

    individuals, uniquebodyparts, multianimalbodyparts = extractindividualsandbodyparts(
        cfg
    )

    if forceindividual is None:
        if len(individuals) == 0:
            print("At least one individual should exist...")
            folders = []
            forceindividual = ""
        else:
            forceindividual = individuals[0]  # note that single is added at then end!

        if forceindividual == "single":  # no specific individual ()
            if len(multianimalbodyparts) > 0:  # there should be an individual name...
                print(
                    "At least one individual should exist beyond 'single', as there are multianimalbodyparts..."
                )
                folders = []

    for folder in folders:
        if userfeedback == True:
            print("Do you want to convert the annotation file in folder:", folder, "?")
            askuser = input("yes/no")
        else:
            askuser = "yes"

        if (
            askuser == "y" or askuser == "yes" or askuser == "Ja" or askuser == "ha"
        ):  # multilanguage support :)
            fn = os.path.join(str(folder), "CollectedData_" + cfg["scorer"])
            Data = pd.read_hdf(fn + ".h5")
            conversioncode.guarantee_multiindex_rows(Data)
            imindex = Data.index

            print("This is a single animal data set, converting to multi...", folder)

            # -> adding (single,bpt) for uniquebodyparts
            for j, bpt in enumerate(uniquebodyparts):
                index = pd.MultiIndex.from_arrays(
                    np.array(
                        [2 * [cfg["scorer"]], 2 * ["single"], 2 * [bpt], ["x", "y"]]
                    ),
                    names=["scorer", "individuals", "bodyparts", "coords"],
                )

                if bpt in Data[cfg["scorer"]].keys():
                    frame = pd.DataFrame(
                        Data[cfg["scorer"]][bpt].values, columns=index, index=imindex
                    )
                else:
                    frame = pd.DataFrame(
                        np.ones((len(imindex), 2)) * np.nan,
                        columns=index,
                        index=imindex,
                    )

                if j == 0:
                    dataFrame = frame
                else:
                    dataFrame = pd.concat([dataFrame, frame], axis=1)

            if len(uniquebodyparts) == 0:
                dataFrame = None

            # -> adding (individual,bpt) for multianimalbodyparts
            for j, bpt in enumerate(multianimalbodyparts):
                index = pd.MultiIndex.from_arrays(
                    np.array(
                        [
                            2 * [cfg["scorer"]],
                            2 * [str(forceindividual)],
                            2 * [bpt],
                            ["x", "y"],
                        ]
                    ),
                    names=["scorer", "individuals", "bodyparts", "coords"],
                )

                if bpt in Data[cfg["scorer"]].keys():
                    frame = pd.DataFrame(
                        Data[cfg["scorer"]][bpt].values, columns=index, index=imindex
                    )
                else:
                    frame = pd.DataFrame(
                        np.ones((len(imindex), 2)) * np.nan,
                        columns=index,
                        index=imindex,
                    )

                if j == 0 and dataFrame is None:
                    dataFrame = frame
                else:
                    dataFrame = pd.concat([dataFrame, frame], axis=1)

            Data.to_hdf(
                fn + "singleanimal.h5",
                "df_with_missing",
            )
            Data.to_csv(fn + "singleanimal.csv")

            dataFrame.to_hdf(fn + ".h5", "df_with_missing")
            dataFrame.to_csv(fn + ".csv")


def convert_single2multiplelegacyAM(config, userfeedback=True, target=None):
    """Convert multi animal to single animal code and vice versa. Note that by providing target='single'/'multi' this will be target!"""
    cfg = auxiliaryfunctions.read_config(config)
    videos = cfg["video_sets"].keys()
    video_names = [Path(i).stem for i in videos]
    folders = [Path(config).parent / "labeled-data" / Path(i) for i in video_names]

    prefixes, uniquebodyparts, multianimalbodyparts = extractindividualsandbodyparts(
        cfg
    )
    for folder in folders:
        if userfeedback == True:
            print("Do you want to convert the annotation file in folder:", folder, "?")
            askuser = input("yes/no")
        else:
            askuser = "yes"

        if (
            askuser == "y" or askuser == "yes" or askuser == "Ja" or askuser == "ha"
        ):  # multilanguage support :)
            fn = os.path.join(str(folder), "CollectedData_" + cfg["scorer"])
            Data = pd.read_hdf(fn + ".h5")
            conversioncode.guarantee_multiindex_rows(Data)
            imindex = Data.index

            if "individuals" in Data.columns.names and (
                target is None or target == "single"
            ):
                print("This is a multianimal data set, converting to single...", folder)
                for prfxindex, prefix in enumerate(prefixes):
                    if prefix == "single":
                        for j, bpt in enumerate(uniquebodyparts):
                            index = pd.MultiIndex.from_product(
                                [[cfg["scorer"]], [bpt], ["x", "y"]],
                                names=["scorer", "bodyparts", "coords"],
                            )
                            frame = pd.DataFrame(
                                Data[cfg["scorer"]][prefix][bpt].values,
                                columns=index,
                                index=imindex,
                            )
                            if j == 0:
                                dataFrame = frame
                            else:
                                dataFrame = pd.concat([dataFrame, frame], axis=1)
                    else:
                        for j, bpt in enumerate(multianimalbodyparts):
                            index = pd.MultiIndex.from_product(
                                [[cfg["scorer"]], [prefix + bpt], ["x", "y"]],
                                names=["scorer", "bodyparts", "coords"],
                            )
                            frame = pd.DataFrame(
                                Data[cfg["scorer"]][prefix][bpt].values,
                                columns=index,
                                index=imindex,
                            )
                            if j == 0:
                                dataFrame = frame
                            else:
                                dataFrame = pd.concat([dataFrame, frame], axis=1)
                    if prfxindex == 0:
                        DataFrame = dataFrame
                    else:
                        DataFrame = pd.concat([DataFrame, dataFrame], axis=1)

                Data.to_hdf(
                    fn + "multianimal.h5",
                    "df_with_missing",
                )
                Data.to_csv(fn + "multianimal.csv")

                DataFrame.to_hdf(
                    fn + ".h5",
                    "df_with_missing",
                )
                DataFrame.to_csv(fn + ".csv")
            elif target is None or target == "multi":
                print(
                    "This is a single animal data set, converting to multi...", folder
                )
                for prfxindex, prefix in enumerate(prefixes):
                    if prefix == "single":
                        if cfg["uniquebodyparts"] != [None]:
                            for j, bpt in enumerate(uniquebodyparts):
                                index = pd.MultiIndex.from_arrays(
                                    np.array(
                                        [
                                            2 * [cfg["scorer"]],
                                            2 * [prefix],
                                            2 * [bpt],
                                            ["x", "y"],
                                        ]
                                    ),
                                    names=[
                                        "scorer",
                                        "individuals",
                                        "bodyparts",
                                        "coords",
                                    ],
                                )
                                if bpt in Data[cfg["scorer"]].keys():
                                    frame = pd.DataFrame(
                                        Data[cfg["scorer"]][bpt].values,
                                        columns=index,
                                        index=imindex,
                                    )
                                else:  # fill with nans...
                                    frame = pd.DataFrame(
                                        np.ones((len(imindex), 2)) * np.nan,
                                        columns=index,
                                        index=imindex,
                                    )

                                if j == 0:
                                    dataFrame = frame
                                else:
                                    dataFrame = pd.concat([dataFrame, frame], axis=1)
                        else:
                            dataFrame = None
                    else:
                        for j, bpt in enumerate(multianimalbodyparts):
                            index = pd.MultiIndex.from_arrays(
                                np.array(
                                    [
                                        2 * [cfg["scorer"]],
                                        2 * [prefix],
                                        2 * [bpt],
                                        ["x", "y"],
                                    ]
                                ),
                                names=["scorer", "individuals", "bodyparts", "coords"],
                            )
                            if prefix + "_" + bpt in Data[cfg["scorer"]].keys():
                                frame = pd.DataFrame(
                                    Data[cfg["scorer"]][prefix + "_" + bpt].values,
                                    columns=index,
                                    index=imindex,
                                )
                            else:
                                frame = pd.DataFrame(
                                    np.ones((len(imindex), 2)) * np.nan,
                                    columns=index,
                                    index=imindex,
                                )

                            if j == 0:
                                dataFrame = frame
                            else:
                                dataFrame = pd.concat([dataFrame, frame], axis=1)
                    if prfxindex == 0:
                        DataFrame = dataFrame
                    else:
                        DataFrame = pd.concat([DataFrame, dataFrame], axis=1)

                Data.to_hdf(
                    fn + "singleanimal.h5",
                    "df_with_missing",
                )
                Data.to_csv(fn + "singleanimal.csv")

                DataFrame.to_hdf(
                    fn + ".h5",
                    "df_with_missing",
                )
                DataFrame.to_csv(fn + ".csv")


def form_default_inferencecfg(cfg):
    # load defaults
    inferencecfg = auxiliaryfunctions.read_plainconfig(
        os.path.join(auxiliaryfunctions.get_deeplabcut_path(), "inference_cfg.yaml")
    )
    # set project specific parameters:
    inferencecfg["minimalnumberofconnections"] = (
        len(cfg["multianimalbodyparts"]) / 2
    )  # reasonable default
    inferencecfg["topktoretain"] = len(cfg["individuals"])
    return inferencecfg


def check_inferencecfg_sanity(cfg, inferencecfg):
    template = form_default_inferencecfg(cfg)
    missing = [key for key in template if key not in inferencecfg]
    if missing:
        raise KeyError(f'Keys {", ".join(missing)} are missing in the inferencecfg.')


def read_inferencecfg(path_inference_config, cfg):
    """Load inferencecfg or initialize it."""
    try:
        inferencecfg = auxiliaryfunctions.read_plainconfig(str(path_inference_config))
    except FileNotFoundError:
        inferencecfg = form_default_inferencecfg(cfg)
        auxiliaryfunctions.write_plainconfig(
            str(path_inference_config), dict(inferencecfg)
        )
    return inferencecfg


--- File: deeplabcut/utils/pseudo_label.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import glob
import json
import os
from collections import defaultdict
from pathlib import Path

import cv2
import matplotlib.pyplot as plt
import numpy as np
from scipy.optimize import linear_sum_assignment
from scipy.spatial import distance
from scipy.spatial.distance import cdist

import deeplabcut.pose_estimation_pytorch.modelzoo as modelzoo
import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.modelzoo.generalized_data_converter.datasets import (
    MaDLCDataFrame,
    SingleDLCDataFrame,
)
from deeplabcut.pose_estimation_pytorch.apis.utils import get_inference_runners
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import (
    select_device,
    update_config,
)


class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()  # Convert ndarray to list
        return super().default(obj)


def xywh2xyxy(bbox):
    temp_bbox = np.copy(bbox)
    temp_bbox[2:] = temp_bbox[:2] + temp_bbox[2:]
    return temp_bbox


def optimal_match(gts_list, preds_list):
    arranged_preds_list = []
    num_gts = len(gts_list)
    num_preds = len(preds_list)
    cost_matrix = np.zeros((num_gts, num_preds))

    for i in range(num_gts):
        for j in range(num_preds):
            cost_matrix[i, j] = distance.euclidean(
                gts_list[i][..., :2].flatten(), preds_list[j][..., :2].flatten()
            )
    row_ind, col_ind = linear_sum_assignment(cost_matrix)

    return col_ind


def calculate_iou(box1, box2):
    # Unpack the coordinates
    x1_1, y1_1, x2_1, y2_1 = box1
    x1_2, y1_2, x2_2, y2_2 = box2

    # Calculate the coordinates of the intersection rectangle
    inter_x1 = max(x1_1, x1_2)
    inter_y1 = max(y1_1, y1_2)
    inter_x2 = min(x2_1, x2_2)
    inter_y2 = min(y2_1, y2_2)

    # Calculate the width and height of the intersection rectangle
    inter_width = max(0, inter_x2 - inter_x1)
    inter_height = max(0, inter_y2 - inter_y1)

    # Calculate the area of the intersection rectangle
    inter_area = inter_width * inter_height

    # Calculate the area of each bounding box
    area_1 = (x2_1 - x1_1) * (y2_1 - y1_1)
    area_2 = (x2_2 - x1_2) * (y2_2 - y1_2)

    # Calculate the area of the union of the two bounding boxes
    union_area = area_1 + area_2 - inter_area

    # Calculate the IoU
    iou = inter_area / union_area

    return iou


def video_to_frames(input_video, output_folder, cropping: list[int] | None = None):
    # Create the output folder if it doesn't exist
    video = cv2.VideoCapture(str(input_video))
    # Get the frames per second (fps) of the video
    fps = int(video.get(cv2.CAP_PROP_FPS))
    # Initialize a frame counter
    frame_count = 0
    while True:
        # Read a frame from the video
        ret, frame = video.read()
        # Break the loop if we have reached the end of the video
        if not ret:
            break
        # Crop the frame if desired
        if cropping is not None:
            x1, x2, y1, y2 = cropping
            frame = frame[y1:y2, x1:x2]

        # Save the frame as an image file.
        frame_str = str(frame_count).zfill(5)
        frame_file = os.path.join(output_folder, "images", f"frame_{frame_str}.png")
        cv2.imwrite(frame_file, frame)
        # Increment the frame counter
        frame_count += 1
    # Release the video object and close the window (if open)
    video.release()
    # cv2.destroyAllWindows()


def plot_cost_matrix(
    matrix, gt_keypoint_names, pred_keypoint_names, conversion_plot_out_path
):

    matrix /= np.max(matrix)
    fig, ax = plt.subplots()
    heatmap = ax.pcolor(matrix, cmap=plt.cm.Blues, vmin=0, vmax=1)
    ax.set_xticks(np.arange(matrix.shape[1]) + 0.5, minor=False)
    ax.set_yticks(np.arange(matrix.shape[0]) + 0.5, minor=False)
    ax.set_xlim(0, int(matrix.shape[1]))
    ax.set_ylim(0, int(matrix.shape[0]))
    ax.set_yticklabels(pred_keypoint_names, minor=False)
    ax.set_xticklabels(gt_keypoint_names, minor=False)
    ax.set_title("cost matrix")
    plt.xticks(rotation=90)
    fig = plt.gcf()
    fig.tight_layout()

    plt.savefig(conversion_plot_out_path, dpi=300)


def keypoint_matching(
    config_path: str | Path,
    superanimal_name: str,
    model_name: str,
    detector_name: str,
    copy_images: bool = False,
    device: str | None = None,
    train_file: str = "train.json",
):
    """Runs the keypoint matching algorithm for a DeepLabCut project

    Matches project keypoints to SuperAnimal keypoints automatically, by running
    SuperAnimal inference on all images in the dataset

    Args:
        config_path: The path of the DeepLabCut project configuration file.
        superanimal_name: SuperAnimal dataset with which to run keypoint matching.
        model_name: Name of the SuperAnimal pose model architecture with which to run
            keypoint matching
        detector_name: Name of the SuperAnimal detector architecture with which to run
            keypoint matching
        copy_images: When False, symlinks are created for the dataset used for keypoint
            matching. Otherwise, images are copied from the `labeled-data` folder to the
            folder used for keypoint matching.
        device: The device on which to run keypoint matching.
        train_file: The name of the file containing the labels to output.
    """
    config_path = Path(config_path)
    cfg = af.read_config(str(config_path))
    dlc_proj_root = config_path.parent

    if "individuals" in cfg:
        temp_dataset = MaDLCDataFrame(str(dlc_proj_root), "temp_dataset")
        max_individuals = len(cfg["individuals"])
    else:
        temp_dataset = SingleDLCDataFrame(str(dlc_proj_root), "temp_dataset")
        max_individuals = 1

    memory_replay_folder = dlc_proj_root / "memory_replay"
    temp_dataset.materialize(
        str(memory_replay_folder), framework="coco", deepcopy=copy_images
    )

    # run inference on the train set
    config = modelzoo.load_super_animal_config(
        super_animal=superanimal_name,
        model_name=model_name,
        detector_name=detector_name,
    )
    if device is None:
        device = select_device()

    # get the SuperAnimal detector and pose model snapshot paths
    pose_model_path = modelzoo.get_super_animal_snapshot_path(
        dataset=superanimal_name, model_name=model_name,
    )
    detector_path = modelzoo.get_super_animal_snapshot_path(
        dataset=superanimal_name, model_name=detector_name,
    )

    config = update_config(config, max_individuals, device)
    individuals = [f"animal{i}" for i in range(max_individuals)]
    config["metadata"]["individuals"] = individuals
    train_file_path = os.path.join(memory_replay_folder, "annotations", train_file)

    pose_runner, detector_runner = get_inference_runners(
        config,
        snapshot_path=pose_model_path,
        max_individuals=max_individuals,
        num_bodyparts=len(config["metadata"]["bodyparts"]),
        num_unique_bodyparts=0,
        detector_path=detector_path,
    )

    with open(train_file_path, "r") as f:
        train_obj = json.load(f)

    images = train_obj["images"]
    annotations = train_obj["annotations"]
    categories = train_obj["categories"]
    image_name_to_id = {}
    image_id_to_name = {}

    image_name_to_gt = defaultdict(list)
    image_name_to_bbox = defaultdict(list)
    image_id_to_annotations = defaultdict(list)

    for image in images:
        # this only works with relative path as the testing image can be at a different folder
        name = image["file_name"].split(os.sep)[-1]
        image_name_to_id[name] = image["id"]
        image_id_to_name[image["id"]] = name

    for anno in annotations:
        name = image_id_to_name[anno["image_id"]]
        image_name_to_gt[name].append(anno)
        image_name_to_bbox[name].append(anno["bbox"])

    image_ids = set(image_name_to_id.values())
    for anno in annotations:
        image_id = anno["image_id"]
        if anno["image_id"] in image_ids:
            image_id_to_annotations[image_id].append(anno)

    # need to support more image types
    image_extensions = ["*.png", "*.jpg", "*.jpeg", "*.bmp", "*.gif", "*.tiff"]
    images_in_folder = []
    for ext in image_extensions:
        images_in_folder.extend(
            glob.glob(os.path.join(memory_replay_folder, "images", ext))
        )

    corresponded_images = []
    for image in images_in_folder:
        image_path = image
        name = image.split(os.sep)[-1]
        if name in image_name_to_id:
            corresponded_images.append(image_path)

    images = corresponded_images
    bbox_gts = [
        {"bboxes": np.array(image_name_to_bbox[image.split(os.sep)[-1]])}
        for image in images
    ]

    pose_inputs = list(zip(images, bbox_gts))

    # pose inference should return meta data for pseudo labeling
    predictions = pose_runner.inference(pose_inputs)

    with open(str(memory_replay_folder / "pseudo_predictions.json"), "w") as f:
        json.dump(pose_inputs, f, cls=NumpyEncoder)

    assert len(images) == len(predictions)

    image_name_to_pred = {}
    for image_path, prediction in zip(images, predictions):
        name = image_path.split(os.sep)[-1]
        image_name_to_pred[name] = prediction

    pred_keypoint_names = config["metadata"]["bodyparts"]
    num_pred_keypoints = len(pred_keypoint_names)
    gt_keypoint_names = categories[0]["keypoints"]
    num_gt_keypoints = len(gt_keypoint_names)

    match_matrix = np.zeros((num_pred_keypoints, num_gt_keypoints))
    match_dict = defaultdict(lambda: defaultdict(int))

    for name, gts in image_name_to_gt.items():
        bbox_gts = [np.array(gt["bbox"]) for gt in gts]
        bbox_gts = [xywh2xyxy(e) for e in bbox_gts]
        prediction = image_name_to_pred[name]
        bbox_preds = [xywh2xyxy(pred) for pred in prediction["bboxes"]]
        optimal_pred_indices = optimal_match(bbox_gts, bbox_preds)

        for idx in range(len(bbox_gts)):
            if idx == len(optimal_pred_indices):
                break

            optimal_index = optimal_pred_indices[idx]
            matched_gt = np.array(gts[idx]["keypoints"])
            matched_pred = prediction["bodyparts"][optimal_index]
            matched_gt = matched_gt.reshape(num_gt_keypoints, -1)
            matched_pred = matched_pred.reshape(num_pred_keypoints, -1)

            pair_distance = cdist(matched_pred, matched_gt)
            row_ind, column_ind = linear_sum_assignment(pair_distance)
            for row, column in zip(row_ind, column_ind):
                pred_kpt_name = pred_keypoint_names[row]
                anno_kpt_name = gt_keypoint_names[column]
                match_matrix[row][column] += 1
                match_dict[pred_kpt_name][anno_kpt_name] += 1

    row_ind, column_ind = linear_sum_assignment(match_matrix * -1)
    keypoint_mapping_list = []

    conversion_matrix_out_path = os.path.join(
        memory_replay_folder, "confusion_matrix.png"
    )

    plot_cost_matrix(
        match_matrix, gt_keypoint_names, pred_keypoint_names, conversion_matrix_out_path
    )

    for row, column in zip(row_ind, column_ind):
        pred_kpt_name = pred_keypoint_names[row]
        anno_kpt_name = gt_keypoint_names[column]
        count = match_dict[pred_kpt_name][anno_kpt_name]
        keypoint_mapping_list.append((pred_kpt_name, anno_kpt_name, count))

    keypoint_mapping_list = sorted(
        keypoint_mapping_list, key=lambda x: x[2], reverse=True
    )

    names = [e[:2] for e in keypoint_mapping_list]
    conversion_table = {}
    for pred, anno in names:
        conversion_table[pred] = anno

    conversion_table_out_path = os.path.join(
        memory_replay_folder, "conversion_table.csv"
    )
    with open(conversion_table_out_path, "w") as f:
        out = "gt, MasterName\n"
        for name in pred_keypoint_names:
            target = name
            source = conversion_table.get(target, "")
            out += f"{source}, {target}\n"
        f.write(out)


# this is to generate a coco project as an intermediate data
def dlc3predictions_2_annotation_from_video(
    predictions,
    dest_proj_folder,
    bodyparts,
    superanimal_name,
    pose_threshold=0.0,
    bbox_threshold=0.0,
):
    """
    For video adaptation, we also need to create a coco project
    dlc3 predictions:

    list of dictionary
    [{
    bodyparts:[] # (n_individuals, n_kpts, 3)
    bboxes: [] # (n_individuals, 4) -> x,y,w,h
    }]

    coco result is a list of dictionary
    # i might get a minimal version that works with my script

    category_id:
    image_id: []
    image_path: []
    keypoints: []
    score: []
    bbox: []

    """

    category_id = 1  # the default for superanimal. But it might be changed

    images = []
    annotations = []
    categories = []
    annotation_id = 0
    image_folder = os.path.join(dest_proj_folder, "images")

    # video_to_frames function by default outputs png or jpg
    image_paths = sorted(glob.glob(os.path.join(image_folder, "*.png")))

    # skipping every 4 frames should speed up and not impact the performance
    predictions, image_paths = predictions[::10], image_paths[::10]

    # Since the inference API does not return the image path, I assume the predictions are provided in the same order as the frames in the video.
    assert len(image_paths) == len(
        predictions
    ), f"number of images must be equal to number of predictions. image_paths: {len(image_paths)} , predictions: {len(predictions)}"
    new_predictions = []

    num_kpts = len(bodyparts)

    if not superanimal_name.startswith("superanimal_"):
        raise ValueError("not supporting non superanimal model video adaptation yet")

    category_name = superanimal_name[len("superanimal_"):]
    categories = [
        {
            "name": category_name,
            "id": 1,
            "supercategory": "animal",
            "keypoints": bodyparts,
        }
    ]

    assert len(predictions) == len(image_paths)
    imageid2annotations = defaultdict(list)
    for image_id, (prediction, image_path) in enumerate(zip(predictions, image_paths)):
        image_obj = cv2.imread(image_path)
        height, width, channels = image_obj.shape
        imagename = image_path.split(os.sep)[-1]
        image = {
            "id": image_id,
            "file_name": imagename,
            "width": width,
            "height": height,
        }

        # iterate through individuals if there are many

        assert (
            len(prediction["bodyparts"])
            == len(prediction["bboxes"])
            == len(prediction["bbox_scores"])
        )
        for pose, bbox, bbox_score in zip(
            prediction["bodyparts"], prediction["bboxes"], prediction["bbox_scores"]
        ):
            if (
                np.all(np.array(pose) <= 0)
                or len(bbox) == 0
                or bbox_score < bbox_threshold
            ):
                continue
            imageid2annotations[image_id].append(pose)
            pose = np.array(pose)
            bbox = np.array(bbox)

            mask = pose[:, -1] < pose_threshold

            pose[mask] = 0

            # by default all visible
            pose[:, -1] = 2
            bbox_confidence = bbox[-1]

            keypoints = list(pose.reshape(-1))
            keypoints = [float(num) for num in keypoints]
            # bbox here is x,y,w,h from dlc3
            bbox = [float(num) for num in bbox][:4]

            anno = {
                "category_id": int(category_id),
                "keypoints": keypoints,
                "num_keypoints": len(keypoints) // 3,
                "image_id": int(image_id),
                "bbox": bbox,
                "area": float(bbox[-2] * bbox[-3]),
                "iscrowd": 0,
                "id": int(annotation_id),
            }

            annotation_id += 1
            annotations.append(anno)

        # this is to prevent images that do not have annotations
        if len(imageid2annotations[image_id]) > 0:
            images.append(image)

    train_obj = {"images": images, "annotations": annotations, "categories": categories}

    test_annotations = []

    # just use the first 10 image annotations for test
    test_obj = {
        "images": images[:10],
        "annotations": annotations[:10],
        "categories": categories,
    }

    # there is no 'test' split of video adaptation. This is essentially train.json
    with open(os.path.join(dest_proj_folder, "annotations", "test.json"), "w") as f:
        json.dump(test_obj, f, indent=4)

    with open(os.path.join(dest_proj_folder, "annotations", "train.json"), "w") as f:
        json.dump(train_obj, f, indent=4)


--- File: deeplabcut/utils/auxfun_videos.py ---
#!/usr/bin/env python3
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""

import skimage.color
from skimage import io
from skimage.util import img_as_ubyte
import cv2
import datetime
import numpy as np
import os
import subprocess
import warnings


# more videos are in principle covered, as OpenCV is used and allows many formats.
SUPPORTED_VIDEOS = "avi", "mp4", "mov", "mpeg", "mpg", "mpv", "mkv", "flv", "qt", "yuv"


class VideoReader:
    def __init__(self, video_path):
        if not os.path.isfile(video_path):
            raise ValueError(f'Video path "{video_path}" does not point to a file.')
        self.video_path = video_path
        self.video = cv2.VideoCapture(video_path)
        if not self.video.isOpened():
            raise IOError("Video could not be opened; it may be corrupted.")
        self.parse_metadata()
        self._bbox = 0, 1, 0, 1
        self._n_frames_robust = None

    def __repr__(self):
        string = "Video (duration={:0.2f}, fps={}, dimensions={}x{})"
        return string.format(self.calc_duration(), self.fps, *self.dimensions)

    def __len__(self):
        return self._n_frames

    def check_integrity(self):
        dest = os.path.join(self.directory, f"{self.name}.log")
        command = f'ffmpeg -v error -i "{self.video_path}" -f null - 2>"{dest}"'
        subprocess.call(command, shell=True)
        if os.path.getsize(dest) != 0:
            warnings.warn(f'Video contains errors. See "{dest}" for a detailed report.')

    def check_integrity_robust(self):
        numframes = self.video.get(cv2.CAP_PROP_FRAME_COUNT)
        fr = 0
        while fr < numframes:
            success, frame = self.video.read()
            if not success or frame is None:
                warnings.warn(
                    f"Opencv failed to load frame {fr}. Use ffmpeg to re-encode video file"
                )
            fr += 1

    @property
    def name(self):
        return os.path.splitext(os.path.split(self.video_path)[1])[0]

    @property
    def format(self):
        return os.path.splitext(self.video_path)[1]

    @property
    def directory(self):
        return os.path.dirname(self.video_path)

    @property
    def metadata(self):
        return dict(
            n_frames=len(self), fps=self.fps, width=self.width, height=self.height
        )

    def get_n_frames(self, robust=False):
        if not robust:
            return self._n_frames
        elif not self._n_frames_robust:
            command = (
                f'ffprobe -i "{self.video_path}" -v error -count_frames '
                f"-select_streams v:0 -show_entries stream=nb_read_frames "
                f"-of default=nokey=1:noprint_wrappers=1"
            )
            output = subprocess.check_output(
                command, shell=True, stderr=subprocess.STDOUT
            )
            self._n_frames_robust = int(output)
        return self._n_frames_robust

    def calc_duration(self, robust=False):
        if robust:
            command = (
                f'ffprobe -i "{self.video_path}" -show_entries '
                f'format=duration -v quiet -of csv="p=0"'
            )
            output = subprocess.check_output(
                command, shell=True, stderr=subprocess.STDOUT
            )
            return float(output)
        return len(self) / self.fps

    def set_to_frame(self, ind):
        if ind < 0:
            raise ValueError("Index must be a positive integer.")
        last_frame = len(self) - 1
        if ind > last_frame:
            warnings.warn(
                "Index exceeds the total number of frames. "
                "Setting to last frame instead."
            )
            ind = last_frame
        self.video.set(cv2.CAP_PROP_POS_FRAMES, ind)

    def reset(self):
        self.set_to_frame(0)

    def read_frame(self, shrink=1, crop=False):
        success, frame = self.video.read()
        if not success:
            return
        frame = frame[..., ::-1]  # return RGB rather than BGR!
        if crop:
            x1, x2, y1, y2 = self.get_bbox(relative=False)
            frame = frame[y1:y2, x1:x2]
        if shrink > 1:
            h, w = frame.shape[:2]
            frame = cv2.resize(
                frame,
                (w // shrink, h // shrink),
                fx=0,
                fy=0,
                interpolation=cv2.INTER_AREA,
            )
        return frame

    def get_bbox(self, relative=False):
        x1, x2, y1, y2 = self._bbox
        if not relative:
            x1 = int(self._width * x1)
            x2 = int(self._width * x2)
            y1 = int(self._height * y1)
            y2 = int(self._height * y2)
        return x1, x2, y1, y2

    def set_bbox(self, x1, x2, y1, y2, relative=False):
        if x2 <= x1 or y2 <= y1:
            raise ValueError(
                f"Coordinates look wrong... " f"Ensure {x1} < {x2} and {y1} < {y2}."
            )
        if not relative:
            x1 /= self._width
            x2 /= self._width
            y1 /= self._height
            y2 /= self._height
        bbox = x1, x2, y1, y2
        if any(coord > 1 for coord in bbox):
            warnings.warn(
                "Bounding box larger than the video... " "Clipping to video dimensions."
            )
            bbox = tuple(map(lambda x: min(x, 1), bbox))
        self._bbox = bbox

    @property
    def fps(self):
        return self._fps

    @fps.setter
    def fps(self, fps):
        if not fps > 0:
            raise ValueError("Frame rate should be positive.")
        self._fps = fps

    @property
    def width(self):
        x1, x2, _, _ = self.get_bbox(relative=False)
        return x2 - x1

    @property
    def height(self):
        _, _, y1, y2 = self.get_bbox(relative=False)
        return y2 - y1

    @property
    def dimensions(self):
        return self.width, self.height

    def parse_metadata(self):
        self._n_frames = int(self.video.get(cv2.CAP_PROP_FRAME_COUNT))
        if self._n_frames >= 1e9:
            warnings.warn(
                "The video has more than 10^9 frames, we recommend chopping it up."
            )
        self._width = int(self.video.get(cv2.CAP_PROP_FRAME_WIDTH))
        self._height = int(self.video.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self._fps = round(self.video.get(cv2.CAP_PROP_FPS), 2)

    def close(self):
        self.video.release()


class VideoWriter(VideoReader):
    def __init__(self, video_path, codec="h264", dpi=100, fps=None):
        super(VideoWriter, self).__init__(video_path)
        self.codec = codec
        self.dpi = dpi
        if fps:
            self.fps = fps

    def shorten(
        self, start, end, suffix="short", dest_folder=None, validate_inputs=True
    ):
        """
        Shorten the video from start to end.

        Parameter
        ----------
        start: str
            Time formatted in hours:minutes:seconds, where shortened video shall start.

        end: str
            Time formatted in hours:minutes:seconds, where shortened video shall end.

        suffix: str, optional
            String added to the name of the shortened video ('short' by default).

        dest_folder: str, optional
            Folder the video is saved into (by default, same as the original video)

        Returns
        -------
        str
            Full path to the shortened video
        """

        def validate_timestamp(stamp):
            if not isinstance(stamp, str):
                raise ValueError(
                    "Timestamp should be a string formatted "
                    "as hours:minutes:seconds."
                )
            time = datetime.datetime.strptime(stamp, "%H:%M:%S").time()
            # The above already raises a ValueError if formatting is wrong
            seconds = (time.hour * 60 + time.minute) * 60 + time.second
            if seconds > self.calc_duration():
                raise ValueError("Timestamps must not exceed the video duration.")

        if validate_inputs:
            for stamp in start, end:
                validate_timestamp(stamp)

        output_path = self.make_output_path(suffix, dest_folder)
        command = (
            f'ffmpeg -n -i "{self.video_path}" -ss {start} -to {end} '
            f'-c:a copy "{output_path}"'
        )
        subprocess.call(command, shell=True)
        return output_path

    def split(self, n_splits, suffix="split", dest_folder=None):
        """
        Split a video into several shorter ones of equal duration.

        Parameters
        ----------
        n_splits : int
            Number of shorter videos to produce

        suffix: str, optional
            String added to the name of the splits ('short' by default).

        dest_folder: str, optional
            Folder the video splits are saved into (by default, same as the original video)

        Returns
        -------
        list
            Paths of the video splits
        """
        if not n_splits > 1:
            raise ValueError("The video should at least be split in half.")
        chunk_dur = self.calc_duration() / n_splits
        splits = np.arange(n_splits + 1) * chunk_dur
        time_formatter = lambda val: str(datetime.timedelta(seconds=val))
        clips = []
        for n, (start, end) in enumerate(zip(splits, splits[1:]), start=1):
            clips.append(
                self.shorten(
                    time_formatter(start),
                    time_formatter(end),
                    f"{suffix}{n}",
                    dest_folder,
                    validate_inputs=False,
                )
            )
        return clips

    def crop(self, suffix="crop", dest_folder=None):
        x1, _, y1, _ = self.get_bbox()
        output_path = self.make_output_path(suffix, dest_folder)
        command = (
            f'ffmpeg -n -i "{self.video_path}" '
            f"-filter:v crop={self.width}:{self.height}:{x1}:{y1} "
            f'-c:a copy "{output_path}"'
        )
        subprocess.call(command, shell=True)
        return output_path

    def rotate(self, angle, rotatecw="Arbitrary", suffix="rotated", dest_folder=None):
        output_path = self.make_output_path(suffix, dest_folder)
        command = f'ffmpeg -n -i "{self.video_path}" -vf '
        if rotatecw == "Arbitrary":
            angle = np.deg2rad(angle)
            command += f'rotate={angle} '
        elif rotatecw == "Yes":
            command += 'transpose=1 '
        else:
            raise ValueError("Unknown rotation direction.")

        command += f'-c:a copy "{output_path}"'
        subprocess.call(command, shell=True)
        return output_path

    def rescale(
        self,
        width,
        height=-1,
        rotatecw="No",
        angle=0.0,
        suffix="rescale",
        dest_folder=None,
    ):
        output_path = self.make_output_path(suffix, dest_folder)
        command = (
            f'ffmpeg -n -i "{self.video_path}" -filter:v '
            f'"scale={width}:{height}{{}}" -c:a copy "{output_path}"'
        )
        # Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg
        # interesting option to just update metadata.
        if rotatecw == "Arbitrary":
            angle = np.deg2rad(angle)
            command = command.format(f", rotate={angle}")
        elif rotatecw == "Yes":
            command = command.format(f", transpose=1")
        else:
            command = command.format("")
        subprocess.call(command, shell=True)
        return output_path

    @staticmethod
    def write_frame(frame, where):
        cv2.imwrite(where, frame[..., ::-1])

    def make_output_path(self, suffix, dest_folder):
        if not dest_folder:
            dest_folder = self.directory
        return os.path.join(dest_folder, f"{self.name}{suffix}{self.format}")


def check_video_integrity(video_path):
    vid = VideoReader(video_path)
    vid.check_integrity()
    vid.check_integrity_robust()


def imread(image_path, mode="skimage"):
    """Read image either with skimage or cv2.
    Returns frame in uint with 3 color channels."""
    if mode == "skimage":
        image = io.imread(image_path)
        if image.ndim == 2 or image.shape[-1] == 1:
            image = skimage.color.gray2rgb(image)
        elif image.shape[-1] == 4:
            image = skimage.color.rgba2rgb(image)

        return img_as_ubyte(image)

    elif mode == "cv2":
        return cv2.imread(image_path, cv2.IMREAD_UNCHANGED)[
            ..., ::-1
        ]  # ~10% faster than using cv2.cvtColor


# https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121
def imresize(img, size=1.0, interpolationmethod=cv2.INTER_AREA):
    if size != 1.0:
        return cv2.resize(
            img, None, fx=size, fy=size, interpolation=interpolationmethod
        )  # (int(height*size),int(width*size)))
    else:
        return img


def ShortenVideo(
    vname, start="00:00:01", stop="00:01:00", outsuffix="short", outpath=None
):
    """
    Auxiliary function to shorten video and output with outsuffix appended.
    to the same folder from start (hours:minutes:seconds) to stop (hours:minutes:seconds).

    Returns the full path to the shortened video!

    Parameter
    ----------
    videos : string
        A string containing the full paths of the video.

    start: hours:minutes:seconds
        Time formatted in hours:minutes:seconds, where shortened video shall start.

    stop: hours:minutes:seconds
        Time formatted in hours:minutes:seconds, where shortened video shall end.

    outsuffix: str
        Suffix for output videoname (see example).

    outpath: str
        Output path for saving video to (by default will be the same folder as the video)

    Examples
    ----------

    Linux/MacOs
    >>> deeplabcut.ShortenVideo('/data/videos/mouse1.avi')

    Extracts (sub)video from 1st second to 1st minutes (default values) and saves it in /data/videos as mouse1short.avi

    Windows:
    >>> deeplabcut.ShortenVideo('C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi', start='00:17:00',stop='00:22:00',outsuffix='brief')

    Extracts (sub)video from minute 17 to 22 and and saves it in C:\\yourusername\\rig-95\\Videos as reachingvideo1brief.avi
    """
    writer = VideoWriter(vname)
    return writer.shorten(start, stop, outsuffix, outpath)


def CropVideo(
    vname,
    width=256,
    height=256,
    origin_x=0,
    origin_y=0,
    outsuffix="cropped",
    outpath=None,
    useGUI=False,
):
    """
    Auxiliary function to crop a video and output it to the same folder with "outsuffix" appended in its name.
    Width and height will control the new dimensions.

    Returns the full path to the downsampled video!

    ffmpeg -i in.mp4 -filter:v "crop=out_w:out_h:x:y" out.mp4

    Parameter
    ----------
    vname : string
        A string containing the full path of the video.

    width: int
        width of output video

    height: int
        height of output video.

    origin_x, origin_y: int
        x- and y- axis origin of bounding box for cropping.

    outsuffix: str
        Suffix for output videoname (see example).

    outpath: str
        Output path for saving video to (by default will be the same folder as the video)

    Examples
    ----------

    Linux/MacOs
    >>> deeplabcut.CropVideo('/data/videos/mouse1.avi')

    Crops the video using default values and saves it in /data/videos as mouse1cropped.avi

    Windows:
    >>> =deeplabcut.CropVideo('C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi', width=220,height=320,outsuffix='cropped')

    Crops the video to a width of 220 and height of 320 starting at the origin (top left) and saves it in C:\\yourusername\\rig-95\\Videos as reachingvideo1cropped.avi
    """
    writer = VideoWriter(vname)

    if useGUI:
        print(
            "Please, select your coordinates (draw from top left to bottom right ...)"
        )
        coords = draw_bbox(vname)

        if not coords:
            return
        origin_x, origin_y = coords[:2]
        width = int(coords[2]) - int(coords[0])
        height = int(coords[3]) - int(coords[1])

    writer.set_bbox(origin_x, origin_x + width, origin_y, origin_y + height)
    return writer.crop(outsuffix, outpath)


def DownSampleVideo(
    vname,
    width=-1,
    height=200,
    outsuffix="downsampled",
    outpath=None,
    rotatecw="No",
    angle=0.0,
):
    """
    Auxiliary function to downsample a video and output it to the same folder with "outsuffix" appended in its name.
    Width and height will control the new dimensions. You can also pass only height or width and set the other one to -1,
    this will keep the aspect ratio identical.

    Returns the full path to the downsampled video!

    Parameter
    ----------
    vname : string
        A string containing the full path of the video.

    width: int
        width of output video

    height: int
        height of output video.

    outsuffix: str
        Suffix for output videoname (see example).

    outpath: str
        Output path for saving video to (by default will be the same folder as the video)

    rotatecw: str
        Default "No", rotates clockwise if "Yes", "Arbitrary" for arbitrary rotation by specified angle.

    angle: float
        Angle to rotate by in degrees, default 0.0. Negative values rotate counter-clockwise

    Examples
    ----------

    Linux/MacOs
    >>> deeplabcut.DownSampleVideo('/data/videos/mouse1.avi')

    Downsamples the video using default values and saves it in /data/videos as mouse1cropped.avi

    Windows:
    >>> shortenedvideoname=deeplabcut.DownSampleVideo('C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi', width=220,height=320,outsuffix='cropped')

    Downsamples the video to a width of 220 and height of 320 and saves it in C:\\yourusername\\rig-95\\Videos as reachingvideo1cropped.avi
    """
    writer = VideoWriter(vname)
    return writer.rescale(width, height, rotatecw, angle, outsuffix, outpath)


def rotate_video(vname, angle, rotatecw="Arbitrary", outsuffix="rotated", outpath=None):
    """
    Auxiliary function to rotate a video and output it to the same folder with "outsuffix" appended in its name.
    Angle is in degrees.

    Returns the full path to the rotated video!

    Parameter
    ----------
    vname : string
        A string containing the full path of the video.

    angle: float
        Angle to rotate by in degrees. Negative values rotate counter-clockwise.

    rotatecw: str
        Default "Arbitrary", rotates clockwise if "Yes", "Arbitrary" for arbitrary rotation by specified angle.

    outsuffix: str
        Suffix for output videoname (see example).

    outpath: str
        Output path for saving video to (by default will be the same folder as the video)

    Examples
    ----------

    Linux/MacOs
    >>> deeplabcut.rotate_video('/data/videos/mouse1.avi',angle=90)

    Rotates the video by 90 degrees and saves it in /data/videos as mouse1rotated.avi

    Windows:
    >>> shortenedvideoname=deeplabcut.rotate_video('C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi', angle=180,rotatecw='Yes')

    Rotates the video by 180 degrees and saves it in C:\\yourusername\\rig-95\\Videos as reachingvideo1rotated.avi
    """
    writer = VideoWriter(vname)
    return writer.rotate(angle, rotatecw, outsuffix, outpath)


def draw_bbox(video):
    import matplotlib.pyplot as plt
    from matplotlib.widgets import RectangleSelector, Button

    clip = VideoWriter(video)
    frame = None
    # Read the video until a frame is successfully read
    while frame is None:
        frame = clip.read_frame()

    bbox = [0, 0, frame.shape[1], frame.shape[0]]

    def line_select_callback(eclick, erelease):
        bbox[:2] = int(eclick.xdata), int(eclick.ydata)  # x1, y1
        bbox[2:] = int(erelease.xdata), int(erelease.ydata)  # x2, y2

    def validate_crop(*args):
        fig.canvas.stop_event_loop()

    def display_help(*args):
        print(
            "1. Use left click to select the region of interest. A red box will be drawn around the selected region. \n\n2. Use the corner points to expand the box and center to move the box around the image. \n\n3. Click "
        )

    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.imshow(frame)
    ax_help = fig.add_axes([0.9, 0.2, 0.1, 0.1])
    ax_save = fig.add_axes([0.9, 0.1, 0.1, 0.1])
    crop_button = Button(ax_save, "Crop")
    crop_button.on_clicked(validate_crop)
    help_button = Button(ax_help, "Help")
    help_button.on_clicked(display_help)

    rs = RectangleSelector(
        ax,
        line_select_callback,
        minspanx=5,
        minspany=5,
        interactive=True,
        spancoords="pixels",
    )
    plt.show(block=False)

    # import platform
    # if platform.system() == "Darwin":  # for OSX use WXAgg
    #    fig.canvas.start_event_loop(timeout=-1)
    # else:
    fig.canvas.start_event_loop(timeout=-1)  # just tested on Ubuntu I also need this.
    #    #fig.canvas.stop_event_loop()

    plt.close(fig)
    return bbox


--- File: deeplabcut/utils/multiprocessing.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.2 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""
import multiprocessing


def _wrapper(func, queue, *args, **kwargs):
    try:
        result = func(*args, **kwargs)
        queue.put(result)  # Pass the result back via the queue
    except Exception as e:
        queue.put(e)  # Pass any exception back via the queue


def call_with_timeout(func, timeout, *args, **kwargs):
    queue = multiprocessing.Queue()
    process = multiprocessing.Process(
        target=_wrapper, args=(func, queue, *args), kwargs=kwargs
    )
    process.start()
    process.join(timeout)

    if process.is_alive():
        process.terminate()  # Forcefully terminate the process
        process.join()
        raise TimeoutError(
            f"Function {func.__name__} did not complete within {timeout} seconds."
        )

    if not queue.empty():
        result = queue.get()
        if isinstance(result, Exception):
            raise result  # Re-raise the exception if it occurred in the function
        return result
    else:
        raise TimeoutError(
            f"Function {func.__name__} completed but did not return a result."
        )


--- File: deeplabcut/utils/frameselectiontools.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0
"""


import math

import cv2
import numpy as np
from skimage.util import img_as_ubyte
from sklearn.cluster import MiniBatchKMeans
from tqdm import tqdm


def UniformFrames(clip, numframes2pick, start, stop, Index=None):
    """Temporally uniformly sampling frames in interval (start,stop).
    Visual information of video is irrelevant for this method. This code is fast and sufficient (to extract distinct frames),
    when behavioral videos naturally covers many states.

    The variable Index allows to pass on a subindex for the frames.
    """
    print(
        "Uniformly extracting of frames from",
        round(start * clip.duration, 2),
        " seconds to",
        round(stop * clip.duration, 2),
        " seconds.",
    )
    if Index is None:
        if start == 0:
            frames2pick = np.random.choice(
                math.ceil(clip.duration * clip.fps * stop),
                size=numframes2pick,
                replace=False,
            )
        else:
            frames2pick = np.random.choice(
                range(
                    math.floor(start * clip.duration * clip.fps),
                    math.ceil(clip.duration * clip.fps * stop),
                ),
                size=numframes2pick,
                replace=False,
            )
        return frames2pick
    else:
        startindex = int(np.floor(clip.fps * clip.duration * start))
        stopindex = int(np.ceil(clip.fps * clip.duration * stop))
        Index = np.array(Index, dtype=int)
        Index = Index[(Index > startindex) * (Index < stopindex)]  # crop to range!
        if len(Index) >= numframes2pick:
            return list(np.random.permutation(Index)[:numframes2pick])
        else:
            return list(Index)


# uses openCV
def UniformFramescv2(cap, numframes2pick, start, stop, Index=None):
    """Temporally uniformly sampling frames in interval (start,stop).
    Visual information of video is irrelevant for this method. This code is fast and sufficient (to extract distinct frames),
    when behavioral videos naturally covers many states.

    The variable Index allows to pass on a subindex for the frames.
    """
    nframes = len(cap)
    print(
        "Uniformly extracting of frames from",
        round(start * nframes * 1.0 / cap.fps, 2),
        " seconds to",
        round(stop * nframes * 1.0 / cap.fps, 2),
        " seconds.",
    )

    if Index is None:
        if start == 0:
            frames2pick = np.random.choice(
                math.ceil(nframes * stop), size=numframes2pick, replace=False
            )
        else:
            frames2pick = np.random.choice(
                range(math.floor(nframes * start), math.ceil(nframes * stop)),
                size=numframes2pick,
                replace=False,
            )
        return frames2pick
    else:
        startindex = int(np.floor(nframes * start))
        stopindex = int(np.ceil(nframes * stop))
        Index = np.array(Index, dtype=int)
        Index = Index[(Index > startindex) * (Index < stopindex)]  # crop to range!
        if len(Index) >= numframes2pick:
            return list(np.random.permutation(Index)[:numframes2pick])
        else:
            return list(Index)


def KmeansbasedFrameselection(
    clip,
    numframes2pick,
    start,
    stop,
    Index=None,
    step=1,
    resizewidth=30,
    batchsize=100,
    max_iter=50,
    color=False,
):
    """This code downsamples the video to a width of resizewidth.

    The video is extracted as a numpy array, which is then clustered with kmeans, whereby each frames is treated as a vector.
    Frames from different clusters are then selected for labeling. This procedure makes sure that the frames "look different",
    i.e. different postures etc. On large videos this code is slow.

    Consider not extracting the frames from the whole video but rather set start and stop to a period around interesting behavior.

    Note: this method can return fewer images than numframes2pick."""

    print(
        "Kmeans-quantization based extracting of frames from",
        round(start * clip.duration, 2),
        " seconds to",
        round(stop * clip.duration, 2),
        " seconds.",
    )
    startindex = int(np.floor(clip.fps * clip.duration * start))
    stopindex = int(np.ceil(clip.fps * clip.duration * stop))

    if Index is None:
        Index = np.arange(startindex, stopindex, step)
    else:
        Index = np.array(Index)
        Index = Index[(Index > startindex) * (Index < stopindex)]  # crop to range!

    nframes = len(Index)
    if batchsize > nframes:
        batchsize = int(nframes / 2)

    if len(Index) >= numframes2pick:
        clipresized = clip.resize(width=resizewidth)
        ny, nx = clipresized.size
        frame0 = img_as_ubyte(clip.get_frame(0))
        if np.ndim(frame0) == 3:
            ncolors = np.shape(frame0)[2]
        else:
            ncolors = 1
        print("Extracting and downsampling...", nframes, " frames from the video.")

        if color and ncolors > 1:
            DATA = np.zeros((nframes, nx * 3, ny))
            for counter, index in tqdm(enumerate(Index)):
                image = img_as_ubyte(
                    clipresized.get_frame(index * 1.0 / clipresized.fps)
                )
                DATA[counter, :, :] = np.vstack(
                    [image[:, :, 0], image[:, :, 1], image[:, :, 2]]
                )
        else:
            DATA = np.zeros((nframes, nx, ny))
            for counter, index in tqdm(enumerate(Index)):
                if ncolors == 1:
                    DATA[counter, :, :] = img_as_ubyte(
                        clipresized.get_frame(index * 1.0 / clipresized.fps)
                    )
                else:  # attention: averages over color channels to keep size small / perhaps you want to use color information?
                    DATA[counter, :, :] = img_as_ubyte(
                        np.array(
                            np.mean(
                                clipresized.get_frame(index * 1.0 / clipresized.fps), 2
                            ),
                            dtype=np.uint8,
                        )
                    )

        print("Kmeans clustering ... (this might take a while)")
        data = DATA - DATA.mean(axis=0)
        data = data.reshape(nframes, -1)  # stacking

        kmeans = MiniBatchKMeans(
            n_clusters=numframes2pick, tol=1e-3, batch_size=batchsize, max_iter=max_iter
        )
        kmeans.fit(data)
        frames2pick = []
        for clusterid in range(numframes2pick):  # pick one frame per cluster
            clusterids = np.where(clusterid == kmeans.labels_)[0]

            numimagesofcluster = len(clusterids)
            if numimagesofcluster > 0:
                frames2pick.append(
                    Index[clusterids[np.random.randint(numimagesofcluster)]]
                )

        clipresized.close()
        del clipresized
        return list(np.array(frames2pick))
    else:
        return list(Index)


def KmeansbasedFrameselectioncv2(
    cap,
    numframes2pick,
    start,
    stop,
    Index=None,
    step=1,
    resizewidth=30,
    batchsize=100,
    max_iter=50,
    color=False,
):
    """This code downsamples the video to a width of resizewidth.
    The video is extracted as a numpy array, which is then clustered with kmeans, whereby each frames is treated as a vector.
    Frames from different clusters are then selected for labeling. This procedure makes sure that the frames "look different",
    i.e. different postures etc. On large videos this code is slow.

    Consider not extracting the frames from the whole video but rather set start and stop to a period around interesting behavior.

    Note: this method can return fewer images than numframes2pick.

    Attention: the flow of commands was not optimized for readability, but rather speed. This is why it might appear tedious and repetitive.
    """
    nframes = len(cap)
    nx, ny = cap.dimensions
    ratio = resizewidth * 1.0 / nx
    if ratio > 1:
        raise Exception("Choice of resizewidth actually upsamples!")

    print(
        "Kmeans-quantization based extracting of frames from",
        round(start * nframes * 1.0 / cap.fps, 2),
        " seconds to",
        round(stop * nframes * 1.0 / cap.fps, 2),
        " seconds.",
    )
    startindex = int(np.floor(nframes * start))
    stopindex = int(np.ceil(nframes * stop))

    if Index is None:
        Index = np.arange(startindex, stopindex, step)
    else:
        Index = np.array(Index)
        Index = Index[(Index > startindex) * (Index < stopindex)]  # crop to range!

    nframes = len(Index)
    if batchsize > nframes:
        batchsize = nframes // 2

    ny_ = np.round(ny * ratio).astype(int)
    nx_ = np.round(nx * ratio).astype(int)
    DATA = np.empty((nframes, ny_, nx_ * 3 if color else nx_))
    if len(Index) >= numframes2pick:
        if (
            np.mean(np.diff(Index)) > 1
        ):  # then non-consecutive indices are present, thus cap.set is required (which slows everything down!)
            print("Extracting and downsampling...", nframes, " frames from the video.")
            if color:
                for counter, index in tqdm(enumerate(Index)):
                    cap.set_to_frame(index)  # extract a particular frame
                    frame = cap.read_frame(crop=True)
                    if frame is not None:
                        image = img_as_ubyte(
                            cv2.resize(
                                frame,
                                None,
                                fx=ratio,
                                fy=ratio,
                                interpolation=cv2.INTER_NEAREST,
                            )
                        )  # color trafo not necessary; lack thereof improves speed.
                        DATA[counter, :, :] = np.hstack(
                            [image[:, :, 0], image[:, :, 1], image[:, :, 2]]
                        )
            else:
                for counter, index in tqdm(enumerate(Index)):
                    cap.set_to_frame(index)  # extract a particular frame
                    frame = cap.read_frame(crop=True)
                    if frame is not None:
                        image = img_as_ubyte(
                            cv2.resize(
                                frame,
                                None,
                                fx=ratio,
                                fy=ratio,
                                interpolation=cv2.INTER_NEAREST,
                            )
                        )  # color trafo not necessary; lack thereof improves speed.
                        DATA[counter, :, :] = np.mean(image, 2)
        else:
            print("Extracting and downsampling...", nframes, " frames from the video.")
            if color:
                for counter, index in tqdm(enumerate(Index)):
                    frame = cap.read_frame(crop=True)
                    if frame is not None:
                        image = img_as_ubyte(
                            cv2.resize(
                                frame,
                                None,
                                fx=ratio,
                                fy=ratio,
                                interpolation=cv2.INTER_NEAREST,
                            )
                        )  # color trafo not necessary; lack thereof improves speed.
                        DATA[counter, :, :] = np.hstack(
                            [image[:, :, 0], image[:, :, 1], image[:, :, 2]]
                        )
            else:
                for counter, index in tqdm(enumerate(Index)):
                    frame = cap.read_frame(crop=True)
                    if frame is not None:
                        image = img_as_ubyte(
                            cv2.resize(
                                frame,
                                None,
                                fx=ratio,
                                fy=ratio,
                                interpolation=cv2.INTER_NEAREST,
                            )
                        )  # color trafo not necessary; lack thereof improves speed.
                        DATA[counter, :, :] = np.mean(image, 2)

        print("Kmeans clustering ... (this might take a while)")
        data = DATA - DATA.mean(axis=0)
        data = data.reshape(nframes, -1)  # stacking

        kmeans = MiniBatchKMeans(
            n_clusters=numframes2pick, tol=1e-3, batch_size=batchsize, max_iter=max_iter
        )
        kmeans.fit(data)
        frames2pick = []
        for clusterid in range(numframes2pick):  # pick one frame per cluster
            clusterids = np.where(clusterid == kmeans.labels_)[0]

            numimagesofcluster = len(clusterids)
            if numimagesofcluster > 0:
                frames2pick.append(
                    Index[clusterids[np.random.randint(numimagesofcluster)]]
                )
        # cap.release() >> still used in frame_extraction!
        return list(np.array(frames2pick))
    else:
        return list(Index)


--- File: deeplabcut/modelzoo/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.modelzoo.weight_initialization import build_weight_init


--- File: deeplabcut/modelzoo/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import os
import warnings
from glob import glob
from pathlib import Path

import numpy as np
import pandas as pd
from matplotlib.colors import ListedColormap

from deeplabcut.core.config import read_config_as_dict
from deeplabcut.core.conversion_table import ConversionTable
from deeplabcut.utils.auxiliaryfunctions import (
    get_bodyparts,
    get_deeplabcut_path,
    read_config,
    write_config,
)


def dlc_modelzoo_path() -> Path:
    """Returns: the path to the `modelzoo` folder in the DeepLabCut installation"""
    dlc_root_path = Path(get_deeplabcut_path())
    return dlc_root_path / "modelzoo"


def get_super_animal_project_cfg(super_animal: str) -> dict:
    """Gets the project configuration file for a SuperAnimal model

    Args:
        super_animal: the name of the SuperAnimal model for which to load the project
            configuration

    Returns:
        the project configuration for the given SuperAnimal model

    Raises:
        ValueError if no such SuperAnimal is found
    """
    project_configs_dir = dlc_modelzoo_path() / "project_configs"
    super_animal_projects = {p.stem: p for p in project_configs_dir.iterdir()}
    if super_animal not in super_animal_projects:
        raise ValueError(
            f"No such SuperAnimal model: {super_animal}. Available SuperAnimal models "
            f"are {', '.join(super_animal_projects.keys())}."
        )

    return read_config_as_dict(super_animal_projects[super_animal])


def get_super_animal_scorer(
    super_animal: str,
    model_snapshot_path: Path,
    detector_snapshot_path: Path | None,
) -> str:
    """
    Args:
        super_animal: The SuperAnimal dataset on which the models were trained
        model_snapshot_path: The path for the SuperAnimal pose model snapshot
        detector_snapshot_path: The path for the SuperAnimal detector snapshot, if a
            detector is being used.

    Returns:
        The DLC scorer name to use for the given SuperAnimal models.
    """
    super_animal_prefix = super_animal + "_"
    dlc_scorer = super_animal_prefix

    if detector_snapshot_path is not None:
        detector_name = detector_snapshot_path.stem
        if detector_name.startswith(super_animal_prefix):
            detector_name = detector_name[len(super_animal_prefix) :]
        dlc_scorer += f"{detector_name}_"

    model_name = model_snapshot_path.stem
    if model_name.startswith(super_animal_prefix):
        model_name = model_name[len(super_animal_prefix) :]
    dlc_scorer += f"{model_name}"

    return dlc_scorer


def create_conversion_table(
    config: str | Path,
    super_animal: str,
    project_to_super_animal: dict[str, str],
) -> ConversionTable:
    """
    Creates a conversion table mapping bodyparts defined for a DeepLabCut project
    to bodyparts defined for a SuperAnimal model. This allows to fine-tune SuperAnimal
    weights instead of transfer learning from ImageNet. The conversion table is directly
    added to the project's configuration file.

    Args:
        config: The path to the project configuration for which the conversion table
            should be created.
        super_animal: The SuperAnimal model for the conversion table
        project_to_super_animal: The conversion table mapping each project bodypart
            to the corresponding SuperAnimal bodypart.

    Returns:
        The conversion table that was added to the project config.

    Raises:
         ValueError: If the conversion table is misconfigured (e.g., if there are
            misnamed bodyparts in the table). See ConversionTable for more.
    """
    cfg = read_config(str(config))
    sa_cfg = get_super_animal_project_cfg(super_animal)
    conversion_table = ConversionTable(
        super_animal=super_animal,
        project_bodyparts=get_bodyparts(cfg),
        super_animal_bodyparts=sa_cfg["bodyparts"],
        table=project_to_super_animal,
    )

    conversion_tables = cfg.get("SuperAnimalConversionTables")
    if conversion_tables is None:
        conversion_tables = {}

    conversion_tables[super_animal] = conversion_table.table
    cfg["SuperAnimalConversionTables"] = conversion_tables
    write_config(str(config), cfg)
    return conversion_table


def get_conversion_table(cfg: dict | str | Path, super_animal: str) -> ConversionTable:
    """Gets the conversion table from a project to a SuperAnimal model

    Args:
        cfg: The path to a project configuration file, or directly the project config.
        super_animal: The SuperAnimal for which to get the configuration file.

    Returns:
        A dictionary mapping {project_bodypart: super_animal_bodypart}

    Raises:
        ValueError: If the conversion table is misconfigured (e.g., if there are
            misnamed bodyparts in the table). See ConversionTable for more.
    """
    if isinstance(cfg, (str, Path)):
        cfg = read_config(str(cfg))

    conversion_tables = cfg.get("SuperAnimalConversionTables", {})
    if conversion_tables is None or super_animal not in conversion_tables:
        raise ValueError(
            f"No conversion table defined in the project config for {super_animal}."
            "Call deeplabcut.modelzoo.create_conversion_table to create one."
        )

    sa_cfg = get_super_animal_project_cfg(super_animal)
    conversion_table = ConversionTable(
        super_animal=super_animal,
        project_bodyparts=get_bodyparts(cfg),
        super_animal_bodyparts=sa_cfg["bodyparts"],
        table=conversion_tables[super_animal],
    )
    return conversion_table


def read_conversion_table_from_csv(csv_path):
    df = pd.read_csv(csv_path, skiprows=1, header=None)
    df = df.dropna()
    df[0] = df[0].str.replace(r"\s+", "", regex=True)
    df[1] = df[1].str.replace(r"\s+", "", regex=True)
    _map = dict(zip(df[0], df[1]))
    return _map


def parse_project_model_name(superanimal_name: str) -> tuple[str, str]:
    """Parses model zoo model names for SuperAnimal models

    Args:
        superanimal_name: the name of the SuperAnimal model name to parse

    Returns:
        project_name: the parsed SuperAnimal model name
        model_name: the model architecture (e.g., dlcrnet, hrnetw32)
    """

    if superanimal_name == "superanimal_quadruped":
        warnings.warn(
            f"{superanimal_name} is deprecated and will be removed in a future version. Use {superanimal_name}_model_suffix instead.",
            DeprecationWarning,
        )
        superanimal_name = "superanimal_quadruped_hrnetw32"

    if superanimal_name == "superanimal_topviewmouse":
        warnings.warn(
            f"{superanimal_name} is deprecated and will be removed in a future version. Use {superanimal_name}_model_suffix instead.",
            DeprecationWarning,
        )
        superanimal_name = "superanimal_topviewmouse_dlcrnet"

    model_name = superanimal_name.split("_")[-1]
    project_name = superanimal_name.replace(f"_{model_name}", "")

    dlc_root_path = get_deeplabcut_path()
    modelzoo_path = os.path.join(dlc_root_path, "modelzoo")

    available_model_configs = glob(
        os.path.join(modelzoo_path, "model_configs", "*.yaml")
    )
    available_models = [
        os.path.splitext(os.path.basename(path))[0] for path in available_model_configs
    ]

    if model_name not in available_models:
        raise ValueError(
            f"Model {model_name} not found. Available models are: {available_models}"
        )

    available_project_configs = glob(
        os.path.join(modelzoo_path, "project_configs", "*.yaml")
    )
    available_projects = [
        os.path.splitext(os.path.basename(path))[0]
        for path in available_project_configs
    ]

    return project_name, model_name


def get_superanimal_colormaps():
    # FIXME(shaokai) - Add colormaps for the SuperBird dataset
    superanimal_bird_colors = (
        np.array(
            [
                (127, 0, 255),
                (115, 18, 254),
                (103, 37, 254),
                (91, 56, 253),
                (79, 74, 252),
                (65, 95, 250),
                (53, 112, 248),
                (41, 128, 246),
                (29, 144, 243),
                (15, 162, 239),
                (3, 176, 236),
                (8, 189, 232),
                (20, 201, 228),
                (34, 214, 223),
                (46, 223, 219),
                (58, 232, 214),
                (70, 239, 209),
                (84, 246, 202),
                (96, 250, 196),
                (108, 253, 190),
                (120, 254, 184),
                (134, 254, 176),
                (146, 253, 169),
                (158, 250, 162),
                (170, 246, 154),
                (184, 239, 146),
                (196, 232, 138),
                (208, 223, 130),
                (220, 214, 122),
                (234, 201, 112),
                (246, 189, 103),
                (255, 176, 95),
                (255, 162, 86),
                (255, 144, 75),
                (255, 128, 66),
                (255, 112, 57),
                (255, 95, 48),
                (255, 74, 37),
                (255, 56, 28),
                (255, 37, 18),
                (255, 18, 9),
                (255, 0, 0),
            ]
        )
        / 255
    )
    superanimal_topviewmouse_colors = (
        np.array(
            [
                [127, 0, 255],
                [109, 28, 254],
                [91, 56, 253],
                [71, 86, 251],
                [53, 112, 248],
                [33, 139, 244],
                [15, 162, 239],
                [4, 185, 234],
                [22, 203, 228],
                [42, 220, 220],
                [60, 233, 213],
                [80, 244, 204],
                [98, 250, 195],
                [118, 254, 185],
                [136, 254, 175],
                [156, 250, 163],
                [174, 244, 152],
                [194, 233, 139],
                [212, 220, 127],
                [232, 203, 113],
                [250, 185, 100],
                [255, 162, 86],
                [255, 139, 72],
                [255, 112, 57],
                [255, 86, 43],
                [255, 56, 28],
                [255, 28, 14],
            ]
        )
        / 255
    )
    superanimal_quadruped_colors = (
        np.array(
            [
                [255.0, 0.0, 0.0],
                [255.0, 39.63408568671726, 0.0],
                [255.0, 79.26817137343453, 0.0],
                [255.0, 118.9022570601518, 0.0],
                [255.0, 158.53634274686905, 0.0],
                [255.0, 198.17042843358632, 0.0],
                [255.0, 237.8045141203036, 0.0],
                [232.56140019297916, 255.0, 0.0],
                [192.92731450626187, 255.0, 0.0],
                [153.2932288195446, 255.0, 0.0],
                [113.65914313282731, 255.0, 0.0],
                [74.02505744611004, 255.0, 0.0],
                [34.390971759392784, 255.0, 0.0],
                [3.5647953575585385, 255.0, 8.807909284882923],
                [0.0, 255.0, 44.87701729490043],
                [0.0, 255.0, 84.51085328820125],
                [0.0, 255.0, 124.14468928150207],
                [0.0, 255.0, 163.77852527480275],
                [0.0, 255.0, 203.4123612681037],
                [0.0, 255.0, 243.04619726140453],
                [0, 220, 255],
                [0, 255, 255],
                [0, 165, 255],
                [0, 150, 255],
                [0.0, 68.78344961404169, 255.0],
                [0.0, 29.14936392732455, 255.0],
                [10.484721759392611, 0.0, 255.0],
                [50.11880744611004, 0.0, 255.0],
                [89.75289313282732, 0.0, 255.0],
                [129.38697881954448, 0.0, 255.0],
                [169.02106450626192, 0.0, 255.0],
                [169.02106450626192, 0.0, 255.0],
                [255.0, 0.0, 142.80850706015173],
                [169.02106450626192, 0.0, 255.0],
                [255.0, 0.0, 142.80850706015173],
                [255.0, 0.0, 142.80850706015173],
                [255.0, 0.0, 103.17442137343447],
                [255.0, 0.0, 63.54033568671722],
                [255.0, 0.0, 23.90625],
            ]
        )
        / 255
    )

    superanimal_colormaps = {
        "superanimal_bird": ListedColormap(
            list(superanimal_bird_colors), name="superanimal_bird"
        ),
        "superanimal_topviewmouse": ListedColormap(
            list(superanimal_topviewmouse_colors), name="superanimal_topviewmouse"
        ),
        "superanimal_quadruped": ListedColormap(
            list(superanimal_quadruped_colors), name="superanimal_quadruped"
        ),
    }
    return superanimal_colormaps


--- File: deeplabcut/modelzoo/video_inference.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Optional, Union

from dlclibrary.dlcmodelzoo.modelzoo_download import download_huggingface_model
from ruamel.yaml import YAML

from deeplabcut.core.config import read_config_as_dict
from deeplabcut.modelzoo.utils import get_super_animal_scorer
from deeplabcut.pose_estimation_pytorch.modelzoo.train_from_coco import adaptation_train
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import (
    get_snapshot_folder_path,
    get_super_animal_snapshot_path,
    load_super_animal_config,
    update_config,
)
from deeplabcut.utils.auxiliaryfunctions import get_deeplabcut_path
from deeplabcut.utils.pseudo_label import (
    dlc3predictions_2_annotation_from_video,
    video_to_frames,
)


def video_inference_superanimal(
    videos: Union[str, list],
    superanimal_name: str,
    model_name: str,
    detector_name: str | None = None,
    scale_list: Optional[list] = None,
    videotype: str = ".mp4",
    dest_folder: Optional[str] = None,
    cropping: list[int] | None = None,
    video_adapt: bool = False,
    plot_trajectories: bool = False,
    batch_size: int = 1,
    detector_batch_size: int = 1,
    pcutoff: float = 0.1,
    adapt_iterations: int = 1000,
    pseudo_threshold: float = 0.1,
    bbox_threshold: float = 0.9,
    detector_epochs: int = 4,
    pose_epochs: int = 4,
    max_individuals: int = 10,
    video_adapt_batch_size: int = 8,
    device: Optional[str] = "auto",
    customized_pose_checkpoint: Optional[str] = None,
    customized_detector_checkpoint: Optional[str] = None,
    customized_model_config: Optional[str] = None,
    plot_bboxes: bool = True,
):
    """
    This function performs inference on videos using a pretrained SuperAnimal model.

    IMPORTANT: Note that since we have both TensorFlow and PyTorch Engines, we will
    route the engine based on the model you select:

        * dlcrnet -> TensorFlow
        * all others - > PyTorch

    Parameters
    ----------

    videos (str or list):
        The path to the video or a list of paths to videos.

    superanimal_name (str):
        The name of the SuperAnimal dataset for which to load a pre-trained model.

    model_name (str):
        The model architecture to use for inference.

    detector_name (str):
        For top-down models (only available with the PyTorch framework), the type of
        object detector to use for inference.

    scale_list (list):
        A list of different resolutions for the spatial pyramid. Used only for bottom up models.

    videotype (str):
        Checks for the extension of the video in case the input to the video is a directory.
        Only videos with this extension are analyzed. The default is ``.mp4``.

    dest_folder (str): The path to the folder where the results should be saved.

    cropping: list or None, optional, default=None
        Only for SuperAnimal models running with the PyTorch engine.
        List of cropping coordinates as [x1, x2, y1, y2].
        Note that the same cropping parameters will then be used for all videos.
        If different video crops are desired, run ``video_inference_superanimal`` on
        individual videos with the corresponding cropping coordinates.

    video_adapt (bool):
        Whether to perform video adaptation. The default is False.
        You only need to perform it on one video because the adaptation generalizes to all videos that are similar.

    plot_trajectories (bool):
        Whether to plot the trajectories. The default is False.

    batch_size (int):
        The batch size to use for video inference. Only for PyTorch models.

    detector_batch_size (int):
        The batch size to use for the detector during video inference. Only for PyTorch.

    pcutoff (float):
        The p-value cutoff for the confidence of the prediction. The default is 0.1.

    adapt_iterations (int):
        Number of iterations for adaptation training. Empirically 1000 is sufficient.

    bbox_threshold (float):
        The pseudo-label threshold for the confidence of the detector. The default is 0.9

    detector_epochs (int):
        Used in the PyTorch engine. The number of epochs for training the detector. The default is 4.

    pose_epochs (int):
        Used in the PyTorch engine. The number of epochs for training the pose estimator. The default is 4.

    pseudo_threshold (float):
        The pseudo-label threshold for the confidence of the prediction. The default is 0.1.

    max_individuals (int):
        The maximum number of individuals in the video. The default is 30. Used only for top down models.

    video_adapt_batch_size (int):
        The batch size to use for video adaptation.

    device (str):
        The device to use for inference. The default is None (CPU). Used only for PyTorch models.

    customized_pose_checkpoint (str):
        Used in the PyTorch engine. If specified, it replaces the default pose checkpoint.

    customized_detector_checkpoint (str):
        Used in the PyTorch engine. If specified, it replaces the default detector checkpoint.

    customized_model_config (str):
        Used for loading customized model config. Only supported in Pytorch

    plot_bboxes (bool):
        If using Top-Down approach, whether to plot the detector's bounding boxes. The default is True.

    Raises:
        NotImplementedError:
        If the model is not found in the modelzoo.
        Warning: If the superanimal_name will be deprecated in the future.

    (Model Explanation) SuperAnimal-Quadruped:
    `superanimal_quadruped` models aim to work across a large range of quadruped
    animals, from horses, dogs, sheep, rodents, to elephants. The camera perspective is
    orthogonal to the animal ("side view"), and most of the data includes the animals
    face (thus the front and side of the animal). You will note we have several variants
    that differ in speed vs. performance, so please do test them out on your data to see
    which is best suited for your application. Also note we have a "video adaptation"
    feature, which lets you adapt your data to the model in a self-supervised way.
    No labeling needed!

    All model snapshots are automatically downloaded to modelzoo/checkpoints when used.

    - PLEASE SEE THE FULL DATASHEET: https://zenodo.org/records/10619173
    - MORE DETAILS ON THE MODELS (detector, pose estimators):
        https://huggingface.co/mwmathis/DeepLabCutModelZoo-SuperAnimal-Quadruped
    - We provide several models:
        - `hrnet_w32` (Top-Down pose estimation model, PyTorch engine)
            An `hrnet_w32` is a top-down model that is paired with a detector. That
            means it takes a cropped image from an object detector and predicts the
            keypoints. When selecting this variant, a `detector_name` must be set with
            one of the provided object detectors.
        - `dlcrnet` (TensorFlow engine)
            This is a bottom-up model that predicts all keypoints then groups them into
            individuals. This can be faster, but more error prone.
    - We provide one object detector (only for the PyTorch engine):
        - `fasterrcnn_resnet50_fpn_v2`
            This is a FasterRCNN model with a ResNet backbone, see
            https://pytorch.org/vision/stable/models/faster_rcnn.html

    (Model Explanation) SuperAnimal-TopViewMouse:
    `superanimal_topviewmouse` aims to work across lab mice in different lab settings
    from a top-view perspective; this is very polar in many behavioral assays in freely
    moving mice.

    All model snapshots are automatically downloaded to modelzoo/checkpoints when used.

    - [PLEASE SEE THE FULL DATASHEET HERE](https://zenodo.org/records/10618947)
    - [MORE DETAILS ON THE MODELS (detector, pose estimators)](https://huggingface.co/mwmathis/DeepLabCutModelZoo-SuperAnimal-TopViewMouse)
    - We provide several models:
        - `hrnet_w32` (Top-Down pose estimation model, PyTorch engine)
            An `hrnet_w32` is a top-down model that is paired with a detector. That
            means it takes a cropped image from an object detector and predicts the
            keypoints. When selecting this variant, a `detector_name` must be set with
            one of the provided object detectors.
        - `dlcrnet` (TensorFlow engine)
            This is a bottom-up model that predicts all keypoints then groups them into
            individuals. This can be faster, but more error prone.
    - We provide one object detector (only for the PyTorch engine):
        - `fasterrcnn_resnet50_fpn_v2`
            This is a FasterRCNN model with a ResNet backbone, see
            https://pytorch.org/vision/stable/models/faster_rcnn.html

    (Model Explanation) SuperAnimal-Bird:
    TODO(shaokai)

    Examples (PyTorch Engine)
    --------
    >>> import deeplabcut.modelzoo.video_inference.video_inference_superanimal as video_inference_superanimal
    >>> video_inference_superanimal(
        videos=["/mnt/md0/shaokai/DLCdev/3mice_video1_short.mp4"],
        superanimal_name="superanimal_topviewmouse",
        model_name="hrnet_w32",
        detector_name="fasterrcnn_resnet50_fpn_v2",
        video_adapt=True,
        max_individuals=3,
        pseudo_threshold=0.1,
        bbox_threshold=0.9,
        detector_epochs=4,
        pose_epochs=4,
    )

    Tips:
    * max_individuals: make sure you correctly give the number of individuals. Our
        inference api will only give up to max_individuals number of predictions.
    * pseudo_threshold: the higher you set, the more aggressive you filter low
        confidence predictions during video adaptation.
    * bbox_threshold: the higher you set, the more aggressive you filter low confidence
        bounding boxes during video adaptation. Different from our paper, we now add
        video adaptation to the object detector as well.
    * detector_epochs and pose_epochs do not need to be to high as video adaptation does
        not require too much training. However, you can make them higher if you see a
        substaintial gain in the training logs.

    Examples
    --------

    >>> from deeplabcut.modelzoo.video_inference import video_inference_superanimal
    >>> videos = ["/path/to/my/video.mp4"]
    >>> superanimal_name = "superanimal_topviewmouse"
    >>> videotype = "mp4"
    >>> scale_list = [200, 300, 400]
    >>> video_inference_superanimal(
            videos,
            superanimal_name,
            model_name="hrnet_w32",
            detector_name="fasterrcnn_resnet50_fpn_v2",
            scale_list = scale_list,
            videotype = videotype,
            video_adapt = True,
        )

    Tips:
    scale_list: it's recommended to leave this as empty list []. Empirically
    [200, 300, 400] works well. We needed to do this as bottom-up models in TensorFlow
    are sensitive to the scales of the image.
    If you find your predictions not good without scale_list or it's too hard to find
    the right scale_list, you can try to use the PyTorch engine.
    """
    if scale_list is None:
        scale_list = []

    print(f"Running video inference on {videos} with {superanimal_name}_{model_name}")
    dlc_root_path = get_deeplabcut_path()
    modelzoo_path = os.path.join(dlc_root_path, "modelzoo")
    available_architectures = json.load(
        open(os.path.join(modelzoo_path, "models_to_framework.json"), "r")
    )
    framework = available_architectures[model_name]
    print(f"Using {framework} for model {model_name}")
    if framework == "tensorflow":
        from deeplabcut.pose_estimation_tensorflow.modelzoo.api.superanimal_inference import (
            _video_inference_superanimal,
        )

        weight_folder = get_snapshot_folder_path() / f"{superanimal_name}_{model_name}"
        if not weight_folder.exists():
            download_huggingface_model(
                superanimal_name, target_dir=str(weight_folder), rename_mapping=None
            )

        if isinstance(videos, str):
            videos = [videos]
        _video_inference_superanimal(
            videos,
            superanimal_name,
            model_name,
            scale_list,
            videotype,
            video_adapt,
            plot_trajectories,
            pcutoff,
            adapt_iterations,
            pseudo_threshold,
        )
    elif framework == "pytorch":
        if detector_name is None:
            raise ValueError(
                "You have to specify a detector_name when using the Pytorch framework."
            )

        from deeplabcut.pose_estimation_pytorch.modelzoo.inference import (
            _video_inference_superanimal,
        )

        if customized_model_config is not None:
            config = read_config_as_dict(customized_model_config)
        else:
            config = load_super_animal_config(
                super_animal=superanimal_name,
                model_name=model_name,
                detector_name=detector_name,
            )

        pose_model_path = customized_pose_checkpoint
        if pose_model_path is None:
            pose_model_path = get_super_animal_snapshot_path(
                dataset=superanimal_name,
                model_name=model_name,
            )

        detector_path = customized_detector_checkpoint
        if detector_path is None:
            detector_path = get_super_animal_snapshot_path(
                dataset=superanimal_name,
                model_name=detector_name,
            )

        dlc_scorer = get_super_animal_scorer(
            superanimal_name, pose_model_path, detector_path
        )

        config = update_config(config, max_individuals, device)
        output_suffix = "_before_adapt"
        if video_adapt:
            # the users can pass in many videos. For now, we only use one video for
            # video adaptation. As reported in Ye et al. 2024, one video should be
            # sufficient for video adaptation.
            video_path = Path(videos[0])
            print(f"Using {video_path} for video adaptation training")

            # video inference to get pseudo label
            _video_inference_superanimal(
                [str(video_path)],
                superanimal_name,
                model_cfg=config,
                model_snapshot_path=pose_model_path,
                detector_snapshot_path=detector_path,
                max_individuals=max_individuals,
                pcutoff=pcutoff,
                batch_size=batch_size,
                detector_batch_size=detector_batch_size,
                cropping=cropping,
                dest_folder=dest_folder,
                output_suffix=output_suffix,
                plot_bboxes=plot_bboxes,
                bboxes_pcutoff=bbox_threshold,
            )

            # we prepare the pseudo dataset in the same folder of the target video
            pseudo_dataset_folder = video_path.with_name(f"pseudo_{video_path.stem}")
            pseudo_dataset_folder.mkdir(exist_ok=True)
            model_folder = pseudo_dataset_folder / "checkpoints"
            model_folder.mkdir(exist_ok=True)

            image_folder = pseudo_dataset_folder / "images"
            if image_folder.exists():
                print(f"{image_folder} exists, skipping the frame extraction")
            else:
                image_folder.mkdir()
                print(
                    f"Video frames being extracted to {image_folder} for video "
                    f"adaptation."
                )
                video_to_frames(video_path, pseudo_dataset_folder, cropping=cropping)

            anno_folder = pseudo_dataset_folder / "annotations"
            if (anno_folder / "train.json").exists() and (
                anno_folder / "test.json"
            ).exists():
                print(
                    f"{anno_folder} exists, skipping the annotation construction. "
                    f"Delete the folder if you want to re-construct pseudo annotations"
                )
            else:
                anno_folder.mkdir()

                if dest_folder is None:
                    pseudo_anno_dir = video_path.parent
                else:
                    pseudo_anno_dir = Path(dest_folder)

                pseudo_anno_name = f"{video_path.stem}_{dlc_scorer}_before_adapt.json"
                with open(pseudo_anno_dir / pseudo_anno_name, "r") as f:
                    predictions = json.load(f)

                # make sure we tune parameters inside this function such as pseudo
                # threshold etc.
                print(f"Constructing pseudo dataset at {pseudo_dataset_folder}")
                dlc3predictions_2_annotation_from_video(
                    predictions,
                    pseudo_dataset_folder,
                    config["metadata"]["bodyparts"],
                    superanimal_name,
                    pose_threshold=pseudo_threshold,
                    bbox_threshold=bbox_threshold,
                )

            model_snapshot_prefix = f"snapshot-{model_name}"
            detector_snapshot_prefix = f"snapshot-{detector_name}"

            config["runner"]["snapshot_prefix"] = model_snapshot_prefix
            config["detector"]["runner"]["snapshot_prefix"] = detector_snapshot_prefix

            # the model config's parameters need to be updated for adaptation training
            model_config_path = model_folder / "pytorch_config.yaml"
            with open(model_config_path, "w") as f:
                yaml = YAML()
                yaml.dump(config, f)

            adapted_detector_checkpoint = (
                model_folder / f"{detector_snapshot_prefix}-{detector_epochs:03}.pt"
            )
            adapted_pose_checkpoint = (
                model_folder / f"{model_snapshot_prefix}-{pose_epochs:03}.pt"
            )

            if (
                adapted_detector_checkpoint.exists()
                and adapted_pose_checkpoint.exists()
            ):
                print(
                    f"Video adaptation already ran; pose ({adapted_pose_checkpoint}) "
                    f"and detector ({adapted_detector_checkpoint}) already exist. To "
                    "rerun video adaptation training, delete the checkpoints or select"
                    "a different number of adaptation epochs. Continuing with the"
                    "existing checkpoints."
                )
            else:
                print(
                    "Running video adaptation with following parameters:\n"
                    f"  (pose training) pose_epochs: {pose_epochs}\n"
                    "  (pose) save_epochs: 1\n"
                    f"  detector_epochs: {detector_epochs}\n"
                    "  detector_save_epochs: 1\n"
                    f"  video adaptation batch size: {video_adapt_batch_size}\n"
                )
                train_file = pseudo_dataset_folder / "annotations" / "train.json"
                with open(train_file, "r") as f:
                    temp_obj = json.load(f)

                annotations = temp_obj["annotations"]
                if len(annotations) == 0:
                    print(
                        f"No valid predictions from {str(video_path)}. Check the "
                        "quality of the video"
                    )
                    return

                adaptation_train(
                    project_root=pseudo_dataset_folder,
                    model_folder=model_folder,
                    train_file="train.json",
                    test_file="test.json",
                    model_config_path=model_config_path,
                    device=device,
                    epochs=pose_epochs,
                    save_epochs=1,
                    detector_epochs=detector_epochs,
                    detector_save_epochs=1,
                    snapshot_path=pose_model_path,
                    detector_path=detector_path,
                    batch_size=video_adapt_batch_size,
                    detector_batch_size=video_adapt_batch_size,
                )

            # Set the customized checkpoint paths and
            output_suffix = "_after_adapt"
            detector_path = adapted_detector_checkpoint
            pose_model_path = adapted_pose_checkpoint

        return _video_inference_superanimal(
            videos,
            superanimal_name,
            model_cfg=config,
            model_snapshot_path=pose_model_path,
            detector_snapshot_path=detector_path,
            max_individuals=max_individuals,
            pcutoff=pcutoff,
            batch_size=batch_size,
            detector_batch_size=detector_batch_size,
            cropping=cropping,
            dest_folder=dest_folder,
            output_suffix=output_suffix,
            plot_bboxes=plot_bboxes,
            bboxes_pcutoff=bbox_threshold,
        )


--- File: deeplabcut/modelzoo/models_to_framework.json ---
{
    "dlcrnet": "tensorflow",
    "hrnet_w32": "pytorch",
    "resnet_50": "pytorch"
}


--- File: deeplabcut/modelzoo/weight_initialization.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Functions to build weight initialization parameters for SuperAnimal models"""
from pathlib import Path

import deeplabcut.modelzoo.utils as utils
from deeplabcut.core.config import read_config_as_dict
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import (
    get_super_animal_snapshot_path
)


def build_weight_init(
    cfg: dict | str | Path,
    super_animal: str,
    model_name: str,
    detector_name: str | None,
    with_decoder: bool = False,
    memory_replay: bool = False,
    customized_pose_checkpoint: str | Path | None = None,
    customized_detector_checkpoint: str | Path | None = None,
) -> WeightInitialization:
    """Builds the WeightInitialization from a SuperAnimal model for a project

    Args:
        cfg: The project's configuration, or the path to the project configuration file.
        super_animal: The SuperAnimal model with which to initialize weights.
        model_name: The type of the model architecture for which to load the weights.
        detector_name: The type of detector architecture for which to load the weights.
        with_decoder: Whether to load the decoder weights as well. If this is true,
            a conversion table must be specified for the given SuperAnimal in the
            project configuration file. See
            ``deeplabcut.modelzoo.utils.create_conversion_table`` to create a
            conversion table.
        memory_replay: Only when ``with_decoder=True``. Whether to train the model
            with memory replay, so that it predicts all SuperAnimal bodyparts.
        customized_pose_checkpoint: A customized SuperAnimal pose checkpoint, as an
            alternative to the Hugging Face one
        customized_detector_checkpoint: A customized SuperAnimal detector checkpoint, as
            an alternative to the Hugging Face one

    To build a WeightInitialization instance for a project using the conversion table
    specified in the project configuration file, use:

        ```
        from pathlib import Path
        from deeplabcut.utils.auxiliaryfunctions import read_config
        from deeplabcut.modelzoo import build_weight_init

        project_cfg = read_config("/path/to/my/project/config.yaml")
        super_animal = "superanimal_quadruped"
        weight_init = build_weight_init(
            cfg=project_cfg,
            super_animal="superanimal_quadruped",
            model_name="hrnet_w32",
            detector_name="fasterrcnn_resnet50_fpn_v2",
            with_decoder=True,
            memory_replay=False,
        )
        ```

    Returns:
        The built WeightInitialization.
    """
    if isinstance(cfg, (str, Path)):
        cfg = read_config_as_dict(cfg)

    conversion_array = None
    bodyparts = None
    if with_decoder:
        conversion_table = utils.get_conversion_table(cfg, super_animal)
        conversion_array = conversion_table.to_array()
        bodyparts = conversion_table.converted_bodyparts()

    snapshot_path = customized_pose_checkpoint
    if snapshot_path is None:
        snapshot_path = get_super_animal_snapshot_path(
            dataset=super_animal,
            model_name=model_name,
            download=True,
        )

    detector_snapshot_path = customized_detector_checkpoint
    if detector_snapshot_path is None and detector_name is not None:
        detector_snapshot_path = get_super_animal_snapshot_path(
            dataset=super_animal,
            model_name=detector_name,
            download=True,
        )

    return WeightInitialization(
        snapshot_path=snapshot_path,
        detector_snapshot_path=detector_snapshot_path,
        dataset=super_animal,
        with_decoder=with_decoder,
        memory_replay=memory_replay,
        conversion_array=conversion_array,
        bodyparts=bodyparts,
    )


--- File: deeplabcut/modelzoo/project_configs/superanimal_bird.yaml ---
# Project definitions (do not edit)
Task:
scorer:
date:
multianimalproject:
identity:


# Project path (change when moving around)
project_path:


# Default DeepLabCut engine to use for shuffle creation (either pytorch or tensorflow)
engine: pytorch


# Annotation data set configuration (and individual video cropping parameters)
video_sets:
bodyparts:
- back
- bill
- belly
- breast
- crown
- forehead
- left_eye
- left_leg
- left_wing_tip
- left_wrist
- nape
- right_eye
- right_leg
- right_wing_tip
- right_wrist
- tail_tip
- throat
- neck
- tail_left
- tail_right
- upper_spine
- upper_half_spine
- lower_half_spine
- right_foot
- left_foot
- left_half_chest
- right_half_chest
- chin
- left_tibia
- right_tibia
- lower_spine
- upper_half_neck
- lower_half_neck
- left_chest
- right_chest
- upper_neck
- left_wing_shoulder
- left_wing_elbow
- right_wing_shoulder
- right_wing_elbow
- upper_cere
- lower_cere


# Fraction of video to start/stop when extracting frames for labeling/refinement
start:
stop:
numframes2pick:


# Plotting configuration
skeleton: []
skeleton_color: black
pcutoff:
dotsize:
alphavalue:
colormap: rainbow


# Training,Evaluation and Analysis configuration
TrainingFraction:
iteration:
default_net_type:
default_augmenter:
snapshotindex:
detector_snapshotindex: -1
batch_size: 1
detector_batch_size: 1


# Cropping Parameters (for analysis and outlier frame detection)
cropping:
#if cropping is true for analysis, then set the values here:
x1:
x2:
y1:
y2:


# Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
move2corner:


# Conversion tables to fine-tune SuperAnimal weights
SuperAnimalConversionTables:


--- File: deeplabcut/modelzoo/project_configs/superanimal_quadruped.yaml ---
# Project definitions (do not edit)
Task:
scorer:
date:
multianimalproject:
identity:


# Project path (change when moving around)
project_path:


# Default DeepLabCut engine to use for shuffle creation (either pytorch or tensorflow)
engine: pytorch


# Annotation data set configuration (and individual video cropping parameters)
video_sets:
bodyparts:
- nose
- upper_jaw
- lower_jaw
- mouth_end_right
- mouth_end_left
- right_eye
- right_earbase
- right_earend
- right_antler_base
- right_antler_end
- left_eye
- left_earbase
- left_earend
- left_antler_base
- left_antler_end
- neck_base
- neck_end
- throat_base
- throat_end
- back_base
- back_end
- back_middle
- tail_base
- tail_end
- front_left_thai
- front_left_knee
- front_left_paw
- front_right_thai
- front_right_knee
- front_right_paw
- back_left_paw
- back_left_thai
- back_right_thai
- back_left_knee
- back_right_knee
- back_right_paw
- belly_bottom
- body_middle_right
- body_middle_left


# Fraction of video to start/stop when extracting frames for labeling/refinement
start:
stop:
numframes2pick:


# Plotting configuration
skeleton: []
skeleton_color: black
pcutoff:
dotsize:
alphavalue:
colormap: rainbow


# Training,Evaluation and Analysis configuration
TrainingFraction:
iteration:
default_net_type:
default_augmenter:
snapshotindex:
detector_snapshotindex: -1
batch_size: 1
detector_batch_size: 1


# Cropping Parameters (for analysis and outlier frame detection)
cropping:
#if cropping is true for analysis, then set the values here:
x1:
x2:
y1:
y2:


# Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
move2corner:


# Conversion tables to fine-tune SuperAnimal weights
SuperAnimalConversionTables:


--- File: deeplabcut/modelzoo/project_configs/superanimal_topviewmouse.yaml ---
# Project definitions (do not edit)
Task:
scorer:
date:
multianimalproject:
identity:


# Project path (change when moving around)
project_path:


# Default DeepLabCut engine to use for shuffle creation (either pytorch or tensorflow)
engine: pytorch


# Annotation data set configuration (and individual video cropping parameters)
video_sets:
bodyparts:
- nose
- left_ear
- right_ear
- left_ear_tip
- right_ear_tip
- left_eye
- right_eye
- neck
- mid_back
- mouse_center
- mid_backend
- mid_backend2
- mid_backend3
- tail_base
- tail1
- tail2
- tail3
- tail4
- tail5
- left_shoulder
- left_midside
- left_hip
- right_shoulder
- right_midside
- right_hip
- tail_end
- head_midpoint


# Fraction of video to start/stop when extracting frames for labeling/refinement
start:
stop:
numframes2pick:


# Plotting configuration
skeleton: []
skeleton_color: black
pcutoff:
dotsize:
alphavalue:
colormap: rainbow


# Training,Evaluation and Analysis configuration
TrainingFraction:
iteration:
default_net_type:
default_augmenter:
snapshotindex:
detector_snapshotindex: -1
batch_size: 1
detector_batch_size: 1


# Cropping Parameters (for analysis and outlier frame detection)
cropping:
#if cropping is true for analysis, then set the values here:
x1:
x2:
y1:
y2:


# Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
move2corner:


# Conversion tables to fine-tune SuperAnimal weights
SuperAnimalConversionTables:


--- File: deeplabcut/modelzoo/webapp/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/modelzoo/webapp/inference.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from typing import Dict

import numpy as np

import deeplabcut.pose_estimation_pytorch.modelzoo as modelzoo
from deeplabcut.pose_estimation_pytorch.apis.utils import get_inference_runners
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import update_config


class SingletonTopDownRunners:
    """Singleton class for topdown runners

    This class is a singleton class for topdown runners. It is used to
    ensure that only one instance of the topdown runners is created.

    Attrs:
        config: Configuration dictionary
        pose_model_path: Path to the pose model
        detector_model_path: Path to the detector model
        num_bodyparts: Number of bodyparts
        max_individuals: Maximum number of individuals
    """

    _instance = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(
        self,
        config,
        pose_model_path: str,
        detector_model_path: str,
        num_bodyparts: int,
        max_individuals: int,
    ):

        pose_runner, detector_runner = get_inference_runners(
            config,
            snapshot_path=pose_model_path,
            max_individuals=max_individuals,
            num_bodyparts=num_bodyparts,
            num_unique_bodyparts=0,
            detector_path=detector_model_path,
        )
        self.pose_runner = pose_runner
        self.detector_runner = detector_runner


class SuperanimalPyTorchInference:
    """Superanimal inference class

    This class is used to perform inference on a superanimal model from the
    DeepLabCut model zoo website.
    """

    def __init__(
        self,
        project_name: str,
        pose_model_type: str = "hrnet_w32",
        detector_model_type: str = "fasterrcnn_resnet50_fpn_v2",
        max_individuals: int = 30,
        device: str = "cpu",
    ):
        self.max_individuals = max_individuals
        config = modelzoo.load_super_animal_config(
            super_animal=project_name,
            model_name=pose_model_type,
            detector_name=detector_model_type,
        )
        config = update_config(config, max_individuals, device)
        self._config = config

    def initialize_models(self, pose_model_path: str, detector_model_path: str):
        self.models = SingletonTopDownRunners(
            self.config,
            pose_model_path,
            detector_model_path,
            len(self.config["bodyparts"]),
            self.max_individuals,
        )

    @property
    def config(self):
        return self._config

    def predict(self, frames: Dict[str, np.array]):

        input_images = np.array(list(frames.values()), dtype=float)

        bbox_predictions = self.models.detector_runner.inference(images=input_images)
        input_images = list(zip(input_images, bbox_predictions))
        predictions = self.models.pose_runner.inference(images=input_images)
        predictions = [
            {("markers" if k == "bodyparts" else k): v for k, v in d.items()}
            for d in predictions
        ]
        predictions = [
            {**item[1], "image_path": item[0]}
            for item in zip(frames.keys(), predictions)
        ]
        responses = {
            "joint_names": self.config["bodyparts"],
            "predictions": predictions,
        }

        return responses


--- File: deeplabcut/modelzoo/model_configs/dlcrnet.yaml ---
    # Project definitions (do not edit)
Task:
scorer:
date:
multianimalproject:
identity:

    # Project path (change when moving around)
project_path:

    # Annotation data set configuration (and individual video cropping parameters)
video_sets:
bodyparts:

    # Fraction of video to start/stop when extracting frames for labeling/refinement
start:
stop:
numframes2pick:

    # Plotting configuration
skeleton: []
skeleton_color: black
pcutoff:
dotsize:
alphavalue:
colormap:

    # Training,Evaluation and Analysis configuration
TrainingFraction:
iteration:
default_net_type:
default_augmenter:
snapshotindex:
batch_size: 1

    # Cropping Parameters (for analysis and outlier frame detection)
cropping:
    #if cropping is true for analysis, then set the values here:
x1:
x2:
y1:
y2:

    # Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
move2corner:
alpha_r: 0.02
apply_prob: 0.5
clahe: true
claheratio: 0.1
crop_sampling: hybrid
crop_size:
- 400
- 400
cropratio: 0.4
dataset:
dataset_type: multi-animal-imgaug
decay_steps: 30000
display_iters: 500
edge: false
emboss:
  alpha:
  - 0.0
  - 1.0
  embossratio: 0.1
  strength:
  - 0.5
  - 1.5
global_scale: 0.8
histeq: true
histeqratio: 0.1
init_weights:
intermediate_supervision: false
intermediate_supervision_layer: 12
location_refinement: true
locref_huber_loss: true
locref_loss_weight: 0.05
locref_stdev: 7.2801
lr_init: 0.0005
max_input_size: 1500
max_shift: 0.4
mean_pixel:
- 123.68
- 116.779
- 103.939
metadataset:
min_input_size: 64
mirror: false
multi_stage: true
multi_step:
- - 0.0001
  - 7500
- - 5.0e-05
  - 12000
- - 1.0e-05
  - 200000
net_type: resnet_50
num_idchannel: 0
num_joints: 27
num_limbs: 351
optimizer: adam
pafwidth: 20
pairwise_huber_loss: false
pairwise_loss_weight: 0.1
pairwise_predict: false
partaffinityfield_graph: []
partaffinityfield_predict: false
pos_dist_thresh: 17
pre_resize: []
rotation: 25
rotratio: 0.4
save_iters: 10000
scale_jitter_lo: 0.5
scale_jitter_up: 1.25
sharpen: false
sharpenratio: 0.3
stride: 8.0
weigh_only_present_joints: false
gradient_masking: true
weight_decay: 0.0001
weigh_part_predictions: false


--- File: deeplabcut/modelzoo/model_configs/ssdlite.yaml ---
data:
  colormode: RGB
  inference:
    normalize_images: true
  train:
    affine:
      p: 0.5
      rotation: 30
      scaling: [ 1.0, 1.0 ]
      translation: 40
    collate:
      type: ResizeFromDataSizeCollate
      min_scale: 0.4
      max_scale: 1.0
      min_short_side: 128
      max_short_side: 1152
      multiple_of: 32
      to_square: false
    hflip: true
    normalize_images: true
device: auto
model:
  type: SSDLite
  box_score_thresh: 0.6
  freeze_bn_stats: true
  freeze_bn_weights: false
runner:
  type: DetectorTrainingRunner
  key_metric: "test.mAP@50:95"
  key_metric_asc: true
  eval_interval: 10
  optimizer:
    type: AdamW
    params:
      lr: 1e-5
  scheduler:
    type: LRListScheduler
    params:
      milestones: [ 90 ]
      lr_list: [ [ 1e-6 ] ]
  snapshots:
    max_snapshots: 5
    save_epochs: 25
    save_optimizer_state: false
train_settings:
  batch_size: 8
  dataloader_workers: 0
  dataloader_pin_memory: false
  display_iters: 500
  epochs: 250

--- File: deeplabcut/modelzoo/model_configs/fasterrcnn_mobilenet_v3_large_fpn.yaml ---
data:
  colormode: RGB
  inference:
    normalize_images: true
  train:
    affine:
      p: 0.5
      rotation: 30
      scaling: [ 1.0, 1.0 ]
      translation: 40
    collate:
      type: ResizeFromDataSizeCollate
      min_scale: 0.4
      max_scale: 1.0
      min_short_side: 128
      max_short_side: 1152
      multiple_of: 32
      to_square: false
    hflip: true
    normalize_images: true
device: auto
model:
  type: FasterRCNN
  variant: fasterrcnn_mobilenet_v3_large_fpn
  box_score_thresh: 0.6
  freeze_bn_stats: true
  freeze_bn_weights: false
runner:
  type: DetectorTrainingRunner
  key_metric: "test.mAP@50:95"
  key_metric_asc: true
  eval_interval: 10
  optimizer:
    type: AdamW
    params:
      lr: 1e-5
  scheduler:
    type: LRListScheduler
    params:
      milestones: [ 90 ]
      lr_list: [ [ 1e-6 ] ]
  snapshots:
    max_snapshots: 5
    save_epochs: 25
    save_optimizer_state: false
train_settings:
  batch_size: 1
  dataloader_workers: 0
  dataloader_pin_memory: false
  display_iters: 500
  epochs: 250


--- File: deeplabcut/modelzoo/model_configs/hrnet_w32.yaml ---
data:
  colormode: RGB
  inference:
    auto_padding:
      pad_width_divisor: 32
      pad_height_divisor: 32
    normalize_images: true
  train:
    affine:
      p: 0.5
      scaling: [1.0, 1.0]
      rotation: 30
      translation: 0
    gaussian_noise: 12.75
    normalize_images: true
    auto_padding:
      pad_width_divisor: 32
      pad_height_divisor: 32
device: auto
method: td
model:
  backbone:
    type: HRNet
    model_name: hrnet_w32
    pretrained: false
    freeze_bn_stats: True
    freeze_bn_weights: False
    interpolate_branches: false
    increased_channel_count: false
  backbone_output_channels: 32
  heads:
    bodypart:
      type: HeatmapHead
      weight_init: "normal"
      predictor:
        type: HeatmapPredictor
        apply_sigmoid: false
        clip_scores: true
        location_refinement: false
        locref_std: 7.2801
      target_generator:
        type: HeatmapGaussianGenerator
        num_heatmaps: "num_bodyparts"
        pos_dist_thresh: 17
        heatmap_mode: KEYPOINT
        generate_locref: false
        locref_std: 7.2801
      criterion:
        heatmap:
          type: WeightedMSECriterion
          weight: 1.0
      heatmap_config:
        channels: [32, "num_bodyparts"]
        kernel_size: [1]
        strides: [1]
net_type: hrnet_w32
runner:
  type: PoseTrainingRunner
  key_metric: "test.mAP"
  key_metric_asc: true
  eval_interval: 10
  optimizer:
    type: AdamW
    params:
      lr: 1e-5
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-6 ], [ 1e-7 ] ]
      milestones: [ 160, 190 ]
  snapshots:
    max_snapshots: 5
    save_epochs: 25
    save_optimizer_state: false
train_settings:
  batch_size: 1
  dataloader_workers: 0
  dataloader_pin_memory: false
  display_iters: 500
  epochs: 200
  seed: 42


--- File: deeplabcut/modelzoo/model_configs/resnet_50.yaml ---
data:
  colormode: RGB
  inference:
    normalize_images: true
  train:
    affine:
      p: 0.5
      scaling: [1.0, 1.0]
      rotation: 30
      translation: 0
    gaussian_noise: 12.75
    normalize_images: true
device: auto
method: td
model:
  backbone:
    type: ResNet
    model_name: resnet50_gn
    output_stride: 16
    freeze_bn_stats: false
    freeze_bn_weights: false
  backbone_output_channels: 2048
  heads:
    bodypart:
      type: HeatmapHead
      weight_init: normal
      predictor:
        type: HeatmapPredictor
        apply_sigmoid: false
        clip_scores: true
        location_refinement: true
        locref_std: 7.2801
      target_generator:
        type: HeatmapGaussianGenerator
        num_heatmaps: "num_bodyparts"
        pos_dist_thresh: 17
        heatmap_mode: KEYPOINT
        generate_locref: true
        locref_std: 7.2801
      criterion:
        heatmap:
          type: WeightedMSECriterion
          weight: 1.0
        locref:
          type: WeightedHuberCriterion
          weight: 0.05
      heatmap_config:
        channels:
        - 2048
        - "num_bodyparts"
        kernel_size:
        - 3
        strides:
        - 2
      locref_config:
        channels:
        - 2048
        - "num_bodyparts x 2"
        kernel_size:
        - 3
        strides:
        - 2
net_type: resnet_50
runner:
  type: PoseTrainingRunner
  key_metric: "test.mAP"
  key_metric_asc: true
  eval_interval: 10
  optimizer:
    type: AdamW
    params:
      lr: 1e-5
  scheduler:
    type: LRListScheduler
    params:
      lr_list: [ [ 1e-6 ], [ 1e-7 ] ]
      milestones: [ 160, 190 ]
  snapshots:
    max_snapshots: 5
    save_epochs: 25
    save_optimizer_state: false
train_settings:
  batch_size: 1
  dataloader_workers: 0
  dataloader_pin_memory: false
  display_iters: 500
  epochs: 100
  seed: 42


--- File: deeplabcut/modelzoo/model_configs/fasterrcnn_resnet50_fpn_v2.yaml ---
data:
  colormode: RGB
  inference:
    normalize_images: true
  train:
    affine:
      p: 0.5
      rotation: 30
      scaling: [ 1.0, 1.0 ]
      translation: 40
    collate:
      type: ResizeFromDataSizeCollate
      min_scale: 0.4
      max_scale: 1.0
      min_short_side: 128
      max_short_side: 1152
      multiple_of: 32
      to_square: false
    hflip: true
    normalize_images: true
device: auto
model:
  type: FasterRCNN
  variant: fasterrcnn_resnet50_fpn_v2
  box_score_thresh: 0.6
  freeze_bn_stats: true
  freeze_bn_weights: false
runner:
  type: DetectorTrainingRunner
  key_metric: "test.mAP@50:95"
  key_metric_asc: true
  eval_interval: 10
  optimizer:
    type: AdamW
    params:
      lr: 1e-5
  scheduler:
    type: LRListScheduler
    params:
      milestones: [ 90 ]
      lr_list: [ [ 1e-6 ] ]
  snapshots:
    max_snapshots: 5
    save_epochs: 25
    save_optimizer_state: false
train_settings:
  batch_size: 1
  dataloader_workers: 0
  dataloader_pin_memory: false
  display_iters: 500
  epochs: 250

--- File: deeplabcut/modelzoo/conversion_tables/conversion_table_quadruped.csv ---
ap10k,animalpose,stanforddogs,cheetah,horse,webapp,MasterName
nose,nose,Nose,nose,Nose,nose,nose
,,,,,,upper_jaw
,,,,,,lower_jaw
,,,,,,mouth_end_right
,,,,,,mouth_end_left
right_eye,right_eye,R_Eye,r_eye,Eye,right_eye,right_eye
,right_ear,R_EarBase,,,right_ear,right_earbase
,,R_EarTip,,,,right_earend
,,,,,,right_antler_base
,,,,,,right_antler_end
left_eye,left_eye,L_Eye,l_eye,,left_eye,left_eye
,left_ear,L_EarBase,,,left_ear,left_earbase
,,L_EarTip,,,,left_earend
,,,,,,left_antler_base
,,,,,,left_antler_end
neck,,,neck_base,,,neck_base
,,,,,,neck_end
,throat,Throat,,,throat,throat_base
,,,,,,throat_end
,withers,Withers,,Wither,withers,back_base
,,,,,,back_end
,,,spine,,,back_middle
root_of_tail,tailbase,TailBase,tail_base,,tailset,tail_base
,,TailEnd,tail_tip,,,tail_end
left_shoulder,left_front_elbow,L_F_Elbow,l_shoulder,,left_front_elbow,front_left_thai
,left_front_knee,L_F_Knee,l_front_knee,,,front_left_knee
left_front_paw,left_front_paw,L_F_Paw,l_front_paw,Nearfrontfoot,left_front_paw,front_left_paw
right_shoulder,right_front_elbow,R_F_Elbow,r_shoulder,Elbow,right_front_elbow,front_right_thai
,right_front_knee,R_F_Knee,r_front_knee,,,front_right_knee
right_front_paw,right_front_paw,R_F_Paw,r_front_paw,Offfrontfoot,right_front_paw,front_right_paw
left_back_paw,left_back_paw,L_B_Paw,l_back_paw,Nearhindfoot,left_back_paw,back_left_paw
left_hip,left_back_elbow,L_B_Elbow,l_hip,,left_back_stifle,back_left_thai
right_hip,right_back_elbow,R_B_Elbow,r_hip,Stifle,right_back_stifle,back_right_thai
left_knee,left_back_knee,L_B_Knee,l_back_knee,,,back_left_knee
right_knee,right_back_knee,R_B_Knee,r_back_knee,,,back_right_knee
right_back_paw,right_back_paw,R_B_Paw,r_back_paw,Offhindfoot,right_back_paw,back_right_paw
,,,,,,belly_bottom
,,,,,,body_middle_right
,,,,,,body_middle_left

--- File: deeplabcut/modelzoo/conversion_tables/conversion_table_topview.csv ---
treadmill_ole,swimming_ole,openfield_ole,MackenzieMausHaus, ChanLab,Daniel3Mouse,dlc-openfield,EPM ,FST,LBD,OFT,Mostafizur,3CSI,BM,TwoWhiteMice,MasterName
head,head,head,nose,Nose,snout,snout,nose,nose,nose,nose,snout,nose,nose,Nose,nose
,,,leftearbase,Ear_left,leftear,leftear,earl,earl,earl,earl,leftear,earl,earl,Left_ear,left_ear
,,,rightearbase,Ear_right,rightear,rightear,earr,earr,earr,earr,rightear,earr,earr,Right_ear,right_ear
,,,lefteartip,,,,,,,,,,,,left_ear_tip
,,,righteartip,,,,,,,,,,,,right_ear_tip
,,,lefteye,,,,,,,,,,,,left_eye
,,,righteye,,,,,,,,,,,,right_eye
spine 1,spine 1,,spine1,,shoulder,,neck,neck,neck,neck,shoulder,neck,neck,,neck
,,,spine2,,spine1,,,,,,spine1,,,,mid_back
spine 2,spine 2,middle,spine3,Center,spine2,,bodycentre,bodycentre,bodycentre,bodycentre,spine2,bodycenter,bodycenter,Centroid,mouse_center
,,,spine4,,spine3,,,,,,spine3,,,,mid_backend
spine 3,spine 3,,spine5,,spine4,,,,,,spine4,,,,mid_backend2
spine 4,spine 4,,spine6,,,,,,,,,,,,mid_backend3
base ,base ,tailbase,tailbase,Tail_base,tailbase,tailbase,tailbase,tailbase,tailbase,tailbase,tailbase,tailbase,tailbase,Tail_base,tail_base
,,,tail1,,tail1,,,,,,tail1,,,,tail1
tail 25,tail 25,,tail2,,tail2,,,,,,tail2,,,,tail2
,,,tail3,,,,tailcentre,tailcentre,tailcentre,tailcentre,,tailcenter,tailcenter,,tail3
tail 50 ,tail 50,,tail4,, ,,,,,,,,,,tail4
tail 75,tail 75,,tail5,, ,,,,,,,,,,tail5
,,,leftshoulder,, ,,,,,,,,,,left_shoulder
,,,leftside,, ,,bcl,bcl,bcl,bcl,,bcl,bcl,Left_lateral,left_midside
,,,lefthip,Lateral_left,,,hipl,hipl,hipl,hipl,,hipl,hipl,,left_hip
,,,rightshoulder,,,,,,,,,,,,right_shoulder
,,,rightside,,,,bcr,bcr,bcr,bcr,,bcr,bcr,Right_lateral,right_midside
,,,righthip,Lateral_right,,,hipr,hipr,hipr,hipr,,hipr,hipr,,right_hip
tail 100,tail 100,tailtip,,Tail_end,tailend,,tailtip,tailtip,tailtip,tailtip,tailend,tailtip,tailtip,Tail_end,tail_end
,,,,,,,headcentre,headcentre,headcentre,headcentre,,headcenter,headcenter,,head_midpoint


--- File: deeplabcut/modelzoo/generalized_data_converter/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from .utils import add_skeleton, customized_colormap, create_modelprefix


--- File: deeplabcut/modelzoo/generalized_data_converter/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import glob
import os
import pickle
from pathlib import Path

import numpy as np
import pandas as pd

from deeplabcut.utils import auxiliaryfunctions
from deeplabcut.modelzoo.generalized_data_converter.datasets.materialize import (
    SingleDLC_config,
)


def threshold_kpts(config_path, h5path, threshold_mean=0.9, threshold_min=0.1):

    df = pd.read_hdf(h5path)

    scorer = df.columns.get_level_values("scorer").unique()[0]
    try:
        data = df[scorer]["individual0"]
    except:
        data = df[scorer]

    cfg = auxiliaryfunctions.read_config(config_path)

    bodyparts = cfg["multianimalbodyparts"]

    thresholded_bpts = []

    for bpt in bodyparts:
        _mean = data[bpt]["likelihood"].mean()
        _min = data[bpt]["likelihood"].min()
        _var = data[bpt]["likelihood"].var()
        if _mean > threshold_mean and _min > threshold_min:
            thresholded_bpts.append(bpt)
        print(bpt, "mean", _mean)
        print(bpt, "min", _min)
        print(bpt, "var", _var)

    print("thresholded kpts", thresholded_bpts)
    return thresholded_bpts
    ret = []
    print(ret)
    return ret


def create_dummy_config_file_from_h5(
    proj_root, reference_h5, taskname="dummytask", scorer="dummyscorer", date="March30"
):
    """
    Assuming at least labeled-data folder is there
    """

    cfg_template = SingleDLC_config()

    df = pd.read_hdf(reference_h5)

    print(df)

    pattern = glob.glob(os.path.join(proj_root, "labeled-data", "*"))

    labeled_folders = [f.split("/")[-1] for f in pattern]

    video_sets = {
        f"{folder}.mp4": {"crop": "0, 400, 0, 400"} for folder in labeled_folders
    }

    # bodyparts = df[scorer]['bodyparts']

    bodyparts = list(df.columns.get_level_values("bodyparts").unique())
    scorer = df.columns.get_level_values("scorer").unique()[0]

    modify_dict = dict(
        Task=taskname,
        project_path=proj_root,
        scorer=scorer,
        date=date,
        video_sets=video_sets,
        bodyparts=bodyparts,
        TrainingFraction=[0.95],
    )

    cfg_template.create_cfg(proj_root, modify_dict)


def create_dummy_config_file_from_pickle(
    proj_root,
    reference_pickle,
    video_path,
    taskname="dummytask",
    scorer="dummyscorer",
    date="March30",
):
    """
    Assuming at least labeled-data folder is there
    """

    cfg_template = SingleDLC_config()

    with open(reference_pickle, "rb") as f:

        pickle_obj = pickle.load(f)

    # bodyparts  = pickle_obj['keypoint_names']
    bodyparts = [
        "tail",
        "spine4",
        "spine3",
        "spine2",
        "spine1",
        "head",
        "nose",
        "right ear",
        "left ear",
    ]

    video_name = video_path.split("/")[-1]

    video_sets = {f"{video_path}": {"crop": "0, 400, 0, 400"}}

    modify_dict = dict(
        Task=taskname,
        project_path=proj_root,
        scorer=scorer,
        date=date,
        video_sets=video_sets,
        bodyparts=bodyparts,
        TrainingFraction=[0.95],
    )

    cfg_template.create_cfg(".", modify_dict)


def create_video_h5_from_pickle(proj_root, cfg, reference_pickle, videopath):

    with open(reference_pickle, "rb") as f:

        pickle_obj = pickle.load(f)

    # bodyparts  = pickle_obj['keypoint_names']

    bodyparts = [
        "tail",
        "spine4",
        "spine3",
        "spine2",
        "spine1",
        "head",
        "nose",
        "right ear",
        "left ear",
    ]

    video_name = videopath.split("/")[-1]

    video_key = f"{video_name}"  # .replace('.top.ir.mp4', '')

    print("video_key", video_key)

    print(list(pickle_obj.keys()))

    detections = pickle_obj[video_key]

    nframes = len(detections)

    xyz_labs = ["x", "y", "likelihood"]

    scorer = cfg["scorer"]

    keypoint_names = cfg["bodyparts"]

    product = [[scorer], keypoint_names, xyz_labs]

    names = ["scorer", "bodyparts", "coords"]
    columnindex = pd.MultiIndex.from_product(product, names=names)
    imagenames = [f"frame{i}" for i in range(nframes)]
    data = np.zeros((len(imagenames), len(columnindex))) * np.nan
    df = pd.DataFrame(data, columns=columnindex, index=imagenames)

    for imagename, kpts in zip(imagenames, detections):

        for kpt_id, kpt_name in enumerate(keypoint_names):

            df.loc[imagename][scorer, kpt_name, "x"] = kpts[kpt_id, 0]
            df.loc[imagename][scorer, kpt_name, "y"] = kpts[kpt_id, 1]
            df.loc[imagename][scorer, kpt_name, "likelihood"] = kpts[kpt_id, 2]

    vname = Path(videopath).stem
    DLCscorer = ""

    coords = [0, 400, 0, 400]
    trainFraction = cfg["TrainingFraction"][0]
    modelfolder = os.path.join(
        cfg["project_path"],
        str(auxiliaryfunctions.get_model_folder(trainFraction, 0, cfg)),
    )

    path_test_config = Path(modelfolder) / "test" / "pose_cfg.yaml"
    test_cfg = auxiliaryfunctions.read_plainconfig(path_test_config)

    start = 0
    stop = 10
    fps = 10
    dictionary = {
        "start": start,
        "stop": stop,
        "run_duration": stop - start,
        "Scorer": DLCscorer,
        "DLC-model-config file": test_cfg,
        "fps": fps,
        "batch_size": test_cfg["batch_size"],
        "frame_dimensions": (400, 400),
        "nframes": nframes,
        "iteration (active-learning)": cfg["iteration"],
        "cropping": cfg["cropping"],
        "training set fraction": trainFraction,
        "cropping_parameters": coords,
    }
    metadata = {"data": dictionary}

    dataname = os.path.join(proj_root, vname + DLCscorer + ".h5")

    metadata_path = dataname.split(".h5")[0] + "_meta.pickle"

    with open(metadata_path, "wb") as f:
        pickle.dump(metadata, f, pickle.HIGHEST_PROTOCOL)

    df.to_hdf(dataname, "df_with_missing", format="table", mode="w")


def add_skeleton(config_path, pretrain_model_name):

    modelzoo_names = ["superquadruped", "supertopview"]

    assert pretrain_model_name in modelzoo_names

    super_quadruped = [
        ("left_eye", "right_eye"),
        ("left_eye", "left_earbase"),
        ("right_eye", "right_earbase"),
        ("left_eye", "nose"),
        ("right_eye", "nose"),
        ("nose", "throat_base"),
        ("throat_base", "back_base"),
        ("tail_base", "back_base"),
        ("throat_base", "front_left_thai"),
        ("front_left_thai", "front_left_knee"),
        ("front_left_knee", "front_left_paw"),
        ("throat_base", "front_right_thai"),
        ("front_right_thai", "front_right_knee"),
        ("front_right_knee", "front_right_paw"),
        ("tail_base", "back_left_thai"),
        ("back_left_thai", "back_left_knee"),
        ("back_left_knee", "back_left_paw"),
        ("tail_base", "back_right_thai"),
        ("back_right_thai", "back_right_knee"),
        ("back_right_knee", "back_right_paw"),
    ]

    skeleton_dict = {"superquadruped": super_quadruped, "supertopview": None}

    skeleton = skeleton_dict[pretrain_model_name]

    cfg = auxiliaryfunctions.read_config(config_path)
    cfg["skeleton"] = skeleton
    print(f"overwriting skeleton for {config_path}")
    auxiliaryfunctions.write_config(config_path, cfg)


def customized_colormap(config_path):
    # look for all symmetric keypoints
    # make symmetric keypoints the same color

    cfg = auxiliaryfunctions.read_config(config_path)
    bodyparts = cfg["multianimalbodyparts"]
    n_bodyparts = len(cfg["multianimalbodyparts"])

    import matplotlib.pyplot as plt

    cmap = plt.cm.get_cmap("rainbow", n_bodyparts)

    colors = [cmap(i) for i in range(n_bodyparts)]

    visited = set()
    for kpt_id in range(len(bodyparts)):

        bodypart = bodyparts[kpt_id]
        if "left" in bodypart:
            ref_color = colors[kpt_id]
            temp = bodypart.replace("left", "right")
            if temp in bodyparts:
                temp_id = bodyparts.index(temp)
                colors[temp_id] = ref_color

    def ret_function(i):
        return colors[i]

    return ret_function


def create_modelprefix(modelprefix):
    import shutil

    shutil.copytree(
        "template-dlc-models",
        os.path.join(modelprefix, "dlc-models"),
        dirs_exist_ok=True,
    )


if __name__ == "__main__":

    customized_colormap("hei")


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/coco.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import copy
import json
import os

from deeplabcut.modelzoo.generalized_data_converter.datasets.base import BasePoseDataset


class COCOPoseDataset(BasePoseDataset):
    def __init__(
        self,
        proj_root,
        dataset_name,
        train_filename="train.json",
        shuffle=None,
    ):

        super(COCOPoseDataset, self).__init__()

        self.meta["dataset_name"] = dataset_name
        self.meta["proj_root"] = proj_root

        self.proj_root = proj_root
        self.annotations_by_category = {}

        self.train_json_obj = (
            self._load_json(train_filename)
            if shuffle is None
            else self._load_json(
                train_filename.replace(".json", f"_shuffle{shuffle}.json")
            )
        )
        self.test_json_obj = (
            self._load_json("test.json")
            if shuffle is None
            else self._load_json(f"test_shuffle{shuffle}.json")
        )

        self.populate_generic()

    def _load_json(self, json_fn):
        path = os.path.join(self.proj_root, "annotations", json_fn)
        with open(path, "r") as f:
            json_obj = json.load(f)
        return json_obj

    def populate_generic(self):

        temp_train_images = copy.deepcopy(self.train_json_obj["images"])
        temp_test_images = copy.deepcopy(self.test_json_obj["images"])

        for image in temp_train_images + temp_test_images:
            image_path = image["file_name"]
            # if os.sep not in image_path:
            # assuming the file_name is mmpose style, i.e. only the image name is stored
            # so we need to add back absolute path

            image["file_name"] = os.path.join(self.proj_root, "images", image_path)

        self.generic_train_images = temp_train_images
        self.generic_test_images = temp_test_images

        self.generic_train_annotations = self.train_json_obj["annotations"]

        self.generic_test_annotations = self.test_json_obj["annotations"]

        self.meta["categories"] = self.test_json_obj["categories"][0]

        self._build_maps()

        print(f"Before checking trainset {self.meta['dataset_name']}")

        self.whether_anno_image_match(
            self.generic_train_images, self.generic_train_annotations
        )

        print(f"Before checking testset {self.meta['dataset_name']}")

        self.whether_anno_image_match(
            self.generic_test_images, self.generic_test_annotations
        )


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/base_dlc.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import pickle

import numpy as np
import pandas as pd

from deeplabcut.modelzoo.generalized_data_converter.datasets.base import BasePoseDataset
from deeplabcut.utils import auxiliaryfunctions


class BaseDLCPoseDataset(BasePoseDataset):

    def __init__(self, proj_root, dataset_name, shuffle=1, modelprefix=""):
        super(BaseDLCPoseDataset, self).__init__()

        assert proj_root != None and dataset_name != None

        self.meta["dataset_name"] = dataset_name
        self.meta["proj_root"] = proj_root
        self.meta["shuffle"] = shuffle
        self.meta["modelprefix"] = modelprefix

        self.proj_root = proj_root

        if modelprefix:
            config_file = os.path.join(self.proj_root, modelprefix + "_config.yaml")
        else:
            config_file = os.path.join(self.proj_root, "config.yaml")

        cfg = auxiliaryfunctions.read_config(config_file)

        task = cfg["Task"]

        scorer = cfg["scorer"]

        datasets_folder = os.path.join(
            self.proj_root,
            auxiliaryfunctions.GetTrainingSetFolder(cfg),
        )

        self.datasets_folder = datasets_folder

        trainingFraction = int(cfg["TrainingFraction"][0] * 100)

        path_dlc_collected = os.path.join(datasets_folder, f"CollectedData_{scorer}.h5")

        path_dlc_document = os.path.join(
            datasets_folder,
            f"Documentation_data-{task}_{trainingFraction}shuffle{shuffle}.pickle",
        )

        df = pd.read_hdf(path_dlc_collected)

        self.dlc_df = df

        with open(path_dlc_document, "rb") as f:
            document_data = pickle.load(f)

        train_indices = document_data[1]
        # index 2 is test indices
        test_indices = document_data[2]

        train_images = df.index[train_indices]
        test_images = df.index[test_indices]

        self.dlc_images = np.hstack([train_images, test_images])

        df_train = df.loc[train_images]

        df_test = df.loc[test_images]

        self.coco_train = self._df2generic(df_train)

        offset = len(self.coco_train["images"])

        self.coco_test = self._df2generic(df_test, image_id_offset=offset)

        self.populate_generic()

    def _df2generic(self, df, image_id_offset=0):
        raise NotImplementedError()

    def populate_generic(self):

        self.generic_train_images = self.coco_train["images"]
        self.generic_test_images = self.coco_test["images"]
        self.generic_train_annotations = self.coco_train["annotations"]
        self.generic_test_annotations = self.coco_test["annotations"]

        self.meta["categories"] = self.coco_test["categories"][0]

        # to build maps for later analysis
        self._build_maps()

        print(f"Before checking trainset {self.meta['dataset_name']}")

        self.whether_anno_image_match(
            self.generic_train_images, self.generic_train_annotations
        )

        print(f"Before checking testset {self.meta['dataset_name']}")

        self.whether_anno_image_match(
            self.generic_test_images, self.generic_test_annotations
        )


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/single_dlc_dataframe.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
from pathlib import Path

import numpy as np
import pandas as pd

from deeplabcut.generate_training_dataset.trainingsetmanipulation import (
    parse_video_filenames,
)
from deeplabcut.modelzoo.generalized_data_converter.datasets.base import BasePoseDataset
from deeplabcut.modelzoo.generalized_data_converter.datasets.utils import (
    calc_bboxes_from_keypoints,
    read_image_shape_fast,
)
from deeplabcut.utils import auxfun_multianimal, auxiliaryfunctions, conversioncode


def merge_annotateddatasets(cfg):
    """
    Merges all the h5 files for all labeled-datasets (from individual videos).

    This is a bit of a mess because of cross platform compatibility.

    Within platform comp. is straightforward. But if someone labels on windows and wants to train on a unix cluster or colab...
    """
    AnnotationData = []
    data_path = Path(os.path.join(cfg["project_path"], "labeled-data"))
    videos = cfg["video_sets"].keys()
    video_filenames = parse_video_filenames(videos)
    for filename in video_filenames:
        file_path = os.path.join(
            data_path / filename, f'CollectedData_{cfg["scorer"]}.h5'
        )
        try:
            data = pd.read_hdf(file_path)
            conversioncode.guarantee_multiindex_rows(data)
            if data.columns.levels[0][0] != cfg["scorer"]:
                print(
                    f"{file_path} labeled by a different scorer. This data will not be utilized in training dataset creation. If you need to merge datasets across scorers, see https://github.com/DeepLabCut/DeepLabCut/wiki/Using-labeled-data-in-DeepLabCut-that-was-annotated-elsewhere-(or-merge-across-labelers)"
                )
                continue
            AnnotationData.append(data)
        except FileNotFoundError:
            print(file_path, " not found (perhaps not annotated).")

    if not len(AnnotationData):
        print(
            "Annotation data was not found by splitting video paths (from config['video_sets']). An alternative route is taken..."
        )
        AnnotationData = conversioncode.merge_windowsannotationdataONlinuxsystem(cfg)
        if not len(AnnotationData):
            print("No data was found!")
            return

    AnnotationData = pd.concat(AnnotationData).sort_index()
    # When concatenating DataFrames with misaligned column labels,
    # all sorts of reordering may happen (mainly depending on 'sort' and 'join')
    # Ensure the 'bodyparts' level agrees with the order in the config file.
    if cfg.get("multianimalproject", False):
        (
            _,
            uniquebodyparts,
            multianimalbodyparts,
        ) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
        bodyparts = multianimalbodyparts + uniquebodyparts
    else:
        bodyparts = cfg["bodyparts"]
    AnnotationData = AnnotationData.reindex(
        bodyparts, axis=1, level=AnnotationData.columns.names.index("bodyparts")
    )

    return AnnotationData


class SingleDLCDataFrame(BasePoseDataset):

    def __init__(self, proj_root, dataset_name):
        super(SingleDLCDataFrame, self).__init__()
        self.meta["max_individuals"] = 1
        assert proj_root != None and dataset_name != None
        self.proj_root = proj_root
        self.dataset_name = dataset_name
        self.meta["dataset_name"] = dataset_name
        self.meta["proj_root"] = proj_root
        config_path = Path(proj_root) / "config.yaml"
        # read config
        cfg = auxiliaryfunctions.read_config(config_path)
        # get the train folder

        Data = merge_annotateddatasets(
            cfg,
        )

        # now with this data, we construct necessary generic data

        self.dlc_df = Data

        images = self.dlc_df.index

        ratio = 0.9

        df_train = self.dlc_df.iloc[: int(len(images) * ratio)]
        df_test = self.dlc_df.iloc[int(len(images) * ratio) :]

        self.coco_train = self._df2generic(df_train)

        offset = len(self.coco_train["images"])

        self.coco_test = self._df2generic(df_test, image_id_offset=offset)

        self.populate_generic()

    def populate_generic(self):

        self.generic_train_images = self.coco_train["images"]
        self.generic_test_images = self.coco_test["images"]
        self.generic_train_annotations = self.coco_train["annotations"]
        self.generic_test_annotations = self.coco_test["annotations"]

        self.meta["categories"] = self.coco_test["categories"][0]

        # to build maps for later analysis
        self._build_maps()

        print(f"Before checking trainset {self.meta['dataset_name']}")

        self.whether_anno_image_match(
            self.generic_train_images, self.generic_train_annotations
        )

        print(f"Before checking testset {self.meta['dataset_name']}")

        self.whether_anno_image_match(
            self.generic_test_images, self.generic_test_annotations
        )

    def _df2generic(self, df, image_id_offset=0):

        bpts = df.columns.get_level_values("bodyparts").unique().tolist()

        coco_categories = []

        # single animal only has individual0

        category = {
            "name": "individual0",
            "id": 0,
            "supercategory": "animal",
        }

        category["keypoints"] = bpts

        coco_categories.append(category)

        coco_images = []
        coco_annotations = []

        annotation_id = 0
        image_id = -1

        for _, file_name in enumerate(df.index):
            data = df.loc[file_name]

            # skipping all nan

            if np.isnan(data.to_numpy()).all():
                continue

            image_id += 1
            category_id = 0
            kpts = data.to_numpy().reshape(-1, 2)
            keypoints = np.zeros((len(kpts), 3))

            keypoints[:, :2] = kpts

            is_visible = ~pd.isnull(kpts).all(axis=1)

            keypoints[:, 2] = np.where(is_visible, 2, 0)

            num_keypoints = is_visible.sum()

            bbox_margin = 20

            xmin, ymin, xmax, ymax = calc_bboxes_from_keypoints(
                [keypoints],
                slack=bbox_margin,
                clip=True,
            )[0][:4]

            w = xmax - xmin
            h = ymax - ymin
            area = w * h
            bbox = np.nan_to_num([xmin, ymin, w, h])
            keypoints = np.nan_to_num(keypoints.flatten())

            annotation_id += 1
            annotation = {
                "image_id": image_id + image_id_offset,
                "num_keypoints": num_keypoints,
                "keypoints": keypoints,
                "id": annotation_id,
                "category_id": category_id,
                "area": area,
                "bbox": bbox,
                "iscrowd": 0,
            }
            if np.sum(keypoints) != 0:

                coco_annotations.append(annotation)

            # I think width and height are important

            if isinstance(file_name, tuple):
                image_path = os.path.join(self.proj_root, *list(file_name))
            else:
                image_path = os.path.join(self.proj_root, file_name)

            _, height, width = read_image_shape_fast(image_path)

            image = {
                "file_name": image_path,
                "width": width,
                "height": height,
                "id": image_id + image_id_offset,
            }
            coco_images.append(image)

        ret_obj = {
            "images": coco_images,
            "annotations": coco_annotations,
            "categories": coco_categories,
        }
        return ret_obj


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/ma_dlc_dataframe.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
from pathlib import Path

import numpy as np
import pandas as pd

from deeplabcut.generate_training_dataset.trainingsetmanipulation import (
    parse_video_filenames,
)
from deeplabcut.modelzoo.generalized_data_converter.datasets.base import BasePoseDataset
from deeplabcut.modelzoo.generalized_data_converter.datasets.utils import (
    calc_bboxes_from_keypoints,
    read_image_shape_fast,
)
from deeplabcut.utils import auxfun_multianimal, auxiliaryfunctions, conversioncode


def merge_annotateddatasets(cfg):
    """
    Merges all the h5 files for all labeled-datasets (from individual videos).

    This is a bit of a mess because of cross platform compatibility.

    Within platform comp. is straightforward. But if someone labels on windows and wants to train on a unix cluster or colab...
    """
    AnnotationData = []
    data_path = Path(os.path.join(cfg["project_path"], "labeled-data"))
    videos = cfg["video_sets"].keys()
    video_filenames = parse_video_filenames(videos)
    for filename in video_filenames:
        file_path = os.path.join(
            data_path / filename, f'CollectedData_{cfg["scorer"]}.h5'
        )
        try:
            data = pd.read_hdf(file_path)
            conversioncode.guarantee_multiindex_rows(data)
            if data.columns.levels[0][0] != cfg["scorer"]:
                print(
                    f"{file_path} labeled by a different scorer. This data will not be utilized in training dataset creation. If you need to merge datasets across scorers, see https://github.com/DeepLabCut/DeepLabCut/wiki/Using-labeled-data-in-DeepLabCut-that-was-annotated-elsewhere-(or-merge-across-labelers)"
                )
                continue
            AnnotationData.append(data)
        except FileNotFoundError:
            print(file_path, " not found (perhaps not annotated).")

    if not len(AnnotationData):
        print(
            "Annotation data was not found by splitting video paths (from config['video_sets']). An alternative route is taken..."
        )
        AnnotationData = conversioncode.merge_windowsannotationdataONlinuxsystem(cfg)
        if not len(AnnotationData):
            print("No data was found!")
            return

    AnnotationData = pd.concat(AnnotationData).sort_index()
    # When concatenating DataFrames with misaligned column labels,
    # all sorts of reordering may happen (mainly depending on 'sort' and 'join')
    # Ensure the 'bodyparts' level agrees with the order in the config file.
    if cfg.get("multianimalproject", False):
        (
            _,
            uniquebodyparts,
            multianimalbodyparts,
        ) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
        bodyparts = multianimalbodyparts + uniquebodyparts
    else:
        bodyparts = cfg["bodyparts"]
    AnnotationData = AnnotationData.reindex(
        bodyparts, axis=1, level=AnnotationData.columns.names.index("bodyparts")
    )

    return AnnotationData


class MaDLCDataFrame(BasePoseDataset):

    def __init__(self, proj_root, dataset_name):
        super(MaDLCDataFrame, self).__init__()
        assert proj_root != None and dataset_name != None
        self.proj_root = proj_root
        self.dataset_name = dataset_name
        self.meta["dataset_name"] = dataset_name
        self.meta["proj_root"] = proj_root
        config_path = Path(proj_root) / "config.yaml"
        # read config
        cfg = auxiliaryfunctions.read_config(config_path)
        # get the train folder

        Data = merge_annotateddatasets(
            cfg,
        )

        # now with this data, we construct necessary generic data

        self.dlc_df = Data

        images = self.dlc_df.index

        ratio = 0.9

        df_train = self.dlc_df.iloc[: int(len(images) * ratio)]
        df_test = self.dlc_df.iloc[int(len(images) * ratio) :]

        self.coco_train = self._df2generic(df_train)

        offset = len(self.coco_train["images"])

        self.coco_test = self._df2generic(df_test, image_id_offset=offset)

        self.populate_generic()

    def populate_generic(self):

        self.generic_train_images = self.coco_train["images"]
        self.generic_test_images = self.coco_test["images"]
        self.generic_train_annotations = self.coco_train["annotations"]
        self.generic_test_annotations = self.coco_test["annotations"]

        self.meta["categories"] = self.coco_test["categories"][0]

        # to build maps for later analysis
        self._build_maps()

        print(f"Before checking trainset {self.meta['dataset_name']}")

        self.whether_anno_image_match(
            self.generic_train_images, self.generic_train_annotations
        )

        print(f"Before checking testset {self.meta['dataset_name']}")

        self.whether_anno_image_match(
            self.generic_test_images, self.generic_test_annotations
        )

    def _df2generic(self, df, image_id_offset=0):

        individuals = df.columns.get_level_values("individuals").unique().tolist()

        unique_bpts = []

        if "single" in individuals:
            unique_bpts.extend(
                df.xs("single", level="individuals", axis=1)
                .columns.get_level_values("bodyparts")
                .unique()
            )
        multi_bpts = (
            df.xs(individuals[0], level="individuals", axis=1)
            .columns.get_level_values("bodyparts")
            .unique()
            .tolist()
        )

        coco_categories = []

        # assuming all individuals have the same name and same category id

        individual = individuals[0]

        category = {
            "name": individual,
            "id": 0,
            "supercategory": "animal",
        }

        if individual == "single":
            category["keypoints"] = unique_bpts
        else:
            category["keypoints"] = multi_bpts

        coco_categories.append(category)

        coco_images = []
        coco_annotations = []

        annotation_id = 0
        image_id = -1
        for _, file_name in enumerate(df.index):
            data = df.loc[file_name]

            # skipping all nan
            if np.isnan(data.to_numpy()).all():
                continue

            image_id += 1

            for individual_id, individual in enumerate(individuals):
                category_id = 0
                try:
                    kpts = (
                        data.xs(individual, level="individuals")
                        .to_numpy()
                        .reshape((-1, 2))
                    )
                except:
                    # somehow there are duplicates. So only use the first occurrence
                    data = data.iloc[0]
                    kpts = (
                        data.xs(individual, level="individuals")
                        .to_numpy()
                        .reshape((-1, 2))
                    )

                keypoints = np.zeros((len(kpts), 3))

                keypoints[:, :2] = kpts

                is_visible = ~pd.isnull(kpts).all(axis=1)

                keypoints[:, 2] = np.where(is_visible, 2, 0)

                num_keypoints = is_visible.sum()

                bbox_margin = 20

                xmin, ymin, xmax, ymax = calc_bboxes_from_keypoints(
                    [keypoints],
                    slack=bbox_margin,
                    clip=True,
                )[0][:4]

                w = xmax - xmin
                h = ymax - ymin
                area = w * h
                bbox = np.nan_to_num([xmin, ymin, w, h])
                keypoints = np.nan_to_num(keypoints.flatten())

                annotation_id += 1
                annotation = {
                    "image_id": image_id + image_id_offset,
                    "num_keypoints": num_keypoints,
                    "keypoints": keypoints,
                    "id": annotation_id,
                    "category_id": category_id,
                    "area": area,
                    "bbox": bbox,
                    "iscrowd": 0,
                }
                if np.sum(keypoints) != 0:
                    coco_annotations.append(annotation)

            # I think width and height are important

            if isinstance(file_name, tuple):
                image_path = os.path.join(self.proj_root, *list(file_name))
            else:
                image_path = os.path.join(self.proj_root, file_name)

            _, height, width = read_image_shape_fast(image_path)

            image = {
                "file_name": image_path,
                "width": width,
                "height": height,
                "id": image_id + image_id_offset,
            }
            coco_images.append(image)

        ret_obj = {
            "images": coco_images,
            "annotations": coco_annotations,
            "categories": coco_categories,
        }
        return ret_obj


if __name__ == "__main__":
    dataset = MaDLCDataFrame("/mnt/md0/shaokai/daniel3mouse", "3mouse")
    dataset.summary()


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/multi.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import warnings

from deeplabcut.modelzoo.generalized_data_converter.datasets.materialize import (
    mat_func_factory,
)


class MultiSourceDataset:
    """
    Parameters:
    iid_ood_split:  {'iid' : ['dataset1', 'dataset2'],
                     'ood' : ['dataset3', 'dataset4'] }



    """

    def __init__(self, dataset_name, datasets, table_path):
        self.datasets = datasets
        #
        self.name2genericdataset = {}

        # useful maps for analysis
        self.imageid2filename = {}
        self.imageid2datasetname = {}
        self.datasetname2imageids = {}
        #
        self.dataset_name = dataset_name

        names = []
        for dataset in datasets:

            # Must project datasets to same keypoint space before merging
            if table_path != None:
                dataset.project_with_conversion_table(table_path)
            name = dataset.meta["dataset_name"]
            names.append(name)
            self.name2genericdataset[name] = dataset

        self.meta = {}
        self.meta["dataset_name"] = dataset_name
        # after conversion, all datasets have same categories
        self.meta["categories"] = dataset.meta["categories"]

        # map id from local scope to global
        self._update_imgids()

        (
            self.train_images,
            self.test_images,
            self.train_annotations,
            self.test_annotations,
        ) = self._merge_datasets(self.name2genericdataset)
        self.meta["name2genericdataset"] = self.name2genericdataset

        # only build maps after images are merged and ids are in global scope
        self._build_maps()

    def summary(self):
        print(f"Summary of dataset {self.dataset_name}")
        print("Decomposition of multi source datasets:")
        for dataset_name, dataset in self.name2genericdataset.items():
            n_images = len(dataset.generic_train_images) + len(
                dataset.generic_test_images
            )
            n_annotations = len(dataset.generic_train_annotations) + len(
                dataset.generic_test_annotations
            )
            print(f"{dataset_name} has {n_images} images, {n_annotations} annotations")

        print(f"total train images : {len(self.train_images)}")
        print(f"total test images : {len(self.test_images)}")

    def _build_maps(self):

        # shared by both scenarios

        species_set = set()
        for dataset_name, dataset in self.name2genericdataset.items():
            # I could of course do this during merge to save compute, but doing it here makes the logic cleaner to understand
            total_images = dataset.generic_train_images + dataset.generic_test_images

            for image in total_images:
                image_id = image["id"]
                image_name = image["file_name"]
                self.imageid2filename[image_id] = image_name

                self.imageid2datasetname[image_id] = dataset_name

                if dataset_name == "AwA-Pose":
                    species_set.add(image_name.split("/")[-1].split("_")[0])
        self.meta["imageid2datasetname"] = self.imageid2datasetname

        max_num = 0
        for dataset_name, dataset in self.name2genericdataset.items():
            max_num = max(max_num, dataset.meta["max_individuals"])
        self.meta["max_individuals"] = max_num
        dataset_name = self.meta["dataset_name"]
        print(f"Max individual in {dataset_name} is {max_num}")

    def whether_anno_image_match(self, images, annotations):
        """
        Every image id should be annotated at least once
        There should not be any image that is not being annotated
        There should not be any annotation for beyond the set of given images
        """

        image_ids = set([image["id"] for image in images])

        annotation_image_ids = set([anno["image_id"] for anno in annotations])

        if image_ids != annotation_image_ids:
            print("images-annotations", image_ids - annotation_image_ids)
            print("annotations-images", annotation_image_ids - image_ids)

            warnings.warn("annotation and image ids do not match")

            # This is constrain is too hard
            # assert len(annotation_image_ids - image_ids) == 0, "You can't have annotation on non-existed images"

    def _update_imgids(self):
        """
        update image ids for both image and annotation

        If datasets are merged, their image id, annotation id will conflict because they are defined within their own local scope. Therefore, we will need to put these ids in the global scope

        """

        from collections import defaultdict

        dataset_id_pool = defaultdict(set)
        all_datasets = self.name2genericdataset.values()

        total_number_images = 0
        total_number_annotations = 0
        for dataset in all_datasets:
            total_number_images += len(dataset.generic_train_images) + len(
                dataset.generic_test_images
            )
            total_number_annotations += len(dataset.generic_train_annotations) + len(
                dataset.generic_test_annotations
            )

        global_image_id_pool = set(range(total_number_images))
        global_annotation_id_pool = set(range(total_number_annotations))

        for dataset_name, dataset in self.name2genericdataset.items():

            local_image_id_map = defaultdict(int)
            local_anno_id_map = defaultdict(int)

            traintest_images = (
                dataset.generic_train_images + dataset.generic_test_images
            )
            traintest_annotations = (
                dataset.generic_train_annotations + dataset.generic_test_annotations
            )

            for img in traintest_images:

                new_image_id = global_image_id_pool.pop()
                local_image_id_map[img["id"]] = new_image_id
                img["id"] = new_image_id
                dataset_id_pool[dataset_name].add(img["id"])

            for anno in traintest_annotations:
                anno["image_id"] = local_image_id_map[anno["image_id"]]
                new_anno_id = global_annotation_id_pool.pop()
                local_anno_id_map[anno["id"]] = new_anno_id
                anno["id"] = new_anno_id

            self.whether_anno_image_match(traintest_images, traintest_annotations)

        from functools import reduce

        count = 0
        for k, v in dataset_id_pool.items():
            count += len(v)
        print("size of the summation", count)
        union = reduce(set.union, dataset_id_pool.values())
        print("size of the union", len(union))

    def _merge_datasets(self, name2dataset):
        """
        Merged datasets into common list

        # only do this when iid/ood split is done

        """

        merged_train_images = []
        merged_test_images = []
        merged_train_annotations = []
        merged_test_annotations = []

        for dataset_name, dataset in name2dataset.items():

            train_images = dataset.generic_train_images
            test_images = dataset.generic_test_images
            train_annotations = dataset.generic_train_annotations
            test_annotations = dataset.generic_test_annotations

            merged_train_images.extend(train_images)
            merged_test_images.extend(test_images)
            merged_train_annotations.extend(train_annotations)
            merged_test_annotations.extend(test_annotations)

        print("Checking merged dataset")

        merged_traintest_images = merged_train_images + merged_test_images
        merged_traintest_annotations = (
            merged_train_annotations + merged_test_annotations
        )

        self.whether_anno_image_match(
            merged_traintest_images, merged_traintest_annotations
        )

        return (
            merged_train_images,
            merged_test_images,
            merged_train_annotations,
            merged_test_annotations,
        )

    def __eq__(self, other_dataset):

        if isinstance(other_dataset, BasePoseDataset):

            train_images1 = set(map(raw_2_imagename_with_id, self.train_images))
            train_images2 = set(
                map(raw_2_imagename, other_dataset.generic_train_images)
            )

            test_images1 = set(map(raw_2_imagename_with_id, self.test_images))
            test_images2 = set(map(raw_2_imagename, other_dataset.generic_test_images))
            if train_images1 == train_images2 and test_images1 == test_images2:
                print(
                    f'dataset {self.meta["dataset_name"]} and {other_dataset.meta["dataset_name"]} are equivalent'
                )
                return True
            else:
                print(
                    f'dataset {self.meta["dataset_name"]} and {other_dataset.meta["dataset_name"]} are NOT equivalent'
                )
                return False

        else:
            return NotImplementedError("Not existed")

    def materialize(
        self,
        proj_root,
        framework="coco",
        train_all=False,
        deepcopy=False,
        full_image_path=True,
    ):

        # can't be set to true at the same time. This will cause bugs
        assert sum([train_all, full_image_path]) != 2

        mat_func = mat_func_factory(framework)

        self.meta["mat_datasets"] = self.name2genericdataset

        if train_all:
            # for pretrian phase, we can just train everything including the test part
            self.train_images += self.test_images
            self.train_annotations += self.test_annotations

        mat_func(
            proj_root,
            self.train_images,
            self.test_images,
            self.train_annotations,
            self.test_annotations,
            self.meta,
            deepcopy=deepcopy,
            full_image_path=full_image_path,
        )


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from .ma_dlc import MaDLCPoseDataset
from .multi import MultiSourceDataset
from .coco import COCOPoseDataset
from .materialize import mat_func_factory
from .single_dlc import SingleDLCPoseDataset
from .single_dlc_dataframe import SingleDLCDataFrame
from .ma_dlc_dataframe import MaDLCDataFrame


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/materialize.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import json
import os
import pickle
import shutil
from pathlib import Path

import numpy as np
import pandas as pd
import scipy.io as sio
import yaml

import deeplabcut.compat as compat
from deeplabcut.generate_training_dataset.multiple_individuals_trainingsetmanipulation import (
    create_multianimaltraining_dataset,
    format_multianimal_training_data,
)
from deeplabcut.generate_training_dataset.trainingsetmanipulation import (
    create_training_dataset,
)
from deeplabcut.generate_training_dataset.trainingsetmanipulation import (
    format_training_data as format_single_training_data,
)
from deeplabcut.utils import auxiliaryfunctions


def get_filename(filename):
    if type(filename) == tuple:
        filename = os.path.join(*filename)
    return filename


def modify_train_test_cfg(config_path, shuffle=1, modelprefix=""):
    # get train_cfg from main cfg
    # use dlcr net
    # use gradient masking
    # set batch size as 8
    trainposeconfigfile, testposeconfigfile, snapshotfolder = (
        compat.return_train_network_path(
            config_path, shuffle=shuffle, modelprefix=modelprefix, trainingsetindex=0
        )
    )

    train_cfg = auxiliaryfunctions.read_plainconfig(trainposeconfigfile)
    train_cfg["multi_stage"] = True
    train_cfg["batch_size"] = 8
    train_cfg["gradient_masking"] = True

    auxiliaryfunctions.write_plainconfig(trainposeconfigfile, train_cfg)

    test_cfg = auxiliaryfunctions.read_plainconfig(testposeconfigfile)
    test_cfg["multi_stage"] = True
    test_cfg["batch_size"] = 8
    test_cfg["gradient_masking"] = True

    auxiliaryfunctions.write_plainconfig(testposeconfigfile, test_cfg)


class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return super(NpEncoder, self).default(obj)


class SingleDLC_config:
    def __init__(self):
        Task = ""  # could be dataset name
        project_path = ""
        scorer = ""  # random stuff
        date = ""  # random stuff
        video_sets = ""  # has to be used for labeled data
        skeleton = ""  # could be arbitrary
        bodyparts = ""  # either single or multi
        start = 0  # not sure
        stop = 1  # not sure
        numframes2pick = 42  # does not matter
        skeleton_color = "black"
        pcutoff = 0.6
        dotsize = 8
        alphavalue = 0.7
        colormap = "rainbow"
        TrainingFraction = ""  # need to be filled correctly
        iteration = 0
        default_net_type = "resnet_50"
        default_augmenter = "imgaug"
        snapshotindex = -1
        batch_size = 8
        cropping = False
        croppedtraining = False
        multianimalproject = False
        uniquebodyparts = []
        x1 = 0
        x2 = 640
        y1 = 277
        y2 = 624
        corer2move2 = [50, 50]
        move2corner = True
        identity = False
        self.cfg = {
            k: v for k, v in vars().items() if "__" not in k and "self" not in k
        }

    def create_cfg(self, proj_root, kwargs):
        self.cfg.update(kwargs)
        with open(os.path.join(proj_root, "config.yaml"), "w") as f:
            yaml.dump(self.cfg, f)


class MaDLC_config:
    def __init__(self):
        """
        Plain text only for generating templates
        Some variables can be configured by the user later
        """

        Task = ""  # could be dataset name
        project_path = ""
        scorer = ""  # random stuff
        date = ""  # random stuff
        video_sets = ""  # has to be used for labeled data
        individuals = ""  # number of individuals
        multianimalbodyparts = ""  # keypoints
        skeleton = ""  # could be arbitrary
        bodyparts = ""  # either single or multi
        start = 0  # not sure
        stop = 1  # not sure
        numframes2pick = 42  # does not matter
        skeleton_color = "black"
        pcutoff = 0.6
        dotsize = 8
        alphavalue = 0.7
        colormap = "rainbow"
        TrainingFraction = ""  # need to be filled correctly
        iteration = 0
        default_net_type = "resnet_50"
        default_augmenter = "multi-animal-imgaug"
        snapshotindex = -1
        batch_size = 8
        cropping = False
        croppedtraining = True
        multianimalproject = True
        uniquebodyparts = []
        x1 = 0
        x2 = 640
        y1 = 277
        y2 = 624
        corer2move2 = [50, 50]
        move2corner = True
        identity = False
        self.cfg = {
            k: v for k, v in vars().items() if "__" not in k and "self" not in k
        }

    def create_cfg(self, proj_root, kwargs):
        self.cfg.update(kwargs)
        with open(os.path.join(proj_root, "config.yaml"), "w") as f:
            yaml.dump(self.cfg, f)


def _generic2madlc(
    proj_root,
    train_images,
    test_images,
    train_annotations,
    test_annotations,
    meta,
    deepcopy=False,
    full_image_path=True,
    append_image_id=True,
):
    """
    Within DeepLabCut, if we don't explicitly call deeplabcut.create_traindataset(), the train and test split might just be arbitrarily messed up. So here we need to calculate train and test indices to

    Args:
    proj_root where to materialize the data

    """

    assert full_image_path, "DLC wants full image path"

    os.makedirs(os.path.join(proj_root, "labeled-data"), exist_ok=True)

    cfg_template = MaDLC_config()

    individuals = [f"individual{i}" for i in range(meta["max_individuals"])]

    bodyparts = meta["categories"]["keypoints"]

    scorer = "maDLC_scorer"
    # this line is taken from dlc's multi animal dataset creation function
    train_fraction = round(
        len(train_images) * 1.0 / (len(train_images) + len(test_images)), 2
    )

    # need to fake a video path
    # let's use individual dataset names as fake video name
    # merged_dataset_name = '_'.join(meta['mat_datasets'])
    video_sets = {
        f"{dataset_name}.mp4": {"crop": "0, 400, 0, 400"}
        for dataset_name in meta["mat_datasets"]
    }

    modify_dict = dict(
        Task=meta["dataset_name"],
        project_path=proj_root,
        individuals=individuals,
        scorer=scorer,
        date="March30",
        video_sets=video_sets,
        bodyparts="MULTI!",
        TrainingFraction=[train_fraction],
        multianimalbodyparts=list(bodyparts),
    )

    cfg_template.create_cfg(proj_root, modify_dict)
    # what's special in dlc or madlc creation is that we will need to
    # use dlc's code for creating the project structure
    # because you don't want to write your own. It's a lot of lines of code
    # But at least we can focus on labeled-data

    imageid2datasetname = meta["imageid2datasetname"]

    for dataset_name in meta["mat_datasets"]:
        os.makedirs(
            os.path.join(proj_root, "labeled-data", dataset_name), exist_ok=True
        )

    # also, to make sure the split is right, we will have to pass the right indices

    columnindex = pd.MultiIndex.from_product(
        [[scorer], individuals, bodyparts, ["x", "y"]],
        names=["scorer", "individuals", "bodyparts", "coords"],
    )

    # it's important to put train first so the train_fraction parameter can work correctly
    total_images = train_images + test_images
    total_annotations = train_annotations + test_annotations

    # DLC uses relative dest as index into dataframe
    imageid2relativedest = {}
    count = 0
    for image in total_images:
        image_id = image["id"]
        file_name = image["file_name"]
        image_name = file_name.split(os.sep)[-1]
        pre, suffix = image_name.split(".")
        if append_image_id == True:
            dest_image_name = f"{pre}_{image_id}.{suffix}"
        else:
            dest_image_name = image_name
        # the generic data has original pointers to images in the original folders
        # Here, we have to change the image name and location of these to fit corresponding framework's convention

        dataset_name = imageid2datasetname[image_id]

        dest = os.path.join(proj_root, "labeled-data", dataset_name, dest_image_name)
        if deepcopy:
            shutil.copy(file_name, dest)
        else:
            try:
                os.symlink(file_name, dest)
            except Exception as e:
                pass

        relative_dest = os.path.join("labeled-data", dataset_name, dest_image_name)

        imageid2relativedest[image_id] = relative_dest

    temp_count = 0
    for dataset_name, dataset in meta["mat_datasets"].items():

        dataset_total_images = (
            dataset.generic_train_images + dataset.generic_test_images
        )
        dataset_total_annotations = (
            dataset.generic_train_annotations + dataset.generic_test_annotations
        )

        dataset_index = []

        for image in dataset_total_images:
            image_id = image["id"]
            relative_dest = imageid2relativedest[image_id]
            dataset_index.append(relative_dest)

        raw_data = np.zeros((len(dataset_total_images), len(columnindex))) * np.nan
        df = pd.DataFrame(raw_data, columns=columnindex, index=dataset_index)
        # so we know where to put the next annotation if there are multiple individuals in that image
        imageid2filledindividualcount = {}

        image_ids = []
        for anno in dataset_total_annotations:
            keypoints = anno["keypoints"]
            image_id = anno["image_id"]
            image_ids.append(image_id)
            if image_id not in imageid2filledindividualcount:
                imageid2filledindividualcount[image_id] = 0
            else:
                imageid2filledindividualcount[image_id] += 1
            individual_id = imageid2filledindividualcount[image_id]

            file_name = imageid2relativedest[image_id]
            for kpt_id, kpt_name in enumerate(meta["categories"]["keypoints"]):
                coord = keypoints[3 * kpt_id : 3 * kpt_id + 3]
                # note dlc does not yet have visibility flag
                # need to be careful here to assign right keypoints to right people
                if coord[0] > 0 and coord[1] > 0:
                    # leave them to NaN if values are 0
                    df.loc[file_name][
                        scorer, f"individual{individual_id}", kpt_name, "x"
                    ] = coord[0]
                    df.loc[file_name][
                        scorer, f"individual{individual_id}", kpt_name, "y"
                    ] = coord[1]
                elif coord[2] == -1:
                    df.loc[file_name][
                        scorer, f"individual{individual_id}", kpt_name, "x"
                    ] = -1
                    df.loc[file_name][
                        scorer, f"individual{individual_id}", kpt_name, "y"
                    ] = -1
        df.to_hdf(
            os.path.join(
                proj_root, "labeled-data", dataset_name, f"CollectedData_{scorer}.h5"
            ),
            key="df_with_missing",
            mode="w",
        )
    # paf_graph default as None. But I am not sure how to do better
    create_multianimaltraining_dataset(
        os.path.join(proj_root, "config.yaml"), paf_graph=None
    )

    # dlc's merge_annotation messes up my indices, so I will need to overwrite the documentation file
    # I could have done it in a more elegant way if I could modify part of DLC source code, but for backward compatibility reasons, overriding documentation is smarter

    config_path = os.path.join(proj_root, "config.yaml")

    cfg = auxiliaryfunctions.read_config(config_path)

    train_folder = os.path.join(proj_root, auxiliaryfunctions.GetTrainingSetFolder(cfg))

    datafilename, metafilename = auxiliaryfunctions.GetDataandMetaDataFilenames(
        train_folder, train_fraction, 1, cfg
    )

    modify_train_test_cfg(config_path)

    dlc_df = pd.read_hdf(os.path.join(train_folder, f"CollectedData_{scorer}.h5"))

    # I strip off video info from the naming. For horse10, I need to get it back
    parent_trace = {}

    def _filter(image):
        file_name = image["file_name"]

        image_name = file_name.split(os.sep)[-1]
        video_folder = file_name.split(os.sep)[-2]
        pre, suffix = image_name.split(".")
        image_id = image["id"]
        if append_image_id:
            ret = f"{pre}_{image_id}.{suffix}"
        else:
            ret = image_name
        parent_trace[ret] = video_folder
        return ret

    _filter_train_images = list(map(_filter, train_images))
    _filter_test_images = list(map(_filter, test_images))

    with open(os.path.join(train_folder, "parent_trace.pickle"), "wb") as f:
        pickle.dump(parent_trace, f)

    trainIndices = [
        idx
        for idx, image in enumerate(dlc_df.index)
        if get_filename(image).split(os.sep)[-1] in _filter_train_images
    ]
    testIndices = [
        idx
        for idx, image in enumerate(dlc_df.index)
        if get_filename(image).split(os.sep)[-1] in _filter_test_images
    ]

    with open(metafilename, "rb") as f:
        metafile = pickle.load(f)

    metafile[1] = trainIndices
    metafile[2] = testIndices

    with open(metafilename, "wb") as f:
        pickle.dump(metafile, f)

    # need to overwrite the data pickle file too

    nbodyparts = len(bodyparts)

    if "individuals" not in dlc_df.columns.names:
        old_idx = dlc_df.columns.to_frame()
        old_idx.insert(0, "individuals", "")
        dlc_df.columns = pd.MultiIndex.from_frame(old_idx)

    data = format_multianimal_training_data(dlc_df, trainIndices, cfg["project_path"])

    datafilename = datafilename.split(".mat")[0] + ".pickle"

    print(f"overwriting data file {datafilename}")

    with open(os.path.join(proj_root, datafilename), "wb") as f:

        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)


def _generic2sdlc(
    proj_root,
    train_images,
    test_images,
    train_annotations,
    test_annotations,
    meta,
    deepcopy=False,
    full_image_path=True,
    append_image_id=True,
):

    assert full_image_path, "DLC wants full image path"

    os.makedirs(os.path.join(proj_root, "labeled-data"), exist_ok=True)

    cfg_template = SingleDLC_config()

    bodyparts = meta["categories"]["keypoints"]
    scorer = "singleDLC_scorer"

    train_fraction = round(
        len(train_images) * 1.0 / (len(train_images) + len(test_images)), 2
    )

    # need to fake a video path
    # let's use individual dataset names as fake video name

    video_sets = {
        f"{dataset_name}.mp4": {"crop": "0, 400, 0, 400"}
        for dataset_name in meta["mat_datasets"].keys()
    }

    modify_dict = dict(
        Task=meta["dataset_name"],
        project_path=proj_root,
        scorer=scorer,
        date="March30",
        bodyparts=list(bodyparts),
        video_sets=video_sets,
        TrainingFraction=[train_fraction],
    )

    cfg_template.create_cfg(proj_root, modify_dict)

    imageid2datasetname = meta["imageid2datasetname"]

    for dataset_name in meta["mat_datasets"]:
        os.makedirs(
            os.path.join(proj_root, "labeled-data", dataset_name), exist_ok=True
        )

    columnindex = pd.MultiIndex.from_product(
        [[scorer], bodyparts, ["x", "y"]], names=["scorer", "bodyparts", "coords"]
    )

    total_images = train_images + test_images
    total_annotations = train_annotations + test_annotations

    # DLC uses relative dest as index
    imageid2relativedest = {}

    for image in total_images:
        imageid = image["id"]
        filename = image["file_name"]
        datasetname = imageid2datasetname[imageid]
    count = 0
    for image in total_images:
        image_id = image["id"]
        file_name = image["file_name"]

        image_name = file_name.split(os.sep)[-1]
        pre, suffix = image_name.split(".")

        if append_image_id == True:
            dest_image_name = f"{pre}_{image_id}.{suffix}"
        else:
            dest_image_name = image_name
        # the generic data has original pointers to images in the original folders
        # Here, we have to change the image name and location of these to fit corresponding framework's convention

        dataset_name = imageid2datasetname[image_id]

        dest = os.path.join(proj_root, "labeled-data", dataset_name, dest_image_name)
        if deepcopy:
            shutil.copy(file_name, dest)
        else:
            try:
                os.symlink(file_name, dest)
            except:
                pass

        if dataset_name == "AwA-Pose":
            count += 1

        relative_dest = os.path.join("labeled-data", dataset_name, dest_image_name)
        imageid2relativedest[image_id] = relative_dest

    # so we know where to put the next annotation if there are multiple individuals in that image

    for dataset_name, dataset in meta["mat_datasets"].items():

        dataset_total_images = (
            dataset.generic_train_images + dataset.generic_test_images
        )
        dataset_total_annotations = (
            dataset.generic_train_annotations + dataset.generic_test_annotations
        )

        dataset_index = []
        freq = {}
        for image in dataset_total_images:
            filename = image["file_name"]

            image_id = image["id"]
            relative_dest = imageid2relativedest[image_id]

            dataset_index.append(relative_dest)

        raw_data = np.zeros((len(dataset_total_images), len(columnindex))) * np.nan

        dataset_index = dataset_index

        df = pd.DataFrame(raw_data, columns=columnindex, index=dataset_index)

        for idx, anno in enumerate(dataset_total_annotations):
            keypoints = np.array(anno["keypoints"])
            image_id = anno["image_id"]

            file_name = imageid2relativedest[image_id]

            for kpt_id, kpt_name in enumerate(meta["categories"]["keypoints"]):
                coord = keypoints[3 * kpt_id : 3 * kpt_id + 3]
                # note dlc does not yet have visibility flag
                # need to be careful here to assign right keypoints to right people

                if coord[0] > 0 and coord[1] > 0:

                    df.loc[file_name][scorer, kpt_name, "x"] = coord[0]
                    df.loc[file_name][scorer, kpt_name, "y"] = coord[1]
                elif coord[2] == -1:
                    # if -1, this visibility flag means a given keypoint was not annotated in the original dataset
                    df.loc[file_name][scorer, kpt_name, "x"] = -1
                    df.loc[file_name][scorer, kpt_name, "y"] = -1

        df = df.dropna(how="all")
        df.to_hdf(
            os.path.join(
                proj_root, "labeled-data", dataset_name, f"CollectedData_{scorer}.h5"
            ),
            key="df_with_missing",
            mode="w",
        )

    create_training_dataset(os.path.join(proj_root, "config.yaml"))

    # dlc's merge_annotation messes up my indices, so I will need to overwrite the documentation file
    # I could have done it in a more elegant way if I could modify part of DLC source code, but for backward compatibility reasons, overriding documentation is smarter

    config_path = os.path.join(proj_root, "config.yaml")

    cfg = auxiliaryfunctions.read_config(config_path)

    train_folder = os.path.join(proj_root, auxiliaryfunctions.GetTrainingSetFolder(cfg))

    datafilename, metafilename = auxiliaryfunctions.GetDataandMetaDataFilenames(
        train_folder, train_fraction, 1, cfg
    )

    modify_train_test_cfg(config_path)

    dlc_df = pd.read_hdf(os.path.join(train_folder, f"CollectedData_{scorer}.h5"))

    parent_trace = {}

    def _filter(image):
        file_name = image["file_name"]
        image_name = file_name.split(os.sep)[-1]
        video_folder = file_name.split(os.sep)[-2]
        pre, suffix = image_name.split(".")
        image_id = image["id"]
        if append_image_id:
            ret = f"{pre}_{image_id}.{suffix}"
        else:
            ret = image_name

        parent_trace[ret] = video_folder

        return ret

    _filter_train_images = list(map(_filter, train_images))
    _filter_test_images = list(map(_filter, test_images))

    with open(os.path.join(train_folder, "parent_trace.pickle"), "wb") as f:
        pickle.dump(parent_trace, f)

    trainIndices = [
        idx
        for idx, image in enumerate(dlc_df.index)
        if get_filename(image).split(os.sep)[-1] in _filter_train_images
    ]
    testIndices = [
        idx
        for idx, image in enumerate(dlc_df.index)
        if get_filename(image).split(os.sep)[-1] in _filter_test_images
    ]

    with open(metafilename, "rb") as f:
        metafile = pickle.load(f)

    metafile[1] = trainIndices
    metafile[2] = testIndices

    with open(metafilename, "wb") as f:
        pickle.dump(metafile, f)

    # need to overwrite the true data file too
    nbodyparts = len(bodyparts)

    data, MatlabData = format_single_training_data(
        dlc_df, trainIndices, nbodyparts, cfg["project_path"]
    )

    print(f"overwriting data file {datafilename}")

    sio.savemat(os.path.join(datafilename), {"dataset": MatlabData})


def _generic2coco(
    proj_root,
    train_images,
    test_images,
    train_annotations,
    test_annotations,
    meta,
    deepcopy: bool = False,
    full_image_path: bool = True,
    append_image_id: bool = True,
    no_image_copy: bool = False,
):
    """
    Take generic data and create coco structure
    My generic definition of coco structure:
    images
      ...
    annotations
    - train.json
    - test.json

    Args:
        deepcopy: Only when no_image_copy=False. If False, images are not copied from
            their original location and symlinks are created instead.
        full_image_path: Only when no_image_copy=False. If True, the ``file_name`` for
            the images in the annotation files contain the resolved path to the images.
            Otherwise, a relative path is used.
        append_image_id: Only when no_image_copy=False. Appends the image IDs in the
            dataset to the image names.
        no_image_copy: Instead of copying images to the COCO dataset, the full paths to
            the images in the original dataset are used in the annotations.
    """

    os.makedirs(os.path.join(proj_root, "images"), exist_ok=True)
    os.makedirs(os.path.join(proj_root, "annotations"), exist_ok=True)

    # from new path to old_path
    lookuptable = {}

    for annotation in train_annotations + test_annotations:
        if "iscrowd" not in annotation:
            annotation["iscrowd"] = 0

        keypoints = annotation["keypoints"]
        for kpt_id, kpt_name in enumerate(meta["categories"]["keypoints"]):
            coord = keypoints[3 * kpt_id : 3 * kpt_id + 3]
            if coord[0] < 0 or coord[1] < 0:
                coord[2] = -1

    broken_links = []
    # copying images via symbolic link
    for image in train_images + test_images:
        # important to resolve the filepath! Otherwise, errors can occur when running
        # this code from Jupyter Notebooks
        src = Path(image["file_name"]).resolve()
        image_id = image["id"]

        if not src.exists():
            print("problem comes from", image["source_dataset"])
            print(src)
            broken_links.append(image_id)
            continue

        file_name = str(src)
        dest = src
        if not no_image_copy:
            # in dlc, some images have same name but under different folder
            # we used to use a parent folder to distinguish them, but it's only
            # applicable to DLC so here it's easier to append an id into the filename

            # not to repeatedly add image id in memory replay training
            dest_image_name = src.name
            if append_image_id:
                dest_image_name = f"{src.stem}_{image_id}{src.suffix}"

            dest = Path(proj_root) / "images" / dest_image_name
            dest = dest.resolve()

            file_name = str(Path(*dest.parts[-2:]))
            if full_image_path:
                file_name = str(dest)

            if deepcopy:
                shutil.copy(src, dest)
            else:
                try:
                    os.symlink(src, dest)
                except Exception as err:
                    print(f"Could not create a symlink from {src} to {dest}: {err}")
                    pass

        image["file_name"] = file_name
        lookuptable[dest] = src

    train_annotations = [
        train_anno
        for train_anno in train_annotations
        if train_anno["image_id"] not in broken_links
    ]
    test_annotations = [
        test_anno
        for test_anno in test_annotations
        if test_anno["image_id"] not in broken_links
    ]

    with open(os.path.join(proj_root, "annotations", "train.json"), "w") as f:

        train_json_obj = dict(
            images=train_images,
            annotations=train_annotations,
            categories=[meta["categories"]],
        )

        json.dump(train_json_obj, f, indent=4, cls=NpEncoder)

    with open(os.path.join(proj_root, "annotations", "test.json"), "w") as f:
        test_json_obj = dict(
            images=test_images,
            annotations=test_annotations,
            categories=[meta["categories"]],
        )

        json.dump(test_json_obj, f, indent=4, cls=NpEncoder)

    return lookuptable


def mat_func_factory(framework):
    assert framework in [
        "coco",
        "sdlc",
        "madlc",
    ], f"Does not support framework {framework}"
    if framework == "madlc":
        mat_func = _generic2madlc
    elif framework == "coco":
        mat_func = _generic2coco
    elif framework == "sdlc":
        mat_func = _generic2sdlc

    return mat_func


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from functools import lru_cache

import numpy as np
from PIL import Image


def calc_bboxes_from_keypoints(data, slack=0, offset=0, clip=False):
    data = np.asarray(data)
    if data.shape[-1] < 3:
        raise ValueError("Data should be of shape (n_animals, n_bodyparts, 3)")

    if data.ndim != 3:
        data = np.expand_dims(data, axis=0)
    bboxes = np.full((data.shape[0], 5), np.nan)
    bboxes[:, :2] = np.nanmin(data[..., :2], axis=1) - slack  # X1, Y1
    bboxes[:, 2:4] = np.nanmax(data[..., :2], axis=1) + slack  # X2, Y2
    bboxes[:, -1] = np.nanmean(data[..., 2])  # Average confidence
    bboxes[:, [0, 2]] += offset
    if clip:
        coord = bboxes[:, :4]
        coord[coord < 0] = 0
    return bboxes


@lru_cache(maxsize=None)
def read_image_shape_fast(path):
    # Blazing fast and does not load the image into memory
    with Image.open(path) as img:
        width, height = img.size
        return len(img.getbands()), height, width


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/ma_dlc.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os

import numpy as np
import pandas as pd

from deeplabcut.modelzoo.generalized_data_converter.datasets.base_dlc import (
    BaseDLCPoseDataset,
)
from deeplabcut.modelzoo.generalized_data_converter.datasets.utils import (
    calc_bboxes_from_keypoints,
    read_image_shape_fast,
)


class MaDLCPoseDataset(BaseDLCPoseDataset):
    def __init__(self, proj_root, dataset_name, shuffle=1, modelprefix=""):
        super(MaDLCPoseDataset, self).__init__(
            proj_root, dataset_name, shuffle=shuffle, modelprefix=modelprefix
        )

    def _df2generic(self, df, image_id_offset=0):

        individuals = df.columns.get_level_values("individuals").unique().tolist()

        unique_bpts = []

        if "single" in individuals:
            unique_bpts.extend(
                df.xs("single", level="individuals", axis=1)
                .columns.get_level_values("bodyparts")
                .unique()
            )
        multi_bpts = (
            df.xs(individuals[0], level="individuals", axis=1)
            .columns.get_level_values("bodyparts")
            .unique()
            .tolist()
        )

        coco_categories = []

        # assuming all individuals have the same name and same category id

        individual = individuals[0]

        category = {
            "name": individual,
            "id": 0,
            "supercategory": "animal",
        }

        if individual == "single":
            category["keypoints"] = unique_bpts
        else:
            category["keypoints"] = multi_bpts

        coco_categories.append(category)

        coco_images = []
        coco_annotations = []

        annotation_id = 0
        image_id = -1
        for _, file_name in enumerate(df.index):
            data = df.loc[file_name]

            # skipping all nan
            if np.isnan(data.to_numpy()).all():
                continue

            image_id += 1

            for individual_id, individual in enumerate(individuals):
                category_id = 0
                try:
                    kpts = (
                        data.xs(individual, level="individuals")
                        .to_numpy()
                        .reshape((-1, 2))
                    )
                except:
                    # somehow there are duplicates. So only use the first occurrence
                    data = data.iloc[0]
                    kpts = (
                        data.xs(individual, level="individuals")
                        .to_numpy()
                        .reshape((-1, 2))
                    )

                keypoints = np.zeros((len(kpts), 3))

                keypoints[:, :2] = kpts

                is_visible = ~pd.isnull(kpts).all(axis=1)

                keypoints[:, 2] = np.where(is_visible, 2, 0)

                num_keypoints = is_visible.sum()

                bbox_margin = 20

                xmin, ymin, xmax, ymax = calc_bboxes_from_keypoints(
                    [keypoints],
                    slack=bbox_margin,
                    clip=True,
                )[0][:4]

                w = xmax - xmin
                h = ymax - ymin
                area = w * h
                bbox = np.nan_to_num([xmin, ymin, w, h])
                keypoints = np.nan_to_num(keypoints.flatten())

                annotation_id += 1
                annotation = {
                    "image_id": image_id + image_id_offset,
                    "num_keypoints": num_keypoints,
                    "keypoints": keypoints,
                    "id": annotation_id,
                    "category_id": category_id,
                    "area": area,
                    "bbox": bbox,
                    "iscrowd": 0,
                }
                if np.sum(keypoints) != 0:
                    coco_annotations.append(annotation)

            # I think width and height are important

            if isinstance(file_name, tuple):
                image_path = os.path.join(self.proj_root, *list(file_name))
            else:
                image_path = os.path.join(self.proj_root, file_name)

            _, height, width = read_image_shape_fast(image_path)

            image = {
                "file_name": image_path,
                "width": width,
                "height": height,
                "id": image_id + image_id_offset,
            }
            coco_images.append(image)

        ret_obj = {
            "images": coco_images,
            "annotations": coco_annotations,
            "categories": coco_categories,
        }
        return ret_obj


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/base.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import copy
import os
import warnings

import numpy as np

from deeplabcut.modelzoo.generalized_data_converter.conversion_table import (
    get_conversion_table,
)
from deeplabcut.modelzoo.generalized_data_converter.datasets.materialize import (
    mat_func_factory,
)


def raw_2_imagename_with_id(image):
    """
    raw image data has filename and id.
    we modify the imagename such that itis composed of
    both original imagename and image id
    """

    file_name = image["file_name"]
    image_name = file_name.split(os.sep)[-1]
    pre, suffix = image_name.split(".")
    image_id = image["id"]
    return f"{pre}_{image_id}.{suffix}"


def raw_2_imagename(image):
    """
    Only getting the imagename part from the image object
    """

    file_name = image["file_name"]
    image_name = file_name.split(os.sep)[-1]
    return image_name


class BasePoseDataset:
    """
    Dual representation of generic and raw data. For classes that inherits this class,
    the raw data is kept but generic data is populated so you have dual representation.
    """

    def __init__(self):
        # generic data is what all the manipulation is based on
        self.generic_train_images = []
        self.generic_test_images = []
        self.generic_train_annotations = []
        self.generic_test_annotations = []
        # These maps are very important for later analysis, including max_individuals
        # and trace back the original dataset etc.
        self.imageid2anno = {}
        self.dataset2images = {}
        self.imageid2filename = {}
        self.imageid2datasetname = {}
        self.datasetname2imageids = {}
        # meta keeps information for later analysis
        self.meta = {}
        # if conversion_table is None, dataset is not yet converted to super keypoints
        self.conversion_table = None

    def _build_maps(self):
        self.datasetname2imageids[self.meta["dataset_name"]] = set()

        total_annotations = (
            self.generic_train_annotations + self.generic_test_annotations
        )
        for anno in total_annotations:
            image_id = anno["image_id"]
            if image_id not in self.imageid2anno:
                self.imageid2anno[image_id] = []
            self.imageid2anno[image_id].append(anno)

        total_images = self.generic_train_images + self.generic_test_images
        for image in total_images:
            image_id = image["id"]
            self.imageid2datasetname[image_id] = self.meta["dataset_name"]
            file_name = image["file_name"]
            self.imageid2filename[image_id] = file_name
            self.datasetname2imageids[self.meta["dataset_name"]].add(image_id)

        # in DLC, even if you have more than one annotations in one image, it does not
        # mean it's a multi animal project
        max_num = 0
        for k in self.imageid2anno:
            max_num = max(len(self.imageid2anno[k]), max_num)

        self.meta["max_individuals"] = max_num
        self.meta["imageid2filename"] = self.imageid2filename

    def filter_by_pattern(self, pattern):

        keep_ids = []
        keep_train_images = []
        keep_test_images = []
        for img in self.generic_train_images + self.generic_test_images:
            print(img["file_name"])
            if pattern in img["file_name"]:

                image_id = img["id"]
                keep_ids.append(image_id)

        for image in self.generic_train_images:
            if image["id"] in keep_ids:
                keep_train_images.append(image["id"])

        self.generic_train_images = keep_train_images

        for image in self.generic_test_images:
            if image["id"] in keep_ids:
                keep_test_images.append(image["id"])

        self.generic_test_images = keep_test_images

        keep_train_annotations = []
        keep_test_annotations = []

        for anno in self.generic_train_annotations:
            if anno["image_id"] in keep_ids:
                keep_train_annotations.append(anno)

        self.generic_train_annotations = keep_train_annotations

        for anno in self.generic_test_annotations:
            if anno["image_id"] in keep_ids:
                keep_test_annotations.append(anno)

        self.generic_test_annotations = keep_test_annotations

    def summary(self):
        print(f'Summary of dataset {self.meta["dataset_name"]}')
        print("-------------")
        print(f'max num individuals  is {self.meta["max_individuals"]}')
        print(f"total keypoints : {len(self.meta['categories']['keypoints'])}")
        print(f"total train images : {len(self.generic_train_images)}")
        print(f"total test images : {len(self.generic_test_images)}")
        print(f"total train annotations : {len(self.generic_train_annotations)}")
        print(f"total test annotations : {len(self.generic_test_annotations)}")
        print("-------------")

    def populate_generic(self):
        raise NotImplementedError("Must implement this function")

    def materialize(
        self,
        proj_root,
        framework="coco",
        deepcopy=False,
        append_image_id=True,
        no_image_copy=False,
    ):
        mat_func = mat_func_factory(framework)
        self.meta["mat_datasets"] = {self.meta["dataset_name"]: self}
        self.meta["imageid2datasetname"] = self.imageid2datasetname
        kwargs = dict(deepcopy=deepcopy, append_image_id=append_image_id)
        if framework == "coco":
            kwargs["no_image_copy"] = no_image_copy

        mat_func(
            proj_root,
            self.generic_train_images,
            self.generic_test_images,
            self.generic_train_annotations,
            self.generic_test_annotations,
            self.meta,
            **kwargs,
        )

    def whether_anno_image_match(self, images, annotations):
        """
        Every image id should be annotated at least once
        There should not be any image that is not being annotated
        There should not be any annotation for beyond the set of given images
        """

        image_ids = set([image["id"] for image in images])

        annotation_image_ids = set([anno["image_id"] for anno in annotations])

        if image_ids != annotation_image_ids:
            print("images-annotations", image_ids - annotation_image_ids)
            print("len(images-annotatinos)", len(image_ids - annotation_image_ids))
            print("annotations-images", annotation_image_ids - image_ids)
            print("len(annotations-images)", len(annotation_image_ids - image_ids))
            warnings.warn("annotation and image ids do not match")

    def get_keypoints(self):
        # TODO make sure it's always one element in a list
        return self.meta["categories"]["keypoints"]

    def _proj(self, annotations, conversion_table):

        keypoints = self.get_keypoints()

        kpt2index = {kpt: kpt_id for kpt_id, kpt in enumerate(keypoints)}

        ret = []

        master2src = {}
        for kpt in keypoints:
            conv_kpt = conversion_table.convert(kpt)
            # sometimes a keypoint might not find its corresponding one from mastername
            if conv_kpt is not None:
                master2src[conv_kpt] = kpt

        master_keypoints = conversion_table.master_keypoints

        # need to change this in meta

        for anno in annotations:
            try:
                kpts = anno["keypoints"]
            except:
                print(anno)

            new_kpts = np.zeros(len(master_keypoints) * 3)
            new_num_kpts = len(master_keypoints)

            for master_kpt_id, master_kpt_name in enumerate(master_keypoints):
                # check whether the dataset has the corresponding keypoint
                if master_kpt_name not in master2src:
                    new_kpts[master_kpt_id * 3 : master_kpt_id * 3 + 3] = -1
                    continue

                src_kpt_name = master2src[master_kpt_name]
                src_kpt_id = kpt2index[src_kpt_name]
                new_kpts[master_kpt_id * 3 : master_kpt_id * 3 + 3] = kpts[
                    src_kpt_id * 3 : src_kpt_id * 3 + 3
                ]

            # skipping empty frames after conversion
            new_anno = copy.deepcopy(anno)
            new_anno["keypoints"] = new_kpts
            new_anno["num_keypoints"] = new_num_kpts
            ret.append(new_anno)

        return ret

    def adjust_bbox_and_area(self):
        """Called during conversion.

        This is to remove the impact of keypoints that are potentially environmental
        keypoints to the bbox and area calculation.
        """
        from .utils import calc_bboxes_from_keypoints

        for annotation in (
            self.generic_train_annotations + self.generic_test_annotations
        ):
            keypoints = annotation["keypoints"]
            bbox_margin = 20

            num_kpts = annotation["num_keypoints"]

            keypoints = np.array(keypoints).reshape((num_kpts, 3))

            mask = keypoints[:, 0] > 0
            keypoints = keypoints[mask]

            if keypoints.shape[0] == 0:
                continue

            xmin, ymin, xmax, ymax = calc_bboxes_from_keypoints(
                [keypoints],
                slack=bbox_margin,
                clip=True,
            )[0][:4]

            w = xmax - xmin
            h = ymax - ymin
            area = w * h
            bbox = np.nan_to_num([xmin, ymin, w, h])

            if "bbox" not in annotation:
                annotation["bbox"] = bbox
            if "area" not in annotation:
                annotation["area"] = area

    def project_with_conversion_table(self, table_path="", table_dict=None):
        """
        Replace the generic annotations with those that are in superset keypoint space

        """
        print(f'Converting {self.meta["dataset_name"]}')

        keypoints = self.get_keypoints()

        self.conversion_table = get_conversion_table(
            keypoints=keypoints, table_path=table_path, table_dict=table_dict
        )

        self.generic_train_annotations = self._proj(
            self.generic_train_annotations, self.conversion_table
        )

        self.generic_test_annotations = self._proj(
            self.generic_test_annotations, self.conversion_table
        )

        # all category id fixed to 1. So that it does not conflict with the background
        # category id
        for anno in self.generic_train_annotations + self.generic_test_annotations:
            anno["category_id"] = 1

        for img in self.generic_train_images + self.generic_test_images:
            img["source_dataset"] = self.meta["dataset_name"]

        self.adjust_bbox_and_area()
        self.meta["categories"]["keypoints"] = self.conversion_table.master_keypoints
        self.meta["categories"]["supercategory"] = "animal"
        self.meta["categories"]["name"] = "superanimal"

        # category id fixed to be 1, to avoid to conflict with background category id
        self.meta["categories"]["id"] = 1


--- File: deeplabcut/modelzoo/generalized_data_converter/datasets/single_dlc.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os

import numpy as np
import pandas as pd

from deeplabcut.modelzoo.generalized_data_converter.datasets.base_dlc import (
    BaseDLCPoseDataset,
)
from deeplabcut.modelzoo.generalized_data_converter.datasets.utils import (
    calc_bboxes_from_keypoints,
    read_image_shape_fast,
)


class SingleDLCPoseDataset(BaseDLCPoseDataset):
    """
    The philosophy is to assume the dataset is already created so this class is not
    responsible for creating training dataset
    """

    def __init__(self, proj_root, dataset_name, shuffle=1, modelprefix=""):
        super(SingleDLCPoseDataset, self).__init__(
            proj_root, dataset_name, shuffle=shuffle, modelprefix=modelprefix
        )

        # overriding max_individuals
        self.meta["max_individuals"] = 1

    def _df2generic(self, df, image_id_offset=0):

        bpts = df.columns.get_level_values("bodyparts").unique().tolist()

        coco_categories = []

        # single animal only has individual0

        category = {
            "name": "individual0",
            "id": 0,
            "supercategory": "animal",
        }

        category["keypoints"] = bpts

        coco_categories.append(category)

        coco_images = []
        coco_annotations = []

        annotation_id = 0
        image_id = -1

        for _, file_name in enumerate(df.index):
            data = df.loc[file_name]

            # skipping all nan

            if np.isnan(data.to_numpy()).all():
                continue

            image_id += 1
            category_id = 0
            kpts = data.to_numpy().reshape(-1, 2)
            keypoints = np.zeros((len(kpts), 3))

            keypoints[:, :2] = kpts

            is_visible = ~pd.isnull(kpts).all(axis=1)

            keypoints[:, 2] = np.where(is_visible, 2, 0)

            num_keypoints = is_visible.sum()

            bbox_margin = 20

            xmin, ymin, xmax, ymax = calc_bboxes_from_keypoints(
                [keypoints],
                slack=bbox_margin,
                clip=True,
            )[0][:4]

            w = xmax - xmin
            h = ymax - ymin
            area = w * h
            bbox = np.nan_to_num([xmin, ymin, w, h])
            keypoints = np.nan_to_num(keypoints.flatten())

            annotation_id += 1
            annotation = {
                "image_id": image_id + image_id_offset,
                "num_keypoints": num_keypoints,
                "keypoints": keypoints,
                "id": annotation_id,
                "category_id": category_id,
                "area": area,
                "bbox": bbox,
                "iscrowd": 0,
            }
            if np.sum(keypoints) != 0:

                coco_annotations.append(annotation)

            # I think width and height are important

            if isinstance(file_name, tuple):
                image_path = os.path.join(self.proj_root, *list(file_name))
            else:
                image_path = os.path.join(self.proj_root, file_name)

            _, height, width = read_image_shape_fast(image_path)

            image = {
                "file_name": image_path,
                "width": width,
                "height": height,
                "id": image_id + image_id_offset,
            }
            coco_images.append(image)

        ret_obj = {
            "images": coco_images,
            "annotations": coco_annotations,
            "categories": coco_categories,
        }
        return ret_obj


--- File: deeplabcut/modelzoo/generalized_data_converter/conversion_table/conversion_table.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import warnings

import numpy as np
import pandas as pd


class ConversionTableFromDict:
    def __init__(self, raw_table_dict):
        self.table_dict = raw_table_dict["conversion_table"]
        self.master_keypoints = raw_table_dict["master_keypoints"]

    def convert(self, kpt):
        if kpt not in self.table_dict:
            warnings.warn(
                f"{kpt} is defined in src space but not appeared in the conversion table"
            )
            return None
        else:
            return self.table_dict[kpt]


class ConversionTableFromCSV:
    """
    Base class only reads the table
    """

    def __init__(self, src_keypoints, table_path):
        self.table_path = table_path

        # sep removes leading and tailing white space
        df = pd.read_csv(table_path, sep="\s*,\s*")

        df.dropna(inplace=True, how="all")
        # drop the row is MasterName has nan in the row
        df = df.dropna(subset=["MasterName"])

        self.df = df

        self.src_keypoints = src_keypoints

        kpt_list = df.to_numpy()

        self.lookup_set = []

        for i in range(len(kpt_list)):
            kpts = np.array(kpt_list[i])
            # remove nan

            kpt_alias = set(kpts)

            for k in list(kpt_alias):
                if type(k) != str:
                    kpt_alias.remove(k)

            self.lookup_set.append(kpt_alias)

        target_keypoints = df["MasterName"].values

        # target_keypoints = target_keypoints[~np.isnan(target_keypoints.values)]

        self.master_keypoints = target_keypoints

        # paired when they both exist

        # following assumes that either it's 1vs.1 from src to target
        # or 1 vs. 0
        # it could be 1 vs. 2 in horse data
        self.table = {}
        for src_kpt in src_keypoints:
            for target_kpt in target_keypoints:

                src_kpt_id = self._search(src_kpt)
                target_kpt_id = self._search(target_kpt)

                if src_kpt_id == -1 or target_kpt_id == -1:
                    # if any one of them not exist in the set
                    # skip
                    continue
                if src_kpt_id == target_kpt_id:
                    self.table[src_kpt] = target_kpt

        self.check_inclusion()

    def _search(self, key):
        """
        return -1 if not found
        return kpt id if found

        """
        # [TODO] if it can be mapped to two, I can randomly return one
        for kpt_id in range(len(self.lookup_set)):
            if key in self.lookup_set[kpt_id]:
                return kpt_id
        return -1

    def check_inclusion(self):
        """
        check if conversion table covers
        every keypoint contained in src proj

        """
        count = 0
        print("src keypoints")
        print(self.src_keypoints)
        for kpt in self.src_keypoints:
            index = self._search(kpt)
            if index == -1:
                pass
            else:
                count += 1
        print(f"{count}/{len(self.src_keypoints)} keypoints will be converted")

    def convert(self, kpt):
        if kpt not in self.table:
            warnings.warn(
                f"{kpt} is defined in src space but not appeared in the conversion table"
            )
            return None
        else:
            return self.table[kpt]

    def get_subset(self, labname=""):

        bodyparts = self.df[labname]

        super_bodyparts = self.df["MasterName"]

        ret = []

        for bodypart in bodyparts:
            if bodypart in self.table:
                ret.append(self.table[bodypart])

        return ret


def get_conversion_table(keypoints=None, table_path="", table_dict=None):
    if table_path is not None and keypoints is not None:
        return ConversionTableFromCSV(keypoints, table_path)
    elif table_dict:
        return ConversionTableFromDict(table_dict)
    else:
        raise NotImplementedError("not supported")


--- File: deeplabcut/modelzoo/generalized_data_converter/conversion_table/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from .conversion_table import get_conversion_table


--- File: deeplabcut/create_project/modelzoo.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import os
from pathlib import Path

import yaml
from dlclibrary import get_available_detectors
from dlclibrary.dlcmodelzoo.modelzoo_download import (
    download_huggingface_model,
    MODELOPTIONS,
    get_available_datasets,
    get_available_models,
)

import deeplabcut
from deeplabcut.core.config import read_config_as_dict, write_config
from deeplabcut.core.engine import Engine
from deeplabcut.generate_training_dataset.metadata import (
    TrainingDatasetMetadata,
    ShuffleMetadata,
    DataSplit,
)
from deeplabcut.generate_training_dataset.trainingsetmanipulation import (
    MakeInference_yaml,
)
from deeplabcut.modelzoo.utils import get_super_animal_project_cfg
from deeplabcut.pose_estimation_pytorch.config.make_pose_config import (
    add_metadata,
    make_pytorch_test_config,
)
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import load_super_animal_config
from deeplabcut.utils import auxiliaryfunctions

Modeloptions = MODELOPTIONS  # backwards compatibility for COLAB NOTEBOOK


def MakeTrain_pose_yaml(itemstochange, saveasconfigfile, defaultconfigfile):
    raw = open(defaultconfigfile).read()
    docs = []
    for raw_doc in raw.split("\n---"):
        try:
            docs.append(yaml.load(raw_doc, Loader=yaml.SafeLoader))
        except SyntaxError:
            docs.append(raw_doc)

    for key in itemstochange.keys():
        docs[0][key] = itemstochange[key]
    docs[0]["max_input_size"] = 1500
    with open(saveasconfigfile, "w") as f:
        yaml.dump(docs[0], f)
    return docs[0]


def UpdateTrain_pose_yaml(dict_train, dict2change, saveasfile):
    for key in dict2change.keys():
        dict_train[key] = dict2change[key]
    auxiliaryfunctions.write_plainconfig(saveasfile, dict_train)


def MakeTest_pose_yaml(dictionary, keys2save, saveasfile):
    dict_test = {}
    for key in keys2save:
        dict_test[key] = dictionary[key]
    dict_test["scoremap_dir"] = "test"
    dict_test["global_scale"] = 1.0
    auxiliaryfunctions.write_plainconfig(saveasfile, dict_test)
    # with open(saveasfile, "w") as f:
    #    yaml.dump(dict_test, f)


def create_pretrained_human_project(
    project,
    experimenter,
    videos,
    working_directory=None,
    copy_videos=False,
    videotype="",
    createlabeledvideo=True,
    analyzevideo=True,
):
    """
    LEGACY FUNCTION will be deprecated.

    Use deeplabcut.create_pretrained_project(project, experimenter, videos, model='full_human', ..)

    For now just calls that function....

    Creates a demo human project and analyzes a video with ResNet 101 weights pretrained on
    MPII Human Pose. This is from the DeeperCut paper by Insafutdinov et al. https://arxiv.org/abs/1605.03170
    Please make sure to cite it too if you use this code!
    """
    print(
        "LEGACY FUNCTION will be deprecated.... use  deeplabcut.create_pretrained_project(project, experimenter, videos, model='full_human', ..) in the future!"
    )
    create_pretrained_project(
        project,
        experimenter,
        videos,
        model="full_human",
        working_directory=working_directory,
        copy_videos=copy_videos,
        videotype=videotype,
        createlabeledvideo=createlabeledvideo,
        analyzevideo=analyzevideo,
    )


def create_pretrained_project(
    project: str,
    experimenter: str,
    videos: list[str],
    model: str | None = None,
    working_directory: str | None = None,
    copy_videos: bool = False,
    videotype: str = "",
    analyzevideo: bool = True,
    filtered: bool = True,
    createlabeledvideo: bool = True,
    trainFraction: float | None = None,
    engine: Engine = Engine.PYTORCH,
    multi_animal: bool = False,
    individuals: list[str] | None = None,
    net_name: str | None = None,
    detector_name: str | None = None,
):
    """
    Creates a new project directory, sub-directories and a basic configuration file.
    Change its parameters to your projects need.

    The project will also be initialized with a pre-trained model from the DeepLabCut model zoo!

    http://modelzoo.deeplabcut.org

    Parameters
    ----------
    project : string
        String containing the name of the project.

    experimenter : string
        String containing the name of the experimenter.

    model: string | None, default = None,
        The model / dataset to use as basis for the project.
        If None, the default model / dataset for the selected engine will be used.

    videos : list[string]
        A list of string containing the full paths of the videos to include in the project.

    working_directory : string, optional, default = None
        The directory where the project will be created. If None - the current working directory will be used.

    copy_videos : bool, optional, default = False,
        If this is set to True, the videos are copied to the ``videos`` directory.
        If it is False, symlink of the videos are copied to the project/videos directory.
        Note: on Windows: True is often necessary!

    analyzevideo: bool, optional
        If true, then the video is analyzed and a labeled video is created.
        If false, then only the project will be created and the weights downloaded.

    filtered: bool, default True
        Indicates if filtered pose data output should be plotted rather than frame-by-frame predictions.
        Filtered version can be calculated with deeplabcut.filterpredictions()

    createlabeledvideo: bool, default True,
        Specifies if a labeled video needs to be created.

    trainFraction: float|None, default = None.
            Fraction that will be used in dlc-model/trainingset folder name.
            If None - default value (0.95) from new projects will be used.

    engine: Engine, default Engine.PYTORCH,
        engine on which the pretrained weights are based

    multi_animal: bool = False,
        Specifies if the project is single or multi-animal.
        Implemented only for Pytorch-based models.

    individuals: list[str] | None = None,
        Only if multianimal is True.
        Defines the names of the individuals.

    net_name: str | None, default = None,
        Valid only if using Pytorch engine.
        Name of the pose model on which the superanimal dataset has been trained on.
        If None - "hrnet_w32" will be used as default.

    detector_name: str | None, default = None,
        Valid only if using Pytorch engine.
        Name of the detector model on which the superanimal dataset has been trained on.
        If None - "fasterrcnn_resnet50_fpn_v2" will be used as default.

    Example
    --------
    Linux/MacOs loading full_human model and analyzing video /homosapiens1.avi
    >>> deeplabcut.create_pretrained_project("humanstrokestudy", "Linus", ["/data/videos/homosapiens1.avi"], copy_videos=False)

    Loading full_cat model and analyzing video "felixfeliscatus3.avi"
    >>> deeplabcut.create_pretrained_project("humanstrokestudy", "Linus", ["/data/videos/felixfeliscatus3.avi"], model="full_cat", engine=Engine.TF)

    Windows:
    >>> deeplabcut.create_pretrained_project("humanstrokestudy", "Bill", [r'C:\yourusername\rig-95\Videos\reachingvideo1.avi'], r'C:\yourusername\analysis\project', copy_videos=True)
    Users must format paths with either:  r'C:\ OR 'C:\\ <- i.e. a double backslash \ \ )
    """
    if engine == Engine.TF:
        return create_pretrained_project_tensorflow(
            project=project,
            experimenter=experimenter,
            videos=videos,
            model=model,
            working_directory=working_directory,
            copy_videos=copy_videos,
            videotype=videotype,
            analyzevideo=analyzevideo,
            filtered=filtered,
            createlabeledvideo=createlabeledvideo,
            trainFraction=trainFraction,
        )
    elif engine == Engine.PYTORCH:
        return create_pretrained_project_pytorch(
            project=project,
            experimenter=experimenter,
            videos=videos,
            dataset=model,
            working_directory=working_directory,
            copy_videos=copy_videos,
            video_type=videotype,
            analyze_video=analyzevideo,
            filtered=filtered,
            create_labeled_video=createlabeledvideo,
            train_fraction=trainFraction,
            multi_animal=multi_animal,
            individuals=individuals,
            net_name=net_name,
            detector_name=detector_name,
        )

    raise NotImplementedError(f"This function is not implemented for {engine}")


def create_pretrained_project_pytorch(
    project: str,
    experimenter: str,
    videos: list[str],
    dataset: str | None = None,
    working_directory: str | None = None,
    copy_videos: bool = False,
    video_type: str | None = None,
    analyze_video: bool = True,
    filtered: bool = True,
    create_labeled_video: bool = True,
    train_fraction: float | None = None,
    multi_animal: bool = False,
    individuals: list[str] | None = None,
    net_name: str | None = None,
    detector_name: str | None = None,
):
    """
    Method used specifically for Pytorch-based ModelZoo models.

    Creates a new project directory, sub-directories and a basic configuration file.
    Change its parameters to your projects need.

    The project will also be initialized with a pre-trained model from the DeepLabCut model zoo!

    http://modelzoo.deeplabcut.org

    Parameters
    ----------
    project : string
        String containing the name of the project.

    experimenter : string
        String containing the name of the experimenter.

    dataset: string|None, default = None,
        The superanimal dataset to use as basis for the project.
        If not specified - superanimal_quadruped will be used by default.

    videos : list[string]
        A list of string containing the full paths of the videos to include in the project.

    working_directory : string, optional, default = None
        The directory where the project will be created. If None - the current working directory will be used.

    copy_videos : bool, optional, default = False,
        If this is set to True, the videos are copied to the ``videos`` directory.
        If it is False, symlink of the videos are copied to the project/videos directory.
        Note: on Windows: True is often necessary!

    analyze_video: bool, optional
        If true, then the video is analyzed and a labeled video is created.
        If false, then only the project will be created and the weights downloaded.

    filtered: bool, default True
        Indicates if filtered pose data output should be plotted rather than frame-by-frame predictions.
        Filtered version can be calculated with deeplabcut.filterpredictions()

    create_labeled_video: bool, default True
        Specifies if a labeled video needs to be created.

    train_fraction: float|None, default = None.
            Fraction that will be used in dlc-model/trainingset folder name.
            If None - default value (0.95) from new projects will be used.

    multi_animal: bool = False,
        Specifies if the project is single or multi-animal

    individuals: list[str]|None = None,
        Only if multianimal is True.
        Defines the names of the individuals.

    net_name: str | None, default = None,
        Valid only if using Pytorch engine.
        Name of the pose model on which the superanimal dataset has been trained on.
        If None - "hrnet_w32" will be used as default.

    detector_name: str | None, default = None,
        Valid only if using Pytorch engine.
        Name of the detector model on which the superanimal dataset has been trained on.
        If None - "fasterrcnn_resnet50_fpn_v2" will be used as default.

    Example
    --------
    Linux/MacOs loading full_human model and analyzing video /homosapiens1.avi
    >>> deeplabcut.create_pretrained_project_pytorch("humanstrokestudy", "Linus", ["/data/videos/homosapiens1.avi"], copy_videos=False)

    Loading full_cat model and analyzing video "felixfeliscatus3.avi"
    >>> deeplabcut.create_pretrained_project_pytorch("humanstrokestudy", "Linus", ["/data/videos/felixfeliscatus3.avi"], model="full_cat", engine=Engine.TF)

    Windows:
    >>> deeplabcut.create_pretrained_project_pytorch("humanstrokestudy", "Bill", [r'C:\yourusername\rig-95\Videos\reachingvideo1.avi'], r'C:\yourusername\analysis\project', copy_videos=True)
    Users must format paths with either:  r'C:\ OR 'C:\\ <- i.e. a double backslash \ \ )
    """
    # Check arguments
    if not dataset:
        dataset = "superanimal_quadruped"

    if not net_name:
        net_name = "hrnet_w32"

    # Currently, all Pytorch Superanimal models are Top-Down.
    if not detector_name:
        detector_name = "fasterrcnn_resnet50_fpn_v2"

    if dataset not in get_available_datasets():
        raise ValueError(
            f"Invalid dataset '{dataset}'. Available datasets are: {get_available_datasets()}"
        )

    if net_name not in get_available_models(dataset):
        raise ValueError(
            f"Invalid net_name '{net_name}' for dataset {dataset}. The following net types are available: {get_available_models(dataset)}"
        )

    if detector_name not in get_available_detectors(dataset):
        raise ValueError(
            f"Invalid detector_name '{detector_name}' for dataset {dataset}. The following detectors are available: {get_available_detectors(dataset)}"
        )

    # Create project
    cfg_path = deeplabcut.create_new_project(
        project=project,
        experimenter=experimenter,
        videos=videos,
        working_directory=working_directory,
        copy_videos=copy_videos,
        videotype=video_type,
        multianimal=multi_animal,
        individuals=individuals,
    )

    # Edits to do to the project config
    cfg_edits = {}
    if train_fraction is not None:
        cfg_edits["TrainingFraction"] = [train_fraction]
    super_animal_project_cfg = get_super_animal_project_cfg(dataset)
    super_animal_bodyparts = super_animal_project_cfg.get("bodyparts")
    super_animal_skeleton = super_animal_project_cfg.get("skeleton")
    cfg_edits["skeleton"] = super_animal_skeleton
    if multi_animal:
        cfg_edits["multianimalbodyparts"] = super_animal_bodyparts
    else:
        cfg_edits["bodyparts"] = super_animal_bodyparts
    auxiliaryfunctions.edit_config(cfg_path, edits=cfg_edits)

    # Create the shuffle train and test directories
    config = read_config_as_dict(cfg_path)
    shuffle_dir = Path(cfg_path).parent / auxiliaryfunctions.get_model_folder(
        trainFraction=config["TrainingFraction"][0],
        shuffle=1,
        cfg=config,
        engine=Engine.PYTORCH,
    )
    train_dir = shuffle_dir / "train"
    test_dir = shuffle_dir / "test"
    train_dir.mkdir(parents=True, exist_ok=True)
    test_dir.mkdir(parents=True, exist_ok=True)

    # Download the weights and put them into appropriate directory
    print("Downloading weights...")
    super_animal_detector_name = f"{dataset}_{detector_name}"
    new_detector_name = "snapshot-detector-000.pt"
    download_huggingface_model(
        model_name=super_animal_detector_name,
        target_dir=str(train_dir),
        rename_mapping={f"{super_animal_detector_name}.pt": new_detector_name},
    )
    super_animal_model_name = f"{dataset}_{net_name}"
    new_snapshot_name = "snapshot-000.pt"
    download_huggingface_model(
        model_name=super_animal_model_name,
        target_dir=str(train_dir),
        rename_mapping={f"{super_animal_model_name}.pt": new_snapshot_name},
    )

    # Create pytorch_config.yaml
    train_cfg_path = train_dir / "pytorch_config.yaml"
    pytorch_config = load_super_animal_config(
        super_animal=dataset,
        model_name=net_name,
        detector_name=detector_name,
    )
    pytorch_config = add_metadata(config, pytorch_config, train_cfg_path)
    pytorch_config["resume_training_from"] = str(train_dir / new_snapshot_name)
    pytorch_config["detector"]["resume_training_from"] = str(
        train_dir / new_detector_name
    )
    write_config(train_cfg_path, pytorch_config)

    # Create test pose_cfg.yaml
    test_cfg_path = test_dir / "pose_cfg.yaml"
    make_pytorch_test_config(
        model_config=pytorch_config, test_config_path=test_cfg_path, save=True
    )

    # Create inference_cfg.yaml if needed
    if multi_animal:
        inference_cfg_path = test_dir / "inference_cfg.yaml"
        _create_inference_config(inference_cfg_path, config)

    # Create metadata.yaml with shuffle info in training-data directory
    _create_training_datasets_metadata(config, shuffle_dir.name, Engine.PYTORCH)

    # Process the videos
    _process_videos(
        cfg_path=cfg_path,
        video_type=video_type,
        analyze_video=analyze_video,
        filtered=filtered,
        create_labeled_video=create_labeled_video,
    )
    return cfg_path, str(train_cfg_path)


def _create_inference_config(inference_cfg_path: str | Path, project_cfg: dict):
    inf_updates = dict(
        minimalnumberofconnections=int(len(project_cfg["multianimalbodyparts"]) / 2),
        topktoretain=len(project_cfg["individuals"]),
        withid=project_cfg.get("identity", False),
    )
    default_inf_path = (
        Path(auxiliaryfunctions.get_deeplabcut_path()) / "inference_cfg.yaml"
    )
    MakeInference_yaml(inf_updates, inference_cfg_path, default_inf_path)


def create_pretrained_project_tensorflow(
    project: str,
    experimenter: str,
    videos: list[str],
    model: str | None = None,
    working_directory: str | None = None,
    copy_videos: bool = False,
    videotype: str = "",
    analyzevideo: bool = True,
    filtered: bool = True,
    createlabeledvideo: bool = True,
    trainFraction: float | None = None,
):
    """
    Method used specifically for Tensorflow-based ModelZoo models.

    Creates a new project directory, sub-directories and a basic configuration file.
    Change its parameters to your projects need.

    The project will also be initialized with a pre-trained model from the DeepLabCut model zoo!

    http://modelzoo.deeplabcut.org

    Parameters
    ----------
    project : string
        String containing the name of the project.

    experimenter : string
        String containing the name of the experimenter.

    model: string|None, default = None,
        The model / dataset to use as basis for the project.
        If not specified - full_human will be used by default.

    videos : list[string]
        A list of string containing the full paths of the videos to include in the project.

    working_directory : string, optional, default = None
        The directory where the project will be created. If None - the current working directory will be used.

    copy_videos : bool, optional, default = False,
        If this is set to True, the videos are copied to the ``videos`` directory.
        If it is False, symlink of the videos are copied to the project/videos directory.
        Note: on Windows: True is often necessary!

    analyzevideo: bool, optional
        If true, then the video is analyzed and a labeled video is created.
        If false, then only the project will be created and the weights downloaded.

    filtered: bool, default True
        Indicates if filtered pose data output should be plotted rather than frame-by-frame predictions.
        Filtered version can be calculated with deeplabcut.filterpredictions()

    createlabeledvideo: bool, default True
        Specifies if a labeled video needs to be created.

    trainFraction: float|None, default = None.
            Fraction that will be used in dlc-model/trainingset folder name.
            If None - default value (0.95) from new projects will be used.

    Example
    --------
    Linux/MacOs loading full_human model and analyzing video /homosapiens1.avi
    >>> deeplabcut.create_pretrained_project_tensorflow("humanstrokestudy", "Linus", ["/data/videos/homosapiens1.avi"], copy_videos=False)

    Loading full_cat model and analyzing video "felixfeliscatus3.avi"
    >>> deeplabcut.create_pretrained_project_tensorflow("humanstrokestudy", "Linus", ["/data/videos/felixfeliscatus3.avi"], model="full_cat", engine=Engine.TF)

    Windows:
    >>> deeplabcut.create_pretrained_project_tensorflow("humanstrokestudy", "Bill", [r'C:\yourusername\rig-95\Videos\reachingvideo1.avi'], r'C:\yourusername\analysis\project', copy_videos=True)
    Users must format paths with either:  r'C:\ OR 'C:\\ <- i.e. a double backslash \ \ )
    """
    if not model:
        model = "full_human"

    if model in MODELOPTIONS:
        cwd = os.getcwd()

        cfg = deeplabcut.create_new_project(
            project, experimenter, videos, working_directory, copy_videos, videotype
        )
        if trainFraction is not None:
            auxiliaryfunctions.edit_config(cfg, {"TrainingFraction": [trainFraction]})

        config = auxiliaryfunctions.read_config(cfg)
        if model == "full_human":
            config["bodyparts"] = [
                "ankle1",
                "knee1",
                "hip1",
                "hip2",
                "knee2",
                "ankle2",
                "wrist1",
                "elbow1",
                "shoulder1",
                "shoulder2",
                "elbow2",
                "wrist2",
                "chin",
                "forehead",
            ]
            config["skeleton"] = [
                ["ankle1", "knee1"],
                ["ankle2", "knee2"],
                ["knee1", "hip1"],
                ["knee2", "hip2"],
                ["hip1", "hip2"],
                ["shoulder1", "shoulder2"],
                ["shoulder1", "hip1"],
                ["shoulder2", "hip2"],
                ["shoulder1", "elbow1"],
                ["shoulder2", "elbow2"],
                ["chin", "forehead"],
                ["elbow1", "wrist1"],
                ["elbow2", "wrist2"],
            ]
            config["default_net_type"] = "resnet_101"
        else:  # just make a case and put the stuff you want.
            # TBD: 'partaffinityfield_graph' >> use to set skeleton!
            pass

        auxiliaryfunctions.write_config(cfg, config)
        config = auxiliaryfunctions.read_config(cfg)

        train_dir = Path(
            os.path.join(
                config["project_path"],
                str(
                    auxiliaryfunctions.get_model_folder(
                        trainFraction=config["TrainingFraction"][0],
                        shuffle=1,
                        cfg=config,
                    )
                ),
                "train",
            )
        )
        test_dir = Path(
            os.path.join(
                config["project_path"],
                str(
                    auxiliaryfunctions.get_model_folder(
                        trainFraction=config["TrainingFraction"][0],
                        shuffle=1,
                        cfg=config,
                    )
                ),
                "test",
            )
        )

        # Create the model directory
        train_dir.mkdir(parents=True, exist_ok=True)
        test_dir.mkdir(parents=True, exist_ok=True)

        modelfoldername = auxiliaryfunctions.get_model_folder(
            trainFraction=config["TrainingFraction"][0], shuffle=1, cfg=config
        )
        path_train_config = str(
            os.path.join(
                config["project_path"], Path(modelfoldername), "train", "pose_cfg.yaml"
            )
        )
        path_test_config = str(
            os.path.join(
                config["project_path"], Path(modelfoldername), "test", "pose_cfg.yaml"
            )
        )

        # Download the weights and put then in appropriate directory
        print("Downloading weights...")
        download_huggingface_model(model, train_dir)

        pose_cfg = deeplabcut.auxiliaryfunctions.read_plainconfig(path_train_config)
        pose_cfg["dataset_type"] = "imgaug"
        print(path_train_config)
        # Updating config file:
        dict_ = {
            "default_net_type": pose_cfg["net_type"],
            "default_augmenter": pose_cfg["dataset_type"],
            "bodyparts": pose_cfg["all_joints_names"],
            "dotsize": 6,
        }
        auxiliaryfunctions.edit_config(cfg, dict_)

        # downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway)
        # model_path = auxfun_models.check_for_weights(pose_cfg['net_type'], parent_path)

        # Updating training and test pose_cfg:
        snapshotname = [fn for fn in os.listdir(train_dir) if ".meta" in fn][0].split(
            ".meta"
        )[0]
        dict2change = {
            "init_weights": str(os.path.join(train_dir, snapshotname)),
            "project_path": str(config["project_path"]),
        }

        UpdateTrain_pose_yaml(pose_cfg, dict2change, path_train_config)
        keys2save = [
            "dataset",
            "dataset_type",
            "num_joints",
            "all_joints",
            "all_joints_names",
            "net_type",
            "init_weights",
            "global_scale",
            "location_refinement",
            "locref_stdev",
        ]

        MakeTest_pose_yaml(pose_cfg, keys2save, path_test_config)

        _create_training_datasets_metadata(config, modelfoldername.name, Engine.TF)

        _process_videos(
            cfg_path=cfg,
            video_type=videotype,
            analyze_video=analyzevideo,
            filtered=filtered,
            create_labeled_video=createlabeledvideo,
        )

        os.chdir(cwd)
        return cfg, path_train_config

    else:
        return "N/A", "N/A"


def _create_training_datasets_metadata(
    config: dict, shuffle_dir_name: str, engine: Engine
):
    # First create the metadata object
    metadata = TrainingDatasetMetadata.create(config)

    # Create a new shuffle with TensorFlow engine
    new_shuffle = ShuffleMetadata(
        name=shuffle_dir_name,
        train_fraction=config["TrainingFraction"][0],
        index=1,
        engine=engine,
        split=DataSplit(train_indices=(), test_indices=()),
    )

    # Add the shuffle to metadata
    metadata = metadata.add(new_shuffle)

    # Save the metadata
    metadata.save()

    return metadata


def _process_videos(
    cfg_path: str | Path,
    video_type: str = "",
    analyze_video: bool = True,
    filtered: bool = True,
    create_labeled_video: bool = True,
):
    cfg_path = str(cfg_path)
    video_dir = Path(cfg_path).parent / "videos"

    if analyze_video:
        print("Analyzing video...")
        deeplabcut.analyze_videos(
            cfg_path, [video_dir], videotype=video_type, save_as_csv=True
        )

    if create_labeled_video:
        if filtered:
            deeplabcut.filterpredictions(cfg_path, [video_dir], video_type)

        print("Plotting results...")
        deeplabcut.create_labeled_video(
            cfg_path, [video_dir], video_type, draw_skeleton=True, filtered=filtered
        )
        deeplabcut.plot_trajectories(
            cfg_path, [video_dir], video_type, filtered=filtered
        )


--- File: deeplabcut/create_project/add.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


def add_new_videos(
    config, videos, copy_videos=False, coords=None, extract_frames=False
):
    """
    Add new videos to the config file at any stage of the project.

    Parameters
    ----------
    config : string
        String containing the full path of the config file in the project.

    videos : list
        A list of strings containing the full paths of the videos to include in the project.

    copy_videos : bool, optional
        If this is set to True, the videos will be copied to your project/videos directory. If False, the symlink of the
        videos will be copied instead. The default is
        ``False``; if provided it must be either ``True`` or ``False``.

    coords: list, optional
        A list containing the list of cropping coordinates of the video. The default is set to None.

    extract_frames: bool, optional
        if this is set to True extract_frames will be run on the new videos

    Examples
    --------
    Video will be added, with cropping dimensions according to the frame dimensions of mouse5.avi
    >>> deeplabcut.add_new_videos('/home/project/reaching-task-Tanmay-2018-08-23/config.yaml',['/data/videos/mouse5.avi'])

    Video will be added, with cropping dimensions [0,100,0,200]
    >>> deeplabcut.add_new_videos('/home/project/reaching-task-Tanmay-2018-08-23/config.yaml',['/data/videos/mouse5.avi'],copy_videos=False,coords=[[0,100,0,200]])

    Two videos will be added, with cropping dimensions [0,100,0,200] and [0,100,0,250], respectively.
    >>> deeplabcut.add_new_videos('/home/project/reaching-task-Tanmay-2018-08-23/config.yaml',['/data/videos/mouse5.avi','/data/videos/mouse6.avi'],copy_videos=False,coords=[[0,100,0,200],[0,100,0,250]])

    """
    import os
    import shutil
    from pathlib import Path

    from deeplabcut.utils import auxiliaryfunctions
    from deeplabcut.utils.auxfun_videos import VideoReader
    from deeplabcut.generate_training_dataset import frame_extraction

    # Read the config file
    cfg = auxiliaryfunctions.read_config(config)

    # deal with user passing a single video to add
    if isinstance(videos, str):
        videos = [videos]

    video_path = Path(config).parents[0] / "videos"
    data_path = Path(config).parents[0] / "labeled-data"
    videos = [Path(vp) for vp in videos]

    dirs = [data_path / Path(i.stem) for i in videos]

    for p in dirs:
        """
        Creates directory under data & perhaps copies videos (to /video)
        """
        p.mkdir(parents=True, exist_ok=True)

    destinations = [video_path.joinpath(vp.name) for vp in videos]
    if copy_videos:
        for src, dst in zip(videos, destinations):
            if dst.exists():
                pass
            else:
                print("Copying the videos")
                shutil.copy(os.fspath(src), os.fspath(dst))

    else:
        # creates the symlinks of the video and puts it in the videos directory.
        print("Attempting to create a symbolic link of the video ...")
        for src, dst in zip(videos, destinations):
            if dst.exists():
                print(f"Video {dst} already exists. Skipping...")
                continue
            try:
                src = str(src)
                dst = str(dst)
                os.symlink(src, dst)
                print("Created the symlink of {} to {}".format(src, dst))
            except OSError:
                try:
                    import subprocess

                    subprocess.check_call("mklink %s %s" % (dst, src), shell=True)
                except (OSError, subprocess.CalledProcessError):
                    print(
                        "Symlink creation impossible (exFat architecture?): "
                        "copying the video instead."
                    )
                    shutil.copy(os.fspath(src), os.fspath(dst))
                    print("{} copied to {}".format(src, dst))
            videos = destinations

    if copy_videos:
        videos = destinations  # in this case the *new* location should be added to the config file
    # adds the video list to the config.yaml file
    for idx, video in enumerate(videos):
        try:
            # For windows os.path.realpath does not work and does not link to the real video.
            video_path = str(Path.resolve(Path(video)))
        #           video_path = os.path.realpath(video)
        except:
            video_path = os.readlink(video)

        vid = VideoReader(video_path)
        if coords is not None:
            c = coords[idx]
        else:
            c = vid.get_bbox()
        params = {video_path: {"crop": ", ".join(map(str, c))}}
        if "video_sets_original" not in cfg:
            cfg["video_sets"].update(params)
        else:
            cfg["video_sets_original"].update(params)
    videos_str = [str(video) for video in videos]
    auxiliaryfunctions.write_config(config, cfg)
    if extract_frames:
        frame_extraction.extract_frames(
            config, userfeedback=False, videos_list=videos_str
        )
        print(
            "New videos were added to the project and frames have been extracted for labeling!"
        )
    else:
        print(
            "New videos were added to the project! Use the function 'extract_frames' to select frames for labeling."
        )


--- File: deeplabcut/create_project/new_3d.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import os
from pathlib import Path

from deeplabcut import DEBUG


def create_new_project_3d(project, experimenter, num_cameras=2, working_directory=None):
    """Creates a new project directory, sub-directories and a basic configuration file for 3d project.
    The configuration file is loaded with the default values. Adjust the parameters to your project's needs.

    Parameters
    ----------
    project : string
        String containing the name of the project.

    experimenter : string
        String containing the name of the experimenter.

    num_cameras : int
        An integer value specifying the number of cameras.

    working_directory : string, optional
        The directory where the project will be created. The default is the ``current working directory``; if provided, it must be a string.


    Example
    --------
    Linux/MacOs
    >>> deeplabcut.create_new_project_3d('reaching-task','Linus',2)

    Windows:
    >>> deeplabcut.create_new_project('reaching-task','Bill',2)
    Users must format paths with either:  r'C:\ OR 'C:\\ <- i.e. a double backslash \ \ )

    """
    from datetime import datetime as dt
    from deeplabcut.utils import auxiliaryfunctions

    date = dt.today()
    month = date.strftime("%B")
    day = date.day
    d = str(month[0:3] + str(day))
    date = dt.today().strftime("%Y-%m-%d")

    if working_directory is None:
        working_directory = "."

    wd = Path(working_directory).resolve()
    project_name = "{pn}-{exp}-{date}-{triangulate}".format(
        pn=project, exp=experimenter, date=date, triangulate="3d"
    )
    project_path = wd / project_name
    # Create project and sub-directories
    if not DEBUG and project_path.exists():
        print('Project "{}" already exists!'.format(project_path))
        return

    camera_matrix_path = project_path / "camera_matrix"
    calibration_images_path = project_path / "calibration_images"
    undistortion_path = project_path / "undistortion"
    path_corners = project_path / "corners"
    path_removed_images = project_path / "removed_calibration_images"

    for p in [
        camera_matrix_path,
        calibration_images_path,
        undistortion_path,
        path_corners,
        path_removed_images,
    ]:
        p.mkdir(parents=True, exist_ok=DEBUG)
        print('Created "{}"'.format(p))

    # Create config file
    cfg_file_3d, ruamelFile_3d = auxiliaryfunctions.create_config_template_3d()
    cfg_file_3d["Task"] = project
    cfg_file_3d["scorer"] = experimenter
    cfg_file_3d["date"] = d
    cfg_file_3d["project_path"] = str(project_path)
    #    cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]
    #    cfg_file_3d['config_files']= ['Enter the path of the config file 1']
    cfg_file_3d["colormap"] = "jet"
    cfg_file_3d["dotsize"] = 15
    cfg_file_3d["alphaValue"] = 0.8
    cfg_file_3d["markerType"] = "*"
    cfg_file_3d["markerColor"] = "r"
    cfg_file_3d["pcutoff"] = 0.4
    cfg_file_3d["num_cameras"] = num_cameras
    cfg_file_3d["camera_names"] = [
        str("camera-" + str(i)) for i in range(1, num_cameras + 1)
    ]
    cfg_file_3d["scorername_3d"] = "DLC_3D"

    cfg_file_3d["skeleton"] = [
        ["bodypart1", "bodypart2"],
        ["bodypart2", "bodypart3"],
        ["bodypart3", "bodypart4"],
        ["bodypart4", "bodypart5"],
    ]
    cfg_file_3d["skeleton_color"] = "black"

    for i in range(num_cameras):
        path = str(
            "/home/mackenzie/DEEPLABCUT/DeepLabCut/2DprojectCam"
            + str(i + 1)
            + "-Mackenzie-2019-06-05/config.yaml"
        )
        cfg_file_3d.insert(
            len(cfg_file_3d), str("config_file_camera-" + str(i + 1)), path
        )

    for i in range(num_cameras):
        cfg_file_3d.insert(len(cfg_file_3d), str("shuffle_camera-" + str(i + 1)), 1)
        cfg_file_3d.insert(
            len(cfg_file_3d), str("trainingsetindex_camera-" + str(i + 1)), 0
        )

    projconfigfile = os.path.join(str(project_path), "config.yaml")
    auxiliaryfunctions.write_config_3d(projconfigfile, cfg_file_3d)

    print('Generated "{}"'.format(project_path / "config.yaml"))
    print(
        "\nA new project with name %s is created at %s and a configurable file (config.yaml) is stored there. If you have not calibrated the cameras, then use the function 'calibrate_camera' to start calibrating the camera otherwise use the function ``triangulate`` to triangulate the dataframe"
        % (project_name, wd)
    )
    return projconfigfile


--- File: deeplabcut/create_project/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.create_project.add import add_new_videos
from deeplabcut.create_project.demo_data import load_demo_data
from deeplabcut.create_project.modelzoo import (
    create_pretrained_human_project,
    create_pretrained_project,
)
from deeplabcut.create_project.new import create_new_project
from deeplabcut.create_project.new_3d import create_new_project_3d


--- File: deeplabcut/create_project/demo_data.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import os
from pathlib import Path

import deeplabcut
from deeplabcut.core.engine import Engine
from deeplabcut.utils import auxiliaryfunctions


def load_demo_data(
    config: str,
    createtrainingset: bool = True,
    engine: Engine = Engine.PYTORCH,
):
    """
    Loads the demo data -- subset from trail-tracking data in Mathis et al. 2018.
    When loading, it sets paths correctly to run this project on your system

    Parameter
      ----------
      config : string
          Full path of the config.yaml file of the provided demo dataset as a string.

      createtrainingset : bool
          Boolean variable indicating if a training set shall be created.

      engine: Engine
          The Engine to create the training set for if a training set shall be created.

      Example
      --------
      >>> deeplabcut.load_demo_data('config.yaml')
      --------
    """
    config = Path(config).resolve()
    config = str(config)

    transform_data(config)
    if createtrainingset:
        print("Loaded, now creating training data...")
        deeplabcut.create_training_dataset(config, num_shuffles=1, engine=engine)


def transform_data(config):
    """
    This function adds the full path to labeling dataset.
    It also adds the correct path to the video file in the config file.
    """

    cfg = auxiliaryfunctions.read_config(config)
    project_path = str(Path(config).parents[0])

    cfg["project_path"] = project_path
    if "Reaching" in project_path:
        video_file = os.path.join(project_path, "videos", "reachingvideo1.avi")
    elif "openfield" in project_path:
        video_file = os.path.join(project_path, "videos", "m4s1.mp4")
    else:
        print("This is not an official demo dataset.")

    if "WILL BE AUTOMATICALLY UPDATED BY DEMO CODE" in cfg["video_sets"].keys():
        cfg["video_sets"][str(video_file)] = cfg["video_sets"].pop(
            "WILL BE AUTOMATICALLY UPDATED BY DEMO CODE"
        )

    auxiliaryfunctions.write_config(config, cfg)


--- File: deeplabcut/create_project/new.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import os
import shutil
import warnings
from pathlib import Path

from deeplabcut import DEBUG
from deeplabcut.core.engine import Engine
from deeplabcut.utils.auxfun_videos import VideoReader


def create_new_project(
    project: str,
    experimenter: str,
    videos: list[str],
    working_directory: str | None = None,
    copy_videos: bool = False,
    videotype: str = "",
    multianimal: bool = False,
    individuals: list[str] | None = None,
):
    r"""Create the necessary folders and files for a new project.

    Creating a new project involves creating the project directory, sub-directories and
    a basic configuration file. The configuration file is loaded with the default
    values. Change its parameters to your projects need.

    Parameters
    ----------
    project : string
        The name of the project.

    experimenter : string
        The name of the experimenter.

    videos : list[str]
        A list of strings representing the full paths of the videos to include in the
        project. If the strings represent a directory instead of a file, all videos of
        ``videotype`` will be imported.

    working_directory : string, optional
        The directory where the project will be created. The default is the
        ``current working directory``.

    copy_videos : bool, optional, Default: False.
        If True, the videos are copied to the ``videos`` directory. If False, symlinks
        of the videos will be created in the ``project/videos`` directory; in the event
        of a failure to create symbolic links, videos will be moved instead.

    multianimal: bool, optional. Default: False.
        For creating a multi-animal project (introduced in DLC 2.2)

    individuals: list[str]|None = None,
        Relevant only if multianimal is True.
        list of individuals to be used in the project configuration.
        If None - defaults to ['individual1', 'individual2', 'individual3']

    Returns
    -------
    str
        Path to the new project configuration file.

    Examples
    --------

    Linux/MacOS:

    >>> deeplabcut.create_new_project(
            project='reaching-task',
            experimenter='Linus',
            videos=[
                '/data/videos/mouse1.avi',
                '/data/videos/mouse2.avi',
                '/data/videos/mouse3.avi'
            ],
            working_directory='/analysis/project/',
        )
    >>> deeplabcut.create_new_project(
            project='reaching-task',
            experimenter='Linus',
            videos=['/data/videos'],
            videotype='.mp4',
        )

    Windows:

    >>> deeplabcut.create_new_project(
            'reaching-task',
            'Bill',
            [r'C:\yourusername\rig-95\Videos\reachingvideo1.avi'],
            copy_videos=True,
        )

    Users must format paths with either:  r'C:\ OR 'C:\\ <- i.e. a double backslash \ \ )
    """
    from datetime import datetime as dt
    from deeplabcut.utils import auxiliaryfunctions

    months_3letter = {
        1: "Jan",
        2: "Feb",
        3: "Mar",
        4: "Apr",
        5: "May",
        6: "Jun",
        7: "Jul",
        8: "Aug",
        9: "Sep",
        10: "Oct",
        11: "Nov",
        12: "Dec",
    }

    date = dt.today()
    month = months_3letter[date.month]
    day = date.day
    d = str(month[0:3] + str(day))
    date = dt.today().strftime("%Y-%m-%d")
    if working_directory is None:
        working_directory = "."
    wd = Path(working_directory).resolve()
    project_name = "{pn}-{exp}-{date}".format(pn=project, exp=experimenter, date=date)
    project_path = wd / project_name

    # Create project and sub-directories
    if not DEBUG and project_path.exists():
        print('Project "{}" already exists!'.format(project_path))
        return os.path.join(str(project_path), "config.yaml")
    video_path = project_path / "videos"
    data_path = project_path / "labeled-data"
    shuffles_path = project_path / "training-datasets"
    results_path = project_path / "dlc-models"
    for p in [video_path, data_path, shuffles_path, results_path]:
        p.mkdir(parents=True, exist_ok=DEBUG)
        print('Created "{}"'.format(p))

    # Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!
    vids = []
    for i in videos:
        # Check if it is a folder
        if os.path.isdir(i):
            vids_in_dir = [
                os.path.join(i, vp)
                for vp in os.listdir(i)
                if vp.lower().endswith(videotype)
            ]
            vids = vids + vids_in_dir
            if len(vids_in_dir) == 0:
                print("No videos found in", i)
                print(
                    "Perhaps change the videotype, which is currently set to:",
                    videotype,
                )
            else:
                videos = vids
                print(
                    len(vids_in_dir),
                    " videos from the directory",
                    i,
                    "were added to the project.",
                )
        else:
            if os.path.isfile(i):
                vids = vids + [i]
            videos = vids

    videos = [Path(vp) for vp in videos]
    dirs = [data_path / Path(i.stem) for i in videos]
    for p in dirs:
        """
        Creates directory under data
        """
        p.mkdir(parents=True, exist_ok=True)

    destinations = [video_path.joinpath(vp.name) for vp in videos]
    if copy_videos:
        print("Copying the videos")
        for src, dst in zip(videos, destinations):
            shutil.copy(
                os.fspath(src), os.fspath(dst)
            )  # https://www.python.org/dev/peps/pep-0519/
    else:
        # creates the symlinks of the video and puts it in the videos directory.
        print("Attempting to create a symbolic link of the video ...")
        for src, dst in zip(videos, destinations):
            if dst.exists() and not DEBUG:
                raise FileExistsError("Video {} exists already!".format(dst))
            try:
                src = str(src)
                dst = str(dst)
                os.symlink(src, dst)
                print("Created the symlink of {} to {}".format(src, dst))
            except OSError:
                try:
                    import subprocess

                    subprocess.check_call("mklink %s %s" % (dst, src), shell=True)
                except (OSError, subprocess.CalledProcessError):
                    print(
                        "Symlink creation impossible (exFat architecture?): "
                        "copying the video instead."
                    )
                    shutil.copy(os.fspath(src), os.fspath(dst))
                    print("{} copied to {}".format(src, dst))
            videos = destinations

    if copy_videos:
        videos = destinations  # in this case the *new* location should be added to the config file

    # adds the video list to the config.yaml file
    video_sets = {}
    for video in videos:
        print(video)
        try:
            # For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)]
            rel_video_path = str(Path.resolve(Path(video)))
        except:
            rel_video_path = os.readlink(str(video))

        try:
            vid = VideoReader(rel_video_path)
            video_sets[rel_video_path] = {"crop": ", ".join(map(str, vid.get_bbox()))}
        except IOError:
            warnings.warn("Cannot open the video file! Skipping to the next one...")
            os.remove(video)  # Removing the video or link from the project

    if not len(video_sets):
        # Silently sweep the files that were already written.
        shutil.rmtree(project_path, ignore_errors=True)
        warnings.warn(
            "No valid videos were found. The project was not created... "
            "Verify the video files and re-create the project."
        )
        return "nothingcreated"

    # Set values to config file:
    if multianimal:  # parameters specific to multianimal project
        cfg_file, ruamelFile = auxiliaryfunctions.create_config_template(multianimal)
        cfg_file["multianimalproject"] = multianimal
        cfg_file["identity"] = False
        cfg_file["individuals"] = (
            individuals
            if individuals
            else ["individual1", "individual2", "individual3"]
        )
        cfg_file["multianimalbodyparts"] = ["bodypart1", "bodypart2", "bodypart3"]
        cfg_file["uniquebodyparts"] = []
        cfg_file["bodyparts"] = "MULTI!"
        cfg_file["skeleton"] = [
            ["bodypart1", "bodypart2"],
            ["bodypart2", "bodypart3"],
            ["bodypart1", "bodypart3"],
        ]
        engine = cfg_file.get("engine")
        if engine in Engine.PYTORCH.aliases:
            cfg_file["default_augmenter"] = "albumentations"
            cfg_file["default_net_type"] = "resnet_50"
        elif engine in Engine.TF.aliases:
            cfg_file["default_augmenter"] = "multi-animal-imgaug"
            cfg_file["default_net_type"] = "dlcrnet_ms5"
        else:
            raise ValueError(f"Unknown or undefined engine {engine}")
        cfg_file["default_track_method"] = "ellipse"
    else:
        cfg_file, ruamelFile = auxiliaryfunctions.create_config_template()
        cfg_file["multianimalproject"] = False
        cfg_file["bodyparts"] = ["bodypart1", "bodypart2", "bodypart3", "objectA"]
        cfg_file["skeleton"] = [["bodypart1", "bodypart2"], ["objectA", "bodypart3"]]
        cfg_file["default_augmenter"] = "default"
        cfg_file["default_net_type"] = "resnet_50"

    # common parameters:
    cfg_file["Task"] = project
    cfg_file["scorer"] = experimenter
    cfg_file["video_sets"] = video_sets
    cfg_file["project_path"] = str(project_path)
    cfg_file["date"] = d
    cfg_file["cropping"] = False
    cfg_file["start"] = 0
    cfg_file["stop"] = 1
    cfg_file["numframes2pick"] = 20
    cfg_file["TrainingFraction"] = [0.95]
    cfg_file["iteration"] = 0
    cfg_file["snapshotindex"] = -1
    cfg_file["detector_snapshotindex"] = -1
    cfg_file["x1"] = 0
    cfg_file["x2"] = 640
    cfg_file["y1"] = 277
    cfg_file["y2"] = 624
    cfg_file["batch_size"] = (
        8  # batch size during inference (video - analysis); see https://www.biorxiv.org/content/early/2018/10/30/457242
    )
    cfg_file["detector_batch_size"] = 1
    cfg_file["corner2move2"] = (50, 50)
    cfg_file["move2corner"] = True
    cfg_file["skeleton_color"] = "black"
    cfg_file["pcutoff"] = 0.6
    cfg_file["dotsize"] = 12  # for plots size of dots
    cfg_file["alphavalue"] = 0.7  # for plots transparency of markers
    cfg_file["colormap"] = "rainbow"  # for plots type of colormap

    projconfigfile = os.path.join(str(project_path), "config.yaml")
    # Write dictionary to yaml  config file
    auxiliaryfunctions.write_config(projconfigfile, cfg_file)

    print('Generated "{}"'.format(project_path / "config.yaml"))
    print(
        "\nA new project with name %s is created at %s and a configurable file (config.yaml) is stored there. Change the parameters in this file to adapt to your project's needs.\n Once you have changed the configuration file, use the function 'extract_frames' to select frames for labeling.\n. [OPTIONAL] Use the function 'add_new_videos' to add new videos to your project (at any stage)."
        % (project_name, str(wd))
    )
    return projconfigfile


--- File: deeplabcut/gui/window.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import logging
import subprocess
import sys
from functools import cached_property
from pathlib import Path
from typing import List
from urllib.error import URLError
import qdarkstyle

import deeplabcut
from deeplabcut import auxiliaryfunctions, VERSION, compat
from deeplabcut.core.engine import Engine
from deeplabcut.gui import BASE_DIR, components, utils
from deeplabcut.gui.tabs import *
from deeplabcut.gui.widgets import StreamReceiver, StreamWriter
from deeplabcut.utils.multiprocessing import call_with_timeout
from napari_deeplabcut import misc
from PySide6.QtWidgets import (
    QMessageBox,
    QMenu,
    QWidget,
    QMainWindow,
    QComboBox,
    QLabel,
    QSizePolicy,
)
from PySide6 import QtCore
from PySide6.QtGui import QIcon, QAction, QPixmap
from PySide6 import QtWidgets, QtGui
from PySide6.QtCore import Qt, QTimer


def _check_for_updates(silent=True):
    try:
        is_latest, latest_version = call_with_timeout(
            utils.is_latest_deeplabcut_version, 5
        )
        is_latest_plugin, latest_plugin_version = call_with_timeout(
            misc.is_latest_version, 5
        )
    except (URLError, TimeoutError):  # Handle internet connectivity issues
        is_latest = is_latest_plugin = True

    if is_latest and is_latest_plugin:
        if not silent:
            msg = QtWidgets.QMessageBox(
                text=f"DeepLabCut is up-to-date",
            )
            msg.exec_()
    else:
        if not is_latest and is_latest_plugin:
            text = f"DeepLabCut {latest_version} available"
            command = "pip", "install", "-U", "deeplabcut"
        elif not is_latest_plugin and is_latest:
            text = f"DeepLabCut labeling plugin {latest_plugin_version} available"
            command = "pip", "install", "-U", "napari-deeplabcut"
        else:
            text = f"DeepLabCut {latest_version}\nand labeling plugin {latest_plugin_version} available"
            command = "pip", "install", "-U", "deeplabcut", "napari-deeplabcut"

        msg = QtWidgets.QMessageBox(
            text=text,
        )
        msg.setIcon(QtWidgets.QMessageBox.Information)
        update_btn = msg.addButton("Update", QtWidgets.QMessageBox.AcceptRole)
        msg.setDefaultButton(update_btn)
        _ = msg.addButton("Skip", QtWidgets.QMessageBox.RejectRole)
        msg.exec_()
        if msg.clickedButton() is update_btn:
            subprocess.check_call([sys.executable, "-m", *command])


class MainWindow(QMainWindow):
    config_loaded = QtCore.Signal()
    video_type_ = QtCore.Signal(str)
    video_files_ = QtCore.Signal(set)
    engine_change = QtCore.Signal(Engine)
    shuffle_change = QtCore.Signal(int)
    shuffle_created = QtCore.Signal(int)

    def __init__(self, app):
        super(MainWindow, self).__init__()
        self.app = app
        screen_size = app.screens()[0].size()
        self.screen_width = screen_size.width()
        self.screen_height = screen_size.height()

        self.logger = logging.getLogger("GUI")

        self.config = None
        self.loaded = False

        self.shuffle_value = 1
        self.trainingset_index = 0
        self.videotype = "mp4"
        self.files = set()

        self._engine = Engine.PYTORCH

        self.default_set()

        self._generate_welcome_page()
        self.window_set()
        self.default_set()

        names = ["new_project.png", "open.png", "help.png"]
        self.create_actions(names)
        self.create_menu_bar()
        self.load_settings()
        self._toolbar = None
        self.create_toolbar()

        # Thread-safe Stdout redirector
        self.writer = StreamWriter()
        sys.stdout = self.writer
        self.receiver = StreamReceiver(self.writer.queue)
        self.receiver.new_text.connect(self.print_to_status_bar)

        # create logger to also log to the console
        logging.basicConfig()
        logging.getLogger("console").setLevel(logging.INFO)

        self._progress_bar = QtWidgets.QProgressBar()
        self._progress_bar.setMaximum(0)
        self._progress_bar.hide()
        self.status_bar.addPermanentWidget(self._progress_bar)

    def print_to_status_bar(self, text):
        self.status_bar.showMessage(text)
        self.status_bar.repaint()
        logging.getLogger("console").info(text)

    @property
    def toolbar(self):
        if self._toolbar is None:
            self._toolbar = self.addToolBar("File")
        return self._toolbar

    @cached_property
    def settings(self):
        return QtCore.QSettings()

    def load_settings(self):
        filenames = self.settings.value("recent_files") or []
        for filename in filenames:
            self.add_recent_filename(filename)

    def save_settings(self):
        recent_files = []
        for action in self.recentfiles_menu.actions()[::-1]:
            recent_files.append(action.text())
        self.settings.setValue("recent_files", recent_files)

    def add_recent_filename(self, filename):
        actions = self.recentfiles_menu.actions()
        filenames = [action.text() for action in actions]
        if filename in filenames:
            return
        action = QAction(filename, self)
        before_action = actions[0] if actions else None
        self.recentfiles_menu.insertAction(before_action, action)

    @property
    def cfg(self):
        try:
            cfg = auxiliaryfunctions.read_config(self.config)
        except TypeError:
            cfg = {}
        return cfg

    @property
    def engine(self) -> Engine:
        return self._engine

    @engine.setter
    def engine(self, e: Engine) -> None:
        if self._engine == e:
            return

        if e == e.TF:
            try:
                import tensorflow
            except ModuleNotFoundError as err:
                msg = QtWidgets.QMessageBox()
                msg.setIcon(QtWidgets.QMessageBox.Warning)
                msg.setText("Cannot use the TensorFlow engine.")
                msg.setInformativeText(
                    f"Error `{err}`\nCannot use the TensorFlow engine as TensorFlow "
                    "is not installed. To use it, install TensorFlow with\n"
                    "    Windows/Linux:\n"
                    "        pip install 'deeplabcut[tf]'\n"
                    "    Apple Silicon:\n"
                    "        pip install 'deeplabcut[apple_mchips]'\n\n"
                    "Please switch back to the PyTorch engine to use DeepLabCut, or install TensorFlow."
                )

                msg.setWindowTitle("Info")
                msg.setMinimumWidth(900)
                logo_dir = os.path.dirname(os.path.realpath("logo.png")) + os.path.sep
                logo = logo_dir + "/assets/logo.png"
                msg.setWindowIcon(QIcon(logo))
                msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
                msg.exec_()

        self._engine = e
        self.engine_change.emit(e)

    @property
    def project_folder(self) -> str:
        return self.cfg.get("project_path", os.path.expanduser("~/Desktop"))

    @property
    def is_multianimal(self) -> bool:
        return bool(self.cfg.get("multianimalproject"))

    @property
    def all_bodyparts(self) -> List:
        if self.is_multianimal:
            return self.cfg.get("multianimalbodyparts")
        else:
            return self.cfg["bodyparts"]

    @property
    def all_individuals(self) -> List:
        if self.is_multianimal:
            return self.cfg.get("individuals")
        else:
            return [""]

    @property
    def pose_cfg_path(self) -> str:
        try:
            return str(
                compat.return_train_network_path(
                    self.config,
                    shuffle=int(self.shuffle_value),
                    trainingsetindex=int(self.trainingset_index),
                    modelprefix="",
                )[0]
            )
        except FileNotFoundError:
            return str(Path(deeplabcut.__file__).parent / "pose_cfg.yaml")

    @property
    def models_folder(self) -> str:
        try:
            return str(
                compat.return_train_network_path(
                    self.config,
                    shuffle=int(self.shuffle_value),
                    trainingsetindex=int(self.trainingset_index),
                    modelprefix="",
                )[2]
            )
        except FileNotFoundError:
            return self.project_folder()

    @property
    def inference_cfg_path(self) -> str:
        return os.path.join(
            self.cfg["project_path"],
            auxiliaryfunctions.get_model_folder(
                self.cfg["TrainingFraction"][int(self.trainingset_index)],
                int(self.shuffle_value),
                self.cfg,
            ),
            "test",
            "inference_cfg.yaml",
        )

    def update_cfg(self, text):
        self.root.config = text
        self.unsupervised_id_tracking.setEnabled(self.is_transreid_available())

    def update_shuffle(self, value):
        self.shuffle_value = value
        self.shuffle_change.emit(value)
        self.logger.info(f"Shuffle set to {self.shuffle_value}")

    @property
    def video_type(self):
        return self.videotype

    @video_type.setter
    def video_type(self, ext):
        self.videotype = ext
        self.video_type_.emit(ext)
        self.logger.info(f"Video type set to {self.video_type}")

    @property
    def video_files(self):
        return self.files

    def add_video_files(self, new_video_files):
        """
        Add new video files to the existing set of files. This method ensures no duplicates are added.
        Emits a signal to notify about the updated set of files.
        """
        new_video_files = set(new_video_files)
        self.files.update(new_video_files) # Add new items to the existing set
        self.video_files_.emit(self.files) # Emit the updated set of files
        self.logger.info(f"Videos added to analyze:\n{new_video_files}\nCurrent video files:\n{self.files}")

    def clear_video_files(self):
        """
        Clear all video files from the existing set. Emits a signal to notify the change.
        """
        self.files.clear()  # Reset the set to be empty
        self.video_files_.emit(self.files)  # Emit the empty set
        self.logger.info("All video files have been cleared.")

    def window_set(self):
        self.setWindowTitle("DeepLabCut")

        palette = QtGui.QPalette()
        palette.setColor(QtGui.QPalette.Window, QtGui.QColor("#ffffff"))
        self.setPalette(palette)

        icon = os.path.join(BASE_DIR, "assets", "logo.png")
        self.setWindowIcon(QIcon(icon))

        self.status_bar = self.statusBar()
        self.status_bar.setObjectName("Status Bar")
        self.status_bar.showMessage("www.deeplabcut.org")

    def _generate_welcome_page(self):
        self.layout = QtWidgets.QVBoxLayout()
        self.layout.setAlignment(Qt.AlignCenter | Qt.AlignTop)
        self.layout.setSpacing(30)

        title = components._create_label_widget(
            f"Welcome to the DeepLabCut Project Manager GUI {VERSION}!",
            "font:bold; font-size:18px;",
            margins=(0, 30, 0, 0),
        )
        title.setAlignment(Qt.AlignCenter)
        self.layout.addWidget(title)

        image_widget = QtWidgets.QLabel(self)
        image_widget.setAlignment(Qt.AlignCenter)
        image_widget.setContentsMargins(0, 0, 0, 0)
        logo = os.path.join(BASE_DIR, "assets", "logo_transparent.png")
        pixmap = QtGui.QPixmap(logo)
        image_widget.setPixmap(
            pixmap.scaledToHeight(400, QtCore.Qt.SmoothTransformation)
        )
        self.layout.addWidget(image_widget)

        description = "DeepLabCut™ is an open source tool for markerless pose estimation of user-defined body parts with deep learning.\nA.  and M.W.  Mathis Labs | http://www.deeplabcut.org\n\n To get started,  create a new project, load an existing one, or try one of our pretrained models from the Model Zoo."
        label = components._create_label_widget(
            description,
            "font-size:12px; text-align: center;",
            margins=(0, 0, 0, 0),
        )
        label.setMinimumWidth(400)
        label.setWordWrap(True)
        label.setAlignment(Qt.AlignCenter)
        self.layout.addWidget(label)

        self.layout_buttons = QtWidgets.QHBoxLayout()
        self.layout_buttons.setAlignment(Qt.AlignCenter | Qt.AlignCenter)
        self.create_project_button = QtWidgets.QPushButton("Create New Project")
        self.create_project_button.setFixedWidth(200)
        self.create_project_button.clicked.connect(self._create_project)

        self.load_project_button = QtWidgets.QPushButton("Load Project")
        self.load_project_button.setFixedWidth(200)
        self.load_project_button.clicked.connect(self._open_project)

        self.run_superanimal_button = QtWidgets.QPushButton("Model Zoo")
        self.run_superanimal_button.setFixedWidth(200)
        self.run_superanimal_button.clicked.connect(self._goto_superanimal)

        self.layout_buttons.addWidget(self.create_project_button)
        self.layout_buttons.addWidget(self.load_project_button)
        self.layout_buttons.addWidget(self.run_superanimal_button)

        self.layout.addLayout(self.layout_buttons)

        widget = QWidget()
        widget.setLayout(self.layout)
        self.setCentralWidget(widget)

        QTimer.singleShot(1000, lambda: _check_for_updates(silent=True))

    def default_set(self):
        self.name_default = ""
        self.proj_default = ""
        self.exp_default = ""
        self.loc_default = str(Path.home())

    def create_actions(self, names):
        # Creating action using the first constructor
        self.newAction = QAction(self)
        self.newAction.setText("&New Project...")

        self.newAction.setIcon(
            QIcon(os.path.join(BASE_DIR, "assets", "icons", names[0]))
        )
        self.newAction.setShortcut("Ctrl+N")
        self.newAction.setStatusTip("Create a new project...")

        self.newAction.triggered.connect(self._create_project)

        # Creating actions using the second constructor
        self.openAction = QAction("&Open...", self)
        self.openAction.setIcon(
            QIcon(os.path.join(BASE_DIR, "assets", "icons", names[1]))
        )
        self.openAction.setShortcut("Ctrl+O")
        self.openAction.setStatusTip("Open a project...")
        self.openAction.triggered.connect(self._open_project)

        self.saveAction = QAction("&Save", self)
        self.exitAction = QAction("&Exit", self)

        self.lightmodeAction = QAction("&Light theme", self)
        self.lightmodeAction.triggered.connect(self.lightmode)
        self.darkmodeAction = QAction("&Dark theme", self)
        self.darkmodeAction.triggered.connect(self.darkmode)

        self.helpAction = QAction("&Help", self)
        self.helpAction.setIcon(
            QIcon(os.path.join(BASE_DIR, "assets", "icons", names[2]))
        )
        self.helpAction.setStatusTip("Ask for help...")
        self.helpAction.triggered.connect(self._ask_for_help)

        self.aboutAction = QAction("&Learn DLC", self)
        self.aboutAction.triggered.connect(self._learn_dlc)

        self.check_updates = QAction("&Check for Updates...", self)
        self.check_updates.triggered.connect(lambda: _check_for_updates(silent=False))

    def create_menu_bar(self):
        menu_bar = self.menuBar()

        # File menu
        self.file_menu = QMenu("&File", self)
        menu_bar.addMenu(self.file_menu)

        self.file_menu.addAction(self.newAction)
        self.file_menu.addAction(self.openAction)

        self.recentfiles_menu = self.file_menu.addMenu("Open Recent")
        self.recentfiles_menu.triggered.connect(
            lambda a: self._update_project_state(a.text(), True)
        )
        self.file_menu.addAction(self.saveAction)
        self.file_menu.addAction(self.exitAction)

        # View menu
        view_menu = QMenu("&View", self)
        mode = view_menu.addMenu("Appearance")
        menu_bar.addMenu(view_menu)
        mode.addAction(self.lightmodeAction)
        mode.addAction(self.darkmodeAction)

        # Help menu
        help_menu = QMenu("&Help", self)
        menu_bar.addMenu(help_menu)
        help_menu.addAction(self.helpAction)
        help_menu.adjustSize()
        help_menu.addAction(self.check_updates)
        help_menu.addAction(self.aboutAction)

    def update_menu_bar(self):
        self.file_menu.removeAction(self.newAction)
        self.file_menu.removeAction(self.openAction)

    def create_toolbar(self):
        self.toolbar.clear()
        self.toolbar.addAction(self.newAction)
        self.toolbar.addAction(self.openAction)
        self.toolbar.addAction(self.helpAction)

        size_policy = QSizePolicy()  # QtWidgets.QSizePolicy.Policy.Expanding
        size_policy.setHorizontalPolicy(QSizePolicy.Policy.Expanding)
        spacer = QLabel()
        spacer.setSizePolicy(size_policy)
        spacer.setStyleSheet("background: transparent;")

        engine_label = QLabel()
        engine_label.autoFillBackground()
        engine_label.setText("Engine")
        engine_label.setStyleSheet("background: transparent;")

        engine_icon = QLabel()
        engine_icon.setStyleSheet("background: transparent;")

        def _update_icon(engine: str):
            pixmap = QPixmap(f"deeplabcut/gui/media/dlc-{engine}.png")
            engine_icon.setPixmap(
                pixmap.scaled(56, 56, Qt.AspectRatioMode.KeepAspectRatio)
            )

        _update_icon("pt" if self.engine == Engine.PYTORCH else "tf")

        engines = [engine for engine in Engine]

        def _update_engine(index: int) -> None:
            self.logger.info(f"Changed engine to {engines[index]}")
            self.engine = engines[index]
            _update_icon("pt" if self.engine == Engine.PYTORCH else "tf")

        change_engine_widget = QComboBox()
        change_engine_widget.addItems([e.aliases[0] for e in engines])
        change_engine_widget.setFixedWidth(180)
        change_engine_widget.currentIndexChanged.connect(_update_engine)
        change_engine_widget.setCurrentIndex(engines.index(self.engine))

        self.toolbar.addWidget(spacer)
        self.toolbar.addWidget(engine_icon)
        self.toolbar.addWidget(engine_label)
        self.toolbar.addWidget(change_engine_widget)

    def remove_action(self):
        self.toolbar.removeAction(self.newAction)
        self.toolbar.removeAction(self.openAction)
        self.toolbar.removeAction(self.helpAction)

    def _update_project_state(self, config, loaded):
        self.config = config
        self.loaded = loaded
        if loaded:
            self.add_recent_filename(self.config)
            self.add_tabs()

    def _ask_for_help(self):
        dlg = QMessageBox(self)
        dlg.setWindowTitle("Ask for help")
        dlg.setText(
            """Ask our community for help on <a href='https://forum.image.sc/tag/deeplabcut'>the forum</a>!"""
        )
        _ = dlg.exec()

    def _learn_dlc(self):
        dlg = QMessageBox(self)
        dlg.setWindowTitle("Learn DLC")
        dlg.setText(
            """Learn DLC with <a href='https://deeplabcut.github.io/DeepLabCut/docs/UseOverviewGuide.html'>our docs and how-to guides</a>!"""
        )
        _ = dlg.exec()

    def _create_project(self):
        dlg = ProjectCreator(self)
        dlg.show()

    def _open_project(self):
        open_project = OpenProject(self)
        open_project.load_config()
        if not open_project.config:
            return

        open_project.loaded = True
        self._update_project_state(
            open_project.config,
            open_project.loaded,
        )

    def _goto_superanimal(self):
        self.tab_widget = QtWidgets.QTabWidget()
        self.tab_widget.setContentsMargins(0, 20, 0, 0)
        self.modelzoo = ModelZoo(
            root=self, parent=None, h1_description="DeepLabCut - Model Zoo"
        )
        self.tab_widget.addTab(self.modelzoo, "Model Zoo")
        self.setCentralWidget(self.tab_widget)

    def load_config(self, config):
        self.config = config
        self.config_loaded.emit()
        print(f'Project "{self.cfg["Task"]}" successfully loaded.')

    def darkmode(self):
        dark_stylesheet = qdarkstyle.load_stylesheet_pyside2()
        self.app.setStyleSheet(dark_stylesheet)

        names = ["new_project2.png", "open2.png", "help2.png"]
        self.remove_action()
        self.create_actions(names)
        self.update_menu_bar()
        self.create_toolbar()

    def lightmode(self):
        from qdarkstyle.light.palette import LightPalette

        style = qdarkstyle.load_stylesheet(palette=LightPalette)
        self.app.setStyleSheet(style)

        names = ["new_project.png", "open.png", "help.png"]
        self.remove_action()
        self.create_actions(names)
        self.create_toolbar()
        self.update_menu_bar()

    def add_tabs(self):
        self.tab_widget = QtWidgets.QTabWidget()
        self.tab_widget.setContentsMargins(0, 20, 0, 0)
        self.manage_project = ManageProject(
            root=self, parent=None, h1_description="DeepLabCut - Manage Project"
        )
        self.extract_frames = ExtractFrames(
            root=self, parent=None, h1_description="DeepLabCut - Extract Frames"
        )
        self.label_frames = LabelFrames(
            root=self, parent=None, h1_description="DeepLabCut - Label Frames"
        )
        self.create_training_dataset = CreateTrainingDataset(
            root=self,
            parent=None,
            h1_description="DeepLabCut - Step 4. Create training dataset",
        )
        self.train_network = TrainNetwork(
            root=self,
            parent=None,
            h1_description="DeepLabCut - Train network",
        )
        self.evaluate_network = EvaluateNetwork(
            root=self,
            parent=None,
            h1_description="DeepLabCut - Evaluate Network",
        )
        self.analyze_videos = AnalyzeVideos(
            root=self, parent=None, h1_description="DeepLabCut - Analyze Videos"
        )
        self.unsupervised_id_tracking = UnsupervizedIdTracking(
            root=self,
            parent=None,
            h1_description="DeepLabCut - Optional Unsupervised ID Tracking with Transformer",
        )
        self.create_videos = CreateVideos(
            root=self,
            parent=None,
            h1_description="DeepLabCut - Create Videos",
        )
        self.extract_outlier_frames = ExtractOutlierFrames(
            root=self,
            parent=None,
            h1_description="DeepLabCut - Step 8. Extract outlier frames",
        )
        self.refine_tracklets = RefineTracklets(
            root=self, parent=None, h1_description="DeepLabCut - Refine labels"
        )
        self.modelzoo = ModelZoo(
            root=self, parent=None, h1_description="DeepLabCut - Model Zoo"
        )
        self.video_editor = VideoEditor(
            root=self, parent=None, h1_description="DeepLabCut - Optional Video Editor"
        )

        self.tab_widget.addTab(self.manage_project, "Manage project")
        self.tab_widget.addTab(self.extract_frames, "Extract frames")
        self.tab_widget.addTab(self.label_frames, "Label frames")
        self.tab_widget.addTab(self.create_training_dataset, "Create training dataset")
        self.tab_widget.addTab(self.train_network, "Train network")
        self.tab_widget.addTab(self.evaluate_network, "Evaluate network")
        self.tab_widget.addTab(self.analyze_videos, "Analyze videos")
        self.tab_widget.addTab(
            self.unsupervised_id_tracking, "Unsupervised ID Tracking (*)"
        )
        self.tab_widget.addTab(self.create_videos, "Create videos")
        self.tab_widget.addTab(
            self.extract_outlier_frames, "Extract outlier frames (*)"
        )
        self.tab_widget.addTab(self.refine_tracklets, "Refine tracklets (*)")
        self.tab_widget.addTab(self.modelzoo, "Model Zoo")
        self.tab_widget.addTab(self.video_editor, "Video editor (*)")

        if not self.is_multianimal:
            self.tab_widget.removeTab(
                self.tab_widget.indexOf(self.unsupervised_id_tracking)
            )
            self.tab_widget.removeTab(self.tab_widget.indexOf(self.refine_tracklets))

        self.setCentralWidget(self.tab_widget)
        self.tab_widget.currentChanged.connect(self.refresh_active_tab)

    def refresh_active_tab(self):
        active_tab = self.tab_widget.currentWidget()
        tab_label = self.tab_widget.tabText(self.tab_widget.currentIndex())

        widget_to_attribute_map = {
            QtWidgets.QSpinBox: "setValue",
            components.ShuffleSpinBox: "setValue",
            components.TrainingSetSpinBox: "setValue",
            QtWidgets.QLineEdit: "setText",
        }

        def _attempt_attribute_update(widget_name, updated_value):
            try:
                widget = getattr(active_tab, widget_name)
                method = getattr(widget, widget_to_attribute_map[type(widget)])
                self.logger.debug(
                    f"Setting {widget_name}={updated_value} in tab '{tab_label}'"
                )
                method(updated_value)
            except AttributeError:
                pass

        _attempt_attribute_update("shuffle", self.shuffle_value)
        _attempt_attribute_update("cfg_line", self.config)

    def is_transreid_available(self):
        if self.is_multianimal:
            try:
                from deeplabcut.pose_tracking_pytorch import transformer_reID

                return True
            except ModuleNotFoundError:
                return False
        else:
            return False

    def closeEvent(self, event):
        print("Exiting...")
        answer = QtWidgets.QMessageBox.question(
            self,
            "Quit",
            "Are you sure you want to quit?",
            QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.Cancel,
            QtWidgets.QMessageBox.Cancel,
        )
        if answer == QtWidgets.QMessageBox.Yes:
            self.receiver.terminate()
            event.accept()
            self.save_settings()
        else:
            event.ignore()
            print("")


--- File: deeplabcut/gui/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import os

os.environ["QT_API"] = "pyside6"
import qtpy  # Necessary unused import to properly store the env variable

BASE_DIR = os.path.dirname(__file__)


--- File: deeplabcut/gui/widgets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import ast
import os
import warnings

import matplotlib.colors as mcolors
import napari
import numpy as np
import pandas as pd
from matplotlib.collections import LineCollection
from matplotlib.path import Path
from matplotlib.backends.backend_qt5agg import (
    NavigationToolbar2QT,
    FigureCanvasQTAgg as FigureCanvas,
)
from matplotlib.figure import Figure
from matplotlib.widgets import RectangleSelector, Button, LassoSelector
from queue import Queue
from PySide6 import QtCore, QtWidgets
from PySide6.QtGui import QStandardItemModel, QStandardItem, QCursor, QAction
from scipy.spatial import cKDTree as KDTree
from skimage import io

from deeplabcut.utils import auxiliaryfunctions
from deeplabcut.utils.auxfun_videos import VideoWriter


def launch_napari(files=None, plugin="napari-deeplabcut", stack=False):
    viewer = napari.Viewer()
    if plugin == "napari-deeplabcut":
        # Automatically activate the napari-deeplabcut plugin
        for action in viewer.window.plugins_menu.actions():
            if "deeplabcut" in action.text():
                action.trigger()
                break
    if files is not None:
        viewer.open(files, plugin=plugin, stack=stack)
    return viewer


class BaseFrame(QtWidgets.QFrame):
    def __init__(self, parent, **kwargs):
        super().__init__(parent)

        self.figure = Figure()
        self.axes = self.figure.add_subplot(1, 1, 1)
        self.canvas = FigureCanvas(self.figure)
        self.orig_xlim = None
        self.orig_ylim = None

        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(self.canvas)

    def getfigure(self):
        """
        Returns the figure, axes and canvas
        """
        return self.figure, self.axes, self.canvas

    def resetView(self):
        self.axes.set_xlim(self.orig_xlim)
        self.axes.set_ylim(self.orig_ylim)


class DragDropListView(QtWidgets.QListView):
    def __init__(self, parent=None):
        super(DragDropListView, self).__init__(parent)
        self.parent = parent
        self.setAcceptDrops(True)
        self.setDropIndicatorShown(True)
        self.setDragDropMode(QtWidgets.QAbstractItemView.InternalMove)
        self.model = QStandardItemModel(self)
        self.setModel(self.model)
        self._default_style = self.styleSheet()

    @property
    def items(self):
        for i in range(self.model.rowCount()):
            yield self.model.item(i)

    @property
    def state(self):
        tests = [item.checkState() == QtCore.Qt.Checked for item in self.items]
        n_checked = sum(tests)
        if all(tests):
            state = QtCore.Qt.Checked
        elif any(tests):
            state = QtCore.Qt.PartiallyChecked
        else:
            state = QtCore.Qt.Unchecked
        return state, n_checked

    def add_item(self, path):
        item = QStandardItem(path)
        item.setCheckable(True)
        item.setCheckState(QtCore.Qt.Checked)
        self.model.appendRow(item)

    def clear(self):
        self.model.removeRows(0, self.model.rowCount())

    def dragEnterEvent(self, event):
        if event.mimeData().hasUrls():
            event.accept()
        else:
            event.ignore()

    def dropEvent(self, event):
        for url in event.mimeData().urls():
            path = url.toLocalFile()
            if os.path.isfile(path):
                self.add_item(path)
            elif os.path.isdir(path):
                for root, _, files in os.walk(path):
                    for file in files:
                        if not file.startswith("."):
                            self.add_item(os.path.join(root, file))


class ItemSelectionFrame(QtWidgets.QFrame):
    def __init__(self, items, parent=None):
        super(ItemSelectionFrame, self).__init__(parent)
        self.setFrameShape(self.Shape.StyledPanel)
        self.setLineWidth(0)

        self.select_box = QtWidgets.QCheckBox("Files")
        self.select_box.setChecked(True)
        self.select_box.stateChanged.connect(self.toggle_select)

        self.fancy_list = DragDropListView(self)
        self._model = self.fancy_list.model
        self._model.rowsInserted.connect(self.check_select_box)
        self._model.rowsRemoved.connect(self.check_select_box)
        self._model.itemChanged.connect(self.check_select_box)
        for item in items:
            self.fancy_list.add_item(item)

        self.layout = QtWidgets.QVBoxLayout(self)
        self.layout.addWidget(self.select_box)
        self.layout.addWidget(self.fancy_list)

    @property
    def selected_items(self):
        for item in self.fancy_list.items:
            if item.checkState() == QtCore.Qt.Checked:
                yield item.text()

    def check_select_box(self):
        state, n_checked = self.fancy_list.state
        if self.select_box.checkState() != state:
            self.select_box.blockSignals(True)
            self.select_box.setCheckState(state)
            self.select_box.blockSignals(False)
        string = "file"
        if n_checked > 1:
            string += "s"
        self.select_box.setText(f"{n_checked} {string} selected")

    def toggle_select(self, state):
        state = QtCore.Qt.CheckState(state)
        if state == QtCore.Qt.PartiallyChecked:
            return
        for item in self.fancy_list.items:
            if item.checkState() != state:
                item.setCheckState(state)


class NavigationToolbar(NavigationToolbar2QT):
    toolitems = [
        t for t in NavigationToolbar2QT.toolitems if t[0] in ("Home", "Pan", "Zoom")
    ]

    def set_message(self, msg):
        pass

    def release_zoom(self, event):
        super(NavigationToolbar, self).release_zoom(event)
        self.zoom()


class StreamWriter:
    def __init__(self):
        self.queue = Queue()

    def write(self, text):
        if text != "\n":
            self.queue.put(text)

    def flush(self):
        pass


class StreamReceiver(QtCore.QThread):
    new_text = QtCore.Signal(str)

    def __init__(self, queue):
        super(StreamReceiver, self).__init__()
        self.queue = queue

    def run(self):
        while True:
            text = self.queue.get()
            self.new_text.emit(text)


class ClickableLabel(QtWidgets.QLabel):
    signal = QtCore.Signal()

    def __init__(self, text="", color="turquoise", parent=None):
        super(ClickableLabel, self).__init__(text, parent)
        self._default_style = self.styleSheet()
        self.color = color
        self.setStyleSheet(f"color: {self.color}")

    def mouseReleaseEvent(self, event):
        self.signal.emit()

    def enterEvent(self, event):
        self.setCursor(QCursor(QtCore.Qt.PointingHandCursor))
        self.setStyleSheet(f"color: {self.color}")

    def leaveEvent(self, event):
        self.unsetCursor()
        self.setStyleSheet(self._default_style)


class ItemCreator(QtWidgets.QDialog):
    created = QtCore.Signal(QtWidgets.QTreeWidgetItem)

    def __init__(self, parent=None):
        super(ItemCreator, self).__init__(parent)
        self.parent = parent
        vbox = QtWidgets.QVBoxLayout(self)
        self.field1 = QtWidgets.QLineEdit(self)
        self.field1.setPlaceholderText("Parameter")
        self.field2 = QtWidgets.QLineEdit(self)
        self.field2.setPlaceholderText("Value")
        create_button = QtWidgets.QPushButton(self)
        create_button.setText("Create")
        create_button.clicked.connect(self.form_item)
        vbox.addWidget(self.field1)
        vbox.addWidget(self.field2)
        vbox.addWidget(create_button)
        self.show()

    def form_item(self):
        key = self.field1.text()
        value = self.field2.text()
        item = QtWidgets.QTreeWidgetItem([key, value])
        item.setFlags(item.flags() | QtCore.Qt.ItemIsEditable)
        self.created.emit(item)
        self.accept()


# TODO Insert new video
# TODO Insert skeleton link
class ContextMenu(QtWidgets.QMenu):
    def __init__(self, parent):
        super(ContextMenu, self).__init__(parent)
        self.parent = parent
        self.current_item = parent.tree.currentItem()
        insert = QAction("Insert", self)
        insert.triggered.connect(self.create_item)
        delete = QAction("Delete", self)
        delete.triggered.connect(parent.remove_items)
        self.addAction(insert)
        self.addAction(delete)
        if self.current_item.text(0) == "project_path":
            fix_path = QAction("Fix Path", self)
            fix_path.triggered.connect(self.fix_path)
            self.addAction(fix_path)

    def create_item(self):
        creator = ItemCreator(self)
        creator.created.connect(self.parent.insert)

    def fix_path(self):
        self.current_item.setText(1, os.path.split(self.parent.filename)[0])


class CustomDelegate(QtWidgets.QItemDelegate):
    # Hack to make the first column read-only, as we do not want users to touch it.
    # The cleaner solution would be to use a QTreeView and QAbstractItemModel,
    # but that is a lot of rework for little benefits.
    def createEditor(self, parent, option, index):
        if index.column() != 0:
            return super(CustomDelegate, self).createEditor(parent, option, index)
        return None


class DictViewer(QtWidgets.QWidget):
    def __init__(self, cfg, filename="", parent=None):
        super(DictViewer, self).__init__(parent)
        self.cfg = cfg
        self.filename = filename
        self.parent = parent
        self.tree = QtWidgets.QTreeWidget()
        self.tree.setItemDelegate(CustomDelegate())
        self.tree.setHeaderLabels(["Parameter", "Value"])
        self.tree.header().setSectionResizeMode(QtWidgets.QHeaderView.ResizeToContents)
        self.tree.setSelectionMode(QtWidgets.QAbstractItemView.ExtendedSelection)
        self.tree.setSelectionBehavior(QtWidgets.QAbstractItemView.SelectItems)
        self.tree.setAlternatingRowColors(True)
        self.tree.setSortingEnabled(False)
        self.tree.setHeaderHidden(False)
        self.tree.itemChanged.connect(self.edit_value)
        self.tree.setContextMenuPolicy(QtCore.Qt.CustomContextMenu)
        self.tree.customContextMenuRequested.connect(self.pop_context_menu)

        self.root = self.tree.invisibleRootItem()
        self.tree.addTopLevelItem(self.root)
        self.populate_tree(cfg, self.root)

        layout = QtWidgets.QHBoxLayout()
        layout.addWidget(self.tree)
        layout2 = QtWidgets.QVBoxLayout()
        layout2.addWidget(QtWidgets.QLabel(filename))
        layout2.addWidget(self.tree)
        self.setLayout(layout2)

    def pop_context_menu(self, point):
        index = self.tree.indexAt(point)
        if not index.isValid():
            return
        menu = ContextMenu(self)
        menu.exec_(self.tree.mapToGlobal(point))

    def get_position_in_parent(self, item):
        parent = item.parent() or self.root
        index = parent.indexOfChild(item)
        return index, parent

    def insert(self, item):
        current = self.tree.selectedItems()[0]
        ind, parent = self.get_position_in_parent(current)
        parent.insertChild(ind + 1, item)

        value = self.cast_to_right_type(item.text(1))
        if parent is self.root:
            self.set_value(self.cfg, [item.text(0)], value)
        else:
            keys, _ = self.walk_recursively_to_root(current)
            self.set_value(self.cfg, keys, value, ind + 1)

    def remove(self, item):
        ind, parent = self.get_position_in_parent(item)
        keys, value = self.walk_recursively_to_root(item)
        if item.parent() and item.childCount():  # Handle nested dict or list
            keys = [keys[0], value]
        success = self.remove_key(self.cfg, keys, ind)
        if success:
            parent.removeChild(item)

    def remove_items(self):
        for item in self.tree.selectedItems():
            self.remove(item)

    @staticmethod
    def cast_to_right_type(val):
        try:
            val = ast.literal_eval(val)
        except ValueError:
            # Leave untouched when it is already a string
            pass
        except SyntaxError:
            # Slashes also raise the error, but no need to print anything since it is then likely to be a path
            if os.path.sep not in val:
                print("Consider removing leading zeros or spaces in the string.")
        return val

    @staticmethod
    def walk_recursively_to_root(item):
        vals = []
        # Walk backwards across parents to get all keys
        while item is not None:
            for i in range(item.columnCount() - 1, -1, -1):
                vals.append(item.text(i))
            item = item.parent()
        *keys, value = vals[::-1]
        return keys, value

    @staticmethod
    def get_nested_key(cfg, keys):
        temp = cfg
        for key in keys[:-1]:
            try:
                temp = temp.setdefault(key, {})
            except AttributeError:  # Handle nested lists
                temp = temp[int(key)]
        return temp

    def edit_value(self, item):
        keys, value = self.walk_recursively_to_root(item)
        if (
            "crop" not in keys
        ):  # 'crop' should not be cast, otherwise it is understood as a list
            value = self.cast_to_right_type(value)
        self.set_value(self.cfg, keys, value)

    def set_value(self, cfg, keys, value, ind=None):
        temp = self.get_nested_key(cfg, keys)
        try:  # Work for a dict
            temp[keys[-1]] = value
        except TypeError:  # Needed to index a list
            if ind is None:  # Edit the list in place
                temp[self.tree.currentIndex().row()] = value
            else:
                temp.insert(ind, value)

    def remove_key(self, cfg, keys, ind=None):
        if not len(keys):  # Avoid deleting a parent list or dict
            return
        temp = self.get_nested_key(cfg, keys)
        try:
            temp.pop(keys[-1])
        except TypeError:
            if ind is None:
                ind = self.tree.currentIndex().row()
            temp.pop(ind)
        return True

    def populate_tree(self, data, tree_widget):
        if isinstance(data, dict):
            for key, val in data.items():
                self.add_row(key, val, tree_widget)
        elif isinstance(data, list):
            for i, val in enumerate(data):
                self.add_row(str(i), val, tree_widget)
        else:
            print("This should never be reached!")

    def add_row(self, key, val, tree_widget):
        if isinstance(val, dict) or isinstance(val, list):
            item = QtWidgets.QTreeWidgetItem([key])
            self.populate_tree(val, item)
        else:
            item = QtWidgets.QTreeWidgetItem([key, str(val)])
            item.setFlags(item.flags() | QtCore.Qt.ItemIsEditable)
        tree_widget.addChild(item)


class ConfigEditor(QtWidgets.QDialog):
    def __init__(self, config, parent=None):
        super(ConfigEditor, self).__init__(parent)
        self.config = config
        if (
            config.endswith("config.yaml")
            and not config.endswith("pytorch_config.yaml")
        ):
            self.read_func = auxiliaryfunctions.read_config
            self.write_func = auxiliaryfunctions.write_config
        else:
            self.read_func = auxiliaryfunctions.read_plainconfig
            self.write_func = auxiliaryfunctions.write_plainconfig
        self.cfg = self.read_func(config)
        self.parent = parent
        self.setWindowTitle("Configuration Editor")
        if parent is not None:
            self.setMinimumWidth(parent.screen_width // 2)
            self.setMinimumHeight(parent.screen_height // 2)
        self.viewer = DictViewer(self.cfg, config, self)

        self.save_button = QtWidgets.QPushButton("Save", self)
        self.save_button.setDefault(True)
        self.save_button.clicked.connect(self.accept)
        self.cancel_button = QtWidgets.QPushButton("Cancel", self)
        self.cancel_button.clicked.connect(self.close)

        vbox = QtWidgets.QVBoxLayout(self)
        vbox.addWidget(self.viewer)
        hbox = QtWidgets.QHBoxLayout()
        hbox.addWidget(self.save_button)
        hbox.addWidget(self.cancel_button)
        vbox.addLayout(hbox)

    def keyPressEvent(self, e):
        if e.key() == QtCore.Qt.Key_Escape:
            self.close()

    def accept(self):
        self.write_func(self.config, self.cfg)
        super(ConfigEditor, self).accept()


class FrameCropper(QtWidgets.QDialog):
    def __init__(self, video, parent=None):
        super(FrameCropper, self).__init__(parent)
        self.clip = VideoWriter(video)

        self.fig = Figure()
        self.ax = self.fig.add_subplot(111)
        self.ax_help = self.fig.add_axes([0.9, 0.2, 0.1, 0.1])
        self.ax_save = self.fig.add_axes([0.9, 0.1, 0.1, 0.1])
        self.crop_button = Button(self.ax_save, "Crop")
        self.crop_button.on_clicked(self.validate_crop)
        self.help_button = Button(self.ax_help, "Help")
        self.help_button.on_clicked(self.display_help)

        self.canvas = FigureCanvas(self.fig)
        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(self.canvas)
        self.setLayout(layout)

        self.bbox = [0, 0, 0, 0]

    def draw_bbox(self):
        frame = None
        # Read the video until a frame is successfully read
        while frame is None:
            frame = self.clip.read_frame()
        self.bbox[-2:] = frame.shape[1], frame.shape[0]
        self.ax.imshow(frame[:, :, ::-1])

        self.rs = RectangleSelector(
            self.ax,
            self.line_select_callback,
            minspanx=5,
            minspany=5,
            interactive=True,
            spancoords="pixels",
        )
        self.show()
        self.fig.canvas.start_event_loop(timeout=-1)
        return self.bbox

    def line_select_callback(self, eclick, erelease):
        self.bbox[:2] = int(eclick.xdata), int(eclick.ydata)  # x1, y1
        self.bbox[2:] = int(erelease.xdata), int(erelease.ydata)  # x2, y2

    def validate_crop(self, *args):
        self.fig.canvas.stop_event_loop()
        self.close()

    def display_help(self, *args):
        print(
            "1. Use left click to select the region of interest. A red box will be drawn around the selected region. \n\n2. Use the corner points to expand the box and center to move the box around the image. \n\n3. Click "
        )


class SkeletonBuilder(QtWidgets.QDialog):
    def __init__(self, config_path, parent=None):
        super(SkeletonBuilder, self).__init__(parent)
        self.config_path = config_path
        self.cfg = auxiliaryfunctions.read_config(config_path)
        # Find uncropped labeled data
        self.df = None
        found = False
        root = os.path.join(self.cfg["project_path"], "labeled-data")
        for dir_ in os.listdir(root):
            folder = os.path.join(root, dir_)
            if os.path.isdir(folder) and not any(
                folder.endswith(s) for s in ("cropped", "labeled")
            ):
                self.df = pd.read_hdf(
                    os.path.join(folder, f'CollectedData_{self.cfg["scorer"]}.h5')
                )
                row, col = self.pick_labeled_frame()
                if "individuals" in self.df.columns.names:
                    self.df = self.df.xs(col, axis=1, level="individuals")
                self.xy = self.df.loc[row].values.reshape((-1, 2))
                missing = np.flatnonzero(np.isnan(self.xy).all(axis=1))
                if not missing.size:
                    found = True
                    break
        if self.df is None:
            raise IOError("No labeled data were found.")

        self.bpts = self.df.columns.get_level_values("bodyparts").unique()
        if not found:
            warnings.warn(
                f"A fully labeled animal could not be found. "
                f"{', '.join(self.bpts[missing])} will need to be manually connected in the config.yaml."
            )
        self.tree = KDTree(self.xy)
        # Handle image previously annotated on a different platform
        if isinstance(row, str):
            sep = "/" if "/" in row else "\\"
            row = row.split(sep)
        self.image = io.imread(os.path.join(self.cfg["project_path"], *row))
        self.inds = set()
        self.segs = set()
        # Draw the skeleton if already existent
        if self.cfg["skeleton"]:
            for bone in self.cfg["skeleton"]:
                pair = np.flatnonzero(self.bpts.isin(bone))
                if len(pair) != 2:
                    continue
                pair_sorted = tuple(sorted(pair))
                self.inds.add(pair_sorted)
                self.segs.add(tuple(map(tuple, self.xy[pair_sorted, :])))

        self.fig = Figure()
        self.ax = self.fig.add_subplot(111)
        self.ax.axis("off")
        ax_clear = self.fig.add_axes([0.85, 0.55, 0.1, 0.1])
        ax_export = self.fig.add_axes([0.85, 0.45, 0.1, 0.1])
        self.clear_button = Button(ax_clear, "Clear")
        self.clear_button.on_clicked(self.clear)
        self.export_button = Button(ax_export, "Export")
        self.export_button.on_clicked(self.export)
        self.fig.canvas.mpl_connect("pick_event", self.on_pick)
        self.canvas = FigureCanvas(self.fig)
        layout = QtWidgets.QVBoxLayout(self)
        layout.addWidget(self.canvas)
        self.setLayout(layout)

        self.lines = LineCollection(
            self.segs, colors=mcolors.to_rgba(self.cfg["skeleton_color"])
        )
        self.lines.set_picker(True)
        self._show()

    def pick_labeled_frame(self):
        # Find the most 'complete' animal
        try:
            count = self.df.groupby(level="individuals", axis=1).count()
            if "single" in count:
                count.drop("single", axis=1, inplace=True)
        except KeyError:
            count = self.df.count(axis=1).to_frame()
        mask = count.where(count == count.values.max())
        kept = mask.stack().index.to_list()
        np.random.shuffle(kept)
        picked = kept.pop()
        row = picked[:-1]
        col = picked[-1]
        return row, col

    def _show(self):
        lo = np.nanmin(self.xy, axis=0)
        hi = np.nanmax(self.xy, axis=0)
        center = (hi + lo) / 2
        w, h = hi - lo
        ampl = 1.3
        w *= ampl
        h *= ampl
        self.ax.set_xlim(center[0] - w / 2, center[0] + w / 2)
        self.ax.set_ylim(center[1] - h / 2, center[1] + h / 2)
        self.ax.imshow(self.image)
        self.ax.scatter(*self.xy.T, s=self.cfg["dotsize"] ** 2)
        self.ax.add_collection(self.lines)
        self.ax.invert_yaxis()

        self.lasso = LassoSelector(self.ax, onselect=self.on_select)
        self.show()

    def clear(self, *args):
        self.inds.clear()
        self.segs.clear()
        self.lines.set_segments(self.segs)

    def export(self, *args):
        inds_flat = set(ind for pair in self.inds for ind in pair)
        unconnected = [i for i in range(len(self.xy)) if i not in inds_flat]
        if len(unconnected):
            warnings.warn(
                f"You didn't connect all the bodyparts (which is fine!). This is just a note to let you know."
            )
        self.cfg["skeleton"] = [tuple(self.bpts[list(pair)]) for pair in self.inds]
        auxiliaryfunctions.write_config(self.config_path, self.cfg)

    def on_pick(self, event):
        if event.mouseevent.button == 3:
            removed = event.artist.get_segments().pop(event.ind[0])
            self.segs.remove(tuple(map(tuple, removed)))
            self.inds.remove(tuple(self.tree.query(removed)[1]))

    def on_select(self, verts):
        self.path = Path(verts)
        self.verts = verts
        inds = self.tree.query_ball_point(verts, 5)
        inds_unique = []
        for lst in inds:
            if len(lst) and lst[0] not in inds_unique:
                inds_unique.append(lst[0])
        for pair in zip(inds_unique, inds_unique[1:]):
            pair_sorted = tuple(sorted(pair))
            self.inds.add(pair_sorted)
            self.segs.add(tuple(map(tuple, self.xy[pair_sorted, :])))
        self.lines.set_segments(self.segs)
        self.fig.canvas.draw_idle()


--- File: deeplabcut/gui/style.qss ---
 /* 
 Variables used
 --------------

 widgets height: 25px 

 */

QPushButton{
    height: 25px;
    min-width: 100px;
}

QSpinBox{
    height: 25px;
    width: 100px
}

QDoubleSpinBox{
    height: 25px;
    width: 100px
}

QComboBox{
    height: 25px;
    min-width: 100px;
}

QLineEdit{
    height: 25px;
}

--- File: deeplabcut/gui/launch_script.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0

"""
import sys
import os
import logging

import PySide6.QtWidgets as QtWidgets
import qdarkstyle
from deeplabcut.gui import BASE_DIR
from PySide6.QtCore import Qt
from PySide6.QtGui import QIcon, QPixmap


def launch_dlc():
    app = QtWidgets.QApplication(sys.argv)
    app.setWindowIcon(QIcon(os.path.join(BASE_DIR, "assets", "logo.png")))
    screen_size = app.screens()[0].size()
    pixmap = QPixmap(os.path.join(BASE_DIR, "assets", "welcome.png")).scaledToWidth(
        int(0.7 * screen_size.width()), Qt.SmoothTransformation
    )
    splash = QtWidgets.QSplashScreen(pixmap)
    splash.show()

    stylefile = os.path.join(BASE_DIR, "style.qss")
    with open(stylefile, "r") as f:
        app.setStyleSheet(f.read())

    dark_stylesheet = qdarkstyle.load_stylesheet_pyside2()
    app.setStyleSheet(dark_stylesheet)

    # Set up a logger and add an stdout handler.
    # A single logger can have many handlers:
    # https://docs.python.org/3/howto/logging.html#handler-basic
    # TODO Dump to log file instead
    # logger = logging.getLogger("GUI")
    # logger.setLevel(logging.DEBUG)
    # handler = logging.StreamHandler(stream=sys.stdout)
    # handler.setLevel(logging.DEBUG)
    # formatter = logging.Formatter(
    #     "%(asctime)s - %(name)s - %(levelname)s - %(message)s", "%Y-%m-%d %H:%M:%S"
    # )
    # handler.setFormatter(formatter)
    # logger.addHandler(handler)

    from deeplabcut.gui.window import MainWindow

    window = MainWindow(app)
    window.receiver.start()
    window.showMaximized()
    splash.finish(window)
    sys.exit(app.exec_())


if __name__ == "__main__":
    launch_dlc()


--- File: deeplabcut/gui/dlc_params.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
class DLCParams:
    VIDEOTYPES = [
        "",
        "avi",
        "mp4",
        "mkv",
        "mov",
    ]

    NNETS = [
        "dlcrnet_ms5",
        "resnet_50",
        "resnet_101",
        "resnet_152",
        "mobilenet_v2_1.0",
        "mobilenet_v2_0.75",
        "mobilenet_v2_0.5",
        "mobilenet_v2_0.35",
        "efficientnet-b0",
        "efficientnet-b3",
        "efficientnet-b6",
    ]

    FRAME_EXTRACTION_ALGORITHMS = ["kmeans", "uniform"]

    OUTLIER_EXTRACTION_ALGORITHMS = ["jump", "fitting", "uncertain", "manual"]

    TRACKERS = ["ellipse", "box", "skeleton"]


--- File: deeplabcut/gui/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from typing import Callable, Tuple

from PySide6 import QtCore
import re


class Worker(QtCore.QObject):
    finished = QtCore.Signal()

    def __init__(self, func):
        super().__init__()
        self.func = func

    def run(self):
        self.func()
        self.finished.emit()


class CaptureWorker(Worker):
    """A worker that captures outputs from methods that are run."""

    def __init__(self, func: Callable):
        super().__init__(func)
        self.outputs = None

    def run(self):
        self.outputs = self.func()
        self.finished.emit()


def move_to_separate_thread(func: Callable, capture_outputs: bool = False):
    thread = QtCore.QThread()
    if capture_outputs:
        worker = CaptureWorker(func)
    else:
        worker = Worker(func)

    worker.finished.connect(worker.deleteLater)
    worker.moveToThread(thread)
    thread.started.connect(worker.run)

    def stop_thread():
        thread.quit()
        thread.wait()

    worker.finished.connect(stop_thread)
    return worker, thread


def parse_version(version: str) -> Tuple[int, int, int]:
    """
    Parses a version string into a tuple of (major, minor, patch).
    """
    match = re.search(r"(\d+)\.(\d+)\.(\d+)", version)
    if match:
        return tuple(int(part) for part in match.groups())
    else:
        raise ValueError(f"Invalid version format: {version}")


def is_latest_deeplabcut_version():
    import json
    import urllib.request
    from deeplabcut import VERSION

    url = "https://pypi.org/pypi/deeplabcut/json"
    contents = urllib.request.urlopen(url).read()
    latest_version = json.loads(contents)["info"]["version"]
    return parse_version(VERSION) >= parse_version(latest_version), latest_version


--- File: deeplabcut/gui/components.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os

from PySide6 import QtWidgets
from PySide6.QtCore import Qt, Slot
from deeplabcut.gui.dlc_params import DLCParams
from deeplabcut.gui.widgets import ConfigEditor


def _create_label_widget(
    text: str,
    style: str = "",
    margins: tuple = (20, 10, 0, 10),
) -> QtWidgets.QLabel:
    label = QtWidgets.QLabel(text)
    label.setContentsMargins(*margins)
    label.setStyleSheet(style)

    return label


def _create_horizontal_layout(
    alignment=None, spacing: int = 20, margins: tuple = (20, 0, 0, 0)
) -> QtWidgets.QHBoxLayout():
    layout = QtWidgets.QHBoxLayout()
    layout.setAlignment(Qt.AlignLeft | Qt.AlignTop)
    layout.setSpacing(spacing)
    layout.setContentsMargins(*margins)

    return layout


def _create_vertical_layout(
    alignment=None, spacing: int = 20, margins: tuple = (20, 0, 0, 0)
) -> QtWidgets.QVBoxLayout():
    layout = QtWidgets.QVBoxLayout()
    layout.setAlignment(Qt.AlignLeft | Qt.AlignTop)
    layout.setSpacing(spacing)
    layout.setContentsMargins(*margins)

    return layout


def _create_grid_layout(
    alignment=None,
    spacing: int = 20,
    margins: tuple = None,
) -> QtWidgets.QGridLayout:
    layout = QtWidgets.QGridLayout()
    layout.setAlignment(Qt.AlignLeft | Qt.AlignTop)
    layout.setSpacing(spacing)
    if margins:
        layout.setContentsMargins(*margins)

    return layout


class BodypartListWidget(QtWidgets.QListWidget):
    def __init__(
        self,
        root: QtWidgets.QMainWindow,
        parent: QtWidgets.QWidget,
        # all_bodyparts: List
        # NOTE: Is there a case where a specific list should
        # have bodyparts other than the root? I don't think so.
    ):
        super(BodypartListWidget, self).__init__()

        self.root = root
        self.parent = parent
        self.selected_bodyparts = self.root.all_bodyparts

        self.setEnabled(False)
        self.setMaximumWidth(600)
        self.setMaximumHeight(500)
        self.hide()

        self.addItems(self.root.all_bodyparts)
        self.setSelectionMode(QtWidgets.QAbstractItemView.MultiSelection)

        self.itemSelectionChanged.connect(self.update_selected_bodyparts)

    def refresh(self):
        self.clear()
        self.addItems(self.root.all_bodyparts)
        self.update_selected_bodyparts()

    def update_selected_bodyparts(self):
        self.selected_bodyparts = [item.text() for item in self.selectedItems()]
        self.root.logger.info(f"Selected bodyparts:\n\t{self.selected_bodyparts}")


class VideoSelectionWidget(QtWidgets.QWidget):
    def __init__(self, root: QtWidgets.QMainWindow, parent: QtWidgets.QWidget):
        super(VideoSelectionWidget, self).__init__(parent)

        self.root = root
        self.parent = parent

        self._init_layout()

    def _init_layout(self):
        layout = _create_horizontal_layout()

        # Videotype selection
        self.videotype_widget = QtWidgets.QComboBox()
        self.videotype_widget.setMinimumWidth(100)
        self.videotype_widget.addItems(DLCParams.VIDEOTYPES)
        self.videotype_widget.setCurrentText(self.root.video_type)
        self.root.video_type_.connect(self.videotype_widget.setCurrentText)
        self.videotype_widget.currentTextChanged.connect(self.update_videotype)

        # Select videos
        self.select_video_button = QtWidgets.QPushButton("Select videos")
        self.select_video_button.setMaximumWidth(200)
        self.select_video_button.clicked.connect(self.update_videos)
        self.root.video_files_.connect(self._update_video_selection)

        # Number of selected videos text
        self.selected_videos_text = QtWidgets.QLabel(
            ""
        )  # updated when videos are selected

        # Clear video selection
        self.clear_videos = QtWidgets.QPushButton("Clear selection")
        self.clear_videos.clicked.connect(self.clear_selected_videos)

        layout.addWidget(self.videotype_widget)
        layout.addWidget(self.select_video_button)
        layout.addWidget(self.selected_videos_text)
        layout.addWidget(self.clear_videos, alignment=Qt.AlignRight)

        self.setLayout(layout)

    @property
    def files(self):
        return self.root.video_files

    def update_videotype(self, vtype):
        self.clear_selected_videos()
        self.root.video_type = vtype

    def _update_video_selection(self, videopaths):
        n_videos = len(self.root.video_files)
        if n_videos:
            self.selected_videos_text.setText(f"{n_videos} videos selected")
            self.select_video_button.setText("Add more videos")
        else:
            self.selected_videos_text.setText("")
            self.select_video_button.setText("Select videos")

    def update_videos(self):
        cwd = self.root.project_folder

        # Create a filter string with both lowercase and uppercase extensions

        video_types = [f"*.{ext.lower()}" for ext in DLCParams.VIDEOTYPES[1:]] + [
            f"*.{ext.upper()}" for ext in DLCParams.VIDEOTYPES[1:]
        ]
        video_files = f"Videos ({' '.join(video_types)})"

        filenames = QtWidgets.QFileDialog.getOpenFileNames(
            self,
            "Select video(s) to analyze",
            cwd,
            video_files,
        )

        if filenames[0]:
            # Qt returns a tuple (list of files, filetype)
            self.root.add_video_files([os.path.abspath(vid) for vid in filenames[0]])

    def clear_selected_videos(self):
        self.root.clear_video_files()
        self.root.logger.info(f"Cleared selected videos")


class SnapshotSelectionWidget(QtWidgets.QWidget):
    def __init__(
        self,
        root: QtWidgets.QMainWindow,
        parent: QtWidgets.QWidget,
        margins: tuple,
        select_button_text: str,
    ):
        super(SnapshotSelectionWidget, self).__init__(parent)

        self.root = root
        self.parent = parent

        self.selected_snapshot = None

        self._init_layout(margins, select_button_text)

    def _init_layout(self, margins, select_button_text):
        layout = _create_horizontal_layout(margins=margins)

        # Select videos
        self.select_snapshot_button = QtWidgets.QPushButton(select_button_text)
        self.select_snapshot_button.setMaximumWidth(200)
        self.select_snapshot_button.clicked.connect(self.select_snapshot)

        # Selected snapshot text
        self.selected_snapshot_text = QtWidgets.QLabel(
            ""
        )  # updated when snapshot is selected

        # Clear snapshot selection
        self.clear_snapshot_button = QtWidgets.QPushButton("Clear selection")
        self.clear_snapshot_button.clicked.connect(self.clear_selected_snapshot)
        self.clear_snapshot_button.hide()

        layout.addWidget(self.select_snapshot_button)
        layout.addWidget(self.selected_snapshot_text)
        layout.addWidget(self.clear_snapshot_button, alignment=Qt.AlignRight)

        self.setLayout(layout)

    def _update_selected_snapshot_display(self):
        if self.selected_snapshot is None:
            self.selected_snapshot_text.setText("")
            self.clear_snapshot_button.hide()
        else:
            self.selected_snapshot_text.setText(
                f"{os.path.basename(self.selected_snapshot)}"
            )
            self.clear_snapshot_button.show()

    def select_snapshot(self):
        # Create a filter string with both lowercase and uppercase extensions
        snapshot_types = ["*.pt", "*.PT"]
        snapshot_files = f"Snapshots ({' '.join(snapshot_types)})"

        directory_to_open = self.root.models_folder

        selected_snapshot, _ = QtWidgets.QFileDialog.getOpenFileName(
            self,
            "Select snapshot to start training from",
            directory_to_open,
            snapshot_files,
        )
        # When Canceling a file selection, Qt returns an empty string as selected file
        if selected_snapshot:
            self.selected_snapshot = os.path.abspath(selected_snapshot)

        self._update_selected_snapshot_display()

    def clear_selected_snapshot(self):
        self.selected_snapshot = None
        self._update_selected_snapshot_display()


class TrainingSetSpinBox(QtWidgets.QSpinBox):
    def __init__(self, root, parent):
        super(TrainingSetSpinBox, self).__init__(parent)

        self.root = root
        self.parent = parent

        self.setMaximum(100)
        self.setValue(self.root.trainingset_index)
        self.valueChanged.connect(self.root.update_trainingset)


class ShuffleSpinBox(QtWidgets.QSpinBox):
    def __init__(self, root, parent):
        super(ShuffleSpinBox, self).__init__(parent)

        self.root = root
        self.parent = parent

        self.setMaximum(10_000)
        self.setValue(self.root.shuffle_value)
        self.valueChanged.connect(self.root.update_shuffle)
        self.root.shuffle_change.connect(self.update_shuffle)

    @Slot(int)
    def update_shuffle(self, new_shuffle: int):
        if new_shuffle != self.value():
            self.setValue(new_shuffle)


class DefaultTab(QtWidgets.QWidget):
    def __init__(
        self,
        root: QtWidgets.QMainWindow,
        parent: QtWidgets.QWidget = None,
        h1_description: str = "",
    ):
        super(DefaultTab, self).__init__(parent)

        self.parent = parent
        self.root = root

        self.h1_description = h1_description

        self.main_layout = QtWidgets.QVBoxLayout()
        self.main_layout.setAlignment(Qt.AlignLeft | Qt.AlignTop)
        self.setLayout(self.main_layout)

        self._init_default_layout()

    def _init_default_layout(self):
        # Add tab header
        self.main_layout.addWidget(
            _create_label_widget(self.h1_description, "font:bold;", (10, 10, 0, 10))
        )

        # Add separating line
        self.separator = QtWidgets.QFrame()
        self.separator.setFrameShape(QtWidgets.QFrame.HLine)
        self.separator.setFrameShadow(QtWidgets.QFrame.Raised)
        self.separator.setLineWidth(0)
        self.separator.setMidLineWidth(1)
        policy = QtWidgets.QSizePolicy()
        policy.setVerticalPolicy(QtWidgets.QSizePolicy.Policy.Fixed)
        policy.setHorizontalPolicy(QtWidgets.QSizePolicy.Policy.MinimumExpanding)
        self.separator.setSizePolicy(policy)
        self.main_layout.addWidget(self.separator)


class EditYamlButton(QtWidgets.QPushButton):
    def __init__(
        self, button_label: str, filepath: str, parent: QtWidgets.QWidget = None
    ):
        super(EditYamlButton, self).__init__(button_label)
        self.filepath = filepath
        self.parent = parent

        self.clicked.connect(self.open_config)

    def open_config(self):
        editor = ConfigEditor(self.filepath)
        editor.show()


class BrowseFilesButton(QtWidgets.QPushButton):
    def __init__(
        self,
        button_label: str,
        filetype: str = None,
        cwd: str = None,
        single_file: bool = False,
        dialog_text: str = None,
        file_text: str = None,
        parent=None,
    ):
        super(BrowseFilesButton, self).__init__(button_label)
        self.filetype = filetype
        self.single_file_only = single_file
        self.cwd = cwd
        self.parent = parent

        self.dialog_text = dialog_text
        self.file_text = file_text

        self.files = set()

        self.clicked.connect(self.browse_files)

    def browse_files(self):
        # Look for any extension by default
        file_ext = "*"
        if self.filetype:
            # This works both with e.g. .avi and avi
            file_ext = self.filetype.split(".")[-1]

        # Choose multiple files by default
        open_file_func = QtWidgets.QFileDialog.getOpenFileNames
        if self.single_file_only:
            open_file_func = QtWidgets.QFileDialog.getOpenFileName

        cwd = ""
        if self.cwd:
            cwd = self.cwd

        dialog_text = f"Select .{file_ext} files"
        if self.dialog_text:
            dialog_text = self.dialog_text

        file_text = f"Files (*.{file_ext})"
        if self.file_text:
            file_text = self.file_text

        filepaths = open_file_func(self, dialog_text, cwd, file_text)

        if filepaths:
            self.files.update(filepaths[0])


--- File: deeplabcut/gui/tracklet_toolbox.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import matplotlib.transforms as mtransforms
import numpy as np
import pandas as pd
from threading import Event
from deeplabcut.gui.utils import move_to_separate_thread
from deeplabcut.refine_training_dataset.tracklets import TrackletManager
from deeplabcut.utils.auxfun_videos import VideoReader
from deeplabcut.utils.auxiliaryfunctions import attempt_to_make_folder
from matplotlib.path import Path
from matplotlib.widgets import Slider, LassoSelector, Button, CheckButtons, TextBox
from PySide6.QtWidgets import QMessageBox
from PySide6.QtCore import QMutex


class DraggablePoint:
    lock = None  # only one can be animated at a time

    def __init__(self, point, bodyParts, individual_names=None, likelihood=None):
        self.point = point
        self.bodyParts = bodyParts
        self.individual_names = individual_names
        self.likelihood = likelihood
        self.press = None
        self.background = None
        self.final_point = (0.0, 0.0)
        self.annot = self.point.axes.annotate(
            "",
            xy=(0, 0),
            xytext=(20, 20),
            textcoords="offset points",
            bbox=dict(boxstyle="round", fc="w"),
            arrowprops=dict(arrowstyle="->"),
        )
        self.annot.set_visible(False)
        self.coords = []

    def connect(self):
        "connect to all the events we need"

        self.cidpress = self.point.figure.canvas.mpl_connect(
            "button_press_event", self.on_press
        )
        self.cidrelease = self.point.figure.canvas.mpl_connect(
            "button_release_event", self.on_release
        )
        self.cidmotion = self.point.figure.canvas.mpl_connect(
            "motion_notify_event", self.on_motion
        )
        self.cidhover = self.point.figure.canvas.mpl_connect(
            "motion_notify_event", self.on_hover
        )

    def on_press(self, event):
        """
        Define the event for the button press!
        """
        if event.inaxes != self.point.axes:
            return
        if DraggablePoint.lock is not None:
            return
        contains, attrd = self.point.contains(event)
        if not contains:
            return
        if event.button == 1:
            """
            This button press corresponds to the left click
            """
            self.press = (self.point.center), event.xdata, event.ydata
            DraggablePoint.lock = self
            canvas = self.point.figure.canvas
            axes = self.point.axes
            self.point.set_animated(True)
            canvas.draw()
            self.background = canvas.copy_from_bbox(self.point.axes.bbox)
            axes.draw_artist(self.point)
            canvas.blit(axes.bbox)
        elif event.button == 2:
            """
            To remove a predicted label. Internally, the coordinates of the selected predicted label is replaced with nan. The user needs to middle click for the event. After right
            click the data point is removed from the plot.
            """
            message = f"Do you want to remove the label {self.bodyParts}?"
            if self.likelihood is not None:
                message += " You cannot undo this step!"
            msg = QMessageBox()
            msg.setWindowTitle("Warning!")
            msg.setText(message)
            msg.setStandardButtons(msg.Yes | msg.No)
            if msg.exec() == msg.Yes:
                self.delete_data()

    def delete_data(self):
        self.press = None
        DraggablePoint.lock = None
        self.point.set_animated(False)
        self.background = None
        self.final_point = (np.nan, np.nan, self.individual_names, self.bodyParts)
        self.point.center = (np.nan, np.nan)
        self.coords.append(self.final_point)
        self.point.figure.canvas.draw()

    def on_motion(self, event):
        """
        During the drag!
        """
        if DraggablePoint.lock is not self:
            return
        if event.inaxes != self.point.axes:
            return

        if event.button == 1:
            self.point.center, xpress, ypress = self.press
            dx = event.xdata - xpress
            dy = event.ydata - ypress
            self.point.center = (self.point.center[0] + dx, self.point.center[1] + dy)
            canvas = self.point.figure.canvas
            axes = self.point.axes
            # restore the background region
            canvas.restore_region(self.background)
            axes.draw_artist(self.point)
            canvas.blit(axes.bbox)

    def on_release(self, event):
        "on release we reset the press data"
        if DraggablePoint.lock is not self:
            return
        if event.button == 1:
            self.press = None
            DraggablePoint.lock = None
            self.point.set_animated(False)
            self.background = None
            self.point.figure.canvas.draw()
            self.final_point = (
                self.point.center[0],
                self.point.center[1],
                self.individual_names,
                self.bodyParts,
            )
            self.coords.append(self.final_point)

    def on_hover(self, event):
        """
        Annotate the labels and likelihood when the user hovers over the data points.
        """
        vis = self.annot.get_visible()

        if event.inaxes == self.point.axes:
            contains, attrd = self.point.contains(event)
            if contains:
                self.annot.xy = (self.point.center[0], self.point.center[1])
                text = str(self.bodyParts)
                if self.individual_names is not None:
                    text = f"{self.individual_names},{text}"
                if self.likelihood is not None:
                    text += f",p={self.likelihood:.2f}"
                self.annot.set_text(text)
                self.annot.get_bbox_patch().set_alpha(0.4)
                self.annot.set_visible(True)
                self.point.figure.canvas.draw_idle()
            else:
                if vis:
                    self.annot.set_visible(False)

    def disconnect(self):
        "disconnect all the stored connection ids"
        self.point.figure.canvas.mpl_disconnect(self.cidpress)
        self.point.figure.canvas.mpl_disconnect(self.cidrelease)
        self.point.figure.canvas.mpl_disconnect(self.cidmotion)
        self.point.figure.canvas.mpl_disconnect(self.cidhover)


class BackgroundPlayer:
    def __init__(self, viz):
        self.viz = viz
        self.can_run = Event()
        self.can_run.clear()
        self.running = True
        self.paused = True
        self.speed = "F"

    def run(self):
        while self.running:
            self.can_run.wait()
            i = self.viz.curr_frame
            if "F" in self.speed:
                if len(self.speed) == 1:
                    i += 1
                else:
                    i += 2 * (len(self.speed) - 1)
            elif "R" in self.speed:
                if len(self.speed) == 1:
                    i -= 1
                else:
                    i -= 2 * (len(self.speed) - 1)
            if i >= self.viz.manager.nframes:
                i = 0
            elif i < 0:
                i = self.viz.manager.nframes - 1
            self.viz.slider.set_val(i)

    def pause(self):
        self.can_run.clear()
        self.paused = True

    def resume(self):
        self.can_run.set()
        self.paused = False

    def toggle(self):
        if self.paused:
            self.resume()
        else:
            self.pause()

    def forward(self):
        speed = self.speed
        if "R" in speed:
            speed = "F"
        elif len(speed) < 5:
            speed += "F"
        elif len(speed) == 5:
            speed = "F"
        print(speed)
        self.speed = speed
        self.resume()

    def rewind(self):
        speed = self.speed
        if "F" in speed:
            speed = "R"
        elif len(speed) < 5:
            speed += "R"
        elif len(speed) == 5:
            speed = "R"
        print(speed)
        self.speed = speed
        self.resume()

    def terminate(self, *args):
        self.can_run.set()
        self.running = False


class PointSelector:
    def __init__(self, tracker, ax, collection, alpha, alpha_other=0.2):
        self.tracker = tracker
        self.ax = ax
        self.collection = collection
        self.fc = collection.get_facecolors()
        self.alpha = alpha
        self.alpha_other = alpha_other
        self.lasso = LassoSelector(ax, onselect=self.on_select)
        self.is_connected = True
        self.toggle()

    def on_select(self, verts):
        path = Path(verts)
        xy = self.collection.get_offsets()
        self.tracker.picked = list(np.nonzero(path.contains_points(xy))[0])
        self.fc[:, -1] = self.alpha_other
        self.fc[self.tracker.picked, -1] = self.alpha
        self.collection.set_color(self.fc)
        self.tracker.display_traces()
        self.tracker.fig.canvas.draw_idle()  # Force wx backend to redraw the figure

    def toggle(self, *args):
        if self.is_connected:
            self.disconnect()
        else:
            self.reconnect()

    def disconnect(self):
        self.lasso.disconnect_events()
        self.is_connected = False
        self.tracker.picked = []
        self.tracker.picked_pair = []
        self.fc[:, -1] = self.alpha
        self.collection.set_color(self.fc)
        self.tracker.display_traces(only_picked=False)
        self.tracker.fig.canvas.draw_idle()  # Force wx backend to redraw the figure

    def reconnect(self):
        self.lasso.connect_default_events()
        self.is_connected = True


class TrackletVisualizer:
    def __init__(self, manager, videoname, trail_len=50):
        self.manager = manager
        self.cmap = plt.cm.get_cmap(
            manager.cfg["colormap"], len(set(manager.tracklet2id))
        )
        self.videoname = videoname
        self.video = VideoReader(videoname)
        self.nframes = len(self.video)
        # Take into consideration imprecise OpenCV estimation of total number of frames
        if abs(self.nframes - manager.nframes) >= 0.05 * manager.nframes:
            print(
                "Video duration and data length do not match. Continuing nonetheless..."
            )
        self.trail_len = trail_len
        self.help_text = ""
        self.draggable = False
        self._curr_frame = 0
        self.curr_frame = 0

        self.picked = []
        self.picked_pair = []
        self.cuts = []

        self.mutex = QMutex()
        self.player = BackgroundPlayer(self)
        self.worker, self.thread_player = move_to_separate_thread(self.player.run)
        self.thread_player.start()

        self.dps = []

        self.swap_id1 = None
        self.swap_id2 = None

    def _prepare_canvas(self, manager, fig):
        params = {
            "keymap.save": "s",
            "keymap.back": "left",
            "keymap.forward": "right",
            "keymap.yscale": "l",
        }
        for k, v in params.items():
            if v in plt.rcParams[k]:
                plt.rcParams[k].remove(v)

        self.dotsize = manager.cfg["dotsize"]
        self.alpha = manager.cfg["alphavalue"]

        if fig is None:
            self.fig = plt.figure(figsize=(13, 8))
        else:
            self.fig = fig
        gs = self.fig.add_gridspec(2, 2)
        self.ax1 = self.fig.add_subplot(gs[:, 0])
        self.ax2 = self.fig.add_subplot(gs[0, 1])
        self.ax3 = self.fig.add_subplot(gs[1, 1], sharex=self.ax2)
        plt.subplots_adjust(bottom=0.2)
        for ax in self.ax1, self.ax2, self.ax3:
            ax.axis("off")

        self.colors = self.cmap(manager.tracklet2id)
        self.colors[:, -1] = self.alpha

        img = self.video.read_frame()
        self.im = self.ax1.imshow(img)
        self.scat = self.ax1.scatter([], [], s=self.dotsize**2, picker=True)
        self.scat.set_offsets(manager.xy[:, 0])
        self.scat.set_color(self.colors)
        self.trails = sum(
            [self.ax1.plot([], [], "-", lw=2, c=c) for c in self.colors], []
        )
        self.lines_x = sum(
            [self.ax2.plot([], [], "-", lw=1, c=c, pickradius=5) for c in self.colors],
            [],
        )
        self.lines_y = sum(
            [self.ax3.plot([], [], "-", lw=1, c=c, pickradius=5) for c in self.colors],
            [],
        )
        self.vline_x = self.ax2.axvline(0, 0, 1, c="k", ls=":")
        self.vline_y = self.ax3.axvline(0, 0, 1, c="k", ls=":")

        custom_lines = [
            plt.Line2D([0], [0], color=self.cmap(i), lw=4)
            for i in range(len(manager.individuals))
        ]
        self.leg = self.fig.legend(
            custom_lines,
            manager.individuals,
            frameon=False,
            fancybox=None,
            ncol=len(manager.individuals),
            fontsize="small",
            bbox_to_anchor=(0, 0.9, 1, 0.1),
            loc="center",
        )
        for line in self.leg.get_lines():
            line.set_picker(5)

        self.ax_slider = self.fig.add_axes([0.1, 0.1, 0.5, 0.03], facecolor="lightgray")
        self.ax_slider2 = self.fig.add_axes(
            [0.1, 0.05, 0.3, 0.03], facecolor="darkorange"
        )
        self.slider = Slider(
            self.ax_slider,
            "# Frame",
            self.curr_frame,
            manager.nframes - 1,
            valinit=0,
            valstep=1,
            valfmt="%i",
        )
        self.slider.on_changed(self.on_change)
        self.slider2 = Slider(
            self.ax_slider2,
            "Marker size",
            1,
            30,
            valinit=self.dotsize,
            valstep=1,
            valfmt="%i",
        )
        self.slider2.on_changed(self.update_dotsize)
        self.ax_drag = self.fig.add_axes([0.65, 0.1, 0.05, 0.03])
        self.ax_lasso = self.fig.add_axes([0.7, 0.1, 0.05, 0.03])
        self.ax_flag = self.fig.add_axes([0.75, 0.1, 0.05, 0.03])
        self.ax_save = self.fig.add_axes([0.80, 0.1, 0.05, 0.03])
        self.ax_help = self.fig.add_axes([0.85, 0.1, 0.05, 0.03])
        self.ax_swap = self.fig.add_axes([0.90, 0.1, 0.05, 0.03])  # New button

        self.save_button = Button(self.ax_save, "Save", color="darkorange")
        self.save_button.on_clicked(self.save)
        self.help_button = Button(self.ax_help, "Help")
        self.help_button.on_clicked(self.display_help)
        self.swap_button = Button(self.ax_swap, "Swap")  # New button
        self.swap_button.on_clicked(self.swap_tracklets)  # Placeholder action

        self.drag_toggle = CheckButtons(self.ax_drag, ["Drag"])
        self.drag_toggle.on_clicked(self.toggle_draggable_points)
        self.flag_button = Button(self.ax_flag, "Flag")
        self.flag_button.on_clicked(self.flag_frame)

        self.fig.canvas.mpl_connect("pick_event", self.on_pick)
        self.fig.canvas.mpl_connect("key_press_event", self.on_press)
        self.fig.canvas.mpl_connect("button_press_event", self.on_click)
        self.fig.canvas.mpl_connect("close_event", self.player.terminate)

        self.selector = PointSelector(self, self.ax1, self.scat, self.alpha)
        self.lasso_toggle = CheckButtons(self.ax_lasso, ["Lasso"])
        self.lasso_toggle.on_clicked(self.selector.toggle)
        self.display_traces(only_picked=False)
        self.ax1_background = self.fig.canvas.copy_from_bbox(self.ax1.bbox)
        self.fig.show()

        # Create dropdowns for selecting tracklets to swap, placing them near the swap button
        self.ax_dropdown1 = self.fig.add_axes([0.9, 0.15, 0.05, 0.03])
        self.ax_dropdown2 = self.fig.add_axes([0.9, 0.20, 0.05, 0.03])
        self.textbox1 = TextBox(self.ax_dropdown1, "ID 1")
        self.textbox2 = TextBox(self.ax_dropdown2, "ID 2")
        self.textbox1.on_submit(self.set_swap_id1)
        self.textbox2.on_submit(self.set_swap_id2)

    def show(self, fig=None):
        self._prepare_canvas(self.manager, fig)

    def swap_tracklets(self, event):
        if self.swap_id1 is not None and self.swap_id2 is not None:

            # Get tracklet indices for each individual
            inds1 = [
                k
                for k in range(len(self.manager.tracklet2id))
                if self.manager.tracklet2id[k] == self.swap_id1
            ]
            inds2 = [
                k
                for k in range(len(self.manager.tracklet2id))
                if self.manager.tracklet2id[k] == self.swap_id2
            ]

            print(f"Swapping tracklets {self.swap_id1} and {self.swap_id2}")

            # Frames to swap
            frames = []
            if len(self.cuts) == 2:
                frames = list(range(min(self.cuts), max(self.cuts) + 1))
            elif len(self.cuts) == 1:
                frames = [self.cuts[0]]
            else:
                frames = list(range(self.curr_frame, self.manager.nframes))

            # Swap the tracklets
            for i in range(min(len(inds1), len(inds2))):
                self.manager.swap_tracklets(inds1[i], inds2[i], frames)
                self.display_traces()
                self.slider.set_val(self.curr_frame)

    def set_swap_id1(self, val):
        # check that the input is a valid from the list of individuals
        if int(val) in self.manager.tracklet2id:
            self.swap_id1 = int(val)
            print("ID 1 set.")
        else:
            print(
                f"Invalid ID. Please select a valid ID from the list of individuals: {set(self.manager.tracklet2id)}"
            )
            self.swap_id1 = None

    def set_swap_id2(self, val):
        # check that the input is a valid from the list of individuals
        if int(val) in self.manager.tracklet2id:
            self.swap_id2 = int(val)
            print("ID 2 set.")
        else:
            print(
                f"Invalid ID. Please select a valid ID from the list of individuals: {set(self.manager.tracklet2id)}"
            )
            self.swap_id2 = None

    def terminate(self, event):
        plt.close(self.fig)
        self.player.terminate()

    def fill_shaded_areas(self):
        self.clean_collections()
        if self.picked_pair:
            mask = self.manager.get_nonoverlapping_segments(*self.picked_pair)
            for ax in self.ax2, self.ax3:
                ax.fill_between(
                    self.manager.times,
                    *ax.dataLim.intervaly,
                    mask,
                    facecolor="darkgray",
                    alpha=0.2,
                )
            trans = mtransforms.blended_transform_factory(
                self.ax_slider.transData, self.ax_slider.transAxes
            )
            self.ax_slider.vlines(
                np.flatnonzero(mask), 0, 0.5, color="darkorange", transform=trans
            )

    def toggle_draggable_points(self, *args):
        self.draggable = not self.draggable
        if self.draggable:
            self._curr_frame = self.curr_frame
            self.scat.set_offsets(np.empty((0, 2)))
            self.add_draggable_points()
        else:
            self.save_coords()
            self.clean_points()
            self.display_points(self._curr_frame)
        self.fig.canvas.draw_idle()

    def add_point(self, center, animal, bodypart, **kwargs):
        circle = patches.Circle(center, **kwargs)
        self.ax1.add_patch(circle)
        dp = DraggablePoint(circle, bodypart, animal)
        dp.connect()
        self.dps.append(dp)

    def clean_points(self):
        for dp in self.dps:
            dp.annot.set_visible(False)
            dp.disconnect()
        self.dps = []
        for patch in self.ax1.patches[::-1]:
            patch.remove()

    def add_draggable_points(self):
        self.clean_points()
        xy, _, inds = self.manager.get_non_nan_elements(self.curr_frame)
        for i, (animal, bodypart) in enumerate(self.manager._label_pairs):
            if i in inds:
                coords = xy[inds == i].squeeze()
                self.add_point(
                    coords,
                    animal,
                    bodypart,
                    radius=self.dotsize,
                    fc=self.colors[i],
                    alpha=self.alpha,
                )

    def save_coords(self):
        coords, nonempty, inds = self.manager.get_non_nan_elements(self._curr_frame)
        if not inds.size:
            return
        prob = self.manager.prob[:, self._curr_frame]
        for dp in self.dps:
            label = dp.individual_names, dp.bodyParts
            ind = self.manager._label_pairs.index(label)
            nrow = np.flatnonzero(inds == ind)
            if not nrow.size:
                return
            nrow = nrow[0]
            if not np.array_equal(
                coords[nrow], dp.point.center
            ):  # Keypoint has been displaced
                coords[nrow] = dp.point.center
                prob[ind] = 1
        self.manager.xy[nonempty, self._curr_frame] = coords

    def flag_frame(self, *args):
        self.cuts.append(self.curr_frame)
        self.ax_slider.axvline(self.curr_frame, color="r")
        if len(self.cuts) == 2:
            self.cuts.sort()
            mask = np.zeros_like(self.manager.times, dtype=bool)
            mask[self.cuts[0] : self.cuts[1] + 1] = True
            for ax in self.ax2, self.ax3:
                ax.fill_between(
                    self.manager.times,
                    *ax.dataLim.intervaly,
                    mask,
                    facecolor="darkgray",
                    alpha=0.2,
                )
            trans = mtransforms.blended_transform_factory(
                self.ax_slider.transData, self.ax_slider.transAxes
            )
            self.ax_slider.vlines(
                np.flatnonzero(mask), 0, 0.5, color="darkorange", transform=trans
            )
        self.fig.canvas.draw_idle()

    def on_scroll(self, event):
        cur_xlim = self.ax1.get_xlim()
        cur_ylim = self.ax1.get_ylim()
        xdata = event.xdata
        ydata = event.ydata
        if event.button == "up":
            scale_factor = 0.5
        elif event.button == "down":
            scale_factor = 2
        else:  # This should never happen anyway
            scale_factor = 1

        self.ax1.set_xlim(
            [
                xdata - (xdata - cur_xlim[0]) / scale_factor,
                xdata + (cur_xlim[1] - xdata) / scale_factor,
            ]
        )
        self.ax1.set_ylim(
            [
                ydata - (ydata - cur_ylim[0]) / scale_factor,
                ydata + (cur_ylim[1] - ydata) / scale_factor,
            ]
        )
        self.fig.canvas.draw()

    def on_press(self, event):
        if event.key == "n" or event.key == "right":
            self.move_forward()
        elif event.key == "b" or event.key == "left":
            self.move_backward()
        elif event.key == "s":
            self.swap()
        elif event.key == "i":
            self.invert()
        elif event.key == "x":
            self.flag_frame()
            if len(self.cuts) > 1:
                self.cuts.sort()
                if self.picked_pair:
                    self.manager.tracklet_swaps[self.picked_pair][self.cuts] = (
                        ~self.manager.tracklet_swaps[self.picked_pair][self.cuts]
                    )
                    self.fill_shaded_areas()
                    self.cuts = []
                    for line in self.ax_slider.lines:
                        line.remove()
        elif event.key == "backspace":
            if not self.dps:  # Last flag deletion
                try:
                    self.cuts.pop()
                    self.ax_slider.lines.pop()
                    if not len(self.cuts) == 2:
                        self.clean_collections()
                except IndexError:
                    pass
            else:  # Smart point removal
                i = np.nanargmin(
                    [
                        self.calc_distance(*dp.point.center, event.xdata, event.ydata)
                        for dp in self.dps
                    ]
                )
                closest_dp = self.dps[i]
                label = closest_dp.individual_names, closest_dp.bodyParts
                closest_dp.disconnect()
                closest_dp.point.remove()
                self.dps.remove(closest_dp)
                ind = self.manager._label_pairs.index(label)
                self.manager.xy[ind, self._curr_frame] = np.nan
                self.manager.prob[ind, self._curr_frame] = np.nan
            self.fig.canvas.draw_idle()
        elif event.key == "l":
            self.lasso_toggle.set_active(not self.lasso_toggle.get_active)
        elif event.key == "d":
            self.drag_toggle.set_active(not self.drag_toggle.get_active)
        elif event.key == "alt+right":
            self.player.forward()
        elif event.key == "alt+left":
            self.player.rewind()
        elif event.key == " " or event.key == "tab":
            self.player.toggle()

    def move_forward(self):
        if self.curr_frame < self.manager.nframes - 1:
            self.curr_frame += 1
            self.slider.set_val(self.curr_frame)

    def move_backward(self):
        if self.curr_frame > 0:
            self.curr_frame -= 1
            self.slider.set_val(self.curr_frame)

    def swap(self):
        if self.picked_pair:
            swap_inds = self.manager.get_swap_indices(*self.picked_pair)
            inds = np.insert(
                swap_inds, [0, len(swap_inds)], [0, self.manager.nframes - 1]
            )
            if len(inds):
                ind = np.argmax(inds > self.curr_frame)
                self.manager.swap_tracklets(
                    *self.picked_pair, range(inds[ind - 1], inds[ind] + 1)
                )
                self.display_traces()
                self.slider.set_val(self.curr_frame)

    def invert(self):
        if not self.picked_pair and len(self.picked) == 2:
            self.picked_pair = self.picked
        if self.picked_pair:
            self.manager.swap_tracklets(*self.picked_pair, [self.curr_frame])
            self.display_traces()
            self.slider.set_val(self.curr_frame)

    def on_pick(self, event):
        artist = event.artist
        if artist.axes == self.ax1:
            self.picked = list(event.ind)
        elif artist.axes == self.ax2:
            if isinstance(artist, plt.Line2D):
                self.picked = [self.lines_x.index(artist)]
        elif artist.axes == self.ax3:
            if isinstance(artist, plt.Line2D):
                self.picked = [self.lines_y.index(artist)]
        else:  # Click on the legend lines
            if self.picked:
                num_individual = self.leg.get_lines().index(artist)
                nrow = self.manager.tracklet2id.index(num_individual)
                inds = [
                    nrow + self.manager.to_num_bodypart(pick) for pick in self.picked
                ]
                xy = self.manager.xy[self.picked]
                p = self.manager.prob[self.picked]
                mask = np.zeros(xy.shape[1], dtype=bool)
                if len(self.cuts) > 1:
                    mask[self.cuts[-2] : self.cuts[-1] + 1] = True
                    self.cuts = []
                    for line in self.ax_slider.lines:
                        line.remove()
                    self.clean_collections()
                else:
                    return
                sl_inds = np.ix_(inds, mask)
                sl_picks = np.ix_(self.picked, mask)
                old_xy = self.manager.xy[sl_inds].copy()
                old_prob = self.manager.prob[sl_inds].copy()
                self.manager.xy[sl_inds] = xy[:, mask]
                self.manager.prob[sl_inds] = p[:, mask]
                self.manager.xy[sl_picks] = old_xy
                self.manager.prob[sl_picks] = old_prob
        self.picked_pair = []
        if len(self.picked) == 1:
            for pair in self.manager.swapping_pairs:
                if self.picked[0] in pair:
                    self.picked_pair = pair
                    break
        self.clean_collections()
        self.display_traces()
        if self.picked_pair:
            self.fill_shaded_areas()
        self.slider.set_val(self.curr_frame)

    def on_click(self, event):
        if (
            event.inaxes in (self.ax2, self.ax3)
            and event.button == 1
            and not any(line.contains(event)[0] for line in self.lines_x + self.lines_y)
        ):
            x = max(0, min(event.xdata, self.manager.nframes - 1))
            self.update_vlines(x)
            self.slider.set_val(x)
        elif event.inaxes == self.ax1 and not self.scat.contains(event)[0]:
            self.display_traces(only_picked=False)
            self.clean_collections()

    def clean_collections(self):
        for coll in (
            self.ax2.collections + self.ax3.collections + self.ax_slider.collections
        ):
            coll.remove()

    def display_points(self, val):
        data = self.manager.xy[:, val]
        self.scat.set_offsets(data)

    def display_trails(self, val):
        sl = slice(val - self.trail_len // 2, val + self.trail_len // 2)
        for n, trail in enumerate(self.trails):
            if n in self.picked:
                xy = self.manager.xy[n, sl]
                trail.set_data(*xy.T)
            else:
                trail.set_data([], [])

    def display_traces(self, only_picked=True):
        if only_picked:
            inds = self.picked + list(self.picked_pair)
        else:
            inds = self.manager.swapping_bodyparts
        for n, (line_x, line_y) in enumerate(zip(self.lines_x, self.lines_y)):
            if n in inds:
                line_x.set_data(self.manager.times, self.manager.xy[n, :, 0])
                line_y.set_data(self.manager.times, self.manager.xy[n, :, 1])
            else:
                line_x.set_data([], [])
                line_y.set_data([], [])
        for ax in self.ax2, self.ax3:
            ax.relim()
            ax.autoscale_view()

    def display_help(self, event):
        if not self.help_text:
            self.help_text = """
            Key D: activate "drag" so you can adjust bodyparts in that particular frame
            Key I: invert the position of a pair of bodyparts
            Key L: toggle the lasso selector
            Key S: swap two tracklets
            Key X: cut swapping tracklets
            Left/Right arrow OR Key B/Key N: navigate through the video (back/next)
            Tab or SPACE: play/pause the video
            Alt+Right/Left: fast forward/rewind - toggles through 5 speed levels
            Backspace: deletes last flag (if set) or deletes point
            Key P: toggles on pan/zoom tool - left button and drag to pan, right button and drag to zoom
            """
            self.text = self.fig.text(
                0.5,
                0.5,
                self.help_text,
                horizontalalignment="center",
                verticalalignment="center",
                fontsize=12,
                color="red",
            )
        else:
            self.help_text = ""
            self.text.remove()

    def update_vlines(self, val):
        self.vline_x.set_xdata([val, val])
        self.vline_y.set_xdata([val, val])

    def on_change(self, val):
        self.mutex.lock()  # Make video frame retrieval thread-safe
        self.curr_frame = int(val)
        self.video.set_to_frame(self.curr_frame)
        img = self.video.read_frame()
        self.mutex.unlock()
        if img is not None:
            # Automatically disable the draggable points
            if self.draggable:
                self.drag_toggle.set_active(False)

            self.im.set_array(img)
            self.display_points(self.curr_frame)
            self.display_trails(self.curr_frame)
            self.update_vlines(self.curr_frame)

    def update_dotsize(self, val):
        self.dotsize = val
        self.scat.set_sizes([self.dotsize**2])

    @staticmethod
    def calc_distance(x1, y1, x2, y2):
        return np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)

    def save(self, *args):
        self.save_coords()
        self.manager.save()

    def export_to_training_data(self, pcutoff=0.1):
        import os
        from skimage import io

        inds = self.manager.find_edited_frames()
        if not len(inds):
            print("No frames have been manually edited.")
            return

        # Save additional frames to the labeled-data directory
        strwidth = int(np.ceil(np.log10(self.nframes)))
        tmpfolder = os.path.join(
            self.manager.cfg["project_path"], "labeled-data", self.video.name
        )
        if os.path.isdir(tmpfolder):
            print(
                "Frames from video",
                self.video.name,
                " already extracted (more will be added)!",
            )
        else:
            attempt_to_make_folder(tmpfolder)
        index = []
        for ind in inds:
            imagename = os.path.join(
                tmpfolder, "img" + str(ind).zfill(strwidth) + ".png"
            )
            index.append(
                tuple(
                    (os.path.join(*imagename.rsplit(os.path.sep, 3)[-3:])).split("\\")
                )
            )
            if not os.path.isfile(imagename):
                self.video.set_to_frame(ind)
                frame = self.video.read_frame()
                if frame is None:
                    print("Frame could not be read. Skipping...")
                    continue
                frame = frame.astype(np.ubyte)
                if self.manager.cfg["cropping"]:
                    x1, x2, y1, y2 = [
                        int(self.manager.cfg[key]) for key in ("x1", "x2", "y1", "y2")
                    ]
                    frame = frame[y1:y2, x1:x2]
                io.imsave(imagename, frame)

        # Store the newly-refined data
        data = self.manager.format_data()
        df = data.iloc[inds]

        # Uncertain keypoints are ignored
        def filter_low_prob(cols, prob):
            mask = cols.iloc[:, 2] < prob
            cols.loc[mask] = np.nan
            return cols

        df = df.groupby(level="bodyparts", axis=1, group_keys=False).apply(
            filter_low_prob, prob=pcutoff
        )
        df.index = pd.MultiIndex.from_tuples(index)

        machinefile = os.path.join(
            tmpfolder, "machinelabels-iter" + str(self.manager.cfg["iteration"]) + ".h5"
        )
        if os.path.isfile(machinefile):
            df_old = pd.read_hdf(machinefile)
            df_joint = pd.concat([df_old, df])
            df_joint = df_joint[~df_joint.index.duplicated(keep="first")]
            df_joint.to_hdf(machinefile, key="df_with_missing", mode="w")
            df_joint.to_csv(os.path.join(tmpfolder, "machinelabels.csv"))
        else:
            df.to_hdf(machinefile, key="df_with_missing", mode="w")
            df.to_csv(os.path.join(tmpfolder, "machinelabels.csv"))

        # Merge with the already existing annotated data
        df.columns = df.columns.set_levels([self.manager.cfg["scorer"]], level="scorer")
        df.drop("likelihood", level="coords", axis=1, inplace=True)
        output_path = os.path.join(
            tmpfolder, f'CollectedData_{self.manager.cfg["scorer"]}.h5'
        )
        if os.path.isfile(output_path):
            print(
                "A training dataset file is already found for this video. The refined machine labels are merged to this data!"
            )
            df_orig = pd.read_hdf(output_path)
            df_joint = pd.concat([df, df_orig])
            # Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference]
            df_joint = df_joint[~df_joint.index.duplicated(keep="first")]
            df_joint.sort_index(inplace=True)
            df_joint.to_hdf(output_path, key="df_with_missing", mode="w")
            df_joint.to_csv(output_path.replace("h5", "csv"))
        else:
            df.sort_index(inplace=True)
            df.to_hdf(output_path, key="df_with_missing", mode="w")
            df.to_csv(output_path.replace("h5", "csv"))


def refine_tracklets(
    config,
    pickle_or_h5_file,
    video,
    min_swap_len=2,
    min_tracklet_len=2,
    max_gap=2,
    trail_len=0,
):
    """
    Refine tracklets stored either in pickle or h5 format.
    The procedure is done in two stages:
    (i) freshly-converted detections are read by the TrackletManager,
    which automatically attempts to optimize tracklet continuity by
    assigning higher priority to long tracks while maximizing
    keypoint likelihood;
    (ii) loaded tracklets are displayed into the TrackletVisualizer
    for manual editing. Individual labels can be dragged around
    like in the labeling toolbox; several of them can also be simultaneously
    selected using the Lasso tool in order to re-assign multiple tracks
    to another identity at once.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    pickle_or_h5_file: str
        Full path of either the pickle file obtained after calling
        deeplabcut.convert_detections2tracklets, or the h5 file written after
        refining the tracklets a first time. Note that refined tracklets are
        always stored in the h5 format.

    video: str
        Full path of the corresponding video.
        If the video duration and the total length of the tracklets disagree
        by more than 5%, a message is printed indicating that the selected
        video may not be the right one.

    min_swap_len : float, optional (default=2)
        Minimum swap length.
        Set to 2 by default. Retained swaps appear in the right panel in
        shaded regions.

    min_tracklet_len : float, optional (default=2)
        Minimum tracklet length.
        By default, tracklets shorter than 2 frames are discarded,
        leaving missing data instead. If set to 0, all tracklets are kept.

    max_gap : int, optional (default=2).
        Maximal gap size (in number of frames) of missing data to be filled.
        The procedure fits a cubic spline over all individual trajectories,
        and fills all gaps smaller than or equal to 2 frames by default.

    trail_len : int, optional (default=0)
        Number of trailing points. None by default, to accelerate visualization.
    """
    manager = TrackletManager(config, min_swap_len, min_tracklet_len, max_gap)
    if pickle_or_h5_file.endswith("pickle"):
        manager.load_tracklets_from_pickle(pickle_or_h5_file)
    else:
        manager.load_tracklets_from_hdf(pickle_or_h5_file)
    manager.find_swapping_bodypart_pairs()
    viz = TrackletVisualizer(manager, video, trail_len)
    viz.show()
    return manager, viz


--- File: deeplabcut/gui/tabs/unsupervised_id_tracking.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from functools import partial
from PySide6 import QtWidgets
from PySide6.QtCore import Qt

from deeplabcut.gui.components import (
    DefaultTab,
    ShuffleSpinBox,
    VideoSelectionWidget,
    _create_grid_layout,
    _create_label_widget,
)
from deeplabcut.gui.utils import move_to_separate_thread

import deeplabcut


class UnsupervizedIdTracking(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(UnsupervizedIdTracking, self).__init__(root, parent, h1_description)

        self._set_page()

    @property
    def files(self):
        return self.root.video_files

    def _set_page(self):
        self.main_layout.addWidget(_create_label_widget("Video Selection", "font:bold"))
        self.video_selection_widget = VideoSelectionWidget(self.root, self)
        self.main_layout.addWidget(self.video_selection_widget)

        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self.layout_attributes = _create_grid_layout(margins=(20, 0, 0, 0))
        self._generate_layout_attributes(self.layout_attributes)
        self.main_layout.addLayout(self.layout_attributes)

        self.run_transformer_button = QtWidgets.QPushButton("Run transformer")
        self.run_transformer_button.clicked.connect(self.run_transformer)

        self.main_layout.addWidget(self.run_transformer_button, alignment=Qt.AlignRight)

        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        label = QtWidgets.QLabel(deeplabcut.transformer_reID.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _generate_layout_attributes(self, layout):
        # Shuffle
        shuffle_label = QtWidgets.QLabel("Shuffle")
        self.shuffle = ShuffleSpinBox(root=self.root, parent=self)

        # Tracker Type
        trackingtype_label = QtWidgets.QLabel("Tracking method")
        self.tracker_type_widget = QtWidgets.QComboBox()
        self.tracker_type_widget.addItems(["ellipse", "box"])
        self.tracker_type_widget.currentTextChanged.connect(self.log_tracker_type)

        # Num animals
        num_animals_label = QtWidgets.QLabel("Number of animals in videos")
        self.num_animals_in_videos = QtWidgets.QSpinBox()
        self.num_animals_in_videos.setValue(len(self.root.all_individuals))
        self.num_animals_in_videos.setMaximum(100)
        self.num_animals_in_videos.valueChanged.connect(self.log_num_animals)

        # Num triplets
        num_triplets_label = QtWidgets.QLabel("Number of triplets")
        self.num_triplets = QtWidgets.QSpinBox()
        self.num_triplets.setMaximum(1000000)
        self.num_triplets.setValue(1000)
        self.num_triplets.valueChanged.connect(self.log_num_triplets)

        layout.addWidget(shuffle_label, 0, 0)
        layout.addWidget(self.shuffle, 0, 1)
        layout.addWidget(trackingtype_label, 0, 2)
        layout.addWidget(self.tracker_type_widget, 0, 3)
        layout.addWidget(num_animals_label, 1, 0)
        layout.addWidget(self.num_animals_in_videos, 1, 1)
        layout.addWidget(num_triplets_label, 1, 2)
        layout.addWidget(self.num_triplets, 1, 3)
        # layout.addWidget()

    def log_tracker_type(self, tracker):
        self.root.logger.info(f"Tracker type set to {tracker.upper()}")

    def log_num_animals(self, value):
        self.root.logger.info(f"Num animals set to {value}")

    def log_num_triplets(self, value):
        self.root.logger.info(f"Num triplets set to {value}")

    def run_transformer(self):
        config = self.root.config
        videos = self.files
        videotype = self.video_selection_widget.videotype_widget.currentText()
        n_tracks = self.num_animals_in_videos.value()
        shuffle = self.shuffle.value()
        track_method = self.tracker_type_widget.currentText()

        func = partial(
            deeplabcut.transformer_reID,
            config=config,
            videos=videos,
            videotype=videotype,
            n_tracks=n_tracks,
            shuffle=shuffle,
            track_method=track_method,
        )
        self.worker, self.thread = move_to_separate_thread(func)
        self.worker.finished.connect(
            lambda: self.run_transformer_button.setEnabled(True)
        )
        self.worker.finished.connect(lambda: self.root._progress_bar.hide())
        self.thread.start()
        self.run_transformer_button.setEnabled(False)
        self.root._progress_bar.show()


--- File: deeplabcut/gui/tabs/modelzoo.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import webbrowser
from functools import partial

import dlclibrary
from PySide6 import QtWidgets
from PySide6.QtCore import QRegularExpression, Qt, QTimer, Signal, Slot
from PySide6.QtGui import QIcon, QPixmap, QRegularExpressionValidator

import deeplabcut
from deeplabcut.core.engine import Engine
from deeplabcut.gui import BASE_DIR
from deeplabcut.gui.components import (
    _create_grid_layout,
    _create_label_widget,
    DefaultTab,
    VideoSelectionWidget,
)
from deeplabcut.gui.utils import move_to_separate_thread
from deeplabcut.gui.widgets import ClickableLabel


class RegExpValidator(QRegularExpressionValidator):
    validationChanged = Signal(QRegularExpressionValidator.State)

    def validate(self, input_, pos):
        state, input_, pos = super().validate(input_, pos)
        self.validationChanged.emit(state)
        return state, input_, pos


class ModelZoo(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super().__init__(root, parent, h1_description)
        self._val_pattern = QRegularExpression(r"(\d{3,5},\s*)+\d{3,5}")
        self._set_page()
        self.root.engine_change.connect(self._on_engine_change)
        self.root.engine_change.connect(self._update_available_models)
        self._update_pose_models(self.model_combo.currentText())
        self._update_detectors(self.model_combo.currentText())
        self._destfolder = None

    @property
    def files(self):
        return self.video_selection_widget.files

    def _set_page(self):
        self.main_layout.addWidget(_create_label_widget("Video Selection", "font:bold"))
        self.video_selection_widget = VideoSelectionWidget(self.root, self)
        self.main_layout.addWidget(self.video_selection_widget)

        self._build_common_attributes()
        self._build_tf_attributes()
        self._build_torch_attributes()

        self.run_button = QtWidgets.QPushButton("Run")
        self.run_button.clicked.connect(self.run_video_adaptation)
        self.main_layout.addWidget(self.run_button, alignment=Qt.AlignRight)

        self.home_button = QtWidgets.QPushButton("Return to Welcome page")
        self.home_button.clicked.connect(self.root._generate_welcome_page)
        self.main_layout.addWidget(self.home_button, alignment=Qt.AlignLeft)
        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

        self.go_to_button = QtWidgets.QPushButton("Read Documentation")
        # go to url https://deeplabcut.github.io/DeepLabCut/docs/ModelZoo.html#about-the-superanimal-models when button is clicked
        self.go_to_button.clicked.connect(
            lambda: webbrowser.open(
                "https://deeplabcut.github.io/DeepLabCut/docs/ModelZoo.html#about-the-superanimal-models"
            )
        )
        self.main_layout.addWidget(self.go_to_button, alignment=Qt.AlignLeft)
        self._on_engine_change(self.root.engine)

    def _build_common_attributes(self) -> None:
        settings_layout = _create_grid_layout(margins=(20, 0, 0, 0))
        section_title = _create_label_widget(
            "Supermodel Settings", "font:bold", (0, 50, 0, 0)
        )

        model_combo_text = QtWidgets.QLabel("Supermodel name")
        model_combo_text.setMinimumWidth(300)
        self.model_combo = QtWidgets.QComboBox()
        self.model_combo.setMinimumWidth(250)

        net_type_text = QtWidgets.QLabel("Net Type")
        net_type_text.setMinimumWidth(300)
        self.net_type_selector = QtWidgets.QComboBox()

        self.detector_type_text = QtWidgets.QLabel("Detector Type")
        self.detector_type_text.setMinimumWidth(300)
        self.detector_type_selector = QtWidgets.QComboBox()

        loc_label = ClickableLabel("Folder to store results:", parent=self)
        loc_label.signal.connect(self.select_folder)
        self.loc_line = QtWidgets.QLineEdit(
            "<Select a folder - Default: store in same folder as video>",
            self,
        )
        self.loc_line.setReadOnly(True)
        action = self.loc_line.addAction(
            QIcon(os.path.join(BASE_DIR, "assets", "icons", "open2.png")),
            QtWidgets.QLineEdit.TrailingPosition,
        )
        action.triggered.connect(self.select_folder)

        settings_layout.addWidget(section_title, 0, 0)
        settings_layout.addWidget(model_combo_text, 1, 0)
        settings_layout.addWidget(self.model_combo, 1, 1)
        settings_layout.addWidget(net_type_text, 2, 0)
        settings_layout.addWidget(self.net_type_selector, 2, 1)
        settings_layout.addWidget(self.detector_type_text, 3, 0)
        settings_layout.addWidget(self.detector_type_selector, 3, 1)

        settings_layout.addWidget(loc_label, 4, 0)
        settings_layout.addWidget(self.loc_line, 4, 1)

        self.settings_widget = QtWidgets.QWidget()
        self.settings_widget.setLayout(settings_layout)
        self.main_layout.addWidget(self.settings_widget)
        self.model_combo.currentTextChanged.connect(self._update_pose_models)
        self.model_combo.currentTextChanged.connect(self._update_detectors)

    def _build_tf_attributes(self) -> None:
        model_settings_layout = _create_grid_layout(margins=(20, 0, 0, 0))

        scales_label = QtWidgets.QLabel("Scale list")
        scales_label.setMinimumWidth(300)
        self.scales_line = QtWidgets.QLineEdit("", parent=self)
        self.scales_line.setPlaceholderText(
            "Optionally input a list of integer sizes separated by commas..."
        )
        validator = RegExpValidator(self._val_pattern, self)
        validator.validationChanged.connect(self._handle_validation_change)
        self.scales_line.setValidator(validator)

        tooltip_label = QtWidgets.QLabel()
        tooltip_label.setPixmap(
            QPixmap(
                os.path.join(BASE_DIR, "assets", "icons", "help2.png")
            ).scaledToWidth(30)
        )
        tooltip_label.setToolTip(
            "Approximate animal sizes in pixels, for spatial pyramid search. If left "
            "blank, defaults to video height +/- 50 pixels"
        )

        self.adapt_checkbox = QtWidgets.QCheckBox("Use video adaptation")
        self.adapt_checkbox.setChecked(True)

        pseudo_threshold_label = QtWidgets.QLabel("Pseudo-label confidence threshold")
        self.pseudo_threshold_spinbox = QtWidgets.QDoubleSpinBox(
            decimals=2,
            minimum=0.01,
            maximum=1.0,
            singleStep=0.05,
            value=0.1,
            wrapping=True,
        )
        self.pseudo_threshold_spinbox.setMaximumWidth(300)

        adapt_iter_label = QtWidgets.QLabel("Number of adaptation iterations")
        adapt_iter_label.setMinimumWidth(300)
        self.adapt_iter_spinbox = QtWidgets.QSpinBox()
        self.adapt_iter_spinbox.setRange(100, 10000)
        self.adapt_iter_spinbox.setValue(1000)
        self.adapt_iter_spinbox.setSingleStep(100)
        self.adapt_iter_spinbox.setGroupSeparatorShown(True)
        self.adapt_iter_spinbox.setMaximumWidth(300)

        model_settings_layout.addWidget(scales_label, 1, 0)
        model_settings_layout.addWidget(self.scales_line, 1, 1)
        model_settings_layout.addWidget(tooltip_label, 1, 2)
        model_settings_layout.addWidget(self.adapt_checkbox, 2, 0)
        model_settings_layout.addWidget(pseudo_threshold_label, 3, 0)
        model_settings_layout.addWidget(self.pseudo_threshold_spinbox, 3, 1)
        model_settings_layout.addWidget(adapt_iter_label, 4, 0)
        model_settings_layout.addWidget(self.adapt_iter_spinbox, 4, 1)
        self.tf_widget = QtWidgets.QWidget()
        self.tf_widget.setLayout(model_settings_layout)
        self.tf_widget.hide()
        self.main_layout.addWidget(self.tf_widget)

    def _build_torch_attributes(self) -> None:
        torch_settings_layout = _create_grid_layout(margins=(20, 0, 0, 0))

        self.torch_adapt_checkbox = QtWidgets.QCheckBox("Use video adaptation")
        self.torch_adapt_checkbox.setChecked(True)

        pseudo_threshold_label = QtWidgets.QLabel("Pseudo-label confidence threshold")
        pseudo_threshold_label.setMinimumWidth(300)
        self.torch_pseudo_threshold_spinbox = QtWidgets.QDoubleSpinBox(
            decimals=2,
            minimum=0.01,
            maximum=1.0,
            singleStep=0.05,
            value=0.1,
            wrapping=True,
        )
        self.torch_pseudo_threshold_spinbox.setMaximumWidth(300)

        adapt_epoch_label = QtWidgets.QLabel("Number of adaptation epochs")
        adapt_epoch_label.setMinimumWidth(300)
        self.torch_adapt_epoch_spinbox = QtWidgets.QSpinBox()
        self.torch_adapt_epoch_spinbox.setRange(1, 50)
        self.torch_adapt_epoch_spinbox.setValue(4)
        self.torch_adapt_epoch_spinbox.setMaximumWidth(300)

        adapt_det_epoch_label = QtWidgets.QLabel("Number of detector adaptation epochs")
        adapt_det_epoch_label.setMinimumWidth(300)
        self.torch_adapt_det_epoch_spinbox = QtWidgets.QSpinBox()
        self.torch_adapt_det_epoch_spinbox.setRange(1, 50)
        self.torch_adapt_det_epoch_spinbox.setValue(4)
        self.torch_adapt_det_epoch_spinbox.setMaximumWidth(300)

        torch_settings_layout.addWidget(self.torch_adapt_checkbox, 1, 0)
        torch_settings_layout.addWidget(pseudo_threshold_label, 2, 0)
        torch_settings_layout.addWidget(self.torch_pseudo_threshold_spinbox, 2, 1)
        torch_settings_layout.addWidget(adapt_epoch_label, 3, 0)
        torch_settings_layout.addWidget(self.torch_adapt_epoch_spinbox, 3, 1)
        torch_settings_layout.addWidget(adapt_det_epoch_label, 4, 0)
        torch_settings_layout.addWidget(self.torch_adapt_det_epoch_spinbox, 4, 1)
        self.torch_widget = QtWidgets.QWidget()
        self.torch_widget.setLayout(torch_settings_layout)
        self.torch_widget.hide()
        self.main_layout.addWidget(self.torch_widget)

    def select_folder(self):
        dirname = QtWidgets.QFileDialog.getExistingDirectory(
            self, "Please select a folder", self.root.project_folder
        )
        if not dirname:
            return

        self._destfolder = dirname
        self.loc_line.setText(dirname)

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        label = QtWidgets.QLabel(deeplabcut.video_inference_superanimal.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _handle_validation_change(self, state):
        if state == RegExpValidator.Invalid:
            color = "red"
        elif state == RegExpValidator.Intermediate:
            color = "gold"
        elif state == RegExpValidator.Acceptable:
            color = "lime"
        self.scales_line.setStyleSheet(f"border: 1px solid {color}")
        QTimer.singleShot(500, lambda: self.scales_line.setStyleSheet(""))

    def run_video_adaptation(self):
        videos = list(self.files)
        if not videos:
            msg = QtWidgets.QMessageBox()
            msg.setIcon(QtWidgets.QMessageBox.Critical)
            msg.setText("You must select a video file")
            msg.setWindowTitle("Error")
            msg.setMinimumWidth(400)
            msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
            msg.exec_()
            return

        supermodel_name = self.model_combo.currentText()
        videotype = self.video_selection_widget.videotype_widget.currentText()
        kwargs = self._gather_kwargs()

        can_run_in_background = False
        if can_run_in_background:
            func = partial(
                deeplabcut.video_inference_superanimal,
                videos,
                supermodel_name,
                videotype=videotype,
                dest_folder=self._destfolder,
                **kwargs,
            )

            self.worker, self.thread = move_to_separate_thread(func)
            self.worker.finished.connect(self.signal_analysis_complete)
            self.thread.start()
            self.run_button.setEnabled(False)
            self.root._progress_bar.show()
        else:
            print(f"Calling video_inference_superanimal with kwargs={kwargs}")
            deeplabcut.video_inference_superanimal(
                videos,
                supermodel_name,
                videotype=videotype,
                dest_folder=self._destfolder,
                **kwargs,
            )
            self.signal_analysis_complete()

    def signal_analysis_complete(self):
        self.run_button.setEnabled(True)
        self.root._progress_bar.hide()
        msg = QtWidgets.QMessageBox(text="SuperAnimal video inference complete!")
        msg.setIcon(QtWidgets.QMessageBox.Information)
        msg.exec_()

    def _gather_kwargs(self) -> dict:
        kwargs = dict(model_name=self.net_type_selector.currentText())
        if self.root.engine == Engine.TF:
            scales = []
            scales_ = self.scales_line.text()
            if scales_:
                if (
                    self.scales_line.validator().validate(scales_, 0)[0]
                    == RegExpValidator.Acceptable
                ):
                    scales = list(map(int, scales_.split(",")))
            kwargs["scale_list"] = scales
            kwargs["video_adapt"] = self.adapt_checkbox.isChecked()
            kwargs["pseudo_threshold"] = self.pseudo_threshold_spinbox.value()
            kwargs["adapt_iterations"] = self.adapt_iter_spinbox.value()
        else:
            kwargs["detector_name"] = self.detector_type_selector.currentText()
            kwargs["video_adapt"] = self.torch_adapt_checkbox.isChecked()
            kwargs["pseudo_threshold"] = self.torch_pseudo_threshold_spinbox.value()
            kwargs["detector_epochs"] = self.torch_adapt_det_epoch_spinbox.value()
            kwargs["pose_epochs"] = self.torch_adapt_epoch_spinbox.value()

        return kwargs

    def _update_available_models(self, engine: Engine) -> None:
        current_dataset = self.model_combo.currentText()

        while self.model_combo.count() > 0:
            self.model_combo.removeItem(0)

        if engine == Engine.TF:
            supermodels = ["superanimal_topviewmouse", "superanimal_quadruped"]
        else:
            supermodels = dlclibrary.get_available_datasets()

        self.model_combo.addItems(supermodels)
        if current_dataset in supermodels:
            self.model_combo.setCurrentIndex(supermodels.index(current_dataset))

    def _update_pose_models(self, super_animal: str) -> None:
        while self.net_type_selector.count() > 0:
            self.net_type_selector.removeItem(0)

        if len(super_animal) == 0:
            return

        if self.root.engine == Engine.TF:
            self.net_type_selector.addItems(["dlcrnet"])
        else:
            self.net_type_selector.addItems(
                dlclibrary.get_available_models(super_animal)
            )

    def _update_detectors(self, super_animal: str) -> None:
        while self.detector_type_selector.count() > 0:
            self.detector_type_selector.removeItem(0)

        if len(super_animal) == 0:
            return

        if self.root.engine == Engine.TF:
            self.detector_type_selector.addItems(["dlcrnet"])
        else:
            self.detector_type_selector.addItems(
                dlclibrary.get_available_detectors(super_animal)
            )

    @Slot(Engine)
    def _on_engine_change(self, engine: Engine) -> None:
        self._update_available_models(engine)
        if engine == Engine.PYTORCH:
            self.tf_widget.hide()
            self.detector_type_text.show()
            self.detector_type_selector.show()
            self.torch_widget.show()
        else:
            self.torch_widget.hide()
            self.detector_type_text.hide()
            self.detector_type_selector.hide()
            self.tf_widget.show()


--- File: deeplabcut/gui/tabs/analyze_videos.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from functools import partial
from PySide6 import QtWidgets
from PySide6.QtCore import Qt

from deeplabcut.gui.utils import move_to_separate_thread
from deeplabcut.gui.widgets import ConfigEditor
from deeplabcut.gui.components import (
    DefaultTab,
    BodypartListWidget,
    ShuffleSpinBox,
    VideoSelectionWidget,
    _create_grid_layout,
    _create_label_widget,
    _create_horizontal_layout,
    _create_vertical_layout,
)

import deeplabcut
from deeplabcut.utils.auxiliaryfunctions import edit_config
from deeplabcut.utils import auxfun_multianimal


class AnalyzeVideos(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(AnalyzeVideos, self).__init__(root, parent, h1_description)

        self._set_page()

    @property
    def files(self):
        return self.video_selection_widget.files

    def _set_page(self):
        self.main_layout.addWidget(_create_label_widget("Video Selection", "font:bold"))
        self.video_selection_widget = VideoSelectionWidget(self.root, self)
        self.main_layout.addWidget(self.video_selection_widget)

        tmp_layout = _create_horizontal_layout()

        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self.layout_attributes = _create_grid_layout()
        self._generate_layout_attributes(self.layout_attributes)
        tmp_layout.addLayout(self.layout_attributes)

        # Single / Multi animal Only Layouts
        self.layout_singleanimal = _create_horizontal_layout()
        self.layout_multianimal = _create_horizontal_layout()

        if self.root.is_multianimal:
            self._generate_layout_multianimal(self.layout_multianimal)
            tmp_layout.addLayout(self.layout_multianimal)
        else:
            self._generate_layout_single_animal(self.layout_singleanimal)
            tmp_layout.addLayout(self.layout_singleanimal)

        self.main_layout.addLayout(tmp_layout)

        self.main_layout.addWidget(_create_label_widget("", "font:bold"))
        self.layout_other_options = _create_vertical_layout()
        self._generate_layout_other_options(self.layout_other_options)
        self.main_layout.addLayout(self.layout_other_options)

        self.analyze_videos_btn = QtWidgets.QPushButton("Analyze Videos")
        self.analyze_videos_btn.clicked.connect(self.analyze_videos)

        self.edit_config_file_btn = QtWidgets.QPushButton("Edit config.yaml")
        self.edit_config_file_btn.clicked.connect(self.edit_config_file)

        self.main_layout.addWidget(self.analyze_videos_btn, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.edit_config_file_btn, alignment=Qt.AlignRight)

        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        label = QtWidgets.QLabel(deeplabcut.analyze_videos.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _generate_layout_single_animal(self, layout):
        # Dynamic bodypart cropping
        self.crop_bodyparts = QtWidgets.QCheckBox("Dynamically crop bodyparts")
        self.crop_bodyparts.setCheckState(Qt.Unchecked)
        self.crop_bodyparts.stateChanged.connect(self.update_crop_choice)

        self.dynamic_cropping = False
        layout.addWidget(self.crop_bodyparts)

    def _generate_layout_other_options(self, layout):
        tmp_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))

        # Save results as csv
        self.save_as_csv = QtWidgets.QCheckBox("Save result(s) as csv")
        self.save_as_csv.setCheckState(Qt.Unchecked)
        self.save_as_csv.stateChanged.connect(self.update_csv_choice)

        tmp_layout.addWidget(self.save_as_csv)

        # Filter predictions
        self.filter_predictions = QtWidgets.QCheckBox("Filter predictions")
        self.filter_predictions.setCheckState(Qt.Unchecked)
        self.filter_predictions.stateChanged.connect(self.update_filter_choice)

        tmp_layout.addWidget(self.filter_predictions)

        # Plot Trajectories
        self.plot_trajectories = QtWidgets.QCheckBox("Plot trajectories")
        self.plot_trajectories.setCheckState(Qt.Unchecked)
        self.plot_trajectories.stateChanged.connect(self.update_plot_trajectory_choice)

        tmp_layout.addWidget(self.plot_trajectories)

        # Show trajectory plots
        self.show_trajectory_plots = QtWidgets.QCheckBox("Show trajectory plots")
        self.show_trajectory_plots.setCheckState(Qt.Unchecked)
        self.show_trajectory_plots.setEnabled(False)
        self.show_trajectory_plots.stateChanged.connect(self.update_showfigs_choice)

        tmp_layout.addWidget(self.show_trajectory_plots)

        layout.addLayout(tmp_layout)

        self.bodyparts_list_widget = BodypartListWidget(root=self.root, parent=self)
        layout.addWidget(self.bodyparts_list_widget, Qt.AlignLeft)

    def _generate_layout_attributes(self, layout):
        # Shuffle
        opt_text = QtWidgets.QLabel("Shuffle")
        self.shuffle = ShuffleSpinBox(root=self.root, parent=self)

        layout.addWidget(opt_text, 0, 0)
        layout.addWidget(self.shuffle, 0, 1)

    def _generate_layout_multianimal(self, layout):
        tmp_layout = QtWidgets.QGridLayout()

        opt_text = QtWidgets.QLabel("Tracking method")
        self.tracker_type_widget = QtWidgets.QComboBox()
        self.tracker_type_widget.addItems(["ellipse", "box", "skeleton"])
        self.tracker_type_widget.currentTextChanged.connect(self.update_tracker_type)
        tmp_layout.addWidget(opt_text, 0, 0)
        tmp_layout.addWidget(self.tracker_type_widget, 0, 1)

        opt_text = QtWidgets.QLabel("Number of animals in videos")
        self.num_animals_in_videos = QtWidgets.QSpinBox()
        self.num_animals_in_videos.setMaximum(100)
        self.num_animals_in_videos.setValue(len(self.root.all_individuals))
        tmp_layout.addWidget(opt_text, 1, 0)
        tmp_layout.addWidget(self.num_animals_in_videos, 1, 1)

        # layout.addLayout(tmp_layout)

        # tmp_layout = QtWidgets.QGridLayout()

        self.calibrate_assembly_checkbox = QtWidgets.QCheckBox("Calibrate assembly")
        self.calibrate_assembly_checkbox.setCheckState(Qt.Unchecked)
        self.calibrate_assembly_checkbox.stateChanged.connect(
            self.update_calibrate_assembly
        )
        tmp_layout.addWidget(self.calibrate_assembly_checkbox, 0, 2)

        self.assemble_with_ID_only_checkbox = QtWidgets.QCheckBox(
            "Assemble with ID only"
        )
        self.assemble_with_ID_only_checkbox.setCheckState(Qt.Unchecked)
        self.assemble_with_ID_only_checkbox.stateChanged.connect(
            self.update_assemble_with_ID_only
        )
        tmp_layout.addWidget(self.assemble_with_ID_only_checkbox, 0, 3)

        self.create_detections_video_checkbox = QtWidgets.QCheckBox(
            "Create video with all detections"
        )
        self.create_detections_video_checkbox.setCheckState(Qt.Unchecked)
        self.create_detections_video_checkbox.stateChanged.connect(
            self.update_create_video_detections
        )
        tmp_layout.addWidget(self.create_detections_video_checkbox, 0, 4)

        layout.addLayout(tmp_layout)

    def update_create_video_detections(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"Create video with all detections {s}")

    def update_assemble_with_ID_only(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"Assembly with ID only {s}")

    def update_calibrate_assembly(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"Assembly calibration {s}")

    def update_tracker_type(self, method):
        self.root.logger.info(f"Using {method.upper()} tracker")

    def update_csv_choice(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"Save results as CSV {s}")

    def update_filter_choice(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"Filtering predictions {s}")

    def update_showfigs_choice(self, state):
        if Qt.CheckState(state) == Qt.Checked:
            self.root.logger.info("Plots will show as pop ups.")
        else:
            self.root.logger.info("Plots will not show up.")

    def update_crop_choice(self, state):
        if Qt.CheckState(state) == Qt.Checked:
            self.root.logger.info("Dynamic bodypart cropping ENABLED.")
            self.dynamic_cropping = True
        else:
            self.root.logger.info("Dynamic bodypart cropping DISABLED.")
            self.dynamic_cropping = False

    def update_plot_trajectory_choice(self, state):
        if Qt.CheckState(state) == Qt.Checked:
            self.bodyparts_list_widget.refresh()
            self.bodyparts_list_widget.show()
            self.bodyparts_list_widget.setEnabled(True)
            self.show_trajectory_plots.setEnabled(True)
            self.root.logger.info("Plot trajectories ENABLED.")

        else:
            self.bodyparts_list_widget.hide()
            self.bodyparts_list_widget.setEnabled(False)
            self.show_trajectory_plots.setEnabled(False)
            self.show_trajectory_plots.setCheckState(Qt.Unchecked)
            self.root.logger.info("Plot trajectories DISABLED.")

    def edit_config_file(self):
        if not self.root.config:
            return
        editor = ConfigEditor(self.root.config)
        editor.show()

    def analyze_videos(self):
        config = self.root.config
        shuffle = self.root.shuffle_value

        videos = list(self.files)
        save_as_csv = self.save_as_csv.isChecked()
        videotype = self.video_selection_widget.videotype_widget.currentText()

        if self.root.is_multianimal:
            calibrate_assembly = self.calibrate_assembly_checkbox.isChecked()
            assemble_with_ID_only = self.assemble_with_ID_only_checkbox.isChecked()
            track_method = self.tracker_type_widget.currentText()
            edit_config(self.root.config, {"default_track_method": track_method})
            num_animals_in_videos = self.num_animals_in_videos.value()
        else:
            calibrate_assembly = False
            num_animals_in_videos = None
            assemble_with_ID_only = False

        cropping = None

        if self.root.cfg["cropping"] == "True":
            cropping = (
                self.root.cfg["x1"],
                self.root.cfg["x2"],
                self.root.cfg["y1"],
                self.root.cfg["y2"],
            )

        dynamic_cropping_params = (False, 0.5, 10)
        try:
            if self.dynamic_cropping:
                dynamic_cropping_params = (True, 0.5, 10)
        except AttributeError:
            pass

        func = partial(
            deeplabcut.analyze_videos,
            config,
            videos=videos,
            videotype=videotype,
            shuffle=shuffle,
            save_as_csv=save_as_csv,
            cropping=cropping,
            dynamic=dynamic_cropping_params,
            auto_track=self.root.is_multianimal,
            n_tracks=num_animals_in_videos,
            calibrate=calibrate_assembly,
            identity_only=assemble_with_ID_only,
        )

        self.worker, self.thread = move_to_separate_thread(func)
        self.worker.finished.connect(lambda: self.analyze_videos_btn.setEnabled(True))
        self.worker.finished.connect(lambda: self.root._progress_bar.hide())
        self.worker.finished.connect(lambda: self.run_enabled())
        self.thread.start()
        self.analyze_videos_btn.setEnabled(False)
        self.root._progress_bar.show()

    def run_enabled(self):
        config = self.root.config
        shuffle = self.root.shuffle_value

        videos = list(self.files)
        save_as_csv = self.save_as_csv.isChecked()
        filter_data = self.filter_predictions.isChecked()
        videotype = self.video_selection_widget.videotype_widget.currentText()
        try:
            create_video_all_detections = (
                self.create_detections_video_checkbox.isChecked()
            )
        except AttributeError:
            create_video_all_detections = False
        if create_video_all_detections:
            deeplabcut.create_video_with_all_detections(
                config,
                videos=videos,
                videotype=videotype,
                shuffle=shuffle,
            )

        track_method = auxfun_multianimal.get_track_method(self.root.cfg)
        if filter_data:
            deeplabcut.filterpredictions(
                config,
                video=videos,
                videotype=videotype,
                shuffle=shuffle,
                filtertype="median",
                windowlength=5,
                save_as_csv=save_as_csv,
                track_method=track_method,
            )

        if self.plot_trajectories.isChecked():
            bdpts = self.bodyparts_list_widget.selected_bodyparts
            self.root.logger.debug(
                f"Selected body parts for plot_trajectories: {bdpts}"
            )
            deeplabcut.plot_trajectories(
                config,
                videos=videos,
                displayedbodyparts=bdpts,
                videotype=videotype,
                shuffle=shuffle,
                filtered=filter_data,
                showfigures=self.show_trajectory_plots.isChecked(),
                track_method=track_method,
            )

        if self.root.is_multianimal and save_as_csv:
            deeplabcut.analyze_videos_converth5_to_csv(
                videos,
                listofvideos=True,
            )


--- File: deeplabcut/gui/tabs/create_videos.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from PySide6 import QtWidgets
from PySide6.QtCore import Qt

from deeplabcut.gui.components import (
    BodypartListWidget,
    DefaultTab,
    ShuffleSpinBox,
    VideoSelectionWidget,
    _create_horizontal_layout,
    _create_label_widget,
    _create_vertical_layout,
)

import deeplabcut


class CreateVideos(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(CreateVideos, self).__init__(root, parent, h1_description)

        self.bodyparts_to_use = self.root.all_bodyparts
        self._set_page()

    @property
    def files(self):
        return self.video_selection_widget.files

    def _set_page(self):
        self.main_layout.addWidget(_create_label_widget("Video Selection", "font:bold"))
        self.video_selection_widget = VideoSelectionWidget(self.root, self)
        self.main_layout.addWidget(self.video_selection_widget)

        tmp_layout = _create_horizontal_layout()

        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self.layout_attributes = _create_horizontal_layout(margins=(0, 0, 0, 0))
        self._generate_layout_attributes(self.layout_attributes)
        tmp_layout.addLayout(self.layout_attributes)

        self.layout_multianimal = _create_horizontal_layout()

        if self.root.is_multianimal:
            self._generate_layout_multianimal(self.layout_multianimal)
            tmp_layout.addLayout(self.layout_multianimal)

        self.main_layout.addLayout(tmp_layout)

        self.main_layout.addWidget(
            _create_label_widget("Video Parameters", "font:bold")
        )
        self.layout_video_parameters = _create_vertical_layout()
        self._generate_layout_video_parameters(self.layout_video_parameters)
        self.main_layout.addLayout(self.layout_video_parameters)

        self.sk_button = QtWidgets.QPushButton("Build skeleton")
        self.sk_button.clicked.connect(self.build_skeleton)
        self.main_layout.addWidget(self.sk_button, alignment=Qt.AlignRight)

        self.run_button = QtWidgets.QPushButton("Create videos")
        self.run_button.clicked.connect(self.create_videos)
        self.main_layout.addWidget(self.run_button, alignment=Qt.AlignRight)

        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        label = QtWidgets.QLabel(deeplabcut.create_labeled_video.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _generate_layout_multianimal(self, layout):
        tmp_text = QtWidgets.QLabel("Color keypoints by:")
        self.color_by_widget = QtWidgets.QComboBox()
        self.color_by_widget.addItems(["bodypart", "individual"])
        self.color_by_widget.setCurrentText("bodypart")
        self.color_by_widget.currentTextChanged.connect(self.update_color_by)

        layout.addWidget(tmp_text)
        layout.addWidget(self.color_by_widget)

    def _generate_layout_attributes(self, layout):
        # Shuffle
        opt_text = QtWidgets.QLabel("Shuffle")
        self.shuffle = ShuffleSpinBox(root=self.root, parent=self)

        layout.addWidget(opt_text)
        layout.addWidget(self.shuffle)

        # Overwrite videos
        self.overwrite_videos = QtWidgets.QCheckBox("Overwrite videos")
        self.overwrite_videos.setCheckState(Qt.Unchecked)
        self.overwrite_videos.stateChanged.connect(self.update_overwrite_videos)

        layout.addWidget(self.overwrite_videos)

    def _generate_layout_video_parameters(self, layout):
        tmp_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))

        # Trail Points
        opt_text = QtWidgets.QLabel("Specify the number of trail points")
        self.trail_points = QtWidgets.QSpinBox()
        self.trail_points.setValue(0)
        tmp_layout.addWidget(opt_text)
        tmp_layout.addWidget(self.trail_points)

        layout.addLayout(tmp_layout)

        tmp_layout = _create_vertical_layout(margins=(0, 0, 0, 0))

        # Plot all bodyparts
        self.plot_all_bodyparts = QtWidgets.QCheckBox("Plot all bodyparts")
        self.plot_all_bodyparts.setCheckState(Qt.Checked)
        self.plot_all_bodyparts.stateChanged.connect(self.update_use_all_bodyparts)
        tmp_layout.addWidget(self.plot_all_bodyparts)

        # Skeleton
        self.draw_skeleton_checkbox = QtWidgets.QCheckBox("Draw skeleton")
        self.draw_skeleton_checkbox.setCheckState(Qt.Unchecked)
        self.draw_skeleton_checkbox.stateChanged.connect(self.update_draw_skeleton)
        tmp_layout.addWidget(self.draw_skeleton_checkbox)

        # Filtered data
        self.use_filtered_data_checkbox = QtWidgets.QCheckBox("Use filtered data")
        self.use_filtered_data_checkbox.setCheckState(Qt.Unchecked)
        self.use_filtered_data_checkbox.stateChanged.connect(
            self.update_use_filtered_data
        )
        tmp_layout.addWidget(self.use_filtered_data_checkbox)

        # Selector for p-cutoff
        pcutoff_widget = QtWidgets.QWidget()
        pcutoff_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))
        pcutoff_label = QtWidgets.QLabel("Plotting confidence cutoff (pcutoff)")
        self.pcutoff_selector = QtWidgets.QDoubleSpinBox()
        self.pcutoff_selector.setMinimum(0.0)
        self.pcutoff_selector.setMaximum(1.0)
        self.pcutoff_selector.setValue(0.6)
        self.pcutoff_selector.setSingleStep(0.05)
        pcutoff_layout.addWidget(pcutoff_label)
        pcutoff_layout.addWidget(self.pcutoff_selector)
        pcutoff_widget.setLayout(pcutoff_layout)
        pcutoff_widget.setToolTip(
            "This value sets the confidence threshold, above which predictions are "
            "shown in the labeled videos."
        )
        tmp_layout.addWidget(pcutoff_widget)

        # Plot trajectories
        self.plot_trajectories = QtWidgets.QCheckBox("Plot trajectories")
        self.plot_trajectories.setCheckState(Qt.Unchecked)
        self.plot_trajectories.stateChanged.connect(self.update_plot_trajectory_choice)
        tmp_layout.addWidget(self.plot_trajectories)

        # High quality video
        self.create_high_quality_video = QtWidgets.QCheckBox(
            "High quality video (slow)"
        )
        self.create_high_quality_video.setCheckState(Qt.Unchecked)
        self.create_high_quality_video.stateChanged.connect(
            self.update_high_quality_video
        )
        tmp_layout.addWidget(self.create_high_quality_video)

        nested_tmp_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))
        nested_tmp_layout.addLayout(tmp_layout)

        tmp_layout = _create_vertical_layout(margins=(0, 0, 0, 0))

        # Bodypart list
        self.bodyparts_list_widget = BodypartListWidget(
            root=self.root,
            parent=self,
        )
        nested_tmp_layout.addWidget(self.bodyparts_list_widget, Qt.AlignLeft)

        tmp_layout.addLayout(nested_tmp_layout, Qt.AlignLeft)

        layout.addLayout(tmp_layout, Qt.AlignLeft)

    def update_high_quality_video(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"High quality {s}.")

    def update_plot_trajectory_choice(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"Plot trajectories {s}.")

    def update_selected_bodyparts(self):
        selected_bodyparts = [
            item.text() for item in self.bodyparts_list_widget.selectedItems()
        ]
        self.root.logger.info(
            f"Selected bodyparts for plotting:\n\t{selected_bodyparts}"
        )
        self.bodyparts_to_use = selected_bodyparts

    def update_use_all_bodyparts(self, s):
        if Qt.CheckState(s) == Qt.Checked:
            self.bodyparts_list_widget.setEnabled(False)
            self.bodyparts_list_widget.hide()
            self.root.logger.info("Plot all bodyparts ENABLED.")

        else:
            self.bodyparts_list_widget.setEnabled(True)
            self.bodyparts_list_widget.show()
            self.root.logger.info("Plot all bodyparts DISABLED.")

    def update_use_filtered_data(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"Use filtered data {s}")

    def update_draw_skeleton(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"Draw skeleton {s}")

    def update_overwrite_videos(self, state):
        s = "ENABLED" if Qt.CheckState(state) == Qt.Checked else "DISABLED"
        self.root.logger.info(f"Overwrite videos {s}")

    def update_color_by(self, text):
        self.root.logger.info(f"Coloring keypoints in videos by {text}")

    def update_filter_choice(self, rb):
        self.filtered = rb.text() == "Yes"

    def update_video_slow_choice(self, rb):
        self.slow = rb.text() == "Yes"

    def update_draw_skeleton_choice(self, rb):
        self.draw = rb.text() == "Yes"

    def create_videos(self):
        config = self.root.config
        shuffle = self.root.shuffle_value
        videos = self.files
        trailpoints = self.trail_points.value()
        if hasattr(self, "color_by_widget"):
            # Multianimal scenario.
            # Color is based on individual or bodypart.
            color_by = self.color_by_widget.currentText()
        else:
            # Single animal scenario.
            # Color is based on bodypart.
            color_by = "bodypart"
        filtered = self.use_filtered_data_checkbox.isChecked()

        bodyparts = "all"
        if (
            len(self.bodyparts_to_use) != 0
            and not self.plot_all_bodyparts.isChecked()
        ):
            self.update_selected_bodyparts()
            bodyparts = self.bodyparts_to_use

        videos_created = deeplabcut.create_labeled_video(
            config=config,
            videos=videos,
            shuffle=shuffle,
            filtered=filtered,
            save_frames=self.create_high_quality_video.isChecked(),
            pcutoff=self.pcutoff_selector.value(),
            displayedbodyparts=bodyparts,
            draw_skeleton=self.draw_skeleton_checkbox.isChecked(),
            trailpoints=trailpoints,
            color_by=color_by,
            overwrite=self.overwrite_videos.isChecked(),
        )
        if all(videos_created):
            self.root.writer.write("Labeled videos created.")
        else:
            failed_videos = [
                video for success, video in zip(videos_created, videos) if not success
            ]
            failed_videos_str = ", ".join(failed_videos)
            self.root.writer.write(f"Failed to create videos from {failed_videos_str}.")

        if self.plot_trajectories.isChecked():
            deeplabcut.plot_trajectories(
                config=config,
                videos=videos,
                shuffle=shuffle,
                filtered=filtered,
                displayedbodyparts=bodyparts,
                pcutoff=self.pcutoff_selector.value(),
            )

    def build_skeleton(self, *args):
        from deeplabcut.gui.widgets import SkeletonBuilder

        SkeletonBuilder(self.root.config)


--- File: deeplabcut/gui/tabs/open_project.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os

from PySide6 import QtWidgets, QtCore
from PySide6.QtGui import QIcon
from PySide6.QtWidgets import QCheckBox


class OpenProject(QtWidgets.QDialog):
    def __init__(self, parent):
        super(OpenProject, self).__init__(parent)

        self.setWindowTitle("Load Existing Project")

        self.config = None
        self.loaded = False

        main_layout = QtWidgets.QVBoxLayout(self)
        self.layout_open()

        self.ok_button = QtWidgets.QPushButton("Load")
        self.ok_button.setDefault(True)
        self.ok_button.clicked.connect(self.open_project)

        main_layout.addWidget(self.open_frame)
        main_layout.addWidget(self.ok_button, alignment=QtCore.Qt.AlignRight)

    def layout_open(self):
        self.open_frame = QtWidgets.QFrame(self)
        self.open_frame.setFrameShape(self.open_frame.Shape.StyledPanel)
        self.open_frame.setLineWidth(0)
        self.open_frame.setMinimumWidth(600)

        open_label = QtWidgets.QLabel("Select the config file:", self.open_frame)
        self.open_line = QtWidgets.QLineEdit(self.open_frame)
        self.open_line.textChanged[str].connect(self.open_config_name)

        load_button = QtWidgets.QPushButton("Browse")
        load_button.clicked.connect(self.load_config)

        grid = QtWidgets.QGridLayout(self.open_frame)
        grid.setSpacing(30)
        grid.addWidget(open_label, 0, 0)
        grid.addWidget(self.open_line, 0, 1)
        grid.addWidget(load_button, 1, 1)

        return self.open_frame

    def open_config_name(self):
        self.open_line.text()

    def load_config(self):
        cwd = os.getcwd()
        config = QtWidgets.QFileDialog.getOpenFileName(
            self, "Select a configuration file", cwd, "Config files (*.yaml)"
        )
        if not config:
            return
        self.config = config[0]
        self.open_line.setText(self.config)
        self.ok_button.setFocus()

    def open_project(self):
        if self.config == "":
            msg = QtWidgets.QMessageBox()
            msg.setIcon(QtWidgets.QMessageBox.Critical)
            msg.setText("Please choose the config.yaml file to load the project")

            msg.setWindowTitle("Error")
            msg.setMinimumWidth(400)
            self.logo_dir = os.path.dirname(os.path.realpath("logo.png")) + os.path.sep
            self.logo = self.logo_dir + "/assets/logo.png"
            msg.setWindowIcon(QIcon(self.logo))
            msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
            msg.exec_()

            self.loaded = False
        else:
            self.logo_dir = os.path.dirname(os.path.realpath("logo.png")) + os.path.sep
            self.logo = self.logo_dir + "/assets/logo.png"

            self.loaded = True
            self.accept()
            self.close()


--- File: deeplabcut/gui/tabs/extract_frames.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from functools import partial
from pathlib import Path
from typing import Union

from PySide6 import QtWidgets
from PySide6.QtCore import Qt

from deeplabcut.gui.dlc_params import DLCParams
from deeplabcut.gui.components import (
    DefaultTab,
    VideoSelectionWidget,
    _create_grid_layout,
    _create_label_widget,
)
from deeplabcut.gui.utils import move_to_separate_thread
from deeplabcut.gui.widgets import launch_napari
from deeplabcut.generate_training_dataset import extract_frames


def select_cropping_area(config, videos=None):
    """
    Interactively select the cropping area of all videos in the config.
    A user interface pops up with a frame to select the cropping parameters.
    Use the left click to draw a box and hit the button 'set cropping parameters'
    to store the cropping parameters for a video in the config.yaml file.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    videos : optional (default=None)
        List of videos whose cropping areas are to be defined. Note that full paths are required.
        By default, all videos in the config are successively loaded.

    Returns
    -------
    cfg : dict
        Updated project configuration
    """
    from deeplabcut.utils import auxiliaryfunctions
    from deeplabcut.gui.widgets import FrameCropper

    cfg = auxiliaryfunctions.read_config(config)
    if videos is None:
        videos = list(cfg.get("video_sets_original") or cfg["video_sets"])

    for video in videos:
        fc = FrameCropper(video)
        coords = fc.draw_bbox()
        if coords:
            temp = {
                "crop": ", ".join(
                    map(
                        str,
                        [
                            int(coords[0]),
                            int(coords[2]),
                            int(coords[1]),
                            int(coords[3]),
                        ],
                    )
                )
            }
            try:
                cfg["video_sets"][video] = temp
            except KeyError:
                cfg["video_sets_original"][video] = temp

    auxiliaryfunctions.write_config(config, cfg)
    return cfg


class ExtractFrames(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(ExtractFrames, self).__init__(root, parent, h1_description)
        self.worker = None
        self.thread = None
        self._set_page()

    def _set_page(self):
        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self.layout_attributes = _create_grid_layout(margins=(0, 0, 0, 0))
        self._generate_layout_attributes(self.layout_attributes)
        self.main_layout.addLayout(self.layout_attributes)

        self.main_layout.addWidget(
            _create_label_widget(
                "Frame extraction from a video subset (optional for automatic extraction)",
                "font:bold",
            )
        )
        self.video_selection_widget = VideoSelectionWidget(self.root, self)
        self.main_layout.addWidget(self.video_selection_widget)

        self.ok_button = QtWidgets.QPushButton("Extract Frames")
        self.ok_button.clicked.connect(self.extract_frames)
        self.main_layout.addWidget(self.ok_button, alignment=Qt.AlignRight)

        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        label = QtWidgets.QLabel(extract_frames.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _generate_layout_attributes(self, layout):
        layout.setColumnMinimumWidth(1, 300)
        # Extraction method
        ext_method_label = QtWidgets.QLabel("Extraction method")
        self.extraction_method_widget = QtWidgets.QComboBox()
        options = ["automatic", "manual"]
        self.extraction_method_widget.addItems(options)
        self.extraction_method_widget.currentTextChanged.connect(
            self.log_extraction_method
        )

        # Frame extraction algorithm
        ext_algo_label = QtWidgets.QLabel("Extraction algorithm")
        self.extraction_algorithm_widget = QtWidgets.QComboBox()
        self.extraction_algorithm_widget.addItems(DLCParams.FRAME_EXTRACTION_ALGORITHMS)
        self.extraction_algorithm_widget.currentTextChanged.connect(
            self.log_extraction_algorithm
        )

        # Frame cropping
        frame_crop_label = QtWidgets.QLabel("Frame cropping")
        self.frame_cropping_widget = QtWidgets.QComboBox()
        self.frame_cropping_widget.addItems(["disabled", "read from config", "GUI"])
        self.frame_cropping_widget.currentTextChanged.connect(
            self.log_frame_cropping_choice
        )

        # Cluster step
        cluster_step_label = QtWidgets.QLabel("Cluster step")
        self.cluster_step_widget = QtWidgets.QSpinBox()
        self.cluster_step_widget.setValue(1)

        # GUI Slider width
        gui_slider_label = QtWidgets.QLabel("GUI slider width")
        self.slider_width_widget = QtWidgets.QSpinBox()
        self.slider_width_widget.setValue(25)
        self.slider_width_widget.setEnabled(False)

        layout.addWidget(ext_method_label, 1, 0)
        layout.addWidget(self.extraction_method_widget, 1, 1)
        layout.addWidget(gui_slider_label, 1, 2)
        layout.addWidget(self.slider_width_widget, 1, 3)

        layout.addWidget(ext_algo_label, 2, 0)
        layout.addWidget(self.extraction_algorithm_widget, 2, 1)
        layout.addWidget(cluster_step_label, 2, 2)
        layout.addWidget(self.cluster_step_widget, 2, 3)

        layout.addWidget(frame_crop_label, 3, 0)
        layout.addWidget(self.frame_cropping_widget, 3, 1)

    def log_extraction_algorithm(self, extraction_algorithm):
        self.root.logger.info(f"Extraction method set to {extraction_algorithm}")

    def log_extraction_method(self, extraction_method):
        self.root.logger.info(f"Extraction method set to {extraction_method}")
        if extraction_method == "manual":
            self.extraction_algorithm_widget.setEnabled(False)
            self.cluster_step_widget.setEnabled(False)
            self.frame_cropping_widget.setEnabled(False)
            self.slider_width_widget.setEnabled(True)
        else:
            self.extraction_algorithm_widget.setEnabled(True)
            self.cluster_step_widget.setEnabled(True)
            self.frame_cropping_widget.setEnabled(True)
            self.slider_width_widget.setEnabled(False)

    def log_frame_cropping_choice(self, cropping_option):
        self.root.logger.info(f"Cropping set to '{cropping_option}'")

    def extract_frames(self):
        config = self.root.config
        mode = self.extraction_method_widget.currentText()
        if mode == "manual":
            videos = list(self.video_selection_widget.files)
            if not videos:
                QtWidgets.QMessageBox.critical(
                    self,
                    "Error",
                    "Please select exactly one video to extract frames from.",
                )
                return
            first_video = videos[0]
            if len(videos) > 1:
                self.root.writer.write(
                    f"Only the first video ({first_video}) will be opened."
                )
            video_path_in_folder = self._check_symlink(first_video)
            _ = launch_napari(str(video_path_in_folder))
            return

        algo = self.extraction_algorithm_widget.currentText()
        clusterstep = self.cluster_step_widget.value()
        slider_width = self.slider_width_widget.value()

        crop = False  # default value
        if self.frame_cropping_widget.currentText() == "GUI":
            _ = select_cropping_area(config)
            crop = True
        elif self.frame_cropping_widget.currentText() == "read from config":
            crop = True

        func = partial(
            extract_frames,
            config,
            mode,
            algo,
            crop=crop,
            cluster_step=clusterstep,
            cluster_resizewidth=30,
            cluster_color=False,
            slider_width=slider_width,
            userfeedback=False,
            videos_list=self.video_selection_widget.files or None,
        )

        self.worker, self.thread = move_to_separate_thread(func, capture_outputs=True)
        self.worker.finished.connect(lambda: self.ok_button.setEnabled(True))
        self.worker.finished.connect(lambda: self.root._progress_bar.hide())
        self.thread.finished.connect(self._show_success_message)
        self.thread.start()
        self.ok_button.setEnabled(False)
        self.root._progress_bar.show()

    def _show_success_message(self):
        message = "Failed to create worker: it is None"
        root_message = "failed to extract frames: worker is None"
        if self.worker is not None:
            failed = self.worker.outputs
            if failed is None:
                # outputs are None during manual frame extraction
                return

            if len(failed) == 0:
                message = (
                    "Frame extraction failed. Please check your terminal output "
                    "for more information."
                )
            elif all(failed):
                message = "Frame extraction failed. Video files must be corrupted."
            elif any(failed):
                message = "Although most frames were extracted, some were invalid."
                root_message = "failed to extract (some) frames"
            else:
                message = (
                    "Frames were successfully extracted, for the videos of interest."
                )
                root_message = "successfully extracted frames"

        msg = QtWidgets.QMessageBox()
        msg.setIcon(QtWidgets.QMessageBox.Information)
        msg.setText(message)
        msg.setWindowTitle("Info")
        msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
        msg.exec_()
        self.root.writer.write(root_message)

    def _check_symlink(self, video_path: Union[str, Path]) -> Path:
        """Checks that a video is in the DeepLabCut 'videos' folder

        This is required before launching manual frame extraction. When users select
        a symlink of a video using the VideoSelectionWidget, the path is resolved to the
        true path of the video (which leads napari-deeplabcut to save the frames in the
        incorrect folder).

        Args:
            video_path: the path to a video in a DeepLabCut project or a video that was
                added to the project

        Returns:
            the path to the video (or symlink) in the project's 'videos' folder

        Raises:
            FileNotFoundError if there is no symlink or video in the 'videos' folder for
                the given video
        """
        video_path = Path(video_path).resolve()
        project_videos = (Path(self.root.config).parent / "videos").resolve()
        if video_path.parent == project_videos:
            return video_path

        symlink_path = project_videos / video_path.name
        if not symlink_path.exists():
            raise FileNotFoundError(
                f"Could not find the video {video_path.name} in your project videos. "
                f"Did you add the video (you can do so in the 'Manage Project' tab)? "
                f"There should be a file in {symlink_path}."
            )

        return symlink_path


--- File: deeplabcut/gui/tabs/video_editor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import time

from PySide6 import QtWidgets
from PySide6.QtCore import Qt

from deeplabcut.gui.components import (
    DefaultTab,
    VideoSelectionWidget,
    _create_grid_layout,
    _create_label_widget,
)
from deeplabcut.gui.widgets import FrameCropper
from deeplabcut.utils import auxfun_videos


class VideoEditor(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(VideoEditor, self).__init__(root, parent, h1_description)

        self._set_page()

    @property
    def files(self):
        return self.video_selection_widget.files

    def _set_page(self):
        self.main_layout.addWidget(_create_label_widget("Video Selection", "font:bold"))
        self.video_selection_widget = VideoSelectionWidget(self.root, self)
        self.main_layout.addWidget(self.video_selection_widget)

        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self.layout_attributes = _create_grid_layout(margins=(20, 0, 0, 0))
        self._generate_layout_attributes(self.layout_attributes)
        self.main_layout.addLayout(self.layout_attributes)

        self.down_button = QtWidgets.QPushButton("Downsample")
        self.down_button.setMinimumWidth(150)
        self.down_button.clicked.connect(self.downsample_videos)
        self.main_layout.addWidget(self.down_button, alignment=Qt.AlignRight)

        self.rotate_button = QtWidgets.QPushButton("Rotate")
        self.rotate_button.setMinimumWidth(150)
        self.rotate_button.clicked.connect(self.rotate_videos)
        self.main_layout.addWidget(self.rotate_button, alignment=Qt.AlignRight)

        self.trim_button = QtWidgets.QPushButton("Trim")
        self.trim_button.setMinimumWidth(150)
        self.trim_button.clicked.connect(self.trim_videos)
        self.main_layout.addWidget(self.trim_button, alignment=Qt.AlignRight)

        self.crop_button = QtWidgets.QPushButton("Crop")
        self.crop_button.setMinimumWidth(150)
        self.crop_button.clicked.connect(self.crop_videos)
        self.main_layout.addWidget(self.crop_button, alignment=Qt.AlignRight)

    def _generate_layout_attributes(self, layout):
        videoheight_label = QtWidgets.QLabel("Video height (aspect ratio fixed)")
        self.video_height = QtWidgets.QSpinBox()
        self.video_height.setMaximum(1000)
        self.video_height.setValue(256)
        self.video_height.valueChanged.connect(self.log_video_height)

        rotate_label = QtWidgets.QLabel("Rotate video")
        self.video_rotation = QtWidgets.QComboBox()
        self.video_rotation.addItems(["no", "clockwise", "specific angle"])
        self.video_rotation.currentTextChanged.connect(self.update_video_rotation)

        trim_start_label = QtWidgets.QLabel("Trim start (sec)")
        self.video_start = QtWidgets.QSpinBox()
        self.video_start.setMaximum(3600)
        self.video_start.setValue(1)
        self.video_start.valueChanged.connect(self.log_video_start)

        trim_end_label = QtWidgets.QLabel("Trim end (sec)")
        self.video_stop = QtWidgets.QSpinBox()
        self.video_stop.setMaximum(3600)
        self.video_stop.setMinimum(1)
        self.video_stop.setValue(30)
        self.video_stop.valueChanged.connect(self.log_video_stop)

        angle_label = QtWidgets.QLabel("Rotation angle (deg)")
        self.rotation_angle = QtWidgets.QDoubleSpinBox()
        self.rotation_angle.setMaximum(360.0)
        self.rotation_angle.setMinimum(-360.0)
        self.rotation_angle.setDecimals(2)
        self.rotation_angle.setValue(0.0)
        self.rotation_angle.setEnabled(False)
        self.rotation_angle.valueChanged.connect(self.log_rotation_angle)

        downsample_title = QtWidgets.QLabel("Downsample and rotate:")
        trim_title = QtWidgets.QLabel("Shorten video (trim):")

        layout.addWidget(downsample_title, 0, 0)
        layout.addWidget(trim_title, 0, 4)
        layout.addWidget(videoheight_label, 1, 0)
        layout.addWidget(self.video_height, 1, 1)
        layout.addWidget(rotate_label, 1, 2)
        layout.addWidget(self.video_rotation, 1, 3)
        layout.addWidget(angle_label, 2, 2)
        layout.addWidget(self.rotation_angle, 2, 3)
        layout.addWidget(trim_start_label, 1, 4)
        layout.addWidget(self.video_start, 1, 5)
        layout.addWidget(trim_end_label, 2, 4)
        layout.addWidget(self.video_stop, 2, 5)
        # layout.addWidget()

    def update_video_rotation(self, option):
        self.root.logger.info(f"Video rotation set to {option.upper()}")
        if option == "specific angle":
            self.rotation_angle.setEnabled(True)
        else:
            self.rotation_angle.setEnabled(False)

    def log_video_height(self, value):
        self.root.logger.info(f"Video height set to {value}")

    def log_video_start(self, value):
        start_time = time.strftime("%H:%M:%S", time.gmtime(value))
        self.root.logger.info(f"Video start time set to {start_time}")

    def log_video_stop(self, value):
        stop_time = time.strftime("%H:%M:%S", time.gmtime(value))
        self.root.logger.info(f"Video start time set to {stop_time}")

    def log_rotation_angle(self, value):
        self.root.logger.info(f"Rotation angle set to {value}")

    def rotate_videos(self):
        if self.files:
            for video in self.files:
                if self.video_rotation.currentText() == "specific angle":
                    auxfun_videos.rotate_video(
                        video, self.rotation_angle.value(), "Arbitrary"
                    )
                elif self.video_rotation.currentText() == "clockwise":
                    auxfun_videos.rotate_video(
                        video, 0, "Yes"
                    )
        else:
            self.root.logger.error("No videos selected...")


    def trim_videos(self):
        start = time.strftime("%H:%M:%S", time.gmtime(self.video_start.value()))
        stop = time.strftime("%H:%M:%S", time.gmtime(self.video_stop.value()))
        if self.files:
            for video in self.files:
                auxfun_videos.ShortenVideo(video, start, stop)
        else:
            self.root.logger.error("No videos selected...")

    def downsample_videos(self):
        if self.files:
            for video in self.files:
                auxfun_videos.DownSampleVideo(
                    video,
                    width=-1,
                    height=self.video_height.value(),
                    rotatecw=self.video_rotation.currentData(),
                    angle=self.rotation_angle.value(),
                )
        else:
            self.root.logger.error("No videos selected...")

    def crop_videos(self):
        if self.files:
            for video in self.files:
                _ = _crop_video(video)
        else:
            self.root.logger.error("No videos selected...")


def _crop_video(video_path):
    fc = FrameCropper(video_path)
    coords = fc.draw_bbox()
    if not coords:
        return
    origin_x, origin_y = coords[:2]
    width = int(coords[2]) - int(coords[0])
    height = int(coords[3]) - int(coords[1])
    writer = auxfun_videos.VideoWriter(video_path)
    writer.set_bbox(origin_x, origin_x + width, origin_y, origin_y + height)
    return writer.crop("cropped", None)


--- File: deeplabcut/gui/tabs/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.gui.tabs.analyze_videos import AnalyzeVideos
from deeplabcut.gui.tabs.create_project import ProjectCreator
from deeplabcut.gui.tabs.create_training_dataset import CreateTrainingDataset
from deeplabcut.gui.tabs.create_videos import CreateVideos
from deeplabcut.gui.tabs.evaluate_network import EvaluateNetwork
from deeplabcut.gui.tabs.extract_frames import ExtractFrames
from deeplabcut.gui.tabs.extract_outlier_frames import ExtractOutlierFrames
from deeplabcut.gui.tabs.label_frames import LabelFrames
from deeplabcut.gui.tabs.manage_project import ManageProject
from deeplabcut.gui.tabs.modelzoo import ModelZoo
from deeplabcut.gui.tabs.open_project import OpenProject
from deeplabcut.gui.tabs.refine_tracklets import RefineTracklets
from deeplabcut.gui.tabs.train_network import TrainNetwork
from deeplabcut.gui.tabs.unsupervised_id_tracking import UnsupervizedIdTracking
from deeplabcut.gui.tabs.video_editor import VideoEditor


--- File: deeplabcut/gui/tabs/docs.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
BASE_URL = "https://deeplabcut.github.io/DeepLabCut/docs/"
README = "https://deeplabcut.github.io/DeepLabCut/README.html"
URL_3D = BASE_URL + "Overviewof3D.html"
URL_MA_CONFIGURE = BASE_URL + "maDLC_UserGuide.html#configure-the-project"
URL_USE_GUIDE_SCENARIO = BASE_URL + "UseOverviewGuide.html#what-scenario-do-you-have"


--- File: deeplabcut/gui/tabs/refine_tracklets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
from pathlib import Path
from PySide6 import QtWidgets
from PySide6.QtCore import Qt

from deeplabcut.gui.widgets import ConfigEditor
from deeplabcut.gui.components import (
    DefaultTab,
    ShuffleSpinBox,
    VideoSelectionWidget,
    _create_grid_layout,
    _create_horizontal_layout,
    _create_label_widget,
)

import deeplabcut
from deeplabcut.core import trackingutils
from deeplabcut.utils.auxiliaryfunctions import GetScorerName


class RefineTracklets(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(RefineTracklets, self).__init__(root, parent, h1_description)
        self._set_page()

    @property
    def files(self):
        return self.video_selection_widget.files

    def _set_page(self):
        # TODO: Multi video select.... have to change to single video!
        self.main_layout.addWidget(_create_label_widget("Video Selection", "font:bold"))
        self.video_selection_widget = VideoSelectionWidget(self.root, self)
        self.main_layout.addWidget(self.video_selection_widget)

        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self.layout_attributes = _create_horizontal_layout()
        self._generate_layout_attributes(self.layout_attributes)
        self.main_layout.addLayout(self.layout_attributes)

        self.container_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))

        self.layout_refinement_settings = _create_grid_layout(margins=(20, 0, 0, 0))
        self._generate_layout_refinement(self.layout_refinement_settings)
        self.container_layout.addLayout(self.layout_refinement_settings)

        self.layout_filtering_settings = _create_grid_layout(margins=(20, 0, 0, 0))
        self._generate_layout_filtering(self.layout_filtering_settings)
        self.container_layout.addLayout(self.layout_filtering_settings)

        self.main_layout.addLayout(self.container_layout)

        self.stitch_tracklets_btn = QtWidgets.QPushButton("(Re-)run stitching")
        self.stitch_tracklets_btn.setMinimumWidth(150)
        self.stitch_tracklets_btn.clicked.connect(self.create_tracks)

        self.edit_inferencecfg_btn = QtWidgets.QPushButton("Edit inference_cfg.yaml")
        self.edit_inferencecfg_btn.setMinimumWidth(150)
        self.edit_inferencecfg_btn.clicked.connect(self.open_inferencecfg_editor)

        self.filter_tracks_button = QtWidgets.QPushButton("Filter tracks ( + .csv)")
        self.filter_tracks_button.setMinimumWidth(150)
        self.filter_tracks_button.clicked.connect(self.filter_tracks)

        self.launch_button = QtWidgets.QPushButton("Launch track refinement GUI")
        self.launch_button.setMinimumWidth(150)
        self.launch_button.clicked.connect(self.refine_tracks)

        self.merge_button = QtWidgets.QPushButton("Merge dataset")
        self.merge_button.setMinimumWidth(150)
        self.merge_button.clicked.connect(self.merge_dataset)
        self.merge_button.setEnabled(False)

        self.main_layout.addWidget(self.edit_inferencecfg_btn, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.stitch_tracklets_btn, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.launch_button, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.filter_tracks_button, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.merge_button, alignment=Qt.AlignRight)

        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        label = QtWidgets.QLabel(deeplabcut.stitch_tracklets.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _generate_layout_attributes(self, layout):
        # Shuffle
        shuffle_text = QtWidgets.QLabel("Shuffle")
        self.shuffle = ShuffleSpinBox(root=self.root, parent=self)

        # Num animals
        num_animals_text = QtWidgets.QLabel("Number of animals in video")
        self.num_animals_in_videos = QtWidgets.QSpinBox()
        self.num_animals_in_videos.setValue(len(self.root.all_individuals))
        self.num_animals_in_videos.setMaximum(100)
        self.num_animals_in_videos.valueChanged.connect(self.log_num_animals)

        layout.addWidget(shuffle_text)
        layout.addWidget(self.shuffle)
        layout.addWidget(num_animals_text)
        layout.addWidget(self.num_animals_in_videos)

    def _generate_layout_refinement(self, layout):
        section_title = _create_label_widget(
            "Refinement Settings", "font:bold", (0, 50, 0, 0)
        )

        # Min swap length
        swap_length_label = QtWidgets.QLabel("Min swap length to highlight")
        self.swap_length_widget = QtWidgets.QSpinBox()
        self.swap_length_widget.setValue(2)
        self.swap_length_widget.setMinimumWidth(150)
        self.swap_length_widget.valueChanged.connect(self.log_swap_length)

        # Max gap to fill
        max_gap_label = QtWidgets.QLabel("Max gap of missing data to fill")
        self.max_gap_widget = QtWidgets.QSpinBox()
        self.max_gap_widget.setValue(5)
        self.max_gap_widget.setMinimumWidth(150)
        self.max_gap_widget.valueChanged.connect(self.log_max_gap)

        # Trail length
        trail_length_label = QtWidgets.QLabel("Visualization trail length")
        self.trail_length_widget = QtWidgets.QSpinBox()
        self.trail_length_widget.setValue(20)
        self.trail_length_widget.setMinimumWidth(150)
        self.trail_length_widget.valueChanged.connect(self.log_trail_length)

        layout.addWidget(section_title, 0, 0)
        layout.addWidget(swap_length_label, 1, 0)
        layout.addWidget(self.swap_length_widget, 1, 1)
        layout.addWidget(max_gap_label, 2, 0)
        layout.addWidget(self.max_gap_widget, 2, 1)
        layout.addWidget(trail_length_label, 3, 0)
        layout.addWidget(self.trail_length_widget, 3, 1)

    def _generate_layout_filtering(self, layout):
        section_title = _create_label_widget("Filtering", "font:bold", (0, 50, 0, 0))

        # Filter type
        filter_label = QtWidgets.QLabel("Filter type")
        self.filter_type_widget = QtWidgets.QComboBox()
        self.filter_type_widget.setMinimumWidth(150)
        options = ["median"]
        self.filter_type_widget.addItems(options)
        self.filter_type_widget.currentTextChanged.connect(self.log_filter_type)

        # Filter window length
        window_length_label = QtWidgets.QLabel("Window length")
        self.window_length_widget = QtWidgets.QSpinBox()
        self.window_length_widget.setValue(5)
        self.window_length_widget.setMinimumWidth(150)
        self.window_length_widget.valueChanged.connect(self.log_window_length)

        layout.addWidget(section_title, 0, 0)
        layout.addWidget(filter_label, 1, 0)
        layout.addWidget(self.filter_type_widget, 1, 1)
        layout.addWidget(window_length_label, 2, 0)
        layout.addWidget(self.window_length_widget, 2, 1)

    def log_swap_length(self, value):
        self.root.logger.info(f"Swap length set to {value}")

    def log_max_gap(self, value):
        self.root.logger.info(f"Max gap size of missing data to fill set to {value}")

    def log_trail_length(self, value):
        self.root.logger.info(f"Visualization trail length set to {value}")

    def log_filter_type(self, filter_type):
        self.root.logger.info(f"Filter type set to {filter_type.upper()}")

    def log_window_length(self, window_length):
        self.root.logger.info(f"Window length set to {window_length}")

    def log_num_animals(self, num_animals):
        self.root.logger.info(f"Number of animals in video set to {num_animals}")

    def open_inferencecfg_editor(self):
        editor = ConfigEditor(self.root.inference_cfg_path)
        editor.show()

    def create_tracks(self):
        deeplabcut.stitch_tracklets(
            self.root.config,
            self.files,
            videotype=self.video_selection_widget.videotype_widget.currentText(),
            shuffle=self.shuffle.value(),
            n_tracks=self.num_animals_in_videos.value(),
        )

    def filter_tracks(self):
        window_length = self.window_length_widget.value()
        if window_length % 2 != 1:
            raise ValueError("Window length should be odd.")

        videotype = self.video_selection_widget.videotype_widget.currentText()
        deeplabcut.filterpredictions(
            self.root.config,
            self.files,
            videotype=videotype,
            shuffle=self.shuffle.value(),
            filtertype=self.filter_type_widget.currentText(),
            windowlength=self.window_length_widget.value(),
            save_as_csv=True,
        )

    def merge_dataset(self):
        msg = QtWidgets.QMessageBox()
        msg.setIcon(QtWidgets.QMessageBox.Warning)
        msg.setText(
            "Make sure that you have refined all the labels before merging the dataset.If you merge the dataset, you need to re-create the training dataset before you start the training. Are you ready to merge the dataset?"
        )
        msg.setWindowTitle("Warning")
        msg.setStandardButtons(QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
        result = msg.exec_()
        if result == QtWidgets.QMessageBox.Yes:
            deeplabcut.merge_datasets(self.root.config, forceiterate=None)
            self.viz.export_to_training_data()

    def refine_tracks(self):
        cfg = self.root.cfg
        DLCscorer, _ = GetScorerName(
            cfg,
            self.shuffle.value(),
            cfg["TrainingFraction"][-1],
        )
        video = list(self.files)[0]
        track_method = cfg.get("default_track_method", "ellipse")
        method = trackingutils.TRACK_METHODS[track_method]
        dest = str(Path(video).parents[0])
        vname = Path(video).stem
        datafile = os.path.join(dest, vname + DLCscorer + f"{method}.h5")
        self.manager, self.viz = deeplabcut.refine_tracklets(
            self.root.config,
            datafile,
            video,
            min_swap_len=self.swap_length_widget.value(),
            trail_len=self.trail_length_widget.value(),
            max_gap=self.max_gap_widget.value(),
        )
        self.merge_button.setEnabled(True)


--- File: deeplabcut/gui/tabs/train_network.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import os
from dataclasses import dataclass

from PySide6 import QtWidgets
from PySide6.QtCore import Qt, Slot
from PySide6.QtGui import QIcon

import deeplabcut.compat as compat
from deeplabcut.core.engine import Engine
from deeplabcut.gui.components import (
    DefaultTab,
    ShuffleSpinBox,
    SnapshotSelectionWidget,
    _create_grid_layout,
    _create_label_widget,
)
from deeplabcut.gui.displays.selected_shuffle_display import SelectedShuffleDisplay
from deeplabcut.gui.widgets import ConfigEditor


@dataclass
class IntTrainAttribute:
    label: str
    fn_key: str
    default: int
    min: int
    max: int
    tooltip: str | None = None


@dataclass
class TrainAttributeRow:
    attributes: list[IntTrainAttribute]
    description: str | None = None
    show_when_cfg: tuple[str, str] | None = None


class TrainNetwork(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(TrainNetwork, self).__init__(root, parent, h1_description)
        self._shuffle: ShuffleSpinBox = ShuffleSpinBox(root=self.root, parent=self)
        self._shuffle_display = SelectedShuffleDisplay(self.root)

        self._attribute_layouts: dict[Engine, QtWidgets.QWidget] = {}
        self._attribute_kwargs: dict[Engine, dict] = {}
        self._rows_with_requirements: list = []
        self._set_page()

        self.root.engine_change.connect(self._on_engine_change)
        self._shuffle_display.pose_cfg_signal.connect(self._pose_cfg_change)

    @Slot(Engine)
    def _on_engine_change(self, engine: Engine) -> None:
        for e, layout in self._attribute_layouts.items():
            if e == engine:
                layout.show()
            else:
                layout.hide()
        self._update_snapshot_selection_widgets_visibility()

    def _update_snapshot_selection_widgets_visibility(self):
        if self.root.engine == Engine.PYTORCH:
            self.resume_from_snapshot_label.show()
            self.snapshot_selection_widget.show()
            # Display detector snapshot selection widget only if in Top-Down mode
            if self._shuffle_display.pose_cfg.get("method", "").lower() == "td":
                self.detector_snapshot_selection_widget.show()
            else:
                self.detector_snapshot_selection_widget.hide()
        else:
            self.resume_from_snapshot_label.hide()
            self.snapshot_selection_widget.hide()
            self.detector_snapshot_selection_widget.hide()

    def _set_page(self):
        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self._generate_layout_attributes()

        self.resume_from_snapshot_label = _create_label_widget(
            "[Optional]: Select a snapshot to resume training from", "font:bold"
        )
        self.resume_from_snapshot_label.setToolTip(
            "<span style='font-weight:normal; white-space:nowrap;'>"
            "If you've already trained a model on this shuffle, you can continue training it instead of starting "
            "from scratch again. <br>When using top-down models, you can also choose a detector to resume training from."
            "</span>"
        )
        self.main_layout.addWidget(self.resume_from_snapshot_label)

        self.snapshot_selection_widget = SnapshotSelectionWidget(
            self.root, self, margins=(30, 0, 0, 0), select_button_text="Select snapshot"
        )
        self.main_layout.addWidget(self.snapshot_selection_widget)

        self.detector_snapshot_selection_widget = SnapshotSelectionWidget(
            self.root,
            self,
            margins=(30, 0, 0, 0),
            select_button_text="Select detector snapshot",
        )
        self.main_layout.addWidget(self.detector_snapshot_selection_widget)

        self._pose_cfg_change(
            self._shuffle_display.pose_cfg
        )  # also calls _update_snapshot_selection_widgets_visibility

        self.edit_posecfg_btn = QtWidgets.QPushButton("Edit pose_cfg.yaml")
        self.edit_posecfg_btn.setMinimumWidth(150)
        self.edit_posecfg_btn.clicked.connect(self.open_posecfg_editor)

        self.ok_button = QtWidgets.QPushButton("Train Network")
        self.ok_button.setMinimumWidth(150)
        self.ok_button.clicked.connect(self.train_network)

        self.main_layout.addWidget(self.edit_posecfg_btn, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.ok_button, alignment=Qt.AlignRight)

        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        label = QtWidgets.QLabel(compat.train_network.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _generate_layout_attributes(self) -> None:
        row_margin = 25

        # top layout
        shuffle_label = QtWidgets.QLabel("Shuffle")
        shuffle_label.setStyleSheet(f"margin: 0px 0px {row_margin}px 0px")
        self._shuffle.setStyleSheet(f"margin: 0px 0px {row_margin}px 0px")
        self._shuffle_display.setStyleSheet(f"margin: 0px 0px {row_margin}px 0px")

        base_layout = _create_grid_layout(margins=(20, 0, 0, 0))
        base_layout.addWidget(shuffle_label, 0, 0)
        base_layout.addWidget(self._shuffle, 0, 1)
        base_layout.addWidget(self._shuffle_display, 0, 2)
        base_layout_widget = QtWidgets.QWidget()
        base_layout_widget.setLayout(base_layout)
        self.main_layout.addWidget(base_layout_widget)

        for engine in Engine:
            train_attributes = get_train_attributes(engine)

            # Other parameters
            param_layout = _create_grid_layout(margins=(20, 0, 0, 0))
            param_layout.setVerticalSpacing(0)

            self._attribute_kwargs[engine] = {}
            row_index = 1
            for row in train_attributes:
                row_elements = []
                if row.description is not None:
                    row_label = QtWidgets.QLabel(row.description)
                    row_label.setStyleSheet("font-weight: bold")
                    row_elements.append(row_label)
                    param_layout.addWidget(row_label, row_index, 0)
                    row_index += 1

                for j, attribute in enumerate(row.attributes):
                    label = QtWidgets.QLabel(attribute.label)
                    spin_box = QtWidgets.QSpinBox()
                    spin_box.setMinimum(attribute.min)
                    spin_box.setMaximum(attribute.max)
                    spin_box.setValue(attribute.default)
                    spin_box.valueChanged.connect(
                        lambda new_val: self.log_attribute_change(attribute, new_val)
                    )
                    self._attribute_kwargs[engine][attribute.fn_key] = spin_box

                    # Pad below to create spacing with other rows
                    label.setStyleSheet(f"margin: 0px 0px {row_margin}px 0px")
                    spin_box.setStyleSheet(f"margin: 0px 0px {row_margin}px 0px")

                    row_elements.append(label)
                    row_elements.append(spin_box)

                    param_layout.addWidget(label, row_index, 2 * j)
                    param_layout.addWidget(spin_box, row_index, 2 * j + 1)

                if row.show_when_cfg is not None:
                    self._rows_with_requirements.append(
                        (row.show_when_cfg, row_elements)
                    )

                row_index += 1

            layout_widget = QtWidgets.QWidget()
            layout_widget.setLayout(param_layout)
            self._attribute_layouts[engine] = layout_widget
            if engine != self.root.engine:
                layout_widget.hide()

            self.main_layout.addWidget(layout_widget)

    def log_attribute_change(self, attribute: IntTrainAttribute, value: int) -> None:
        self.root.logger.info(f"{attribute.label} set to {value}")

    def open_posecfg_editor(self):
        editor = ConfigEditor(self.root.pose_cfg_path)
        editor.show()

    def train_network(self):
        config = self.root.config
        shuffle = int(self._shuffle.value())

        kwargs = dict(gputouse=None, autotune=False)
        for k, spin_box in self._attribute_kwargs[self.root.engine].items():
            kwargs[k] = int(spin_box.value())
        if self.root.engine == Engine.PYTORCH:
            snapshot_to_start_training_from = (
                self.snapshot_selection_widget.selected_snapshot
            )
            if snapshot_to_start_training_from is not None:
                kwargs["snapshot_path"] = snapshot_to_start_training_from
            detector_to_start_training_from = (
                self.detector_snapshot_selection_widget.selected_snapshot
            )
            if detector_to_start_training_from is not None:
                kwargs["detector_path"] = detector_to_start_training_from

        compat.train_network(config, shuffle, **kwargs)
        msg = QtWidgets.QMessageBox()
        msg.setIcon(QtWidgets.QMessageBox.Information)
        msg.setText("The network is now trained and ready to evaluate.")
        msg.setInformativeText(
            "Use the function 'evaluate_network' to evaluate the network."
        )

        msg.setWindowTitle("Info")
        msg.setMinimumWidth(900)
        self.logo_dir = os.path.dirname(os.path.realpath("logo.png")) + os.path.sep
        self.logo = self.logo_dir + "/assets/logo.png"
        msg.setWindowIcon(QIcon(self.logo))
        msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
        msg.exec_()

    @Slot(dict)
    def _pose_cfg_change(self, pose_cfg: dict | None) -> None:
        if pose_cfg is None:
            return

        for requirement, widgets in self._rows_with_requirements:
            key, value = requirement
            show = pose_cfg.get(key) == value
            for w in widgets:
                if show:
                    w.show()
                else:
                    w.hide()

        self._update_snapshot_selection_widgets_visibility()


def get_train_attributes(engine: Engine) -> list[TrainAttributeRow]:
    if engine == Engine.TF:
        return [
            TrainAttributeRow(
                attributes=[
                    IntTrainAttribute(
                        label="Display iterations",
                        fn_key="displayiters",
                        default=1000,
                        min=1,
                        max=1000,
                    ),
                    IntTrainAttribute(
                        label="Number of snapshots to keep",
                        fn_key="max_snapshots_to_keep",
                        default=5,
                        min=1,
                        max=100,
                    ),
                ],
            ),
            TrainAttributeRow(
                attributes=[
                    IntTrainAttribute(
                        label="Maximum iterations",
                        fn_key="maxiters",
                        default=100_000,
                        min=1,
                        max=1_030_000,
                    ),
                    IntTrainAttribute(
                        label="Save iterations",
                        fn_key="saveiters",
                        default=50_000,
                        min=1,
                        max=50_000,
                    ),
                ],
            ),
        ]
    elif engine == Engine.PYTORCH:
        return [
            TrainAttributeRow(
                attributes=[
                    IntTrainAttribute(
                        label="Display iterations",
                        fn_key="displayiters",
                        default=1_000,
                        min=1,
                        max=100_000,
                    ),
                    IntTrainAttribute(
                        label="Number of snapshots to keep",
                        fn_key="max_snapshots_to_keep",
                        default=5,
                        min=1,
                        max=100,
                    ),
                ],
            ),
            TrainAttributeRow(
                attributes=[
                    IntTrainAttribute(
                        label="Maximum epochs",
                        fn_key="epochs",
                        default=200,
                        min=1,
                        max=1000,
                    ),
                    IntTrainAttribute(
                        label="Save epochs",
                        fn_key="save_epochs",
                        default=50,
                        min=1,
                        max=250,
                    ),
                ],
            ),
            TrainAttributeRow(
                description="Detector parameters",
                show_when_cfg=("method", "td"),
                attributes=[
                    IntTrainAttribute(
                        label="Detector max epochs",
                        fn_key="detector_epochs",
                        default=200,
                        min=0,
                        max=1000,
                        tooltip="",
                    ),
                    IntTrainAttribute(
                        label="Detector save epochs",
                        fn_key="detector_save_epochs",
                        default=50,
                        min=1,
                        max=250,
                        tooltip="",
                    ),
                ],
            ),
        ]

    raise NotImplementedError(f"Unknown engine: {engine}")


--- File: deeplabcut/gui/tabs/label_frames.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import os
from pathlib import Path

from PySide6 import QtWidgets
from PySide6.QtCore import Qt

from deeplabcut.generate_training_dataset import check_labels
from deeplabcut.gui.components import DefaultTab
from deeplabcut.gui.widgets import launch_napari
from deeplabcut.utils.skeleton import SkeletonBuilder


def label_frames(
    config_path: str | Path | None = None,
    image_folder: str | None = None
):
    """Launches the napari-deeplabcut labelling GUI.

    For more information on labelling data with napari-deeplabcut, see our docs:
        https://github.com/DeepLabCut/napari-deeplabcut?tab=readme-ov-file#usage

    If no parameters are given, the napari-deeplabcut labelling GUI is simply open,
    and the folder containing the images to label can be dropped into the GUI.

    If the `config_path` and the `image_folder` are given as arguments, the given
    `image_folder` for the project is opened in the napari-deeplabcut GUI to be labeled.
    If only the `config_path` is given, the first image folder is opened.

    Parameters
    ----------
    config_path: str, Path, None
        Full path of the project config.yaml file.

    image_folder: str, None
        Name of the image folder to open for labelling.

    Examples
    --------
    Opening the napari-deeplabcut annotation GUI without opening a specific folder of
    images to label. You then need to drag-and-drop your image folder into the GUI.
    See the napari-deeplabcut docs linked above for more information about labelling in
    napari-deeplabcut.
    >>> import deeplabcut
    >>> deeplabcut.label_frames()

    Opening the images extracted from the "2025-01-01-experiment7" video in
    napari-deeplabcut on Windows. The project's folder structure should look as follows:
    reaching-task/                    # project root directory
    ├── config.yaml                   # project configuration file
    └── labeled-data/                 # folder containing all extracted image folders
        ├── ...
        ├── 2025-01-01-experiment7    # folder containing the images to label
        └── ...
    >>> deeplabcut.label_frames(
    >>>     "C:\\myproject\\reaching-task\\config.yaml",
    >>>     "2025-01-01-experiment7",
    >>> )

    Opening the images extracted from the first video listed in the project
    configuration in napari-deeplabcut on a Unix system.
    >>> deeplabcut.label_frames("/users/john/project/config.yaml")
    """
    files = None
    if config_path is None:
        if image_folder is not None:
            raise ValueError(
                f"If the ``config_path`` is None, the ``image_folder`` must be None "
                f"too. Found {image_folder}. To label the images in {image_folder}, "
                f"give the project configuration file as `config_path`."
            )
    else:
        data_dir = Path(config_path).parent / "labeled-data"
        if image_folder is None:
            image_dirs = [path for path in data_dir.iterdir() if path.is_dir()]
            if len(image_dirs) == 0:
                raise ValueError(
                    f"Could not find any image folders in {data_dir}. Please check "
                    f"the config path given to `deeplabcut.label_frames(...)`"
                )
            image_dir = list(sorted(image_dirs))[0]
        else:
            image_dir = data_dir / image_folder

        files = [str(image_dir), str(config_path)]
    _ = launch_napari(files=files)


refine_labels = label_frames


class LabelFrames(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(LabelFrames, self).__init__(root, parent, h1_description)

        self._set_page()

    def _set_page(self):
        self.label_frames_btn = QtWidgets.QPushButton("Label Frames")
        self.label_frames_btn.clicked.connect(self.label_frames)
        self.check_labels_btn = QtWidgets.QPushButton("Check Labels")
        self.check_labels_btn.clicked.connect(self.check_labels)
        self.build_skeleton_btn = QtWidgets.QPushButton("Build skeleton")
        self.build_skeleton_btn.clicked.connect(self.build_skeleton)
        self.main_layout.addWidget(self.label_frames_btn, alignment=Qt.AlignLeft)
        self.main_layout.addWidget(self.check_labels_btn, alignment=Qt.AlignLeft)
        self.main_layout.addWidget(self.build_skeleton_btn, alignment=Qt.AlignLeft)

    def log_color_by_option(self, choice):
        self.root.logger.info(f"Labeled images will by colored by {choice.upper()}")

    def label_frames(self):
        dialog = QtWidgets.QFileDialog(self)
        dialog.setFileMode(QtWidgets.QFileDialog.Directory)
        dialog.setViewMode(QtWidgets.QFileDialog.Detail)
        dialog.setDirectory(
            os.path.join(os.path.dirname(self.root.config), "labeled-data")
        )
        if dialog.exec_():
            folder = dialog.selectedFiles()[0]
            has_h5 = False
            for file in os.listdir(folder):
                if file.endswith(".h5"):
                    has_h5 = True
                    break
            if not has_h5:
                folder = [folder, self.root.config]
            _ = launch_napari(folder)

    def check_labels(self):
        check_labels(self.root.config, visualizeindividuals=self.root.is_multianimal)
        labeled_images = (Path(self.root.config).parent / "labeled-data").rglob("*_labeled/*.png")
        _ = launch_napari(labeled_images, plugin="napari", stack=True)

    def build_skeleton(self, *args):
        SkeletonBuilder(self.root.config)

--- File: deeplabcut/gui/tabs/evaluate_network.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import os
import matplotlib.image as mpimg
from matplotlib.backends.backend_qt5agg import (
    FigureCanvasQTAgg as FigureCanvas,
)
from matplotlib.figure import Figure
from pathlib import Path
from PySide6 import QtWidgets
from PySide6.QtCore import Qt, Slot

import deeplabcut
from deeplabcut.core.engine import Engine
from deeplabcut.gui.displays.selected_shuffle_display import SelectedShuffleDisplay
from deeplabcut.gui.components import (
    BodypartListWidget,
    DefaultTab,
    ShuffleSpinBox,
    _create_horizontal_layout,
    _create_label_widget,
    _create_vertical_layout,
)
from deeplabcut.gui.widgets import ConfigEditor, launch_napari
from deeplabcut.utils import auxiliaryfunctions


class GridCanvas(QtWidgets.QDialog):
    def __init__(self, image_paths, parent=None):
        super().__init__(parent)
        self.image_paths = image_paths
        layout = QtWidgets.QVBoxLayout(self)
        self.figure = Figure()
        self.figure.patch.set_facecolor("None")
        self.grid = self.figure.add_gridspec(3, 3)
        self.canvas = FigureCanvas(self.figure)
        layout.addWidget(self.canvas)

        for image_path, gridspec in zip(image_paths[:9], self.grid):
            ax = self.figure.add_subplot(gridspec)
            ax.set_axis_off()
            img = mpimg.imread(image_path)
            ax.imshow(img)


class EvaluateNetwork(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(EvaluateNetwork, self).__init__(root, parent, h1_description)

        self.bodyparts_to_use = self.root.all_bodyparts

        self._set_page()

    def _set_page(self):
        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self.layout_attributes = _create_horizontal_layout()
        self._generate_layout_attributes(self.layout_attributes)
        self.main_layout.addLayout(self.layout_attributes)

        self.main_layout.addWidget(_create_label_widget(""))  # dummy text
        self.layout_additional_attributes = _create_vertical_layout()
        self._generate_additional_attributes(self.layout_additional_attributes)
        self.main_layout.addLayout(self.layout_additional_attributes)

        self.ev_nw_button = QtWidgets.QPushButton("Evaluate Network")
        self.ev_nw_button.setMinimumWidth(150)
        self.ev_nw_button.clicked.connect(self.evaluate_network)

        self.opt_button = QtWidgets.QPushButton("Plot 3 test maps")
        self.opt_button.setMinimumWidth(150)
        self.opt_button.clicked.connect(self.plot_maps)

        self.edit_inferencecfg_btn = QtWidgets.QPushButton("Edit inference_cfg.yaml")
        self.edit_inferencecfg_btn.setMinimumWidth(150)
        self.edit_inferencecfg_btn.clicked.connect(self.open_inferencecfg_editor)

        if self.root.is_multianimal:
            self.main_layout.addWidget(
                self.edit_inferencecfg_btn, alignment=Qt.AlignRight
            )

        self.main_layout.addWidget(self.ev_nw_button, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.opt_button, alignment=Qt.AlignRight)

        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

        self.root.engine_change.connect(self._on_engine_change)
        self._on_engine_change(self.root.engine)

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        label = QtWidgets.QLabel(deeplabcut.evaluate_network.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _generate_layout_attributes(self, layout):
        opt_text = QtWidgets.QLabel("Shuffle")
        self.shuffle = ShuffleSpinBox(root=self.root, parent=self)
        self.shuffle_display = SelectedShuffleDisplay(self.root, row_margin=0)

        layout.addWidget(opt_text)
        layout.addWidget(self.shuffle)
        layout.addWidget(self.shuffle_display)

    def open_inferencecfg_editor(self):
        editor = ConfigEditor(self.root.inference_cfg_path)
        editor.show()

    def plot_maps(self):
        shuffle = self.root.shuffle_value
        config = self.root.config
        deeplabcut.extract_save_all_maps(config, shuffle=shuffle, Indices=[0, 1, 2])

        # Display all images
        dest_folder = os.path.join(
            self.root.project_folder,
            str(
                auxiliaryfunctions.get_evaluation_folder(
                    self.root.cfg["TrainingFraction"][0], shuffle, self.root.cfg
                )
            ),
            "maps",
        )
        image_paths = [
            os.path.join(dest_folder, file)
            for file in os.listdir(dest_folder)
            if file.endswith(".png")
        ]
        canvas = GridCanvas(image_paths, parent=self)
        canvas.show()

    def _generate_additional_attributes(self, layout):
        tmp_layout = _create_horizontal_layout(margins=(0, 0, 0, 0))

        self.plot_predictions = QtWidgets.QCheckBox(
            "Plot predictions (as in standard DLC projects)"
        )
        self.plot_predictions.stateChanged.connect(self.update_plot_predictions)

        tmp_layout.addWidget(self.plot_predictions)

        self.bodyparts_list_widget = BodypartListWidget(root=self.root, parent=self)
        self.use_all_bodyparts = QtWidgets.QCheckBox("Compare all bodyparts")
        self.use_all_bodyparts.stateChanged.connect(self.update_bodypart_choice)
        self.use_all_bodyparts.setCheckState(Qt.Checked)

        tmp_layout.addWidget(self.use_all_bodyparts)
        layout.addLayout(tmp_layout)

        layout.addWidget(self.bodyparts_list_widget, alignment=Qt.AlignLeft)

    def update_map_choice(self, state):
        if Qt.CheckState(state) == Qt.Checked:
            self.root.logger.info("Plot scoremaps ENABLED")
        else:
            self.root.logger.info("Plot predictions DISABLED")

    def update_plot_predictions(self, s):
        if Qt.CheckState(s) == Qt.Checked:
            self.root.logger.info("Plot predictions ENABLED")
        else:
            self.root.logger.info("Plot predictions DISABLED")

    def update_bodypart_choice(self, s):
        if Qt.CheckState(s) == Qt.Checked:
            self.bodyparts_list_widget.setEnabled(False)
            self.bodyparts_list_widget.hide()
            self.root.logger.info("Use all bodyparts")
        else:
            self.bodyparts_list_widget.setEnabled(True)
            self.bodyparts_list_widget.show()
            self.root.logger.info(
                f"Use selected bodyparts only: {self.bodyparts_list_widget.selected_bodyparts}"
            )

    def evaluate_network(self):
        config = self.root.config
        shuffle = self.root.shuffle_value
        plotting = self.plot_predictions.isChecked()

        bodyparts_to_use = "all"
        if (
            len(self.root.all_bodyparts)
            != len(self.bodyparts_list_widget.selected_bodyparts)
        ) and not self.use_all_bodyparts.isChecked():
            bodyparts_to_use = self.bodyparts_list_widget.selected_bodyparts

        deeplabcut.evaluate_network(
            config,
            Shuffles=[shuffle],
            plotting=plotting,
            show_errors=True,
            comparisonbodyparts=bodyparts_to_use,
        )

        if plotting:
            project_cfg = self.root.cfg
            eval_folder = auxiliaryfunctions.get_evaluation_folder(
                trainFraction=project_cfg["TrainingFraction"][0],
                shuffle=shuffle,
                cfg=project_cfg,
            )
            scorer, _ = auxiliaryfunctions.get_scorer_name(
                cfg=project_cfg,
                shuffle=shuffle,
                trainFraction=project_cfg["TrainingFraction"][0],
            )

            image_dir = (
                Path(self.root.project_folder)
                / eval_folder
                / f"LabeledImages_{scorer}"
            )
            labeled_images = [str(p) for p in image_dir.rglob("*.png")]
            if len(labeled_images) > 0:
                _ = launch_napari(image_dir)

    @Slot(Engine)
    def _on_engine_change(self, engine: Engine) -> None:
        if engine == Engine.PYTORCH:
            self.opt_button.hide()
            return

        self.opt_button.show()


--- File: deeplabcut/gui/tabs/extract_outlier_frames.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from PySide6 import QtWidgets
from PySide6.QtCore import Qt

from deeplabcut.gui.dlc_params import DLCParams
from deeplabcut.gui.components import (
    DefaultTab,
    ShuffleSpinBox,
    VideoSelectionWidget,
    _create_horizontal_layout,
    _create_label_widget,
)
from deeplabcut.gui.widgets import launch_napari

import deeplabcut


class ExtractOutlierFrames(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(ExtractOutlierFrames, self).__init__(root, parent, h1_description)
        self.filelist = []

        self._set_page()

    @property
    def files(self):
        return self.video_selection_widget.files

    def _set_page(self):
        self.main_layout.addWidget(_create_label_widget("Video Selection", "font:bold"))
        self.video_selection_widget = VideoSelectionWidget(self.root, self)
        self.main_layout.addWidget(self.video_selection_widget)

        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self.layout_attributes = _create_horizontal_layout()
        self._generate_layout_attributes(self.layout_attributes)

        self._generate_multianimal_options(self.layout_attributes)
        self.main_layout.addLayout(self.layout_attributes)

        self.main_layout.addWidget(
            _create_label_widget("Frame extraction options", "font:bold")
        )
        self.layout_extraction_options = _create_horizontal_layout()
        self._generate_layout_extraction_options(self.layout_extraction_options)
        self.main_layout.addLayout(self.layout_extraction_options)

        self.extract_outlierframes_button = QtWidgets.QPushButton("Extract frames")
        self.extract_outlierframes_button.clicked.connect(self.extract_outlier_frames)
        self.extract_outlierframes_button.setMinimumWidth(150)

        self.label_outliers_button = QtWidgets.QPushButton("Labeling GUI")
        self.label_outliers_button.setEnabled(True)
        self.label_outliers_button.clicked.connect(self.launch_refinement_gui)
        self.label_outliers_button.setMinimumWidth(150)

        self.merge_data_button = QtWidgets.QPushButton("Merge data")
        self.merge_data_button.clicked.connect(self.merge_dataset)
        self.merge_data_button.setMinimumWidth(150)

        self.main_layout.addWidget(
            self.extract_outlierframes_button, alignment=Qt.AlignRight
        )
        self.main_layout.addWidget(self.label_outliers_button, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.merge_data_button, alignment=Qt.AlignRight)

        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        label = QtWidgets.QLabel(deeplabcut.extract_outlier_frames.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _generate_layout_attributes(self, layout):
        # Shuffle
        opt_text = QtWidgets.QLabel("Shuffle")
        self.shuffle = ShuffleSpinBox(root=self.root, parent=self)

        layout.addWidget(opt_text)
        layout.addWidget(self.shuffle)

    def _generate_multianimal_options(self, layout):
        opt_text = QtWidgets.QLabel("Tracking method")
        self.tracker_type_widget = QtWidgets.QComboBox()
        self.tracker_type_widget.addItems(DLCParams.TRACKERS)
        self.tracker_type_widget.currentTextChanged.connect(self.update_tracker_type)

        layout.addWidget(opt_text)
        layout.addWidget(self.tracker_type_widget)
        if not self.root.is_multianimal:
            opt_text.hide()
            self.tracker_type_widget.hide()

    def _generate_layout_extraction_options(self, layout):
        opt_text = QtWidgets.QLabel("Specify the algorithm")
        self.outlier_algorithm_widget = QtWidgets.QComboBox()
        self.outlier_algorithm_widget.addItems(DLCParams.OUTLIER_EXTRACTION_ALGORITHMS)
        self.outlier_algorithm_widget.setMinimumWidth(200)
        self.outlier_algorithm_widget.currentTextChanged.connect(
            self.update_outlier_algorithm
        )

        layout.addWidget(opt_text)
        layout.addWidget(self.outlier_algorithm_widget)

    def update_tracker_type(self, method):
        self.root.logger.info(f"Using {method.upper()} tracker")

    def update_outlier_algorithm(self, algorithm):
        self.root.logger.info(
            f"Using {algorithm.upper()} algorithm for frame extraction"
        )

    def extract_outlier_frames(self):
        config = self.root.config
        shuffle = self.root.shuffle_value
        videos = self.files
        videotype = self.video_selection_widget.videotype_widget.currentText()
        outlieralgorithm = self.outlier_algorithm_widget.currentText()
        track_method = ""
        if self.root.is_multianimal:
            track_method = self.tracker_type_widget.currentText()

        self.root.logger.debug(
            f"""Running extract outlier frames with options:
        config: {config},
        shuffle: {shuffle},
        videos: {videos},
        videotype: {videotype},
        outlier algorithm: {outlieralgorithm},
        track method: {track_method}
        """
        )
        deeplabcut.extract_outlier_frames(
            config=config,
            videos=videos,
            videotype=videotype,
            shuffle=shuffle,
            outlieralgorithm=outlieralgorithm,
            track_method=track_method,
            automatic=True,
        )

    def launch_refinement_gui(self):
        self.merge_data_button.setEnabled(True)
        _ = launch_napari()

    def merge_dataset(self):
        msg = QtWidgets.QMessageBox()
        msg.setIcon(QtWidgets.QMessageBox.Warning)
        msg.setText(
            "Make sure that you have refined all the labels before merging the dataset.If you merge the dataset, you need to re-create the training dataset before you start the training. Are you ready to merge the dataset?"
        )
        msg.setWindowTitle("Warning")
        msg.setStandardButtons(QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
        result = msg.exec_()
        if result == QtWidgets.QMessageBox.Yes:
            deeplabcut.merge_datasets(self.root.config, forceiterate=None)


--- File: deeplabcut/gui/tabs/manage_project.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
from PySide6.QtCore import Qt
from PySide6.QtWidgets import (
    QPushButton,
    QFileDialog,
    QLabel,
    QLineEdit,
)
from deeplabcut.create_project import add_new_videos
from deeplabcut.gui.dlc_params import DLCParams
from deeplabcut.gui.components import DefaultTab, _create_horizontal_layout
from deeplabcut.gui.widgets import ConfigEditor


class ManageProject(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super().__init__(root, parent, h1_description)
        self._set_page()
        self._videos = []

    def _set_page(self):
        # Add config text field and button
        project_config_layout = _create_horizontal_layout()

        cfg_text = QLabel("Active config file:")

        self.cfg_line = QLineEdit()
        self.cfg_line.setText(self.root.config)
        self.cfg_line.textChanged[str].connect(self.root.update_cfg)

        browse_button = QPushButton("Browse")
        browse_button.setMaximumWidth(100)
        browse_button.clicked.connect(self.root._open_project)

        project_config_layout.addWidget(cfg_text)
        project_config_layout.addWidget(self.cfg_line)
        project_config_layout.addWidget(browse_button)

        self.main_layout.addLayout(project_config_layout)

        self.edit_btn = QPushButton("Edit config.yaml")
        self.edit_btn.setMinimumWidth(150)
        self.edit_btn.clicked.connect(self.open_config_editor)

        self.add_videos_btn = QPushButton("Add new videos")
        self.add_videos_btn.clicked.connect(self.add_new_videos)

        self.main_layout.addWidget(self.edit_btn, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.add_videos_btn, alignment=Qt.AlignRight)

    def open_config_editor(self):
        editor = ConfigEditor(self.root.config)
        editor.show()

    def add_new_videos(self):
        cwd = os.getcwd()
        files = QFileDialog.getOpenFileNames(
            self,
            "Select videos to add to the project",
            cwd,
            f"Videos ({' *.'.join(DLCParams.VIDEOTYPES)[1:]})",
        )[0]
        if not files:
            return

        add_new_videos(self.root.config, files)


--- File: deeplabcut/gui/tabs/create_project.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
from datetime import datetime

from PySide6 import QtCore, QtWidgets
from PySide6.QtGui import QBrush, QColor, QDesktopServices, QIcon, QPainter, QPen

import deeplabcut
from deeplabcut.gui import BASE_DIR
from deeplabcut.gui.dlc_params import DLCParams
from deeplabcut.gui.widgets import ClickableLabel, ItemSelectionFrame
from deeplabcut.gui.tabs.docs import (
    URL_3D,
    URL_MA_CONFIGURE,
    URL_USE_GUIDE_SCENARIO,
)
from deeplabcut.utils import auxiliaryfunctions


class DynamicTextList(QtWidgets.QWidget):
    """Dynamically add text entries"""

    def __init__(self, label_text="bodyparts", parent=None):
        super(DynamicTextList, self).__init__(parent)
        self.label_text = label_text
        self.layout = QtWidgets.QVBoxLayout(self)
        self.layout.setContentsMargins(0, 0, 0, 0)

        # Set maximum width for the widget
        self.setMaximumWidth(300)

        # Add explanatory label
        label = QtWidgets.QLabel(label_text)
        self.layout.addWidget(label)

        # Create scroll area and its widget
        self.scroll = QtWidgets.QScrollArea()
        self.scroll.setWidgetResizable(True)
        self.scroll.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOff)
        self.scroll.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAsNeeded)
        self.scroll.setFrameShape(QtWidgets.QFrame.NoFrame)  # Remove frame border

        # Create widget to hold the entries
        self.entries_widget = QtWidgets.QWidget()
        self.entries_layout = QtWidgets.QVBoxLayout(self.entries_widget)
        self.entries_layout.setContentsMargins(0, 0, 0, 0)
        self.entries_layout.setSpacing(5)  # Consistent spacing between entries
        self.entries_layout.setAlignment(QtCore.Qt.AlignTop)  # Align entries to top

        # Add stretch at the bottom to keep entries at top
        self.entries_layout.addStretch()

        self.scroll.setWidget(self.entries_widget)

        # Set fixed height for 6 items
        self.entry_height = 30  # Fixed height for each entry
        self.padding = 10  # Extra padding
        self.scroll.setFixedHeight(5 * self.entry_height + self.padding)

        # Add scroll area to main layout
        self.layout.addWidget(self.scroll)

        self.entries = []
        self.add_entry()

    def add_entry(self):
        # Create horizontal layout for index and entry
        entry_layout = QtWidgets.QHBoxLayout()
        entry_layout.setContentsMargins(0, 0, 10, 0)
        entry_layout.setSpacing(5)  # Consistent spacing between index and entry

        # Create container widget for the entry row
        entry_widget = QtWidgets.QWidget()
        entry_widget.setFixedHeight(self.entry_height)
        entry_widget.setLayout(entry_layout)

        # Add index label
        index_label = QtWidgets.QLabel(str(len(self.entries) + 1) + ".")
        index_label.setFixedWidth(20)  # Set fixed width for alignment
        entry_layout.addWidget(index_label)

        # Add text entry
        entry = QtWidgets.QLineEdit()
        entry.setFixedHeight(self.entry_height - 6)  # Slightly smaller than container
        entry.textChanged.connect(self._on_text_changed)
        entry.textEdited.connect(lambda text: self._check_for_spaces(entry, text))
        self.entries.append((entry, index_label))  # Store both widgets
        entry_layout.addWidget(entry)

        # Insert the new entry before the stretch
        self.entries_layout.insertWidget(len(self.entries) - 1, entry_widget)

    def _check_for_spaces(self, entry, text):
        if " " in text:
            msg = QtWidgets.QMessageBox()
            msg.setIcon(QtWidgets.QMessageBox.Warning)
            msg.setText(
                f"Spaces are not allowed in the {self.label_text} list. Use underscores "
                f"instead."
            )
            msg.setWindowTitle("Warning")
            msg.exec_()
            entry.setText(entry.text().replace(" ", "_"))

    def _on_text_changed(self):
        # If the last entry has text, add a new empty entry
        if self.entries[-1][0].text():
            self.add_entry()

        # Remove any empty entries except the last one
        entries_to_remove = []
        for i, (entry, _) in enumerate(self.entries[:-1]):
            if not entry.text():
                entries_to_remove.append(i)

        for i in reversed(entries_to_remove):
            entry_widget = self.entries[i][0].parent()
            self.entries_layout.removeWidget(entry_widget)
            entry_widget.deleteLater()
            self.entries.pop(i)

        self._update_indices()  # Update the indices after removal

    def get_entries(self):
        return [entry[0].text() for entry in self.entries if entry[0].text()]

    def _update_indices(self):
        for i, (entry, index_label) in enumerate(self.entries):
            index_label.setText(str(i + 1) + ".")


class Switch(QtWidgets.QPushButton):

    def __init__(self, on_text="Yes", off_text="No", width=80, parent=None):
        super().__init__(parent)
        self.on_text = on_text
        self.off_text = off_text
        self.setCheckable(True)
        self.setFixedWidth(width)
        self.setMinimumHeight(22)

    def paintEvent(self, event):
        # Colors: https://qdarkstylesheet.readthedocs.io/en/latest/color_reference.html
        label = self.on_text if self.isChecked() else self.off_text
        bg_color = "#00ff00" if self.isChecked() else "#9DA9B5"

        radius = 10
        width = 32
        center = self.rect().center()

        painter = QPainter(self)
        painter.setRenderHint(QPainter.Antialiasing)
        painter.translate(center)
        painter.setBrush(QColor(69, 83, 100))  # Lighter gray background

        pen = QPen("#455364")
        pen.setWidth(2)
        painter.setPen(pen)

        painter.drawRoundedRect(
            QtCore.QRect(-width, -radius, 2 * width, 2 * radius), radius, radius
        )
        painter.setBrush(QBrush(bg_color))
        sw_rect = QtCore.QRect(-radius, -radius, width + radius, 2 * radius)
        if not self.isChecked():
            sw_rect.moveLeft(-width)

        painter.drawRoundedRect(sw_rect, radius, radius)

        pen = QPen("#000000")
        pen.setWidth(2)
        painter.setPen(pen)
        painter.drawText(sw_rect, QtCore.Qt.AlignCenter, label)


class ProjectCreator(QtWidgets.QDialog):
    """Project creation dialog"""

    def __init__(self, parent):
        super(ProjectCreator, self).__init__(parent)
        self.parent = parent
        self.setWindowTitle("New Project")
        self.setModal(True)
        self.setMinimumWidth(parent.screen_width // 2)
        today = datetime.today().strftime("%Y-%m-%d")
        self.name_default = "-".join(("{}", "{}", today))
        self.proj_default = ""
        self.exp_default = ""
        self.loc_default = parent.project_folder

        self.bodypart_list = None
        self.individuals_list = None
        self.unique_bodyparts_list = None

        self.toggle_3d = Switch()
        self.toggle_3d.setChecked(False)
        self.madlc_toggle = Switch()
        self.madlc_toggle.setChecked(False)
        self.unique_toggle = Switch()
        self.unique_toggle.setChecked(False)
        self.identity_toggle = Switch()
        self.identity_toggle.setChecked(False)

        main_layout = QtWidgets.QVBoxLayout(self)
        self.user_frame = self.lay_out_user_frame()
        self.video_frame = self.lay_out_video_frame()
        self.create_button = QtWidgets.QPushButton("Create")
        self.create_button.setDefault(True)
        self.create_button.clicked.connect(self.finalize_project)
        main_layout.addWidget(self.user_frame)
        main_layout.addWidget(self.video_frame)
        main_layout.addWidget(self.create_button, alignment=QtCore.Qt.AlignRight)

    def lay_out_user_frame(self):
        user_frame = QtWidgets.QFrame(self)
        user_frame.setFrameShape(user_frame.Shape.StyledPanel)
        user_frame.setLineWidth(0)

        proj_label = QtWidgets.QLabel("Project:", user_frame)
        self.proj_line = QtWidgets.QLineEdit(self.proj_default, user_frame)
        self.proj_line.setPlaceholderText("my project's name")
        self._default_style = self.proj_line.styleSheet()
        self.proj_line.textEdited.connect(self.update_project_name)

        exp_label = QtWidgets.QLabel("Experimenter:", user_frame)
        self.exp_line = QtWidgets.QLineEdit(self.exp_default, user_frame)
        self.exp_line.setPlaceholderText("my nickname")
        self.exp_line.textEdited.connect(self.update_experimenter_name)

        loc_label = ClickableLabel("Location:", parent=user_frame)
        loc_label.signal.connect(self.on_click)
        self.loc_line = QtWidgets.QLineEdit(self.loc_default, user_frame)
        self.loc_line.setReadOnly(True)
        action = self.loc_line.addAction(
            QIcon(os.path.join(BASE_DIR, "assets", "icons", "open2.png")),
            QtWidgets.QLineEdit.TrailingPosition,
        )
        action.triggered.connect(self.on_click)

        vbox = QtWidgets.QVBoxLayout(user_frame)
        grid = QtWidgets.QGridLayout()
        grid.addWidget(proj_label, 0, 0)
        grid.addWidget(self.proj_line, 0, 1)
        grid.addWidget(exp_label, 1, 0)
        grid.addWidget(self.exp_line, 1, 1)
        grid.addWidget(loc_label, 2, 0)
        grid.addWidget(self.loc_line, 2, 1)
        vbox.addLayout(grid)

        widget_3d = self.build_toggle_widget(
            switch=self.toggle_3d,
            question="Do you want to create a 3D pose estimation project?",
            help_text="(What is needed for a 3D project?)",
            docs_link=URL_3D,
        )
        madlc_widget = self.build_toggle_widget(
            switch=self.madlc_toggle,
            question="Are there multiple individuals in your videos?",
            help_text="(Why does this matter?)",
            docs_link=URL_USE_GUIDE_SCENARIO,
        )

        # Only visible when the maDLC widget is checked
        unique_widget = self.build_toggle_widget(
            switch=self.unique_toggle,
            question="Do you have unique bodyparts in your video?",
            help_text="(What are unique bodyparts?)",
            docs_link=URL_MA_CONFIGURE,
        )
        unique_widget.setVisible(False)

        # Labelling with identity
        identity_widget = self.build_toggle_widget(
            switch=self.identity_toggle,
            question="Label with identity?",
            help_text="(What is labeling with identity?)",
            docs_link=URL_MA_CONFIGURE,
        )
        identity_widget.setVisible(False)

        vbox.addWidget(widget_3d, alignment=QtCore.Qt.AlignTop)
        vbox.addWidget(madlc_widget, alignment=QtCore.Qt.AlignTop)
        vbox.addWidget(unique_widget, alignment=QtCore.Qt.AlignTop)
        vbox.addWidget(identity_widget, alignment=QtCore.Qt.AlignTop)

        # Create horizontal layout for the two lists
        lists_layout = QtWidgets.QHBoxLayout()
        lists_layout.setAlignment(QtCore.Qt.AlignTop)

        # Create both DynamicTextList widgets as class attributes
        self.bodypart_list = DynamicTextList(
            label_text="Bodyparts to track",
            parent=self,
        )

        self.individuals_list = DynamicTextList(
            label_text="Individual names",
            parent=self,
        )
        self.individuals_list.setVisible(False)

        self.unique_bodyparts_list = DynamicTextList(
            label_text="Unique bodyparts to track",
            parent=self,
        )
        self.unique_bodyparts_list.setVisible(False)

        # Connect toggle state to individuals list visibility, unique, identity
        self.madlc_toggle.toggled.connect(self.individuals_list.setVisible)
        self.madlc_toggle.toggled.connect(unique_widget.setVisible)
        self.madlc_toggle.toggled.connect(identity_widget.setVisible)

        # Connect the unique_toggle to the unique_bodyparts_list
        self.unique_toggle.toggled.connect(
            lambda yes: self.unique_bodyparts_list.setVisible(
                yes and self.madlc_toggle.isChecked()
            )
        )

        # Connect 3d toggle to all other option visibility
        self.toggle_3d.toggled.connect(lambda yes: madlc_widget.setVisible(not yes))
        self.toggle_3d.toggled.connect(
            lambda checked_3d: unique_widget.setVisible(
                not checked_3d and self.madlc_toggle.isChecked()
            )
        )
        self.toggle_3d.toggled.connect(
            lambda checked_3d: identity_widget.setVisible(
                not checked_3d and self.madlc_toggle.isChecked()
            )
        )
        self.toggle_3d.toggled.connect(
            lambda checked_3d: self.bodypart_list.setVisible(not checked_3d)
        )
        self.toggle_3d.toggled.connect(
            lambda checked_3d: self.individuals_list.setVisible(
                not checked_3d and self.madlc_toggle.isChecked()
            )
        )
        self.toggle_3d.toggled.connect(
            lambda checked_3d: self.unique_bodyparts_list.setVisible(
                not checked_3d
                and self.madlc_toggle.isChecked()
                and self.unique_toggle.isChecked()
            )
        )

        # Add both lists to the horizontal layout with top alignment
        lists_layout.addWidget(self.bodypart_list, alignment=QtCore.Qt.AlignTop)
        lists_layout.addWidget(self.individuals_list, alignment=QtCore.Qt.AlignTop)
        lists_layout.addWidget(self.unique_bodyparts_list, alignment=QtCore.Qt.AlignTop)

        # Add the horizontal layout to the main vertical layout
        vbox.addLayout(lists_layout)
        return user_frame

    def build_toggle_widget(
        self,
        switch: Switch,
        question: str,
        help_text: str,
        docs_link: str,
    ) -> QtWidgets.QWidget:
        toggle_layout = QtWidgets.QHBoxLayout()
        toggle_layout.setContentsMargins(0, 0, 0, 0)
        toggle_layout.setSpacing(10)

        toggle_label = QtWidgets.QLabel(question)
        toggle_label.setAlignment(QtCore.Qt.AlignLeft)
        help_label = ClickableLabel(help_text, parent=self)
        help_label.setStyleSheet("text-decoration: underline; font-weight: bold;")
        help_label.setCursor(QtCore.Qt.PointingHandCursor)
        help_label.signal.connect(
            lambda: QDesktopServices.openUrl(QtCore.QUrl(docs_link))
        )

        toggle_layout.addWidget(switch, alignment=QtCore.Qt.AlignLeft)
        toggle_layout.addWidget(toggle_label, alignment=QtCore.Qt.AlignLeft)
        toggle_layout.addStretch()
        toggle_layout.addWidget(help_label, alignment=QtCore.Qt.AlignRight)
        toggle_widget = QtWidgets.QWidget()
        toggle_widget.setLayout(toggle_layout)
        return toggle_widget

    def lay_out_video_frame(self):
        video_frame = ItemSelectionFrame([], self)

        self.copy_box = QtWidgets.QCheckBox("Copy videos to project folder")
        self.copy_box.setChecked(False)

        browse_button = QtWidgets.QPushButton("Browse folders for videos")
        browse_button.clicked.connect(self.browse_videos)
        clear_button = QtWidgets.QPushButton("Clear")
        clear_button.clicked.connect(video_frame.fancy_list.clear)

        layout = QtWidgets.QHBoxLayout()
        layout.addWidget(browse_button)
        layout.addWidget(clear_button)
        video_frame.layout.addLayout(layout)
        video_frame.layout.addWidget(self.copy_box)

        self.toggle_3d.toggled.connect(lambda yes: self.copy_box.setVisible(not yes))
        self.toggle_3d.toggled.connect(lambda yes: browse_button.setVisible(not yes))
        self.toggle_3d.toggled.connect(lambda yes: clear_button.setVisible(not yes))
        self.toggle_3d.toggled.connect(lambda yes: video_frame.setVisible(not yes))
        return video_frame

    def browse_videos(self):
        options = QtWidgets.QFileDialog.Options()
        options |= QtWidgets.QFileDialog.DontUseNativeDialog
        folder = QtWidgets.QFileDialog.getExistingDirectory(
            self,
            "Please select a folder",
            self.loc_default,
            options,
        )
        if not folder:
            return

        for video in auxiliaryfunctions.grab_files_in_folder(
            folder,
            relative=False,
        ):
            if os.path.splitext(video)[1][1:].lower() in DLCParams.VIDEOTYPES[1:]:
                self.video_frame.fancy_list.add_item(video)

    def finalize_project(self):
        fields = [self.proj_line, self.exp_line]
        empty = [i for i, field in enumerate(fields) if not field.text()]
        for i, field in enumerate(fields):
            if i in empty:
                field.setStyleSheet("border: 1px solid red;")
            else:
                field.setStyleSheet(self._default_style)
        if empty:
            return

        create_3d = self.toggle_3d.isChecked()
        try:
            if create_3d:
                _ = deeplabcut.create_new_project_3d(
                    self.proj_default,
                    self.exp_default,
                    2,
                    self.loc_default,
                )
            else:
                videos = list(self.video_frame.selected_items)
                if not len(videos):
                    print("Add at least a video to the project.")
                    self.video_frame.fancy_list.setStyleSheet("border: 1px solid red")
                    return
                else:
                    self.video_frame.fancy_list.setStyleSheet(
                        self.video_frame.fancy_list._default_style
                    )
                to_copy = self.copy_box.isChecked()
                is_madlc = self.madlc_toggle.isChecked()
                config = deeplabcut.create_new_project(
                    self.proj_default,
                    self.exp_default,
                    videos,
                    self.loc_default,
                    to_copy,
                    multianimal=is_madlc,
                )

                if self.bodypart_list is not None:
                    bodypart_key = "bodyparts"
                    updates = {}
                    if is_madlc:
                        bodypart_key = "multianimalbodyparts"
                        if self.individuals_list is not None:
                            individuals = self.individuals_list.get_entries()
                            if len(individuals) > 0:
                                updates["individuals"] = individuals

                        if (
                            self.unique_toggle.isChecked()
                            and self.unique_bodyparts_list is not None
                        ):
                            unique_bodyparts = self.unique_bodyparts_list.get_entries()
                            if len(unique_bodyparts) > 0:
                                updates["uniquebodyparts"] = unique_bodyparts

                        if self.identity_toggle.isChecked():
                            updates["identity"] = True

                    bodyparts = self.bodypart_list.get_entries()
                    if len(bodyparts) > 0:
                        updates[bodypart_key] = bodyparts

                    if len(updates) > 0:
                        cfg: dict = auxiliaryfunctions.read_config(config)
                        cfg.update(**updates)
                        auxiliaryfunctions.write_config(config, cfg)

                self.parent.load_config(config)
                self.parent._update_project_state(config=config, loaded=True)
        except FileExistsError:
            print('Project "{}" already exists!'.format(self.proj_default))
            return

        msg = QtWidgets.QMessageBox(text="New project created")
        msg.setIcon(QtWidgets.QMessageBox.Information)
        msg.exec_()

        self.close()

    def on_click(self):
        dirname = QtWidgets.QFileDialog.getExistingDirectory(
            self, "Please select a folder", self.loc_default
        )
        if not dirname:
            return
        self.loc_default = dirname
        self.update_project_location()

    def update_project_name(self, text):
        self.proj_default = text
        self.update_project_location()

    def update_experimenter_name(self, text):
        self.exp_default = text
        self.update_project_location()

    def update_project_location(self):
        full_name = self.name_default.format(self.proj_default, self.exp_default)
        full_path = os.path.join(self.loc_default, full_name)
        self.loc_line.setText(full_path)


--- File: deeplabcut/gui/tabs/create_training_dataset.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import os
from pathlib import Path

import dlclibrary
from PySide6 import QtWidgets
from PySide6.QtCore import Qt, Slot
from PySide6.QtGui import QIcon

import deeplabcut
import deeplabcut.compat as compat
from deeplabcut.core.engine import Engine
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.generate_training_dataset import get_existing_shuffle_indices
from deeplabcut.generate_training_dataset.metadata import get_shuffle_engine
from deeplabcut.gui.components import (
    DefaultTab,
    ShuffleSpinBox,
    _create_grid_layout,
    _create_label_widget,
)
from deeplabcut.gui.displays.shuffle_metadata_viewer import ShuffleMetadataViewer
from deeplabcut.gui.dlc_params import DLCParams
from deeplabcut.gui.widgets import launch_napari
from deeplabcut.modelzoo import build_weight_init
from deeplabcut.pose_estimation_pytorch import available_models
from deeplabcut.utils.auxiliaryfunctions import (
    get_data_and_metadata_filenames,
    get_training_set_folder,
)


class CreateTrainingDataset(DefaultTab):
    def __init__(self, root, parent, h1_description):
        super(CreateTrainingDataset, self).__init__(root, parent, h1_description)

        self.model_comparison = False

        self.main_layout.addWidget(_create_label_widget("Attributes", "font:bold"))
        self.layout_attributes = _create_grid_layout(margins=(20, 0, 0, 0))
        self._generate_layout_attributes(self.layout_attributes)
        self.main_layout.addLayout(self.layout_attributes)

        self.mapping_button = QtWidgets.QPushButton("Edit Conversion Table")
        self.mapping_button.clicked.connect(self.edit_conversion_table)
        self.mapping_button.setVisible(False)
        self.root.engine_change.connect(self.set_edit_table_visibility)

        self.ok_button = QtWidgets.QPushButton("Create Training Dataset")
        self.ok_button.setMinimumWidth(150)
        self.ok_button.clicked.connect(self.create_training_dataset)

        self.main_layout.addWidget(self.mapping_button, alignment=Qt.AlignRight)
        self.main_layout.addWidget(self.ok_button, alignment=Qt.AlignRight)

        self.view_shuffles_button = QtWidgets.QPushButton("View Existing Shuffles")
        self.view_shuffles_button.clicked.connect(self.view_shuffles)
        self.main_layout.addWidget(self.view_shuffles_button, alignment=Qt.AlignLeft)

        self.help_button = QtWidgets.QPushButton("Help")
        self.help_button.clicked.connect(self.show_help_dialog)
        self.main_layout.addWidget(self.help_button, alignment=Qt.AlignLeft)

    def set_edit_table_visibility(self) -> None:
        has_conversion_tables = bool(
            self.root.cfg.get("SuperAnimalConversionTables", {})
        )
        is_pytorch_engine = self.root.engine == Engine.PYTORCH
        is_finetuning = self.weight_init_selector.with_decoder
        self.mapping_button.setVisible(
            has_conversion_tables & is_pytorch_engine & is_finetuning
        )

    def show_help_dialog(self):
        dialog = QtWidgets.QDialog(self)
        layout = QtWidgets.QVBoxLayout()
        if self.root.is_multianimal:
            func = deeplabcut.create_multianimaltraining_dataset
        else:
            func = deeplabcut.create_training_dataset
        label = QtWidgets.QLabel(func.__doc__, self)
        scroll = QtWidgets.QScrollArea()
        scroll.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOn)
        scroll.setHorizontalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
        scroll.setWidgetResizable(True)
        scroll.setWidget(label)
        layout.addWidget(scroll)
        dialog.setLayout(layout)
        dialog.exec_()

    def _generate_layout_attributes(self, layout):
        layout.setColumnMinimumWidth(3, 300)

        # Shuffle
        shuffle_label = QtWidgets.QLabel("Shuffle")
        self.shuffle = ShuffleSpinBox(root=self.root, parent=self)

        # Dataset choices
        self.weight_init_label = QtWidgets.QLabel("Weight Initialization")
        self.weight_init_selector = WeightInitializationSelector(self.root)
        self.update_weight_init_methods(self.root.engine)
        self.root.engine_change.connect(self.update_weight_init_methods)

        # Augmentation method
        augmentation_label = QtWidgets.QLabel("Augmentation method")
        self.aug_choice = QtWidgets.QComboBox()
        self.update_aug_methods(self.root.engine)
        self.root.engine_change.connect(self.update_aug_methods)
        self.aug_choice.currentTextChanged.connect(self.log_augmentation_choice)

        # Neural Network
        nnet_label = QtWidgets.QLabel("Network architecture")
        self.net_choice = QtWidgets.QComboBox()
        self.net_choice.setMinimumWidth(200)
        self.update_nets(self.root.engine)
        self.root.engine_change.connect(self.update_nets)
        self.net_choice.currentTextChanged.connect(self.log_net_choice)

        # Update Net types when selected weight init changes
        self.weight_init_selector.weight_init_choice.currentTextChanged.connect(
            lambda _: self.update_nets(None)
        )
        self.weight_init_selector.weight_init_choice.currentTextChanged.connect(
            lambda _: self.set_edit_table_visibility()
        )

        # Detector selection for top-down models
        self.detector_label = QtWidgets.QLabel("Detector architecture")
        self.detector_choice = QtWidgets.QComboBox()
        self.detector_choice.setMinimumWidth(200)
        self.update_detectors(engine=self.root.engine)
        self.root.engine_change.connect(
            lambda engine: self.update_detectors(engine=engine)
        )
        self.net_choice.currentTextChanged.connect(
            lambda new_net_choice: self.update_detectors(net_choice=new_net_choice)
        )

        # Overwrite selection
        self.overwrite = QtWidgets.QCheckBox("Overwrite if exists")
        self.overwrite.setChecked(False)
        self.overwrite.setToolTip(
            "When checked, creating a new shuffle with an index that already exists "
            "will overwrite the existing index. Be careful with this option as you "
            "might lose data."
        )
        self.overwrite.stateChanged.connect(
            lambda s: self.root.logger.info(f"Overwrite: {s}")
        )

        # Use same data split as another shuffle
        self.data_split_selection = DataSplitSelector(self.root, self)

        layout.addWidget(shuffle_label, 0, 0)
        layout.addWidget(self.shuffle, 0, 1)
        layout.addWidget(self.weight_init_label, 0, 2)
        layout.addWidget(self.weight_init_selector, 0, 3)

        layout.addWidget(nnet_label, 1, 0)
        layout.addWidget(self.net_choice, 1, 1)
        layout.addWidget(augmentation_label, 1, 2)
        layout.addWidget(self.aug_choice, 1, 3)

        layout.addWidget(self.detector_label, 2, 0)
        layout.addWidget(self.detector_choice, 2, 1)

        layout.addWidget(self.overwrite, 3, 0)
        layout.addWidget(self.data_split_selection, 4, 0)

    def log_net_choice(self, net):
        self.root.logger.info(f"Network architecture set to {net.upper()}")

    def log_augmentation_choice(self, augmentation):
        self.root.logger.info(f"Image augmentation set to {augmentation.upper()}")

    def edit_conversion_table(self):
        # Test beforehand whether a conversion table exists
        memory_replay_folder = Path(self.root.project_folder) / "memory_replay"
        conversion_matrix_out_path = str(memory_replay_folder / "confusion_matrix.png")
        files = [self.root.config]
        if os.path.exists(conversion_matrix_out_path):
            files.append(conversion_matrix_out_path)
        _ = launch_napari(files)

    def create_training_dataset(self):
        shuffle = self.shuffle.value()
        cfg = self.root.cfg
        existing_indices = get_existing_shuffle_indices(
            cfg=cfg, train_fraction=cfg["TrainingFraction"][self.root.trainingset_index]
        )

        overwrite = self.overwrite.isChecked()
        if shuffle in existing_indices:
            if overwrite:
                if not self._confirm_overwrite(shuffle, existing_indices):
                    return
            else:
                msg = _create_message_box(
                    f"The training dataset could not be created.",
                    (
                        f"Shuffle {shuffle} already exists - you can create a new "
                        "training dataset with an unused shuffle index (existing "
                        f"shuffles are {existing_indices}) or you can overwrite the "
                        f"shuffle by ticking the 'Overwrite' checkbox"
                    ),
                )
                msg.exec_()
                self.root.writer.write("Training dataset creation failed.")
                return

        if self.model_comparison:
            raise NotImplementedError
            # TODO: finish model_comparison
            # deeplabcut.create_training_model_comparison(
            #     config_file,
            #     num_shuffles=shuffle,
            #     net_types=self.net_type,
            #     augmenter_types=self.aug_type,
            # )
        else:
            try:
                engine = self.root.engine
                net_type = self.net_choice.currentText()
                detector_type = None
                if engine == Engine.TF:
                    import tensorflow

                    # try importing TF so they can't create shuffles for it if they
                    # don't have it installed
                elif engine == Engine.PYTORCH and "top_down" in net_type:
                    detector_type = self.detector_choice.currentText()

                try:
                    weight_init = (
                        self.weight_init_selector.get_super_animal_weight_init(
                            net_type,
                            detector_type,
                        )
                    )
                except ValueError as err:
                    print(f"The training dataset could not be created: {err}.")
                    return

                if self.data_split_selection.selected:
                    deeplabcut.create_training_dataset_from_existing_split(
                        self.root.config,
                        from_shuffle=self.data_split_selection.from_shuffle,
                        shuffles=[self.shuffle.value()],
                        net_type=net_type,
                        detector_type=detector_type,
                        userfeedback=not overwrite,
                        weight_init=weight_init,
                        engine=engine,
                    )

                elif self.root.is_multianimal:
                    deeplabcut.create_multianimaltraining_dataset(
                        self.root.config,
                        shuffle,
                        Shuffles=[self.shuffle.value()],
                        net_type=net_type,
                        detector_type=detector_type,
                        userfeedback=not overwrite,
                        weight_init=weight_init,
                        engine=engine,
                    )
                else:
                    deeplabcut.create_training_dataset(
                        self.root.config,
                        shuffle,
                        Shuffles=[self.shuffle.value()],
                        net_type=net_type,
                        detector_type=detector_type,
                        augmenter_type=self.aug_choice.currentText(),
                        userfeedback=not overwrite,
                        weight_init=weight_init,
                        engine=engine,
                    )
            except ValueError as err:
                msg = _create_message_box(
                    f"The training dataset could not be created.",
                    str(err),
                )
                msg.exec_()
                return
            except ModuleNotFoundError as err:
                info_text = (
                    f"Error `{err}`. If the error is `ModuleNotFoundError: No module "
                    "named 'tensorflow'`, this is because you tried creating a "
                    "TensorFlow shuffle, but TensorFlow is not installed in your "
                    "environment. To create TensorFlow shuffles (and use TensorFlow "
                    "models), install it with\n"
                    "    Windows/Linux:\n"
                    "      pip install 'deeplabcut[tf]'\n"
                    "    Apple Silicon:\n"
                    "      pip install 'deeplabcut[apple_mchips]'"
                )
                msg = _create_message_box(
                    f"The training dataset could not be created.", info_text
                )
                msg.exec_()
                return

            # Check that training data files were indeed created.
            trainingsetfolder = get_training_set_folder(self.root.cfg)
            filenames = list(
                get_data_and_metadata_filenames(
                    trainingsetfolder,
                    self.root.cfg["TrainingFraction"][0],
                    self.shuffle.value(),
                    self.root.cfg,
                )
            )
            if self.root.is_multianimal:
                filenames[0] = filenames[0].replace("mat", "pickle")
            if all(
                os.path.exists(os.path.join(self.root.project_folder, file))
                for file in filenames
            ):
                self.root.shuffle_created.emit(self.shuffle.value())
                msg = _create_message_box(
                    "The training dataset is successfully created.",
                    "Use the function 'train_network' to start training. Happy training!",
                )
                msg.exec_()
                self.root.writer.write("Training dataset successfully created.")
            else:
                msg = _create_message_box(
                    "The training dataset could not be created.",
                    "Make sure there are annotated data under labeled-data.",
                )
                msg.exec_()
                self.root.writer.write("Training dataset creation failed.")

    def _confirm_overwrite(self, shuffle: int, existing_indices: list[int]) -> bool:
        """
        Asks the user to confirm that they want to overwrite a shuffle.

        Args:
            shuffle: the shuffle the user wants to overwrite
            existing_indices: the indices of existing shuffles

        Returns:
            whether the user confirmed overwriting the shuffle
        """
        try:
            engine = get_shuffle_engine(
                self.root.cfg, self.root.trainingset_index, shuffle
            )
            engine_str = f" (with engine '{engine.aliases[0]}')"
        except ValueError:
            engine_str = ""

        conf = _create_confirmation_box(
            title=f"Are you sure you want to overwrite shuffle {shuffle}?",
            description=(
                f"As shuffle {shuffle} already exists{engine_str}, "
                f"the training-dataset files would be overwritten."
            ),
        )
        result = conf.exec()
        if result != QtWidgets.QMessageBox.Yes:
            msg = _create_message_box(
                text="The training dataset was not be created.",
                info_text=(
                    "You can create a shuffle with another index. Existing indices "
                    f"are {existing_indices}"
                ),
            )
            msg.exec_()
            self.root.writer.write("Training dataset creation interrupted.")
            return False

        return True

    @Slot(Engine)
    def update_nets(self, engine: Engine | None) -> None:
        if engine is None:
            engine = self.root.engine

        default_net = None
        if engine == Engine.TF:
            nets = DLCParams.NNETS.copy()
            if not self.root.is_multianimal:
                nets.remove("dlcrnet_ms5")
        else:
            nets = available_models()
            net_filter = self.get_net_filter()
            default_net = self.get_default_net()
            td_prefix = "top_down_"
            if net_filter is not None:
                nets = [
                    n
                    for n in nets
                    if (
                        n in net_filter
                        or (
                            n.startswith(td_prefix)
                            and n[len(td_prefix) :] in net_filter
                        )
                    )
                ]

        while self.net_choice.count() > 0:
            self.net_choice.removeItem(0)

        self.net_choice.addItems(nets)
        if default_net is None:
            default_net = self.root.cfg.get("default_net_type", "resnet_50")

        if (
            engine == Engine.TF
            and default_net not in DLCParams.NNETS
            or engine == Engine.PYTORCH
            and default_net not in available_models()
        ):
            default_net = "resnet_50"

        if default_net in nets:
            self.net_choice.setCurrentIndex(nets.index(default_net))

    @Slot(Engine)
    def update_detectors(
        self,
        engine: Engine | None = None,
        net_choice: str | None = None,
    ) -> None:
        if engine is None:
            engine = self.root.engine

        if engine == Engine.TF:
            detectors = []
        else:
            # FIXME: Circular imports make it impossible to import this at the top
            from deeplabcut.pose_estimation_pytorch import available_detectors

            detectors = available_detectors()
            det_filter = self.get_detector_filter()
            if det_filter is not None:
                detectors = [d for d in detectors if d in det_filter]

        while self.detector_choice.count() > 0:
            self.detector_choice.removeItem(0)

        self.detector_choice.addItems(detectors)
        default_detector = self.get_default_detector()
        if default_detector in detectors:
            self.detector_choice.setCurrentIndex(detectors.index(default_detector))
        elif "ssdlite" in detectors:
            self.detector_choice.setCurrentIndex(detectors.index("ssdlite"))

        if net_choice is None:
            net_choice = self.net_choice.currentText()

        if "top_down" in net_choice:
            self.detector_label.show()
            self.detector_choice.show()
        else:
            self.detector_label.hide()
            self.detector_choice.hide()

    @Slot(Engine)
    def update_aug_methods(self, engine: Engine) -> None:
        methods = compat.get_available_aug_methods(engine)
        while self.aug_choice.count() > 0:
            self.aug_choice.removeItem(0)

        self.aug_choice.addItems(methods)
        self.aug_choice.setCurrentText(methods[0])

    @Slot(Engine)
    def update_weight_init_methods(self, engine: Engine) -> None:
        if engine != Engine.PYTORCH:
            self.weight_init_label.hide()
            self.weight_init_selector.hide()
            return

        self.weight_init_label.show()
        self.weight_init_selector.update_choices(list(_WEIGHT_INIT_OPTIONS.keys()))
        self.weight_init_selector.show()

    def get_net_filter(self) -> list[str] | None:
        """Returns: the net type that can be used based on weight initialization"""
        if self.root.engine != Engine.PYTORCH:
            return None

        if self.weight_init_selector.weight_init not in _WEIGHT_INIT_OPTIONS:
            return None

        weight_init_cfg = _WEIGHT_INIT_OPTIONS[self.weight_init_selector.weight_init]
        if "super_animal" in weight_init_cfg:
            return dlclibrary.get_available_models(weight_init_cfg["super_animal"])

        return None

    def get_detector_filter(self) -> list[str] | None:
        """Returns: the detectors that can be used based on weight initialization"""
        if self.root.engine != Engine.PYTORCH:
            return None

        if self.weight_init_selector.weight_init not in _WEIGHT_INIT_OPTIONS:
            return None

        weight_init_cfg = _WEIGHT_INIT_OPTIONS[self.weight_init_selector.weight_init]
        if "super_animal" in weight_init_cfg:
            return dlclibrary.get_available_detectors(weight_init_cfg["super_animal"])

        return None

    def get_default_net(self) -> str | None:
        """Returns: the net type that can be used based on weight initialization"""
        if self.root.engine != Engine.PYTORCH:
            return None

        if self.weight_init_selector.weight_init not in _WEIGHT_INIT_OPTIONS:
            return None

        weight_init_cfg = _WEIGHT_INIT_OPTIONS[self.weight_init_selector.weight_init]
        return weight_init_cfg.get("default_net")

    def get_default_detector(self) -> str | None:
        """Returns: the detector type that can be used based on weight initialization"""
        if self.root.engine != Engine.PYTORCH:
            return None

        if self.weight_init_selector.weight_init not in _WEIGHT_INIT_OPTIONS:
            return None

        weight_init_cfg = _WEIGHT_INIT_OPTIONS[self.weight_init_selector.weight_init]
        return weight_init_cfg.get("default_detector")

    def view_shuffles(self) -> None:
        viewer = ShuffleMetadataViewer(root=self.root, parent=self)
        viewer.show()


class WeightInitializationSelector(QtWidgets.QWidget):
    """Widget to select weight initialization"""

    def __init__(self, root):
        super().__init__()
        self.root = root

        self.weight_init_choice = QtWidgets.QComboBox()

        self.memory_replay_label = QtWidgets.QLabel("With memory replay")
        self.memory_replay_box = QtWidgets.QCheckBox()
        self.memory_replay_label.hide()
        self.memory_replay_box.hide()

        memory_replay_layout = QtWidgets.QHBoxLayout()
        memory_replay_layout.addWidget(self.memory_replay_label)
        memory_replay_layout.addWidget(self.memory_replay_box)

        layout = QtWidgets.QHBoxLayout()
        layout.addWidget(self.weight_init_choice)
        layout.addLayout(memory_replay_layout)
        self.setLayout(layout)

        self.weight_init_choice.currentTextChanged.connect(self._choice_changed)

    @property
    def weight_init(self) -> str:
        return self.weight_init_choice.currentText()

    @property
    def with_decoder(self) -> bool:
        weight_init_choice = self.weight_init_choice.currentText()
        return "fine-tuning" in weight_init_choice.lower()

    @property
    def memory_replay(self) -> bool:
        return self.memory_replay_box.isChecked()

    def update_choices(self, choices: list[str]) -> None:
        """Updates the WeightInitialization methods that can be selected"""
        while self.weight_init_choice.count() > 0:
            self.weight_init_choice.removeItem(0)
        self.weight_init_choice.addItems(choices)

    def get_super_animal_weight_init(
        self,
        net_type: str,
        detector_type: str,
    ) -> WeightInitialization | None:
        """
        Args:
            net_type: The architecture of the pose model from which to fine-tune a
                SuperAnimal model.
            detector_type: The architecture of the detector from which to fine-tune a
                SuperAnimal model.

        Raises:
            ValueError if WeightInitialization should be defined but could not be
                created (e.g. if there's no conversion table).
        """
        if self.root.engine != Engine.PYTORCH:
            return None

        weight_init_choice = self.weight_init_choice.currentText()
        if "imagenet" in weight_init_choice.lower():
            return

        weight_init_data = _WEIGHT_INIT_OPTIONS[weight_init_choice]
        super_animal = weight_init_data["super_animal"]
        if net_type.startswith("top_down_"):
            net_type = net_type[len("top_down_") :]
        try:
            weight_init = build_weight_init(
                self.root.cfg,
                super_animal=super_animal,
                model_name=net_type,
                detector_name=detector_type,
                with_decoder=self.with_decoder,
                memory_replay=self.memory_replay,
            )
        except ValueError as err:
            QtWidgets.QMessageBox.critical(
                self,
                "Error",
                (
                    f"No Conversion table specified for {super_animal} in the project "
                    "configuration file. Please create a conversion table using the GUI"
                    ", with ``deeplabcut.modelzoo.utils.create_conversion_table``, or "
                    "by adding it to your project's configuration file manually."
                ),
            )
            raise err

        return weight_init

    def _choice_changed(self, state: str) -> None:
        if "fine-tuning" in str(state).lower():
            self.memory_replay_label.show()
            self.memory_replay_box.show()
        else:
            self.memory_replay_label.hide()
            self.memory_replay_box.hide()


class DataSplitSelector(QtWidgets.QWidget):
    """Allows users to create training sets with the same train/test split as another"""

    def __init__(self, root: QtWidgets.QMainWindow, parent: QtWidgets.QWidget):
        super().__init__()
        self.root = root
        self.parent = parent

        self.setToolTip(
            "This allows you to create a shuffle where the data split is the same as "
            "one of your existing shuffles (the images on which the model is "
            "trained/tested are the same)."
        )

        layout = QtWidgets.QVBoxLayout()
        layout.setSpacing(0)
        layout.setContentsMargins(0, 0, 0, 0)

        box_layout = QtWidgets.QHBoxLayout()
        box_layout.setSpacing(0)
        box_layout.setContentsMargins(0, 0, 0, 0)

        selector_layout = QtWidgets.QHBoxLayout()
        selector_layout.setSpacing(0)
        selector_layout.setContentsMargins(0, 0, 0, 0)

        self.shuffle_label = QtWidgets.QLabel("From shuffle:")
        self.shuffle_label.hide()
        self.shuffle_selector = QtWidgets.QSpinBox()
        self.shuffle_selector.setMaximum(10_000)
        self.shuffle_selector.setValue(0)
        self.shuffle_selector.hide()

        self.box = QtWidgets.QCheckBox(parent=self)
        self.box.stateChanged.connect(self._checkbox_status_changed)
        self.box_label = QtWidgets.QLabel("Use an existing data split")

        box_layout.addWidget(self.box)
        box_layout.addWidget(self.box_label)
        selector_layout.addWidget(self.shuffle_label)
        selector_layout.addWidget(self.shuffle_selector)
        layout.addLayout(box_layout)
        layout.addLayout(selector_layout)
        self.setLayout(layout)

    @property
    def selected(self) -> bool:
        return self.box.isChecked()

    @property
    def from_shuffle(self) -> int:
        """The shuffle from which to copy the data split"""
        return self.shuffle_selector.value()

    def _checkbox_status_changed(self, state: int) -> None:
        if Qt.CheckState(state) == Qt.Checked:
            self.shuffle_selector.show()
            self.shuffle_label.show()
        else:
            self.shuffle_selector.hide()
            self.shuffle_label.hide()


def _create_message_box(text, info_text):
    msg = QtWidgets.QMessageBox()
    msg.setIcon(QtWidgets.QMessageBox.Information)
    msg.setText(text)
    msg.setInformativeText(info_text)

    msg.setWindowTitle("Info")
    msg.setMinimumWidth(900)
    logo_dir = os.path.dirname(os.path.realpath("logo.png")) + os.path.sep
    logo = logo_dir + "/assets/logo.png"
    msg.setWindowIcon(QIcon(logo))
    msg.setStandardButtons(QtWidgets.QMessageBox.Ok)
    return msg


def _create_confirmation_box(title, description):
    msg = QtWidgets.QMessageBox()
    msg.setIcon(QtWidgets.QMessageBox.Information)
    msg.setText(title)
    msg.setInformativeText(description)

    msg.setWindowTitle("Confirmation")
    msg.setMinimumWidth(900)
    logo_dir = os.path.dirname(os.path.realpath("logo.png")) + os.path.sep
    logo = logo_dir + "/assets/logo.png"
    msg.setWindowIcon(QIcon(logo))
    msg.setStandardButtons(QtWidgets.QMessageBox.Yes | QtWidgets.QMessageBox.No)
    return msg


_WEIGHT_INIT_OPTIONS = {  # FIXME - Generate dynamically
    "Transfer Learning - ImageNet": {
        "model_filter": None,
        "detector_filter": None,
    },
    "Transfer Learning - SuperAnimal Bird": {
        "default_net": "top_down_resnet_50",
        "default_detector": "fasterrcnn_mobilenet_v3_large_fpn",
        "super_animal": "superanimal_bird",
    },
    "Transfer Learning - SuperAnimal Quadruped": {
        "default_net": "top_down_hrnet_w32",
        "default_detector": "fasterrcnn_mobilenet_v3_large_fpn",
        "super_animal": "superanimal_quadruped",
    },
    "Transfer Learning - SuperAnimal TopViewMouse": {
        "default_net": "top_down_hrnet_w32",
        "default_detector": "fasterrcnn_mobilenet_v3_large_fpn",
        "super_animal": "superanimal_topviewmouse",
    },
    "Fine-tuning - SuperAnimal Bird": {
        "default_net": "top_down_resnet_50",
        "default_detector": "fasterrcnn_mobilenet_v3_large_fpn",
        "super_animal": "superanimal_bird",
    },
    "Fine-tuning - SuperAnimal Quadruped": {
        "default_net": "top_down_hrnet_w32",
        "default_detector": "fasterrcnn_mobilenet_v3_large_fpn",
        "super_animal": "superanimal_quadruped",
    },
    "Fine-tuning - SuperAnimal TopViewMouse": {
        "default_net": "top_down_hrnet_w32",
        "default_detector": "fasterrcnn_mobilenet_v3_large_fpn",
        "super_animal": "superanimal_topviewmouse",
    },
}


--- File: deeplabcut/gui/displays/selected_shuffle_display.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Module to display information about the selected shuffle in the GUI"""
from __future__ import annotations
from pathlib import Path

import PySide6.QtCore as QtCore
from PySide6 import QtWidgets

from deeplabcut.core.engine import Engine
from deeplabcut.utils import auxiliaryfunctions


class SelectedShuffleDisplay(QtWidgets.QWidget):
    """A widget displaying information about the selected shuffle"""
    pose_cfg_signal = QtCore.Signal(dict)

    def __init__(self, root, row_margin: int = 25):
        super().__init__()
        self.root = root

        self._row_margin = row_margin

        self._current_index: int | None = None
        self._engine: Engine | None = None
        self._is_top_down: bool = False
        self._net_type: str | None = None
        self._pose_cfg: dict | None = None

        self._label = QtWidgets.QLabel("Shuffle info:")
        self._label.setStyleSheet(f"margin: 0px 0px {self._row_margin}px 0px")
        layout = QtWidgets.QHBoxLayout()
        layout.addWidget(self._label)
        self.setLayout(layout)

        # initialize the display
        self._update_display(self.root.shuffle_value)

        # update the display when the shuffle or selected engine changes, or when a new
        # shuffle has been created
        self.root.shuffle_change.connect(self._update_display)
        self.root.engine_change.connect(self._update_display)
        self.root.shuffle_created.connect(self._update_display)

    @property
    def pose_cfg(self) -> dict | None:
        return self._pose_cfg

    @pose_cfg.setter
    def pose_cfg(self, value: dict | None) -> None:
        self._pose_cfg = value
        self.pose_cfg_signal.emit(self._pose_cfg)

    @QtCore.Slot(int)
    def _update_display(self, new_index: int) -> None:
        self._current_index = new_index

        try:
            pose_cfg_path = Path(self.root.pose_cfg_path)
        except ValueError as err:
            self._set_text_error(
                f"Failed to read shuffle {self._current_index} - check that it exists!"
            )
            return
        except ModuleNotFoundError as err:
            # Loading a TF shuffle but TF is not installed
            self._set_text_error(
                f"Failed to read shuffle {self._current_index} due to error `{err}`.\n"
                "If the error is `ModuleNotFoundError: No module named 'tensorflow'`, "
                f"this is because\nshuffle {self._current_index} uses the tensorflow "
                " engine, but TensorFlow is not installed in your environment.\n"
                "Ignore this error if you'll just train PyTorch models. To train "
                "TensorFlow models, install it with \n"
                "    Windows/Linux: pip install 'deeplabcut[tf]'\n"
                "    Apple Silicon: pip install 'deeplabcut[apple_mchips]'"
            )
            return

        if not pose_cfg_path.exists():
            self._set_text_error(
                f"The model configuration file {pose_cfg_path} was not created"
            )
            return

        self._read_pose_config(pose_cfg_path)
        self._set_text()

    def _set_text(self) -> None:
        engine_str = "None"
        if self._engine is not None:
            engine_str = self._engine.aliases[0]

        text = f"net type: {self._net_type}  |  engine: {engine_str}"
        if self._engine == Engine.PYTORCH and self._is_top_down:
            text += f"  |  top-down"

        style = f"margin: 0px 0px {self._row_margin}px 0px;"
        if self._engine != self.root.engine:
            warning = "Change the selected Engine in the top-right to use this shuffle!"
            text = warning + "  |  " + text
            style += " color: orange;"

        self._label.setStyleSheet(style)
        self._label.setText(text)

    def _set_text_error(self, error: str) -> None:
        self._label.setText(error)
        style = f"margin: 0px 0px {self._row_margin}px 0px; color: orange;"
        self._label.setStyleSheet(style)
        self.pose_cfg = None

    def _read_pose_config(self, pose_cfg_path: Path) -> None:
        pose_cfg = auxiliaryfunctions.read_plainconfig(str(pose_cfg_path))

        self._engine = (
            Engine.PYTORCH if "pytorch" in pose_cfg_path.stem.lower() else Engine.TF
        )
        self._net_type = pose_cfg.get("net_type", "UNKNOWN")
        self._is_top_down = (
            self._engine == Engine.PYTORCH and pose_cfg.get("method").lower() == "td"
        )
        self.pose_cfg = pose_cfg


--- File: deeplabcut/gui/displays/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#




--- File: deeplabcut/gui/displays/shuffle_metadata_viewer.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Widget to display existing shuffles"""
from __future__ import annotations

from PySide6 import QtWidgets
from PySide6.QtCore import Qt

import deeplabcut.generate_training_dataset.metadata as metadata


class ShuffleMetadataViewer(QtWidgets.QDialog):
    """Viewer for shuffle metadata"""

    def __init__(self, root: QtWidgets.QMainWindow, parent: QtWidgets.QWidget):
        super().__init__(parent)
        self.root = root
        self.parent = parent
        self.file_content = _load_metadata(self.root.cfg)

        self.setWindowTitle("Existing Shuffles: Metadata")
        self.setMinimumWidth(400)
        self.setMinimumHeight(400)

        scroll = QtWidgets.QScrollArea()
        scroll.setWidgetResizable(True)

        inner_layout = QtWidgets.QVBoxLayout()
        inner_layout.setAlignment(Qt.AlignLeft | Qt.AlignTop)
        inner_layout.setSpacing(0)
        inner_layout.setContentsMargins(0, 0, 0, 0)

        for line in self.file_content:

            inner_layout.addWidget(QtWidgets.QLabel(line))

        inner = QtWidgets.QFrame(scroll)
        inner.setLayout(inner_layout)
        scroll.setWidget(inner)

        layout = QtWidgets.QVBoxLayout()
        layout.addWidget(scroll)
        self.setLayout(layout)


def _load_metadata(cfg: dict) -> list[str]:
    metadata_path = metadata.TrainingDatasetMetadata.path(cfg)
    if not metadata_path.exists():
        trainset_meta = metadata.TrainingDatasetMetadata.create(cfg)
        trainset_meta.save()

    with open(metadata_path, "r") as file:
        raw_metadata = file.read()

    return raw_metadata.split("\n")


--- File: deeplabcut/gui/media/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


--- File: deeplabcut/pose_tracking_pytorch/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from .create_dataset import *
from .tracking_utils.preprocessing import *
from .train_dlctransreid import train_tracking_transformer
from .apis import transformer_reID


--- File: deeplabcut/pose_tracking_pytorch/README.md ---
Our pose-aware ReID transformer takes features from the pre-trained multi-task CNNs and are used to train a shallow transformer with triplet loss. This model is then used to provide a stitching loss for appearance-based tracking.

Part of our transformer code is inspired by: https://github.com/damo-cv/TransReID


--- File: deeplabcut/pose_tracking_pytorch/create_dataset.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import numpy as np
import os
import pickle
import shelve
from deeplabcut.core import trackingutils
from deeplabcut.refine_training_dataset.stitch import TrackletStitcher
from pathlib import Path
from .tracking_utils.preprocessing import query_feature_by_coord_in_img_space

np.random.seed(0)


def generate_train_triplets_from_pickle(path_to_track, n_triplets=1000):
    ts = TrackletStitcher.from_pickle(path_to_track, 3)
    triplets = ts.mine(n_triplets)
    assert len(triplets) == n_triplets
    return triplets


def save_train_triplets(feature_fname, triplets, out_name):
    ret_vecs = []

    feature_dict = shelve.open(feature_fname, protocol=pickle.DEFAULT_PROTOCOL)

    nframes = max(len(feature_dict.keys()), 2)
    zfill_width = int(np.ceil(np.log10(nframes)))

    for triplet in triplets:
        anchor, pos, neg = triplet[0], triplet[1], triplet[2]

        anchor_coord, anchor_frame = anchor
        pos_coord, pos_frame = pos
        neg_coord, neg_frame = neg

        anchor_frame = "frame" + str(anchor_frame).zfill(zfill_width)
        pos_frame = "frame" + str(pos_frame).zfill(zfill_width)
        neg_frame = "frame" + str(neg_frame).zfill(zfill_width)

        if (
            anchor_frame in feature_dict
            and pos_frame in feature_dict
            and neg_frame in feature_dict
        ):
            # only try to find these features if they are in the dictionary

            anchor_vec = query_feature_by_coord_in_img_space(
                feature_dict, anchor_frame, anchor_coord
            )
            pos_vec = query_feature_by_coord_in_img_space(
                feature_dict, pos_frame, pos_coord
            )
            neg_vec = query_feature_by_coord_in_img_space(
                feature_dict, neg_frame, neg_coord
            )

            ret_vecs.append([anchor_vec, pos_vec, neg_vec])

    ret_vecs = np.array(ret_vecs)

    with open(out_name, "wb") as f:
        np.save(f, ret_vecs)


def create_train_using_pickle(feature_fname, path_to_pickle, out_name, n_triplets=1000):
    triplets = generate_train_triplets_from_pickle(
        path_to_pickle, n_triplets=n_triplets
    )
    save_train_triplets(feature_fname, triplets, out_name)


def create_triplets_dataset(
    videos, dlcscorer, track_method, n_triplets=1000, destfolder=None
):
    # 1) reference to video folder and get the proper bpt_feature file for feature table
    # 2) get either the path to gt or the path to track pickle

    for video in videos:
        vname = Path(video).stem
        videofolder = str(Path(video).parents[0])
        if destfolder is None:
            destfolder = videofolder
        feature_fname = os.path.join(
            destfolder, vname + dlcscorer + "_bpt_features.pickle"
        )

        method = trackingutils.TRACK_METHODS[track_method]
        track_file = os.path.join(destfolder, vname + dlcscorer + f"{method}.pickle")
        out_fname = os.path.join(destfolder, vname + dlcscorer + "_triplet_vector.npy")
        create_train_using_pickle(
            feature_fname, track_file, out_fname, n_triplets=n_triplets
        )


--- File: deeplabcut/pose_tracking_pytorch/apis.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


def transformer_reID(
    config: str,
    videos: list[str],
    videotype: str = "",
    shuffle: int = 1,
    trainingsetindex: int = 0,
    track_method: str = "ellipse",
    n_tracks: int | None = None,
    n_triplets: int = 1000,
    train_epochs: int = 100,
    train_frac: float = 0.8,
    modelprefix: str = "",
    destfolder: str = None,
):
    """
    Enables tracking with transformer.

    Substeps include:
        - Mines triplets from tracklets in videos (from another tracker)
        - These triplets are later used to tran a transformer with triplet loss
        - The transformer derived appearance similarity is then used as a stitching loss
            when tracklets are stitched during tracking.

    Outputs: The tracklet file is saved in the same folder where the non-transformer
    tracklet file is stored.

    Parameters
    ----------
    config: string
        Full path of the config.yaml file as a string.

    videos: list
        A list of strings containing the full paths to videos for analysis or a path to
        the directory, where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg',
        'mkv') are kept.

    shuffle : int, optional
        which shuffle to use

    trainingsetindex : int. optional
        which training fraction to use, identified by its index

    track_method: str, optional
        track method from which tracklets are sampled

    n_tracks: int
        number of tracks to be formed in the videos.
        TODO: handling videos with different number of tracks

    n_triplets: (optional) int
        number of triplets to be mined from the videos

    train_epochs: (optional), int
        number of epochs to train the transformer

    train_frac: (optional), fraction
        fraction of triplets used for training/testing of the transformer

    Examples
    --------

    Training model for one video based on ellipse-tracker derived tracklets
    >>> config = "/home/users/.../dlc-project-2025-01-01/config.yaml"
    >>> videos = ['/home/alex/video.mp4']
    >>> deeplabcut.transformer_reID(config, videos, shuffle=1, track_method="ellipse")
    >>> deeplabcut.create_labeled_video(
    >>>     config,
    >>>     videos,
    >>>     shuffle=1,
    >>>     track_method="transformer",
    >>> )
    --------

    """
    import deeplabcut
    import os
    from deeplabcut.utils import auxiliaryfunctions

    # calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets

    cfg = auxiliaryfunctions.read_config(config)

    DLCscorer, _ = deeplabcut.utils.auxiliaryfunctions.GetScorerName(
        cfg,
        shuffle=shuffle,
        trainFraction=cfg["TrainingFraction"][trainingsetindex],
        modelprefix=modelprefix,
    )

    deeplabcut.compat.create_tracking_dataset(
        config,
        videos,
        track_method,
        videotype=videotype,
        shuffle=shuffle,
        trainingsetindex=trainingsetindex,
        modelprefix=modelprefix,
        n_triplets=n_triplets,
        destfolder=destfolder,
    )

    (
        trainposeconfigfile,
        testposeconfigfile,
        snapshotfolder,
    ) = deeplabcut.return_train_network_path(
        config,
        shuffle=shuffle,
        modelprefix=modelprefix,
        trainingsetindex=trainingsetindex,
    )

    deeplabcut.pose_tracking_pytorch.train_tracking_transformer(
        config,
        DLCscorer,
        videos,
        videotype=videotype,
        train_frac=train_frac,
        modelprefix=modelprefix,
        train_epochs=train_epochs,
        ckpt_folder=snapshotfolder,
        destfolder=destfolder,
    )

    transformer_checkpoint = os.path.join(
        snapshotfolder, f"dlc_transreid_{train_epochs}.pth"
    )

    if not os.path.exists(transformer_checkpoint):
        raise FileNotFoundError(f"checkpoint {transformer_checkpoint} not found")

    deeplabcut.stitch_tracklets(
        config,
        videos,
        videotype=videotype,
        shuffle=shuffle,
        trainingsetindex=trainingsetindex,
        track_method=track_method,
        modelprefix=modelprefix,
        n_tracks=n_tracks,
        transformer_checkpoint=transformer_checkpoint,
        destfolder=destfolder,
    )


--- File: deeplabcut/pose_tracking_pytorch/train_dlctransreid.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import random

try:
    import torch
except ModuleNotFoundError:
    raise ModuleNotFoundError(
        "Unsupervised identity learning requires PyTorch. Please run `pip install torch`."
    )
import numpy as np
import os
import glob
from deeplabcut.utils import auxiliaryfunctions
from pathlib import Path
from .config import cfg
from .datasets import make_dlc_dataloader
from .model import make_dlc_model
from .solver import make_easy_optimizer
from .solver.scheduler_factory import create_scheduler
from .loss import easy_triplet_loss
from .processor import do_dlc_train


def set_seed(seed):
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True


def split_train_test(npy_list, train_frac):
    # with npy list form videos, split each to train and test

    x_list = []
    train_list = []
    test_list = []

    for npy in npy_list:
        vectors = np.load(npy)
        n_samples = vectors.shape[0]
        indices = np.random.permutation(n_samples)
        num_train = int(n_samples * train_frac)
        vectors = vectors[indices]
        train = vectors[:num_train]
        test = vectors[num_train:]
        train_list.append(train)
        test_list.append(test)

    train_list = np.concatenate(train_list, axis=0)
    test_list = np.concatenate(test_list, axis=0)

    return train_list, test_list


def train_tracking_transformer(
    path_config_file,
    dlcscorer,
    videos,
    videotype="",
    train_frac=0.8,
    modelprefix="",
    train_epochs=100,
    batch_size=64,
    ckpt_folder="",
    destfolder=None,
):
    npy_list = []
    videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    for video in videos:
        videofolder = str(Path(video).parents[0])
        if destfolder is None:
            destfolder = videofolder
        video_name = Path(video).stem
        # video_name = '.'.join(video.split("/")[-1].split(".")[:-1])
        files = glob.glob(os.path.join(destfolder, video_name + dlcscorer + "*.npy"))

        # assuming there is only one match
        npy_list.append(files[0])

    train_list, test_list = split_train_test(npy_list, train_frac)

    train_loader, val_loader = make_dlc_dataloader(train_list, test_list, batch_size)

    # make my own model factory
    num_kpts = train_list.shape[2]
    feature_dim = train_list.shape[-1]
    model = make_dlc_model(cfg, feature_dim, num_kpts)

    # make my own loss factory
    triplet_loss = easy_triplet_loss()

    optimizer = make_easy_optimizer(cfg, model)
    scheduler = create_scheduler(cfg, optimizer)

    num_query = 1

    do_dlc_train(
        cfg,
        model,
        triplet_loss,
        train_loader,
        val_loader,
        optimizer,
        scheduler,
        num_kpts,
        feature_dim,
        num_query,
        total_epochs=train_epochs,
        ckpt_folder=ckpt_folder,
    )


--- File: deeplabcut/pose_tracking_pytorch/inference.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import torch
import torch.nn as nn
import numpy as np
from deeplabcut.pose_tracking_pytorch.config import cfg
from deeplabcut.pose_tracking_pytorch.model import build_dlc_transformer
from deeplabcut.pose_tracking_pytorch.model.backbones import dlc_base_kpt_TransReID
from deeplabcut.pose_tracking_pytorch.tracking_utils import (
    query_feature_by_coord_in_img_space,
)

from deeplabcut.pose_tracking_pytorch.processor import default_device

inference_factory = {"dlc_transreid": dlc_base_kpt_TransReID}


class DLCTrans:
    def __init__(self, checkpoint):
        self.checkpoint = checkpoint

        ckpt_dict = torch.load(self.checkpoint)

        self.model = build_dlc_transformer(
            cfg, ckpt_dict["feature_dim"], ckpt_dict["num_kpts"], inference_factory
        )

        self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)

        print("loading params")
        self._load_params(ckpt_dict["state_dict"])

        self.model.double()
        self.model.eval()

    def _load_params(self, params):
        self.model.load_state_dict(params)

    def _get_vec(self, inp_a, inp_b, zfill_width, feature_dict):
        coord_a_img, frame_a = inp_a
        coord_b_img, frame_b = inp_b

        frame_a = "frame" + str(frame_a).zfill(zfill_width)
        frame_b = "frame" + str(frame_b).zfill(zfill_width)

        vec_a = query_feature_by_coord_in_img_space(feature_dict, frame_a, coord_a_img)

        vec_b = query_feature_by_coord_in_img_space(feature_dict, frame_b, coord_b_img)

        return vec_a, vec_b

    def __call__(self, inp_a, inp_b, zfill_width, feature_dict, return_features=False):
        # tracklets
        device = default_device("cuda")

        _tuple = self._get_vec(inp_a, inp_b, zfill_width, feature_dict)
        if _tuple is None:
            return None
        vec_a, vec_b = _tuple

        vec_a = np.expand_dims(vec_a, axis=0)
        vec_b = np.expand_dims(vec_b, axis=0)

        vec_a = torch.from_numpy(vec_a).double()
        vec_b = torch.from_numpy(vec_b).double()

        with torch.no_grad():
            vec_a.to(device)
            vec_b.to(device)

            vec_a = self.model(vec_a)
            vec_b = self.model(vec_b)

            dist = self.cos(vec_a, vec_b)
            if return_features:
                return dist, vec_a, vec_b
            else:
                return dist


--- File: deeplabcut/pose_tracking_pytorch/loss/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from .make_loss import easy_triplet_loss


--- File: deeplabcut/pose_tracking_pytorch/loss/make_loss.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import torch


def easy_triplet_loss():
    def loss_func(anchor, positive, neg):
        triplet_loss = torch.nn.TripletMarginLoss()
        loss = triplet_loss(anchor, positive, neg)
        return loss

    return loss_func


--- File: deeplabcut/pose_tracking_pytorch/config/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import os
from deeplabcut.utils.auxiliaryfunctions import (
    read_plainconfig,
    get_deeplabcut_path,
)


dlcparent_path = get_deeplabcut_path()
reid_config = os.path.join(dlcparent_path, "reid_cfg.yaml")
cfg = read_plainconfig(reid_config)


--- File: deeplabcut/pose_tracking_pytorch/tracking_utils/metrics.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import torch
import numpy as np
from ..tracking_utils.reranking import re_ranking


def euclidean_distance(qf, gf):
    m = qf.shape[0]
    n = gf.shape[0]
    dist_mat = (
        torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n)
        + torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()
    )
    dist_mat.addmm_(1, -2, qf, gf.t())
    return dist_mat.cpu().numpy()


def cosine_similarity(qf, gf):
    epsilon = 0.00001
    dist_mat = qf.mm(gf.t())
    qf_norm = torch.norm(qf, p=2, dim=1, keepdim=True)  # mx1
    gf_norm = torch.norm(gf, p=2, dim=1, keepdim=True)  # nx1
    qg_normdot = qf_norm.mm(gf_norm.t())

    dist_mat = dist_mat.mul(1 / qg_normdot).cpu().numpy()
    dist_mat = np.clip(dist_mat, -1 + epsilon, 1 - epsilon)
    dist_mat = np.arccos(dist_mat)
    return dist_mat


def eval_func(distmat, q_pids, g_pids, q_camids, g_camids, max_rank=50):
    """Evaluation with market1501 metric
    Key: for each query identity, its gallery images from the same camera view are discarded.
    """
    num_q, num_g = distmat.shape
    # distmat g
    #    q    1 3 2 4
    #         4 1 2 3
    if num_g < max_rank:
        max_rank = num_g
        print("Note: number of gallery samples is quite small, got {}".format(num_g))
    indices = np.argsort(distmat, axis=1)
    #  0, 2, 1, 3
    #  1, 2, 3, 0
    matches = (g_pids[indices] == q_pids[:, np.newaxis]).astype(np.int32)
    # compute cmc curve for each query
    all_cmc = []
    all_AP = []
    num_valid_q = 0.0  # number of valid query
    for q_idx in range(num_q):
        # get query pid and camid
        q_pid = q_pids[q_idx]
        q_camid = q_camids[q_idx]

        # remove gallery samples that have the same pid and camid with query
        order = indices[q_idx]  # select one row
        remove = (g_pids[order] == q_pid) & (g_camids[order] == q_camid)
        keep = np.invert(remove)

        # compute cmc curve
        # binary vector, positions with value 1 are correct matches
        orig_cmc = matches[q_idx][keep]
        if not np.any(orig_cmc):
            # this condition is true when query identity does not appear in gallery
            continue

        cmc = orig_cmc.cumsum()
        cmc[cmc > 1] = 1

        all_cmc.append(cmc[:max_rank])
        num_valid_q += 1.0

        # compute average precision
        # reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision
        num_rel = orig_cmc.sum()
        tmp_cmc = orig_cmc.cumsum()
        # tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]
        y = np.arange(1, tmp_cmc.shape[0] + 1) * 1.0
        tmp_cmc = tmp_cmc / y
        tmp_cmc = np.asarray(tmp_cmc) * orig_cmc
        AP = tmp_cmc.sum() / num_rel
        all_AP.append(AP)

    assert num_valid_q > 0, "Error: all query identities do not appear in gallery"

    all_cmc = np.asarray(all_cmc).astype(np.float32)
    all_cmc = all_cmc.sum(0) / num_valid_q
    mAP = np.mean(all_AP)

    return all_cmc, mAP


class R1_mAP_eval:
    def __init__(self, num_query, max_rank=50, feat_norm=True, reranking=False):
        super(R1_mAP_eval, self).__init__()
        self.num_query = num_query
        self.max_rank = max_rank
        self.feat_norm = feat_norm
        self.reranking = reranking

    def reset(self):
        self.feats = []
        self.pids = []
        self.camids = []

    def update(self, output):  # called once for each batch
        feat, pid, camid = output
        self.feats.append(feat.cpu())
        self.pids.extend(np.asarray(pid))
        self.camids.extend(np.asarray(camid))

    def compute(self):  # called after each epoch
        feats = torch.cat(self.feats, dim=0)
        if self.feat_norm:
            print("The test feature is normalized")
            feats = torch.nn.functional.normalize(feats, dim=1, p=2)  # along channel
        # query
        qf = feats[: self.num_query]
        q_pids = np.asarray(self.pids[: self.num_query])
        q_camids = np.asarray(self.camids[: self.num_query])
        # gallery
        gf = feats[self.num_query :]
        g_pids = np.asarray(self.pids[self.num_query :])

        g_camids = np.asarray(self.camids[self.num_query :])
        if self.reranking:
            print("=> Enter reranking")
            # distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)
            distmat = re_ranking(qf, gf, k1=50, k2=15, lambda_value=0.3)

        else:
            print("=> Computing DistMat with euclidean_distance")
            distmat = euclidean_distance(qf, gf)
        cmc, mAP = eval_func(distmat, q_pids, g_pids, q_camids, g_camids)

        return cmc, mAP, distmat, self.pids, self.camids, qf, gf


--- File: deeplabcut/pose_tracking_pytorch/tracking_utils/meter.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


--- File: deeplabcut/pose_tracking_pytorch/tracking_utils/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from .preprocessing import (
    load_features_from_coord,
    convert_coord_from_img_space_to_feature_space,
    query_feature_by_coord_in_img_space,
)


--- File: deeplabcut/pose_tracking_pytorch/tracking_utils/reranking.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import torch


def re_ranking(
    probFea, galFea, k1, k2, lambda_value, local_distmat=None, only_local=False
):
    """

    probFea: all feature vectors of the query set (torch tensor)
    galFea: all feature vectors of the gallery set (torch tensor)
    k1,k2,lambda: parameters, the original paper uses (k1=20,k2=6,lambda=0.3)

    Code adapted from  https://github.com/zhunzhong07/person-re-ranking

    Zhong Z, Zheng L, Cao D, et al. Re-ranking Person Re-identification with k-reciprocal Encoding CVPR 2017.

    """
    # if feature vector is numpy, you should use 'torch.tensor' transform it to tensor
    query_num = probFea.size(0)
    all_num = query_num + galFea.size(0)
    if only_local:
        original_dist = local_distmat
    else:
        feat = torch.cat([probFea, galFea])
        # print('using GPU to compute original distance')
        distmat = (
            torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num)
            + torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num).t()
        )
        distmat.addmm_(1, -2, feat, feat.t())
        original_dist = distmat.cpu().numpy()
        del feat
        if not local_distmat is None:
            original_dist = original_dist + local_distmat
    gallery_num = original_dist.shape[0]
    original_dist = np.transpose(original_dist / np.max(original_dist, axis=0))
    V = np.zeros_like(original_dist).astype(np.float16)
    initial_rank = np.argsort(original_dist).astype(np.int32)

    # print('starting re_ranking')
    for i in range(all_num):
        # k-reciprocal neighbors
        forward_k_neigh_index = initial_rank[i, : k1 + 1]
        backward_k_neigh_index = initial_rank[forward_k_neigh_index, : k1 + 1]
        fi = np.where(backward_k_neigh_index == i)[0]
        k_reciprocal_index = forward_k_neigh_index[fi]
        k_reciprocal_expansion_index = k_reciprocal_index
        for j in range(len(k_reciprocal_index)):
            candidate = k_reciprocal_index[j]
            candidate_forward_k_neigh_index = initial_rank[
                candidate, : int(np.around(k1 / 2)) + 1
            ]
            candidate_backward_k_neigh_index = initial_rank[
                candidate_forward_k_neigh_index, : int(np.around(k1 / 2)) + 1
            ]
            fi_candidate = np.where(candidate_backward_k_neigh_index == candidate)[0]
            candidate_k_reciprocal_index = candidate_forward_k_neigh_index[fi_candidate]
            if len(
                np.intersect1d(candidate_k_reciprocal_index, k_reciprocal_index)
            ) > 2 / 3 * len(candidate_k_reciprocal_index):
                k_reciprocal_expansion_index = np.append(
                    k_reciprocal_expansion_index, candidate_k_reciprocal_index
                )

        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)
        weight = np.exp(-original_dist[i, k_reciprocal_expansion_index])
        V[i, k_reciprocal_expansion_index] = weight / np.sum(weight)
    original_dist = original_dist[:query_num,]
    if k2 != 1:
        V_qe = np.zeros_like(V, dtype=np.float16)
        for i in range(all_num):
            V_qe[i, :] = np.mean(V[initial_rank[i, :k2], :], axis=0)
        V = V_qe
        del V_qe
    del initial_rank
    invIndex = []
    for i in range(gallery_num):
        invIndex.append(np.where(V[:, i] != 0)[0])

    jaccard_dist = np.zeros_like(original_dist, dtype=np.float16)

    for i in range(query_num):
        temp_min = np.zeros(shape=[1, gallery_num], dtype=np.float16)
        indNonZero = np.where(V[i, :] != 0)[0]
        indImages = [invIndex[ind] for ind in indNonZero]
        for j in range(len(indNonZero)):
            temp_min[0, indImages[j]] = temp_min[0, indImages[j]] + np.minimum(
                V[i, indNonZero[j]], V[indImages[j], indNonZero[j]]
            )
        jaccard_dist[i] = 1 - temp_min / (2 - temp_min)

    final_dist = jaccard_dist * (1 - lambda_value) + original_dist * lambda_value
    del original_dist
    del V
    del jaccard_dist
    final_dist = final_dist[:query_num, query_num:]
    return final_dist


--- File: deeplabcut/pose_tracking_pytorch/tracking_utils/preprocessing.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np


def load_features_from_coord(feature, coords, valid_mask_for_fish=False):
    """extract the deep feature at the location of the keypoint (x,y)"""
    if valid_mask_for_fish:
        mask = np.array([1, 2, 6])
        coords = coords[mask, :]

    feat_vec = np.zeros((coords.shape[0], coords.shape[1], feature.shape[-1]))

    for animal_idx in range(coords.shape[0]):
        for kpt_idx in range(coords.shape[1]):
            coord = coords[animal_idx][kpt_idx]
            x, y = coord

            vec = feature[y, x, :]
            if np.sum(coord) != 0:
                feat_vec[animal_idx][kpt_idx] = vec

    return feat_vec


def convert_coord_from_img_space_to_feature_space(arr, stride):
    """
    if stride ==8:
        stride = stride * 2
    elif stride == 4:
        stride = stride *4
    elif stride ==2:
        stride = stride *8
    """
    # More elegantly one can simply define:
    stride = 16

    arr = np.nan_to_num(arr).astype(np.int64)

    # take care of difference between feature map space and original image space

    arr = (arr - (stride // 2)) // stride

    return arr.astype(np.int64)


def query_feature_by_coord_in_img_space(feature_dict, frame_id, ref_coord):
    features = feature_dict[frame_id]["features"]
    coordinates = feature_dict[frame_id]["coordinates"]

    diff = coordinates - ref_coord
    diff[np.where(np.logical_or(diff > 9000, diff < 0))] = np.nan
    masked_means = np.ma.masked_invalid(np.nanmean(diff, axis=(1, 2)))
    match_id = np.argmin(masked_means)
    return features[match_id]


--- File: deeplabcut/pose_tracking_pytorch/processor/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from .processor import (
    do_dlc_train,
    do_dlc_inference,
    do_dlc_pair_inference,
    default_device,
)


--- File: deeplabcut/pose_tracking_pytorch/processor/processor.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import logging
import os
import time
import torch
import torch.nn as nn
from ..tracking_utils.meter import AverageMeter
from ..tracking_utils.metrics import R1_mAP_eval
import torch.distributed as dist
import pickle
import numpy as np


def dist(a, b):
    return torch.sqrt(torch.sum((a - b) ** 2, dim=1))


def calc_correct(anchor, pos, neg):
    # cos = torch.cdist
    ap_dist = dist(anchor, pos)
    an_dist = dist(anchor, neg)
    indices = ap_dist < an_dist

    return torch.sum(indices)


def calc_cos_correct(vec1, gt1, vec2, gt2, threshold=0.5):
    cos = nn.CosineSimilarity(dim=1, eps=1e-6)

    confidence = cos(vec1, vec2)

    pred_mask = confidence > threshold

    gt_mask = gt1 == gt2
    n_correct = torch.sum(torch.eq(pred_mask, gt_mask))
    return n_correct


# TODO: maybe find a better spot for this.
def default_device(device="cuda"):  # setting CPU, if no GPU available
    # dev =  device if torch.cuda.is_available() else "cpu"
    dev = torch.device(device) if torch.cuda.is_available() else torch.device("cpu")
    return dev


def do_dlc_train(
    cfg,
    model,
    triplet_loss,
    train_loader,
    val_loader,
    optimizer,
    scheduler,
    num_kpts,
    feature_dim,
    num_query,
    total_epochs=300,
    ckpt_folder="",
):
    log_period = cfg["log_period"]
    checkpoint_period = cfg["checkpoint_period"]
    eval_period = 10

    device = default_device(cfg["device"])

    logger = logging.getLogger("transreid.train")
    logger.info("start training")
    _LOCAL_PROCESS_GROUP = None
    if device:
        model.to(device)

    loss_meter = AverageMeter()
    acc_meter = AverageMeter()

    evaluator = R1_mAP_eval(num_query, max_rank=50, feat_norm=cfg["feat_norm"])

    # train
    epoch_list = []
    train_acc_list = []
    test_acc_list = []
    plot_dict = {}
    for epoch in range(1, total_epochs + 1):
        epoch_list.append(epoch)
        start_time = time.time()
        loss_meter.reset()
        acc_meter.reset()
        evaluator.reset()
        scheduler.step(epoch)
        model.train()
        total_n = 0.0
        total_correct = 0.0
        for n_iter, (anchor, pos, neg) in enumerate(train_loader):
            optimizer.zero_grad()

            anchor = anchor.to(device)
            pos = pos.to(device)
            neg = neg.to(device)

            anchor_feat = model(anchor)
            pos_feat = model(pos)
            neg_feat = model(neg)

            loss = triplet_loss(anchor_feat, pos_feat, neg_feat)

            loss.backward()

            optimizer.step()

            total_n += anchor_feat.shape[0]
            total_correct += calc_correct(anchor_feat, pos_feat, neg_feat)

            loss_meter.update(loss.item())  # , img.shape[0])

            if torch.cuda.is_available():
                torch.cuda.synchronize()

            if (n_iter + 1) % log_period == 0:
                logger.info(
                    "Epoch[{}] Iteration[{}/{}] Loss: {:.3f}, , Base Lr: {:.2e}".format(
                        epoch,
                        (n_iter + 1),
                        len(train_loader),
                        loss_meter.avg,
                        scheduler._get_lr(epoch)[0],
                    )
                )

        end_time = time.time()
        time_per_batch = (end_time - start_time) / (n_iter + 1)
        train_acc = total_correct / total_n
        train_acc_list.append(train_acc.item())

        if cfg["dist_train"]:
            pass
        else:
            logger.info(
                "Epoch {} done. Time per batch: {:.3f}[s] Speed: {:.1f}[samples/s]".format(
                    epoch, time_per_batch, train_loader.batch_size / time_per_batch
                )
            )

        model_name = f"dlc_transreid"

        if epoch % checkpoint_period == 0:
            torch.save(
                {
                    "state_dict": model.state_dict(),
                    "num_kpts": num_kpts,
                    "feature_dim": feature_dim,
                },
                os.path.join(ckpt_folder, model_name + "_{}.pth".format(epoch)),
            )

        if epoch % eval_period == 0:
            model.eval()
            val_loss = 0.0
            total_n = 0.0
            total_correct = 0.0
            for n_iter, (anchor, pos, neg) in enumerate(val_loader):
                with torch.no_grad():
                    anchor = anchor.to(device)
                    pos = pos.to(device)
                    neg = neg.to(device)
                    anchor_feat = model(anchor)
                    pos_feat = model(pos)
                    neg_feat = model(neg)
                    loss = triplet_loss(anchor_feat, pos_feat, neg_feat)
                    val_loss += loss.item()

                    total_n += anchor_feat.shape[0]
                    total_correct += calc_correct(anchor_feat, pos_feat, neg_feat)

            logger.info("Validation Results - Epoch: {}".format(epoch))

            # print (f'validation loss {val_loss/len(val_loader)}')
            test_acc = total_correct / total_n
            test_acc_list.append(test_acc.item())
            print(f"Epoch {epoch}, train acc: {train_acc:.2f}")
            print(f"Epoch {epoch}, test acc {test_acc:.2f}")

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    plot_dict["train_acc"] = train_acc_list
    plot_dict["test_acc"] = test_acc_list
    plot_dict["epochs"] = epoch_list

    with open(
        os.path.join(ckpt_folder, "dlc_transreid_results.pickle"), "wb"
    ) as handle:
        pickle.dump(plot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)


def do_dlc_inference(cfg, model, triplet_loss, val_loader, num_query):
    device = default_device(cfg["device"])
    logger = logging.getLogger("transreid.test")
    logger.info("Enter inferencing")

    evaluator = R1_mAP_eval(num_query, max_rank=50, feat_norm=cfg["feat_norm"])

    evaluator.reset()

    if device:
        if torch.cuda.device_count() > 1:
            print("Using {} GPUs for inference".format(torch.cuda.device_count()))
            model = nn.DataParallel(model)
        model.to(device)

    model.eval()
    val_loss = 0.0

    features_list = []
    labels_list = []
    total_n = 0.0
    total_correct = 0.0
    for n_iter, (anchor, pos, neg) in enumerate(val_loader):
        with torch.no_grad():
            anchor = anchor.to(device)
            pos = pos.to(device)
            neg = neg.to(device)

            anchor_feat = model(anchor)
            pos_feat = model(pos)
            neg_feat = model(neg)

            features_list.append(pos_feat.cpu().detach().numpy())
            features_list.append(neg_feat.cpu().detach().numpy())
            for i in range(neg.shape[0]):
                labels_list.append(0)
                labels_list.append(1)
            total_n += anchor_feat.shape[0]
            total_correct += calc_correct(anchor_feat, pos_feat, neg_feat)

            cos = nn.CosineSimilarity(dim=1, eps=1e-6)
            cos_dist = cos(anchor_feat, pos_feat)
            print("cos_dist ap", cos_dist)
            cos_dist = cos(anchor_feat, neg_feat)
            print("cos_dist an", cos_dist)

            loss = triplet_loss(anchor_feat, pos_feat, neg_feat)
            val_loss += loss.item()

    features_list = np.vstack(features_list)
    with open("video_trans_features.npy", "wb") as f:
        np.save(f, features_list)
    with open("labels.npy", "wb") as f:
        np.save(f, labels_list)
    print(f"validation loss {val_loss/len(val_loader)}")
    print(f" acc {total_correct/total_n}")
    logger.info("Validation Results ")


def do_dlc_pair_inference(cfg, model, val_loader, num_query):
    device = default_device(cfg["device"])
    logger = logging.getLogger("transreid.test")
    logger.info("Enter inferencing")

    evaluator = R1_mAP_eval(num_query, max_rank=50, feat_norm=cfg["feat_norm"])

    evaluator.reset()

    if device and torch.cuda.is_available():
        if torch.cuda.device_count() > 1:
            print("Using {} GPUs for inference".format(torch.cuda.device_count()))
            model = nn.DataParallel(model)
        model.to(device)

    model.eval()
    val_loss = 0.0

    total_n = 0.0
    total_correct = 0.0
    for n_iter, ((vec1, gt1), (vec2, gt2)) in enumerate(val_loader):
        with torch.no_grad():
            gt1 = gt1.to(device)
            gt2 = gt2.to(device)
            vec1 = vec1.to(device)
            vec2 = vec2.to(device)

            vec1_feat = model(vec1)
            vec2_feat = model(vec2)

            total_n += vec1_feat.shape[0]
            total_correct += calc_cos_correct(vec1_feat, gt1, vec2_feat, gt2)

    print(f" acc {total_correct/total_n}")
    logger.info("Validation Results ")


--- File: deeplabcut/pose_tracking_pytorch/datasets/dlc_vec.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from torch.utils.data import Dataset
import numpy as np


class TripletDataset(Dataset):
    def __init__(self, datasource, transform=None):
        self.x = datasource

        # normalize vectors here
        """
        for i in range(self.x.shape[0]):
            for j in range(self.x.shape[1]):
                # i th vector at j th kpt
                v = self.x[i][j]
                normalized_v = v / np.sqrt(np.sum(v**2))
                self.x[i][j] = normalized_v
        """

        self.transform = transform

    def __len__(self):
        return self.x.shape[0]

    def __getitem__(self, index):
        anchor, pos, neg = self.x[index]

        anchor = anchor.astype(np.float32)
        pos = pos.astype(np.float32)
        neg = neg.astype(np.float32)

        if self.transform is not None:
            # maybe needs to convert them to embeddings and position token

            anchor = self.transform(anchor)
            pos = self.transform(pos)
            neg = self.transform(neg)

        return anchor, pos, neg


--- File: deeplabcut/pose_tracking_pytorch/datasets/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from .make_dataloader import make_dlc_dataloader


--- File: deeplabcut/pose_tracking_pytorch/datasets/make_dataloader.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from torch.utils.data import DataLoader
from .dlc_vec import TripletDataset


def make_dlc_dataloader(train_list, test_list, batch_size=64):
    train_dataset = TripletDataset(train_list)
    train_loader = DataLoader(train_dataset, batch_size=batch_size)
    val_dataset = TripletDataset(test_list)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    return train_loader, val_loader


--- File: deeplabcut/pose_tracking_pytorch/solver/make_optimizer.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import torch


def make_easy_optimizer(cfg, model):
    params = []
    for key, value in model.named_parameters():
        if not value.requires_grad:
            continue
        lr = cfg["base_lr"]
        weight_decay = cfg["weight_decay"]
        if "bias" in key:
            lr = cfg["base_lr"] * cfg["bias_lr_factor"]
            weight_decay = cfg["weight_decay_bias"]
        if cfg["large_fc_lr"]:
            if "classifier" in key or "arcface" in key:
                lr = cfg["base_lr"] * 2
                print("Using two times learning rate for fc ")

        params += [{"params": [value], "lr": lr, "weight_decay": weight_decay}]
    optimizer_name = cfg["optimizer_name"]
    if optimizer_name == "SGD":
        optimizer = getattr(torch.optim, optimizer_name)(
            params, momentum=cfg["momentum"]
        )
    elif optimizer_name == "AdamW":
        optimizer = torch.optim.AdamW(
            params, lr=cfg["base_lr"], weight_decay=cfg["weight_decay"]
        )
    else:
        optimizer = getattr(torch.optim, optimizer_name)(params)

    return optimizer


--- File: deeplabcut/pose_tracking_pytorch/solver/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from .make_optimizer import make_easy_optimizer


--- File: deeplabcut/pose_tracking_pytorch/solver/scheduler.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from typing import Dict, Any

import torch


class Scheduler:
    """Parameter Scheduler Base Class
    A scheduler base class that can be used to schedule any optimizer parameter groups.

    Unlike the builtin PyTorch schedulers, this is intended to be consistently called
    * At the END of each epoch, before incrementing the epoch count, to calculate next epoch's value
    * At the END of each optimizer update, after incrementing the update count, to calculate next update's value

    The schedulers built on this should try to remain as stateless as possible (for simplicity).

    This family of schedulers is attempting to avoid the confusion of the meaning of 'last_epoch'
    and -1 values for special behaviour. All epoch and update counts must be tracked in the training
    code and explicitly passed in to the schedulers on the corresponding step or step_update call.

    Based on ideas from:
     * https://github.com/pytorch/fairseq/tree/master/fairseq/optim/lr_scheduler
     * https://github.com/allenai/allennlp/tree/master/allennlp/training/learning_rate_schedulers
    """

    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        param_group_field: str,
        noise_range_t=None,
        noise_type="normal",
        noise_pct=0.67,
        noise_std=1.0,
        noise_seed=None,
        initialize: bool = True,
    ) -> None:
        self.optimizer = optimizer
        self.param_group_field = param_group_field
        self._initial_param_group_field = f"initial_{param_group_field}"
        if initialize:
            for i, group in enumerate(self.optimizer.param_groups):
                if param_group_field not in group:
                    raise KeyError(
                        f"{param_group_field} missing from param_groups[{i}]"
                    )
                group.setdefault(
                    self._initial_param_group_field, group[param_group_field]
                )
        else:
            for i, group in enumerate(self.optimizer.param_groups):
                if self._initial_param_group_field not in group:
                    raise KeyError(
                        f"{self._initial_param_group_field} missing from param_groups[{i}]"
                    )
        self.base_values = [
            group[self._initial_param_group_field]
            for group in self.optimizer.param_groups
        ]
        self.metric = None  # any point to having this for all?
        self.noise_range_t = noise_range_t
        self.noise_pct = noise_pct
        self.noise_type = noise_type
        self.noise_std = noise_std
        self.noise_seed = noise_seed if noise_seed is not None else 42
        self.update_groups(self.base_values)

    def state_dict(self) -> Dict[str, Any]:
        return {
            key: value for key, value in self.__dict__.items() if key != "optimizer"
        }

    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
        self.__dict__.update(state_dict)

    def get_epoch_values(self, epoch: int):
        return None

    def get_update_values(self, num_updates: int):
        return None

    def step(self, epoch: int, metric: float = None) -> None:
        self.metric = metric
        values = self.get_epoch_values(epoch)
        if values is not None:
            values = self._add_noise(values, epoch)
            self.update_groups(values)

    def step_update(self, num_updates: int, metric: float = None):
        self.metric = metric
        values = self.get_update_values(num_updates)
        if values is not None:
            values = self._add_noise(values, num_updates)
            self.update_groups(values)

    def update_groups(self, values):
        if not isinstance(values, (list, tuple)):
            values = [values] * len(self.optimizer.param_groups)
        for param_group, value in zip(self.optimizer.param_groups, values):
            param_group[self.param_group_field] = value

    def _add_noise(self, lrs, t):
        if self.noise_range_t is not None:
            if isinstance(self.noise_range_t, (list, tuple)):
                apply_noise = self.noise_range_t[0] <= t < self.noise_range_t[1]
            else:
                apply_noise = t >= self.noise_range_t
            if apply_noise:
                g = torch.Generator()
                g.manual_seed(self.noise_seed + t)
                if self.noise_type == "normal":
                    while True:
                        # resample if noise out of percent limit, brute force but shouldn't spin much
                        noise = torch.randn(1, generator=g).item()
                        if abs(noise) < self.noise_pct:
                            break
                else:
                    noise = (
                        2 * (torch.rand(1, generator=g).item() - 0.5) * self.noise_pct
                    )
                lrs = [v + v * noise for v in lrs]
        return lrs


--- File: deeplabcut/pose_tracking_pytorch/solver/cosine_lr.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
""" Cosine Scheduler

Cosine LR schedule with warmup, cycle/restarts, noise.

Hacked together by / Copyright 2020 Ross Wightman
"""
import logging
import math
import torch

from .scheduler import Scheduler


_logger = logging.getLogger(__name__)


class CosineLRScheduler(Scheduler):
    """
    Cosine decay with restarts.
    This is described in the paper https://arxiv.org/abs/1608.03983.

    Inspiration from
    https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py
    """

    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        t_initial: int,
        t_mul: float = 1.0,
        lr_min: float = 0.0,
        decay_rate: float = 1.0,
        warmup_t=0,
        warmup_lr_init=0,
        warmup_prefix=False,
        cycle_limit=0,
        t_in_epochs=True,
        noise_range_t=None,
        noise_pct=0.67,
        noise_std=1.0,
        noise_seed=42,
        initialize=True,
    ) -> None:
        super().__init__(
            optimizer,
            param_group_field="lr",
            noise_range_t=noise_range_t,
            noise_pct=noise_pct,
            noise_std=noise_std,
            noise_seed=noise_seed,
            initialize=initialize,
        )

        assert t_initial > 0
        assert lr_min >= 0
        if t_initial == 1 and t_mul == 1 and decay_rate == 1:
            _logger.warning(
                "Cosine annealing scheduler will have no effect on the learning "
                "rate since t_initial = t_mul = eta_mul = 1."
            )
        self.t_initial = t_initial
        self.t_mul = t_mul
        self.lr_min = lr_min
        self.decay_rate = decay_rate
        self.cycle_limit = cycle_limit
        self.warmup_t = warmup_t
        self.warmup_lr_init = warmup_lr_init
        self.warmup_prefix = warmup_prefix
        self.t_in_epochs = t_in_epochs
        if self.warmup_t:
            self.warmup_steps = [
                (v - warmup_lr_init) / self.warmup_t for v in self.base_values
            ]
            super().update_groups(self.warmup_lr_init)
        else:
            self.warmup_steps = [1 for _ in self.base_values]

    def _get_lr(self, t):
        if t < self.warmup_t:
            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]
        else:
            if self.warmup_prefix:
                t = t - self.warmup_t

            if self.t_mul != 1:
                i = math.floor(
                    math.log(1 - t / self.t_initial * (1 - self.t_mul), self.t_mul)
                )
                t_i = self.t_mul**i * self.t_initial
                t_curr = t - (1 - self.t_mul**i) / (1 - self.t_mul) * self.t_initial
            else:
                i = t // self.t_initial
                t_i = self.t_initial
                t_curr = t - (self.t_initial * i)

            gamma = self.decay_rate**i
            lr_min = self.lr_min * gamma
            lr_max_values = [v * gamma for v in self.base_values]

            if self.cycle_limit == 0 or (self.cycle_limit > 0 and i < self.cycle_limit):
                lrs = [
                    lr_min
                    + 0.5 * (lr_max - lr_min) * (1 + math.cos(math.pi * t_curr / t_i))
                    for lr_max in lr_max_values
                ]
            else:
                lrs = [self.lr_min for _ in self.base_values]

        return lrs

    def get_epoch_values(self, epoch: int):
        if self.t_in_epochs:
            return self._get_lr(epoch)
        else:
            return None

    def get_update_values(self, num_updates: int):
        if not self.t_in_epochs:
            return self._get_lr(num_updates)
        else:
            return None

    def get_cycle_length(self, cycles=0):
        if not cycles:
            cycles = self.cycle_limit
        cycles = max(1, cycles)
        if self.t_mul == 1.0:
            return self.t_initial * cycles
        else:
            return int(
                math.floor(
                    -self.t_initial * (self.t_mul**cycles - 1) / (1 - self.t_mul)
                )
            )


--- File: deeplabcut/pose_tracking_pytorch/solver/scheduler_factory.py ---
#
# Copyright 2019 Ross Wightman
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Hacked together by / Copyright 2020 Ross Wightman
# https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py
#
""" Scheduler Factory
Hacked together by / Copyright 2020 Ross Wightman
"""
from .cosine_lr import CosineLRScheduler


def create_scheduler(cfg, optimizer, num_epochs=200):
    num_epochs = num_epochs

    lr_min = 0.002 * cfg["base_lr"]
    warmup_lr_init = 0.01 * cfg["base_lr"]

    warmup_t = cfg["warmup_epochs"]
    noise_range = None

    lr_scheduler = CosineLRScheduler(
        optimizer,
        t_initial=num_epochs,
        lr_min=lr_min,
        t_mul=1.0,
        decay_rate=0.1,
        warmup_lr_init=warmup_lr_init,
        warmup_t=warmup_t,
        cycle_limit=1,
        t_in_epochs=True,
        noise_range_t=noise_range,
        noise_pct=0.67,
        noise_std=1.0,
        noise_seed=42,
    )

    return lr_scheduler


--- File: deeplabcut/pose_tracking_pytorch/model/make_model.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import torch
import torch.nn as nn
from .backbones.vit_pytorch import dlc_base_kpt_TransReID


class build_dlc_transformer(nn.Module):
    def __init__(self, cfg, in_chans, kpt_num, factory):
        super(build_dlc_transformer, self).__init__()
        self.cos_layer = cfg["cos_layer"]
        self.in_planes = 128
        self.kpt_num = kpt_num
        self.base = factory["dlc_transreid"](
            in_chans=in_chans,
            sie_xishu=cfg["sie_coe"],
            drop_path_rate=cfg["drop_path"],
            drop_rate=cfg["drop_out"],
            attn_drop_rate=cfg["att_drop_rate"],
            kpt_num=kpt_num,
        )

        self.classifier = nn.Identity()

        self.bottleneck = nn.Identity()

        self.ID_LOSS_TYPE = "cosface"

    def forward(self, x):
        global_feat = self.base(x)

        feat = self.bottleneck(global_feat)

        q = self.classifier(feat)

        norm = torch.norm(q, p=2, dim=1, keepdim=True)
        q = q.div(norm)
        return q

    def load_param(self, trained_path):
        param_dict = torch.load(trained_path)
        for i in param_dict:
            self.state_dict()[i.replace("module.", "")].copy_(param_dict[i])
        print("Loading pretrained model from {}".format(trained_path))


__factory_T_type = {
    "dlc_transreid": dlc_base_kpt_TransReID,
}


def make_dlc_model(cfg, feature_dim, kpt_num):
    model = build_dlc_transformer(cfg, feature_dim, kpt_num, __factory_T_type)

    return model


--- File: deeplabcut/pose_tracking_pytorch/model/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

from .make_model import make_dlc_model, build_dlc_transformer


--- File: deeplabcut/pose_tracking_pytorch/model/backbones/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from .vit_pytorch import dlc_base_kpt_TransReID


--- File: deeplabcut/pose_tracking_pytorch/model/backbones/vit_pytorch.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
""" Vision Transformer (ViT) in PyTorch

A PyTorch implement of Vision Transformers as described in
'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale' - https://arxiv.org/abs/2010.11929

The official jax code is released and available at https://github.com/google-research/vision_transformer

Status/TODO:
* Models updated to be compatible with official impl. Args added to support backward compat for old PyTorch weights.
* Weights ported from official jax impl for 384x384 base and small models, 16x16 and 32x32 patches.
* Trained (supervised on ImageNet-1k) my custom 'small' patch model to 77.9, 'base' to 79.4 top-1 with this code.
* Hopefully find time and GPUs for SSL or unsupervised pretraining on OpenImages w/ ImageNet fine-tune in future.

Acknowledgments:
* The paper authors for releasing code and weights, thanks!
* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out
for some einops/einsum fun
* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT
* Bert reference code checks against Huggingface Transformers and Tensorflow Bert

Hacked together by / Copyright 2020 Ross Wightman
"""
import math
from functools import partial

import torch
import torch.nn as nn
import torch.nn.functional as F


def drop_path(x, drop_prob: float = 0.0, training: bool = False):
    """Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).

    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,
    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...
    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for
    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use
    'survival rate' as the argument.

    """
    if drop_prob == 0.0 or not training:
        return x
    keep_prob = 1 - drop_prob
    shape = (x.shape[0],) + (1,) * (
        x.ndim - 1
    )  # work with diff dim tensors, not just 2D ConvNets
    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
    random_tensor.floor_()  # binarize
    output = x.div(keep_prob) * random_tensor
    return output


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        return drop_path(x, self.drop_prob, self.training)


class Mlp(nn.Module):
    def __init__(
        self,
        in_features,
        hidden_features=None,
        out_features=None,
        act_layer=nn.GELU,
        drop=0.0,
    ):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class Attention(nn.Module):
    def __init__(
        self,
        dim,
        num_heads=8,
        qkv_bias=False,
        qk_scale=None,
        attn_drop=0.0,
        proj_drop=0.0,
    ):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights
        self.scale = qk_scale or head_dim**-0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        qkv = (
            self.qkv(x)
            .reshape(B, N, 3, self.num_heads, C // self.num_heads)
            .permute(2, 0, 3, 1, 4)
        )
        q, k, v = (
            qkv[0],
            qkv[1],
            qkv[2],
        )  # make torchscript happy (cannot use tensor as tuple)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class Block(nn.Module):
    def __init__(
        self,
        dim,
        num_heads,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        drop=0.0,
        attn_drop=0.0,
        drop_path=0.0,
        act_layer=nn.GELU,
        norm_layer=nn.LayerNorm,
    ):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim,
            num_heads=num_heads,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            attn_drop=attn_drop,
            proj_drop=drop,
        )
        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(
            in_features=dim,
            hidden_features=mlp_hidden_dim,
            act_layer=act_layer,
            drop=drop,
        )

    def forward(self, x):
        x = x + self.drop_path(self.attn(self.norm1(x)))
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


class KptEmbed(nn.Module):
    def __init__(self, channel_size=2048, embed_dim=768):
        super().__init__()
        # only channel pooling
        self.proj = nn.Linear(channel_size, embed_dim, 1)

    def forward(self, x):
        # x -> (B, 12, 2048)
        B, kpt_num, channel_size = x.shape
        # x -> (B, 12, 768)
        x = self.proj(x)
        return x


class DLCTransReID(nn.Module):
    def __init__(
        self,
        in_chans=2048,
        num_kpts=12,
        embed_dim=768,
        depth=4,
        num_heads=4,
        mlp_ratio=4.0,
        qkv_bias=False,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        camera=0,
        view=0,
        drop_path_rate=0.0,
        hybrid_backbone=None,
        norm_layer=nn.LayerNorm,
        local_feature=False,
        sie_xishu=1.0,
        **kwargs,
    ):
        # trying to keep everything simple
        super().__init__()
        self.num_features = embed_dim
        self.local_feature = local_feature

        # need to write my own keypoint embedding
        self.kpt_embed = KptEmbed(channel_size=in_chans, embed_dim=embed_dim)

        self.pos_embed = nn.Parameter(torch.zeros(1, num_kpts, embed_dim))

        self.kpt_num = num_kpts
        self.sie_xishu = sie_xishu
        self.pos_drop = nn.Dropout(p=drop_rate)
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]
        self.blocks = nn.ModuleList(
            [
                Block(
                    dim=embed_dim,
                    num_heads=num_heads,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=qkv_bias,
                    qk_scale=qk_scale,
                    drop=drop_rate,
                    attn_drop=attn_drop_rate,
                    drop_path=dpr[i],
                    norm_layer=norm_layer,
                )
                for i in range(depth)
            ]
        )

        self.norm = norm_layer(embed_dim)

        # we don't need a fc layer here
        trunc_normal_(self.pos_embed, std=0.02)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=0.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {"pos_embed", "cls_token"}

    def get_classifier(self):
        return self.head

    def reset_classifier(self, num_classes, global_pool=""):
        self.num_classes = num_classes
        self.fc = (
            nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()
        )

    def forward_features(self, x):
        # x: inputs
        B = x.shape[0]
        # (B, 12, 768)
        x = self.kpt_embed(x)

        # added pos embed
        x = x + self.pos_embed

        # remove all parts I don't understand

        x = self.pos_drop(x)

        # forwarding through blocks. But num of blocks should be reduced

        # i can't really tell what the difference is
        if self.local_feature:
            for blk in self.blocks[:-1]:
                x = blk(x)
            return x

        else:
            for blk in self.blocks:
                x = blk(x)

            x = self.norm(x)

            return x[:, 0]

    def forward(self, x):
        x = self.forward_features(x)

        return x

    def load_param(self, model_path):
        # will have to change some of these
        param_dict = torch.load(model_path, map_location="cpu")
        if "model" in param_dict:
            param_dict = param_dict["model"]
        if "state_dict" in param_dict:
            param_dict = param_dict["state_dict"]
        for k, v in param_dict.items():
            if "head" in k or "dist" in k:
                continue
            if "patch_embed.proj.weight" in k and len(v.shape) < 4:
                # For old models that I trained prior to conv based patchification
                O, I, H, W = self.patch_embed.proj.weight.shape
                v = v.reshape(O, -1, H, W)
            elif k == "pos_embed" and v.shape != self.pos_embed.shape:
                # To resize pos embedding when using model at different size from pretrained weights
                if "distilled" in model_path:
                    print("distill need to choose right cls token in the pth")
                    v = torch.cat([v[:, 0:1], v[:, 2:]], dim=1)
                v = resize_pos_embed(
                    v, self.pos_embed, self.patch_embed.num_y, self.patch_embed.num_x
                )
            try:
                self.state_dict()[k].copy_(v)
            except:
                print("===========================ERROR=========================")
                print(
                    "shape do not match in k :{}: param_dict{} vs self.state_dict(){}".format(
                        k, v.shape, self.state_dict()[k].shape
                    )
                )


def resize_pos_embed(posemb, posemb_new, height, width):
    # Rescale the grid of position embeddings when loading from state_dict. Adapted from
    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224
    ntok_new = posemb_new.shape[1]

    posemb_token, posemb_grid = posemb[:, :1], posemb[0, 1:]
    ntok_new -= 1

    gs_old = int(math.sqrt(len(posemb_grid)))
    print(
        "Resized position embedding from size:{} to size: {} with height:{} width: {}".format(
            posemb.shape, posemb_new.shape, height, width
        )
    )
    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)
    posemb_grid = F.interpolate(posemb_grid, size=(height, width), mode="bilinear")
    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, height * width, -1)
    posemb = torch.cat([posemb_token, posemb_grid], dim=1)
    return posemb


def dlc_base_kpt_TransReID(
    in_chans=2048,
    drop_rate=0.0,
    attn_drop_rate=0.0,
    drop_path_rate=0.1,
    local_feature=False,
    sie_xishu=1.5,
    kpt_num=12,
    **kwargs,
):
    embed_dim = 128
    depth = 4
    num_heads = 4
    mlp_ratio = 1
    num_kpts = kpt_num
    model = DLCTransReID(
        in_chans=in_chans,
        embed_dim=embed_dim,
        depth=depth,
        num_heads=num_heads,
        mlp_ratio=mlp_ratio,
        qkv_bias=True,
        drop_path_rate=drop_path_rate,
        drop_rate=drop_rate,
        attn_drop_rate=attn_drop_rate,
        num_kpts=num_kpts,
        norm_layer=partial(nn.LayerNorm, eps=1e-6),
        sie_xishu=sie_xishu,
        local_feature=local_feature,
        **kwargs,
    )
    return model


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        print(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


--- File: deeplabcut/pose_estimation_3d/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_3d.camera_calibration import *
from deeplabcut.pose_estimation_3d.plotting3D import *
from deeplabcut.pose_estimation_3d.triangulation import *


--- File: deeplabcut/pose_estimation_3d/plotting3D.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import glob
import os
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib.axes._axes import _log as matplotlib_axes_logger

from deeplabcut.utils import (
    auxiliaryfunctions,
    auxiliaryfunctions_3d,
    make_labeled_video,
)
from deeplabcut.utils.auxfun_videos import VideoReader

matplotlib_axes_logger.setLevel("ERROR")
from matplotlib import gridspec
from matplotlib.animation import FFMpegWriter
from matplotlib.collections import LineCollection
from mpl_toolkits.mplot3d.art3d import Line3DCollection
from tqdm import tqdm


def set_up_grid(figsize, xlim, ylim, zlim, view):
    gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 1])
    fig = plt.figure(figsize=figsize)
    axes1 = fig.add_subplot(gs[0, 0])
    axes2 = fig.add_subplot(gs[0, 1])
    axes3 = fig.add_subplot(gs[0, 2], projection="3d")
    axes3.set_xlim3d(xlim)
    axes3.set_ylim3d(ylim)
    axes3.set_zlim3d(zlim)
    axes3.set_box_aspect((1, 1, 1))
    axes3.set_xticklabels([])
    axes3.set_yticklabels([])
    axes3.set_zticklabels([])
    axes3.xaxis.grid(False)
    axes3.view_init(view[0], view[1])
    axes3.set_xlabel("X", fontsize=10)
    axes3.set_ylabel("Y", fontsize=10)
    axes3.set_zlabel("Z", fontsize=10)
    return fig, axes1, axes2, axes3


def create_labeled_video_3d(
    config,
    path,
    videofolder=None,
    start=0,
    end=None,
    trailpoints=0,
    videotype="",
    view=(-113, -270),
    xlim=None,
    ylim=None,
    zlim=None,
    draw_skeleton=True,
    color_by="bodypart",
    figsize=(20, 8),
    fps=30,
    dpi=300,
):
    """
    Creates a video with views from the two cameras and the 3d reconstruction for a selected number of frames.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    path : list
        A list of strings containing the full paths to triangulated files for analysis or a path to the directory, where all the triangulated files are stored.

    videofolder: string
        Full path of the folder where the videos are stored. Use this if the vidoes are stored in a different location other than where the triangulation files are stored. By default is ``None`` and therefore looks for video files in the directory where the triangulation file is stored.

    start: int
        Integer specifying the start of frame index to select. Default is set to 0.

    end: int
        Integer specifying the end of frame index to select. Default is set to None, where all the frames of the video are used for creating the labeled video.

    trailpoints: int
        Number of revious frames whose body parts are plotted in a frame (for displaying history). Default is set to 0.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.\n Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    view: list
        A list that sets the elevation angle in z plane and azimuthal angle in x,y plane of 3d view. Useful for rotating the axis for 3d view

    xlim: list
        A list of integers specifying the limits for xaxis of 3d view. By default it is set to [None,None], where the x limit is set by taking the minimum and maximum value of the x coordinates for all the bodyparts.

    ylim: list
        A list of integers specifying the limits for yaxis of 3d view. By default it is set to [None,None], where the y limit is set by taking the minimum and maximum value of the y coordinates for all the bodyparts.

    zlim: list
        A list of integers specifying the limits for zaxis of 3d view. By default it is set to [None,None], where the z limit is set by taking the minimum and maximum value of the z coordinates for all the bodyparts.

    draw_skeleton: bool
        If ``True`` adds a line connecting the body parts making a skeleton on on each frame. The body parts to be connected and the color of these connecting lines are specified in the config file. By default: ``True``

    color_by : string, optional (default='bodypart')
        Coloring rule. By default, each bodypart is colored differently.
        If set to 'individual', points belonging to a single individual are colored the same.

    Example
    -------
    Linux/MacOs
    >>> deeplabcut.create_labeled_video_3d(config,['/data/project1/videos/3d.h5'],start=100, end=500)

    To create labeled videos for all the triangulated files in the folder
    >>> deeplabcut.create_labeled_video_3d(config,['/data/project1/videos'],start=100, end=500)

    To set the xlim, ylim, zlim and rotate the view of the 3d axis
    >>> deeplabcut.create_labeled_video_3d(config,['/data/project1/videos'],start=100, end=500,view=[30,90],xlim=[-12,12],ylim=[15,25],zlim=[20,30])

    """
    start_path = os.getcwd()

    # Read the config file and related variables
    cfg_3d = auxiliaryfunctions.read_config(config)
    cam_names = cfg_3d["camera_names"]
    pcutoff = cfg_3d["pcutoff"]
    markerSize = cfg_3d["dotsize"]
    alphaValue = cfg_3d["alphaValue"]
    cmap = cfg_3d["colormap"]
    bodyparts2connect = cfg_3d["skeleton"]
    skeleton_color = cfg_3d["skeleton_color"]
    scorer_3d = cfg_3d["scorername_3d"]

    if color_by not in ("bodypart", "individual"):
        raise ValueError(f"Invalid color_by={color_by}")

    file_list = auxiliaryfunctions_3d.Get_list_of_triangulated_and_videoFiles(
        path, videotype, scorer_3d, cam_names, videofolder
    )
    print(file_list)
    if file_list == []:
        raise Exception(
            "No corresponding video file(s) found for the specified triangulated file or folder. Did you specify the video file type? If videos are stored in a different location, please use the ``videofolder`` argument to specify their path."
        )

    for file in file_list:
        path_h5_file = Path(file[0]).parents[0]
        triangulate_file = file[0]
        # triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]
        # name for output video
        file_name = str(Path(triangulate_file).stem)
        videooutname = os.path.join(path_h5_file, file_name + ".mp4")
        if os.path.isfile(videooutname):
            print("Video already created...")
        else:
            string_to_remove = str(Path(triangulate_file).suffix)
            pickle_file = triangulate_file.replace(string_to_remove, "_meta.pickle")
            metadata_ = auxiliaryfunctions_3d.LoadMetadata3d(pickle_file)

            base_filename_cam1 = str(Path(file[1]).stem).split(videotype)[
                0
            ]  # required for searching the filtered file
            base_filename_cam2 = str(Path(file[2]).stem).split(videotype)[
                0
            ]  # required for searching the filtered file
            cam1_view_video = file[1]
            cam2_view_video = file[2]
            cam1_scorer = metadata_["scorer_name"][cam_names[0]]
            cam2_scorer = metadata_["scorer_name"][cam_names[1]]
            print(
                "Creating 3D video from %s and %s using %s"
                % (
                    Path(cam1_view_video).name,
                    Path(cam2_view_video).name,
                    Path(triangulate_file).name,
                )
            )

            # Read the video files and corresponfing h5 files
            vid_cam1 = VideoReader(cam1_view_video)
            vid_cam2 = VideoReader(cam2_view_video)

            # Look for the filtered predictions file
            try:
                print("Looking for filtered predictions...")
                df_cam1 = pd.read_hdf(
                    glob.glob(
                        os.path.join(
                            path_h5_file,
                            str(
                                "*" + base_filename_cam1 + cam1_scorer + "*filtered.h5"
                            ),
                        )
                    )[0]
                )
                df_cam2 = pd.read_hdf(
                    glob.glob(
                        os.path.join(
                            path_h5_file,
                            str(
                                "*" + base_filename_cam2 + cam2_scorer + "*filtered.h5"
                            ),
                        )
                    )[0]
                )
                # print("Found filtered predictions, will be use these for triangulation.")
                print(
                    "Found the following filtered data: ",
                    os.path.join(
                        path_h5_file,
                        str("*" + base_filename_cam1 + cam1_scorer + "*filtered.h5"),
                    ),
                    os.path.join(
                        path_h5_file,
                        str("*" + base_filename_cam2 + cam2_scorer + "*filtered.h5"),
                    ),
                )
            except IndexError:
                print(
                    "No filtered predictions found, the unfiltered predictions will be used instead."
                )
                df_cam1 = pd.read_hdf(
                    glob.glob(
                        os.path.join(
                            path_h5_file, str(base_filename_cam1 + cam1_scorer + "*.h5")
                        )
                    )[0]
                )
                df_cam2 = pd.read_hdf(
                    glob.glob(
                        os.path.join(
                            path_h5_file, str(base_filename_cam2 + cam2_scorer + "*.h5")
                        )
                    )[0]
                )

            df_3d = pd.read_hdf(triangulate_file)
            try:
                num_animals = (
                    df_3d.columns.get_level_values("individuals").unique().size
                )
            except KeyError:
                num_animals = 1

            if end is None:
                end = len(df_3d)  # All the frames
            end = min(end, min(len(vid_cam1), len(vid_cam2)))
            frames = list(range(start, end))

            output_folder = Path(os.path.join(path_h5_file, "temp_" + file_name))
            output_folder.mkdir(parents=True, exist_ok=True)

            # Flatten the list of bodyparts to connect
            bodyparts2plot = list(
                np.unique([val for sublist in bodyparts2connect for val in sublist])
            )

            # Format data
            mask2d = df_cam1.columns.get_level_values("bodyparts").isin(bodyparts2plot)
            xy1 = (
                df_cam1.iloc[: len(df_3d)]
                .loc[:, mask2d]
                .to_numpy()
                .reshape((len(df_3d), -1, 3))
            )
            visible1 = xy1[..., 2] >= pcutoff
            xy1[~visible1] = np.nan
            xy2 = (
                df_cam2.iloc[: len(df_3d)]
                .loc[:, mask2d]
                .to_numpy()
                .reshape((len(df_3d), -1, 3))
            )
            visible2 = xy2[..., 2] >= pcutoff
            xy2[~visible2] = np.nan
            mask = df_3d.columns.get_level_values("bodyparts").isin(bodyparts2plot)
            xyz = df_3d.loc[:, mask].to_numpy().reshape((len(df_3d), -1, 3))
            xyz[~(visible1 & visible2)] = np.nan

            bpts = df_3d.columns.get_level_values("bodyparts")[mask][::3]
            links = make_labeled_video.get_segment_indices(
                bodyparts2connect,
                bpts,
            )
            ind_links = tuple(zip(*links))

            if color_by == "bodypart":
                color = plt.cm.get_cmap(cmap, len(bodyparts2plot))
                colors_ = color(range(len(bodyparts2plot)))
                colors = np.tile(colors_, (num_animals, 1))
            elif color_by == "individual":
                color = plt.cm.get_cmap(cmap, num_animals)
                colors_ = color(range(num_animals))
                colors = np.repeat(colors_, len(bodyparts2plot), axis=0)

            # Trick to force equal aspect ratio of 3D plots
            minmax = np.nanpercentile(xyz[frames], q=[25, 75], axis=(0, 1)).T
            minmax *= 1.1
            minmax_range = (minmax[:, 1] - minmax[:, 0]).max() / 2
            if xlim is None:
                mid_x = np.mean(minmax[0])
                xlim = mid_x - minmax_range, mid_x + minmax_range
            if ylim is None:
                mid_y = np.mean(minmax[1])
                ylim = mid_y - minmax_range, mid_y + minmax_range
            if zlim is None:
                mid_z = np.mean(minmax[2])
                zlim = mid_z - minmax_range, mid_z + minmax_range

            # Set up the matplotlib figure beforehand
            fig, axes1, axes2, axes3 = set_up_grid(figsize, xlim, ylim, zlim, view)
            points_2d1 = axes1.scatter(
                *np.zeros((2, len(bodyparts2plot))),
                s=markerSize,
                alpha=alphaValue,
            )
            im1 = axes1.imshow(np.zeros((vid_cam1.height, vid_cam1.width)))
            points_2d2 = axes2.scatter(
                *np.zeros((2, len(bodyparts2plot))),
                s=markerSize,
                alpha=alphaValue,
            )
            im2 = axes2.imshow(np.zeros((vid_cam2.height, vid_cam2.width)))
            points_3d = axes3.scatter(
                *np.zeros((3, len(bodyparts2plot))),
                s=markerSize,
                alpha=alphaValue,
            )
            if draw_skeleton:
                # Set up skeleton LineCollections
                segs = np.zeros((2, len(ind_links), 2))
                coll1 = LineCollection(segs, colors=skeleton_color)
                coll2 = LineCollection(segs, colors=skeleton_color)
                axes1.add_collection(coll1)
                axes2.add_collection(coll2)
                segs = np.zeros((2, len(ind_links), 3))
                coll_3d = Line3DCollection(segs, colors=skeleton_color)
                axes3.add_collection(coll_3d)

            writer = FFMpegWriter(fps=fps)
            with writer.saving(fig, videooutname, dpi=dpi):
                for k in tqdm(frames):
                    vid_cam1.set_to_frame(k)
                    vid_cam2.set_to_frame(k)
                    frame_cam1 = vid_cam1.read_frame()
                    frame_cam2 = vid_cam2.read_frame()
                    if frame_cam1 is None or frame_cam2 is None:
                        raise IOError("A video frame is empty.")

                    im1.set_data(frame_cam1)
                    im2.set_data(frame_cam2)

                    sl = slice(max(0, k - trailpoints), k + 1)
                    coords3d = xyz[sl]
                    coords1 = xy1[sl, :, :2]
                    coords2 = xy2[sl, :, :2]
                    points_3d._offsets3d = coords3d.reshape((-1, 3)).T
                    points_3d.set_color(colors)
                    points_2d1.set_offsets(coords1.reshape((-1, 2)))
                    points_2d1.set_color(colors)
                    points_2d2.set_offsets(coords2.reshape((-1, 2)))
                    points_2d2.set_color(colors)
                    if draw_skeleton:
                        segs3d = xyz[k][tuple([ind_links])].swapaxes(0, 1)
                        coll_3d.set_segments(segs3d)
                        segs1 = xy1[k, :, :2][tuple([ind_links])].swapaxes(0, 1)
                        coll1.set_segments(segs1)
                        segs2 = xy2[k, :, :2][tuple([ind_links])].swapaxes(0, 1)
                        coll2.set_segments(segs2)

                    writer.grab_frame()


--- File: deeplabcut/pose_estimation_3d/camera_calibration.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import glob
import os
import pickle
from pathlib import Path

import cv2
import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.axes._axes import _log as matplotlib_axes_logger

from deeplabcut.utils import auxiliaryfunctions
from deeplabcut.utils import auxiliaryfunctions_3d

matplotlib_axes_logger.setLevel("ERROR")


def calibrate_cameras(
    config, cbrow=8, cbcol=6, calibrate=False, alpha=0.4, search_window_size=(11, 11)
):
    """This function extracts the corners points from the calibration images, calibrates the camera and stores the calibration files in the project folder (defined in the config file).

    Make sure you have around 20-60 pairs of calibration images. The function should be used iteratively to select the right set of calibration images.

    A pair of calibration image is considered "correct", if the corners are detected correctly in both the images. It may happen that during the first run of this function,
    the extracted corners are incorrect or the order of detected corners does not align for the corresponding views (i.e. camera-1 and camera-2 images).

    In such a case, remove those pairs of images and re-run this function. Once the right number of calibration images are selected,
    use the parameter ``calibrate=True`` to calibrate the cameras.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    cbrow : int
        Integer specifying the number of rows in the calibration image.

    cbcol : int
        Integer specifying the number of columns in the calibration image.

    calibrate : bool
        If this is set to True, the cameras are calibrated with the current set of calibration images. The default is ``False``
        Set it to True, only after checking the results of the corner detection method and removing dysfunctional images!

    alpha: float
        Floating point number between 0 and 1 specifying the free scaling parameter. When alpha = 0, the rectified images with only valid pixels are stored
        i.e. the rectified images are zoomed in. When alpha = 1, all the pixels from the original images are retained.
        For more details: https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html

    search_window_size: tuple of int
        Half of the side length of the search window when refining detected checkerboard corners for subpixel accuracy.

    Example
    --------
    Linux/MacOs/Windows
    >>> deeplabcut.calibrate_camera(config)

    Once the right set of calibration images are selected,
    >>> deeplabcut.calibrate_camera(config,calibrate=True)

    """
    # Termination criteria
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)

    # Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
    objp = np.zeros((cbrow * cbcol, 3), np.float32)
    objp[:, :2] = np.mgrid[0:cbcol, 0:cbrow].T.reshape(-1, 2)

    # Read the config file
    cfg_3d = auxiliaryfunctions.read_config(config)
    (
        img_path,
        path_corners,
        path_camera_matrix,
        path_undistort,
        path_removed_images,
    ) = auxiliaryfunctions_3d.Foldernames3Dproject(cfg_3d)

    images = glob.glob(os.path.join(img_path, "*.jpg"))
    cam_names = cfg_3d["camera_names"]

    # update the variable snapshot* in config file according to the name of the cameras
    try:
        for i in range(len(cam_names)):
            cfg_3d[str("config_file_" + cam_names[i])] = cfg_3d.pop(
                str("config_file_camera-" + str(i + 1))
            )
        for i in range(len(cam_names)):
            cfg_3d[str("shuffle_" + cam_names[i])] = cfg_3d.pop(
                str("shuffle_camera-" + str(i + 1))
            )
    except:
        pass

    project_path = cfg_3d["project_path"]
    projconfigfile = os.path.join(str(project_path), "config.yaml")
    auxiliaryfunctions.write_config_3d(projconfigfile, cfg_3d)

    # Initialize the dictionary
    img_shape = {}
    objpoints = {}  # 3d point in real world space
    imgpoints = {}  # 2d points in image plane.
    dist_pickle = {}
    stereo_params = {}
    for cam in cam_names:
        objpoints.setdefault(cam, [])
        imgpoints.setdefault(cam, [])
        dist_pickle.setdefault(cam, [])

    # Sort the images.
    images.sort(key=lambda f: int("".join(filter(str.isdigit, f))))
    if len(images) == 0:
        raise Exception(
            "No calibration images found. Make sure the calibration images are saved as .jpg and with prefix as the camera name as specified in the config.yaml file."
        )

    skip_images = []
    for fname in images:
        for cam in cam_names:
            if cam in fname and Path(fname).name not in skip_images:
                filename = Path(fname).stem
                img = cv2.imread(fname)
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

                # Find the chess board corners
                ret, corners = cv2.findChessboardCorners(
                    gray, (cbcol, cbrow), None
                )  #  (8,6) pattern (dimensions = common points of black squares)
                # If found, add object points, image points (after refining them)

                if ret == True:
                    img_shape[cam] = gray.shape[::-1]
                    objpoints[cam].append(objp)
                    corners = cv2.cornerSubPix(
                        gray, corners, search_window_size, (-1, -1), criteria
                    )
                    imgpoints[cam].append(corners)
                    # Draw the corners and store the images
                    img = cv2.drawChessboardCorners(img, (cbcol, cbrow), corners, ret)
                    cv2.imwrite(
                        os.path.join(str(path_corners), filename + "_corner.jpg"), img
                    )
                else:
                    print("Corners not found for the image %s" % Path(fname).name)
                    for new_cam in cam_names:
                        remove_fname = Path(fname).name.replace(cam, new_cam)
                        os.rename(
                            os.path.join(str(img_path), remove_fname),
                            os.path.join(str(path_removed_images), remove_fname),
                        )
                        if new_cam != cam:
                            skip_images.append(remove_fname)

    try:
        h, w = img.shape[:2]
    except:
        raise Exception(
            "It seems that the name of calibration images does not match with the camera names in the config file. Please make sure that the calibration images are named with camera names as specified in the config.yaml file."
        )

    # Perform calibration for each cameras and store the matrices as a pickle file
    if calibrate == True:
        # Calibrating each camera
        for cam in cam_names:
            ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(
                objpoints[cam], imgpoints[cam], img_shape[cam], None, None
            )

            # Save the camera calibration result for later use (we won't use rvecs / tvecs)
            dist_pickle[cam] = {
                "mtx": mtx,
                "dist": dist,
                "objpoints": objpoints[cam],
                "imgpoints": imgpoints[cam],
            }
            pickle.dump(
                dist_pickle,
                open(
                    os.path.join(path_camera_matrix, cam + "_intrinsic_params.pickle"),
                    "wb",
                ),
            )
            print(
                "Saving intrinsic camera calibration matrices for %s as a pickle file in %s"
                % (cam, os.path.join(path_camera_matrix))
            )

            # Compute mean re-projection errors for individual cameras
            mean_error = 0
            for i in range(len(objpoints[cam])):
                imgpoints_proj, _ = cv2.projectPoints(
                    objpoints[cam][i], rvecs[i], tvecs[i], mtx, dist
                )
                error = cv2.norm(imgpoints[cam][i], imgpoints_proj, cv2.NORM_L2) / len(
                    imgpoints_proj
                )
                mean_error += error
            print(
                "Mean re-projection error for %s images: %.3f pixels "
                % (cam, mean_error / len(objpoints[cam]))
            )

        # Compute stereo calibration for each pair of cameras
        camera_pair = [[cam_names[0], cam_names[1]]]
        for pair in camera_pair:
            print("Computing stereo calibration for " % pair)
            (
                retval,
                cameraMatrix1,
                distCoeffs1,
                cameraMatrix2,
                distCoeffs2,
                R,
                T,
                E,
                F,
            ) = cv2.stereoCalibrate(
                objpoints[pair[0]],
                imgpoints[pair[0]],
                imgpoints[pair[1]],
                dist_pickle[pair[0]]["mtx"],
                dist_pickle[pair[0]]["dist"],
                dist_pickle[pair[1]]["mtx"],
                dist_pickle[pair[1]]["dist"],
                (h, w),
                flags=cv2.CALIB_FIX_INTRINSIC,
            )

            # Stereo Rectification
            rectify_scale = alpha  # Free scaling parameter check this https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html#fisheye-stereorectify
            R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(
                cameraMatrix1,
                distCoeffs1,
                cameraMatrix2,
                distCoeffs2,
                (h, w),
                R,
                T,
                alpha=rectify_scale,
            )

            stereo_params[pair[0] + "-" + pair[1]] = {
                "cameraMatrix1": cameraMatrix1,
                "cameraMatrix2": cameraMatrix2,
                "distCoeffs1": distCoeffs1,
                "distCoeffs2": distCoeffs2,
                "R": R,
                "T": T,
                "E": E,
                "F": F,
                "R1": R1,
                "R2": R2,
                "P1": P1,
                "P2": P2,
                "roi1": roi1,
                "roi2": roi2,
                "Q": Q,
                "image_shape": [img_shape[pair[0]], img_shape[pair[1]]],
            }

        print(
            "Saving the stereo parameters for every pair of cameras as a pickle file in %s"
            % str(os.path.join(path_camera_matrix))
        )

        auxiliaryfunctions.write_pickle(
            os.path.join(path_camera_matrix, "stereo_params.pickle"), stereo_params
        )
        print(
            "Camera calibration done! Use the function ``check_undistortion`` to check the check the calibration"
        )
    else:
        print(
            "Corners extracted! You may check for the extracted corners in the directory %s and remove the pair of images where the corners are incorrectly detected. If all the corners are detected correctly with right order, then re-run the same function and use the flag ``calibrate=True``, to calbrate the camera."
            % str(path_corners)
        )


def check_undistortion(config, cbrow=8, cbcol=6, plot=True):
    """
    This function undistorts the calibration images based on the camera matrices and stores them in the project folder(defined in the config file)
    to visually check if the camera matrices are correct.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    cbrow : int
        Int specifying the number of rows in the calibration image.

    cbcol : int
        Int specifying the number of columns in the calibration image.

    plot : bool
        If this is set to True, the results of undistortion are saved as plots. The default is ``True``; if provided it must be either ``True`` or ``False``.

    Example
    --------
    Linux/MacOs/Windows
    >>> deeplabcut.check_undistortion(config, cbrow = 8,cbcol = 6)

    """

    # Read the config file
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
    cfg_3d = auxiliaryfunctions.read_config(config)
    (
        img_path,
        path_corners,
        path_camera_matrix,
        path_undistort,
        path_removed_images,
    ) = auxiliaryfunctions_3d.Foldernames3Dproject(cfg_3d)

    # colormap = plt.get_cmap(cfg_3d['colormap'])
    markerSize = cfg_3d["dotsize"]
    alphaValue = cfg_3d["alphaValue"]
    markerType = cfg_3d["markerType"]
    markerColor = cfg_3d["markerColor"]
    cam_names = cfg_3d["camera_names"]

    images = glob.glob(os.path.join(img_path, "*.jpg"))

    # Sort the images
    images.sort(key=lambda f: int("".join(filter(str.isdigit, f))))
    """
    for fname in images:
        for cam in cam_names:
            if cam in fname:
                filename = Path(fname).stem
                ext = Path(fname).suffix
                img = cv2.imread(fname)
                gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
    """
    camera_pair = [[cam_names[0], cam_names[1]]]
    stereo_params = auxiliaryfunctions.read_pickle(
        os.path.join(path_camera_matrix, "stereo_params.pickle")
    )

    for pair in camera_pair:
        map1_x, map1_y = cv2.initUndistortRectifyMap(
            stereo_params[pair[0] + "-" + pair[1]]["cameraMatrix1"],
            stereo_params[pair[0] + "-" + pair[1]]["distCoeffs1"],
            stereo_params[pair[0] + "-" + pair[1]]["R1"],
            stereo_params[pair[0] + "-" + pair[1]]["P1"],
            (stereo_params[pair[0] + "-" + pair[1]]["image_shape"][0]),
            cv2.CV_16SC2,
        )
        map2_x, map2_y = cv2.initUndistortRectifyMap(
            stereo_params[pair[0] + "-" + pair[1]]["cameraMatrix2"],
            stereo_params[pair[0] + "-" + pair[1]]["distCoeffs2"],
            stereo_params[pair[0] + "-" + pair[1]]["R2"],
            stereo_params[pair[0] + "-" + pair[1]]["P2"],
            (stereo_params[pair[0] + "-" + pair[1]]["image_shape"][1]),
            cv2.CV_16SC2,
        )
        cam1_undistort = []
        cam2_undistort = []

        for fname in images:
            if pair[0] in fname:
                filename = Path(fname).stem
                img1 = cv2.imread(fname)
                gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
                h, w = img1.shape[:2]
                _, corners1 = cv2.findChessboardCorners(gray1, (cbcol, cbrow), None)
                corners_origin1 = cv2.cornerSubPix(
                    gray1, corners1, (11, 11), (-1, -1), criteria
                )

                # Remapping dataFrame_camera1_undistort
                im_remapped1 = cv2.remap(img1, map1_x, map1_y, cv2.INTER_LANCZOS4)
                imgpoints_proj_undistort = cv2.undistortPoints(
                    src=corners_origin1,
                    cameraMatrix=stereo_params[pair[0] + "-" + pair[1]][
                        "cameraMatrix1"
                    ],
                    distCoeffs=stereo_params[pair[0] + "-" + pair[1]]["distCoeffs1"],
                    P=stereo_params[pair[0] + "-" + pair[1]]["P1"],
                    R=stereo_params[pair[0] + "-" + pair[1]]["R1"],
                )
                cam1_undistort.append(imgpoints_proj_undistort)
                cv2.imwrite(
                    os.path.join(str(path_undistort), filename + "_undistort.jpg"),
                    im_remapped1,
                )
                imgpoints_proj_undistort = []

            elif pair[1] in fname:
                filename = Path(fname).stem
                img2 = cv2.imread(fname)
                gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
                h, w = img2.shape[:2]
                _, corners2 = cv2.findChessboardCorners(gray2, (cbcol, cbrow), None)
                corners_origin2 = cv2.cornerSubPix(
                    gray2, corners2, (11, 11), (-1, -1), criteria
                )

                # Remapping
                im_remapped2 = cv2.remap(img2, map2_x, map2_y, cv2.INTER_LANCZOS4)
                imgpoints_proj_undistort2 = cv2.undistortPoints(
                    src=corners_origin2,
                    cameraMatrix=stereo_params[pair[0] + "-" + pair[1]][
                        "cameraMatrix2"
                    ],
                    distCoeffs=stereo_params[pair[0] + "-" + pair[1]]["distCoeffs2"],
                    P=stereo_params[pair[0] + "-" + pair[1]]["P2"],
                    R=stereo_params[pair[0] + "-" + pair[1]]["R2"],
                )
                cam2_undistort.append(imgpoints_proj_undistort2)
                cv2.imwrite(
                    os.path.join(str(path_undistort), filename + "_undistort.jpg"),
                    im_remapped2,
                )
                imgpoints_proj_undistort2 = []

        cam1_undistort = np.array(cam1_undistort)
        cam2_undistort = np.array(cam2_undistort)
        print("All images are undistorted and stored in %s" % str(path_undistort))
        print(
            "Use the function ``triangulate`` to undistort the dataframes and compute the triangulation"
        )

        if plot == True:
            f1, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
            f1.suptitle(
                str("Original Image: Views from " + pair[0] + " and " + pair[1]),
                fontsize=25,
            )

            # Display images in RGB
            ax1.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))
            ax2.imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))

            norm = mcolors.Normalize(vmin=0.0, vmax=cam1_undistort.shape[1])
            plt.savefig(os.path.join(str(path_undistort), "Original_Image.png"))

            # Plot the undistorted corner points
            f2, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))
            f2.suptitle(
                "Undistorted corner points on camera-1 and camera-2", fontsize=25
            )
            ax1.imshow(cv2.cvtColor(im_remapped1, cv2.COLOR_BGR2RGB))
            ax2.imshow(cv2.cvtColor(im_remapped2, cv2.COLOR_BGR2RGB))
            for i in range(0, cam1_undistort.shape[1]):
                ax1.scatter(
                    [cam1_undistort[-1][i, 0, 0]],
                    [cam1_undistort[-1][i, 0, 1]],
                    marker=markerType,
                    s=markerSize,
                    color=markerColor,
                    alpha=alphaValue,
                )
                ax2.scatter(
                    [cam2_undistort[-1][i, 0, 0]],
                    [cam2_undistort[-1][i, 0, 1]],
                    marker=markerType,
                    s=markerSize,
                    color=markerColor,
                    alpha=alphaValue,
                )
            plt.savefig(os.path.join(str(path_undistort), "undistorted_points.png"))

            # Triangulate
            triangulate = (
                auxiliaryfunctions_3d.compute_triangulation_calibration_images(
                    stereo_params[pair[0] + "-" + pair[1]],
                    cam1_undistort,
                    cam2_undistort,
                    path_undistort,
                    cfg_3d,
                    plot=True,
                )
            )
            auxiliaryfunctions.write_pickle("triangulate.pickle", triangulate)


--- File: deeplabcut/pose_estimation_3d/triangulation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#

import os
from pathlib import Path
import cv2
import numpy as np
import pandas as pd

from deeplabcut.utils import auxfun_multianimal, auxiliaryfunctions
from deeplabcut.utils import auxiliaryfunctions_3d
from deeplabcut.core.trackingutils import TRACK_METHODS


def triangulate(
    config,
    video_path,
    videotype="",
    filterpredictions=True,
    filtertype="median",
    gputouse=None,
    destfolder=None,
    save_as_csv=False,
    track_method="",
):
    """
    This function triangulates the detected DLC-keypoints from the two camera views
    using the camera matrices (derived from calibration) to calculate 3D predictions.

    Parameters
    ----------
    config : string
        Full path of the config.yaml file as a string.

    video_path : string/list of list
        Full path of the directory where videos are saved. If the user wants to analyze
        only a pair of videos, the user needs to pass them as a list of list of videos,
        i.e. [['video1-camera-1.avi','video1-camera-2.avi']]

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.\n Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.


    filterpredictions: Bool, optional
        Filter the predictions with filter specified by "filtertype". If specified it
        should be either ``True`` or ``False``.

    filtertype: string
        Select which filter, 'arima' or 'median' filter (currently supported).

    gputouse: int, optional. Natural number indicating the number of your GPU (see number in nvidia-smi).
        If you do not have a GPU put None.
        See: https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries

    destfolder: string, optional
        Specifies the destination folder for analysis data (default is the path of the video)

    save_as_csv: bool, optional
        Saves the predictions in a .csv file. The default is ``False``

    Example
    -------
    Linux/MacOS
    To analyze all the videos in the directory:
    >>> deeplabcut.triangulate(config,'/data/project1/videos/')

    To analyze only a few pairs of videos:
    >>> deeplabcut.triangulate(config,[['/data/project1/videos/video1-camera-1.avi','/data/project1/videos/video1-camera-2.avi'],['/data/project1/videos/video2-camera-1.avi','/data/project1/videos/video2-camera-2.avi']])


    Windows
    To analyze all the videos in the directory:
    >>> deeplabcut.triangulate(config,'C:\\yourusername\\rig-95\\Videos')

    To analyze only a few pair of videos:
    >>> deeplabcut.triangulate(config,[['C:\\yourusername\\rig-95\\Videos\\video1-camera-1.avi','C:\\yourusername\\rig-95\\Videos\\video1-camera-2.avi'],['C:\\yourusername\\rig-95\\Videos\\video2-camera-1.avi','C:\\yourusername\\rig-95\\Videos\\video2-camera-2.avi']])
    """
    from deeplabcut.compat import analyze_videos
    from deeplabcut.post_processing import filtering

    cfg_3d = auxiliaryfunctions.read_config(config)
    cam_names = cfg_3d["camera_names"]
    pcutoff = cfg_3d["pcutoff"]
    scorer_3d = cfg_3d["scorername_3d"]

    snapshots = {}
    for cam in cam_names:
        snapshots[cam] = cfg_3d[str("config_file_" + cam)]
        # Check if the config file exists
        if not os.path.exists(snapshots[cam]):
            raise Exception(
                str(
                    "It seems the file specified in the variable config_file_"
                    + str(cam)
                )
                + " does not exist. Please edit the config file with correct file path and retry."
            )

    # flag to check if the video_path variable is a string or a list of list
    flag = False  # assumes that video path is a list
    if isinstance(video_path, str) == True:
        flag = True
        video_list = auxiliaryfunctions_3d.get_camerawise_videos(
            video_path, cam_names, videotype=videotype
        )
    else:
        video_list = video_path

    if video_list == []:
        print("No videos found in the specified video path.", video_path)
        print(
            "Please make sure that the video names are specified with correct camera names as entered in the config file or"
        )
        print(
            "perhaps the videotype is distinct from the videos in the path, I was looking for:",
            videotype,
        )

    print("List of pairs:", video_list)
    scorer_name = {}
    run_triangulate = False
    for i in range(len(video_list)):
        dataname = []
        for j in range(len(video_list[i])):  # looping over cameras
            if cam_names[j] not in video_list[i][j]:
                raise ValueError(
                    f"Camera name '{cam_names[j]}' "
                    f"not found in video list '{video_list[i][j]}'."
                )
            else:
                print(
                    "Analyzing video %s using %s"
                    % (video_list[i][j], str("config_file_" + cam_names[j]))
                )

                config_2d = snapshots[cam_names[j]]
                cfg = auxiliaryfunctions.read_config(config_2d)

                # Get track_method and do related checks
                track_method = auxfun_multianimal.get_track_method(
                    cfg, track_method=track_method
                )
                if (
                    len(cfg.get("multianimalbodyparts", [])) == 1
                    and track_method != "box"
                ):
                    warnings.warn(
                        "Switching to `box` tracker for single point tracking..."
                    )
                    track_method = "box"

                # Get track method suffix
                tr_method_suffix = TRACK_METHODS.get(track_method, "")

                shuffle = cfg_3d[str("shuffle_" + cam_names[j])]
                trainingsetindex = cfg_3d[str("trainingsetindex_" + cam_names[j])]
                trainFraction = cfg["TrainingFraction"][trainingsetindex]
                if flag == True:
                    video = os.path.join(video_path, video_list[i][j])
                else:
                    video_path = str(Path(video_list[i][j]).parents[0])
                    video = os.path.join(video_path, video_list[i][j])

                if destfolder is None:
                    destfolder = str(Path(video).parents[0])

                vname = Path(video).stem
                prefix = str(vname).split(cam_names[j])[0]
                suffix = str(vname).split(cam_names[j])[-1]
                if prefix == "":
                    pass
                elif prefix[-1] == "_" or prefix[-1] == "-":
                    prefix = prefix[:-1]

                if suffix == "":
                    pass
                elif suffix[0] == "_" or suffix[0] == "-":
                    suffix = suffix[1:]

                if prefix == "":
                    output_file = os.path.join(destfolder, suffix)
                else:
                    if suffix == "":
                        output_file = os.path.join(destfolder, prefix)
                    else:
                        output_file = os.path.join(destfolder, prefix + "_" + suffix)

                output_filename = os.path.join(
                    output_file + "_" + scorer_3d
                )  # Check if the videos are already analyzed for 3d
                if os.path.isfile(output_filename + ".h5"):
                    if save_as_csv is True and not os.path.exists(
                        output_filename + ".csv"
                    ):
                        # In case user adds save_as_csv is True after triangulating
                        pd.read_hdf(output_filename + ".h5").to_csv(
                            str(output_filename + ".csv")
                        )

                    print(
                        "Already analyzed...Checking the meta data for any change in the camera matrices and/or scorer names",
                        vname,
                    )
                    pickle_file = str(output_filename + "_meta.pickle")
                    metadata_ = auxiliaryfunctions_3d.LoadMetadata3d(pickle_file)
                    (
                        img_path,
                        path_corners,
                        path_camera_matrix,
                        path_undistort,
                        _,
                    ) = auxiliaryfunctions_3d.Foldernames3Dproject(cfg_3d)
                    path_stereo_file = os.path.join(
                        path_camera_matrix, "stereo_params.pickle"
                    )
                    stereo_file = auxiliaryfunctions.read_pickle(path_stereo_file)
                    cam_pair = str(cam_names[0] + "-" + cam_names[1])
                    is_video_analyzed = False  # variable to keep track if the video was already analyzed
                    # Check for the camera matrix
                    for k in metadata_["stereo_matrix"].keys():
                        if np.all(
                            metadata_["stereo_matrix"][k] == stereo_file[cam_pair][k]
                        ):
                            pass
                        else:
                            run_triangulate = True

                    # Check for scorer names in the pickle file of 3d output
                    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
                        cfg, shuffle, trainFraction, trainingsiterations="unknown"
                    )

                    if (
                        metadata_["scorer_name"][cam_names[j]] == DLCscorer
                    ):  # TODO: CHECK FOR BOTH?
                        is_video_analyzed = True
                    elif metadata_["scorer_name"][cam_names[j]] == DLCscorerlegacy:
                        is_video_analyzed = True
                    else:
                        is_video_analyzed = False
                        run_triangulate = True

                    if is_video_analyzed:
                        print("This file is already analyzed!")
                        dataname.append(
                            os.path.join(
                                destfolder, vname + DLCscorer + tr_method_suffix + ".h5"
                            )
                        )
                        scorer_name[cam_names[j]] = DLCscorer
                    else:
                        # Analyze video if score name is different
                        DLCscorer = analyze_videos(
                            config_2d,
                            [video],
                            videotype=videotype,
                            shuffle=shuffle,
                            trainingsetindex=trainingsetindex,
                            gputouse=gputouse,
                            destfolder=destfolder,
                        )
                        scorer_name[cam_names[j]] = DLCscorer
                        is_video_analyzed = False
                        run_triangulate = True
                        suffix = tr_method_suffix
                        if filterpredictions:
                            filtering.filterpredictions(
                                config_2d,
                                [video],
                                videotype=videotype,
                                shuffle=shuffle,
                                trainingsetindex=trainingsetindex,
                                filtertype=filtertype,
                                destfolder=destfolder,
                            )
                            suffix += "_filtered"

                        dataname.append(
                            os.path.join(destfolder, vname + DLCscorer + suffix + ".h5")
                        )

                else:  # need to do the whole jam.
                    DLCscorer = analyze_videos(
                        config_2d,
                        [video],
                        videotype=videotype,
                        shuffle=shuffle,
                        trainingsetindex=trainingsetindex,
                        gputouse=gputouse,
                        destfolder=destfolder,
                    )
                    scorer_name[cam_names[j]] = DLCscorer
                    run_triangulate = True
                    print(destfolder, vname, DLCscorer)
                    suffix = tr_method_suffix
                    if filterpredictions:
                        filtering.filterpredictions(
                            config_2d,
                            [video],
                            videotype=videotype,
                            shuffle=shuffle,
                            trainingsetindex=trainingsetindex,
                            filtertype=filtertype,
                            destfolder=destfolder,
                        )
                        suffix += "_filtered"
                    dataname.append(
                        os.path.join(destfolder, vname + DLCscorer + suffix + ".h5")
                    )

        if run_triangulate:
            #        if len(dataname)>0:
            # undistort points for this pair
            print("Undistorting...")
            (
                dataFrame_camera1_undistort,
                dataFrame_camera2_undistort,
                stereomatrix,
                path_stereo_file,
            ) = undistort_points(
                config, dataname, str(cam_names[0] + "-" + cam_names[1])
            )
            if len(dataFrame_camera1_undistort) != len(dataFrame_camera2_undistort):
                import warnings

                warnings.warn(
                    "The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry! Excluding the extra frames from the longer video."
                )
                if len(dataFrame_camera1_undistort) > len(dataFrame_camera2_undistort):
                    dataFrame_camera1_undistort = dataFrame_camera1_undistort[
                        : len(dataFrame_camera2_undistort)
                    ]
                if len(dataFrame_camera2_undistort) > len(dataFrame_camera1_undistort):
                    dataFrame_camera2_undistort = dataFrame_camera2_undistort[
                        : len(dataFrame_camera1_undistort)
                    ]
            #                raise Exception("The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!")
            scorer_cam1 = dataFrame_camera1_undistort.columns.get_level_values(0)[0]
            scorer_cam2 = dataFrame_camera2_undistort.columns.get_level_values(0)[0]

            bodyparts = dataFrame_camera1_undistort.columns.get_level_values(
                "bodyparts"
            ).unique()

            P1 = stereomatrix["P1"]
            P2 = stereomatrix["P2"]
            F = stereomatrix["F"]

            print("Computing the triangulation...")

            num_frames = dataFrame_camera1_undistort.shape[0]
            ### Assign nan to [X,Y] of low likelihood predictions ###
            # Convert the data to a np array to easily mask out the low likelihood predictions
            data_cam1_tmp = dataFrame_camera1_undistort.to_numpy().reshape(
                (num_frames, -1, 3)
            )
            data_cam2_tmp = dataFrame_camera2_undistort.to_numpy().reshape(
                (num_frames, -1, 3)
            )
            # Assign [X,Y] = nan to low likelihood predictions
            data_cam1_tmp[data_cam1_tmp[..., 2] < pcutoff, :2] = np.nan
            data_cam2_tmp[data_cam2_tmp[..., 2] < pcutoff, :2] = np.nan

            # Reshape data back to original shape
            data_cam1_tmp = data_cam1_tmp.reshape(num_frames, -1)
            data_cam2_tmp = data_cam2_tmp.reshape(num_frames, -1)

            # put data back to the dataframes
            dataFrame_camera1_undistort[:] = data_cam1_tmp
            dataFrame_camera2_undistort[:] = data_cam2_tmp

            if cfg.get("multianimalproject"):
                # Check individuals are the same in both views
                individuals_view1 = (
                    dataFrame_camera1_undistort.columns.get_level_values("individuals")
                    .unique()
                    .to_list()
                )
                individuals_view2 = (
                    dataFrame_camera2_undistort.columns.get_level_values("individuals")
                    .unique()
                    .to_list()
                )
                if individuals_view1 != individuals_view2:
                    raise ValueError(
                        "The individuals do not match between the two DataFrames"
                    )

                # Cross-view match individuals
                _, voting = auxiliaryfunctions_3d.cross_view_match_dataframes(
                    dataFrame_camera1_undistort, dataFrame_camera2_undistort, F
                )
            else:
                # Create a dummy variables for single-animal
                individuals_view1 = ["indie"]
                voting = {0: 0}

            # Cleaner variable (since inds view1 == inds view2)
            individuals = individuals_view1

            # Reshape: (num_framex, num_individuals, num_bodyparts , 2)
            all_points_cam1 = dataFrame_camera1_undistort.to_numpy().reshape(
                (num_frames, len(individuals), -1, 3)
            )[..., :2]
            all_points_cam2 = dataFrame_camera2_undistort.to_numpy().reshape(
                (num_frames, len(individuals), -1, 3)
            )[..., :2]

            # Triangulate data
            triangulate = []
            for i, _ in enumerate(individuals):
                # i is individual in view 1
                # voting[i] is the matched individual in view 2

                pts_indv_cam1 = all_points_cam1[:, i].reshape((-1, 2)).T
                pts_indv_cam2 = all_points_cam2[:, voting[i]].reshape((-1, 2)).T

                indv_points_3d = auxiliaryfunctions_3d.triangulatePoints(
                    P1, P2, pts_indv_cam1, pts_indv_cam2
                )

                indv_points_3d = indv_points_3d[:3].T.reshape((num_frames, -1, 3))

                triangulate.append(indv_points_3d)

            triangulate = np.asanyarray(triangulate)
            metadata = {}
            metadata["stereo_matrix"] = stereomatrix
            metadata["stereo_matrix_file"] = path_stereo_file
            metadata["scorer_name"] = {
                cam_names[0]: scorer_name[cam_names[0]],
                cam_names[1]: scorer_name[cam_names[1]],
            }

            # Create 3D DataFrame column and row indices
            cols = [
                [scorer_3d],
                list(auxiliaryfunctions.get_bodyparts(cfg)),
                ["x", "y", "z"],
            ]
            cols_names = ["scorer", "bodyparts", "coords"]
            flag_indiv_single = False
            if cfg.get("multianimalproject"):
                cols_names.insert(1, "individuals")
                if "single" == individuals[-1]:
                    individuals = individuals[:-1]
                    columns_unique = pd.MultiIndex.from_product(
                        [
                            [scorer_3d],
                            ["single"],
                            auxiliaryfunctions.get_unique_bodyparts(cfg),
                            ["x", "y", "z"],
                        ],
                        names=cols_names,
                    )
                    flag_indiv_single = True
                cols.insert(1, individuals)
            columns = pd.MultiIndex.from_product(cols, names=cols_names)
            if flag_indiv_single:
                columns = columns.append(columns_unique)
                individuals.append("single")

            inds = range(num_frames)

            # Swap num_animals with num_frames axes to ensure well-behaving reshape
            triangulate = triangulate.swapaxes(0, 1).reshape((num_frames, -1))

            # Fill up 3D dataframe
            df_3d = pd.DataFrame(triangulate, columns=columns, index=inds)

            df_3d.to_hdf(
                str(output_filename) + ".h5",
                key="df_with_missing",
                mode="w",
                format="table",
            )

            # Reorder 2D dataframe in view 2 to match order of view 1
            if cfg.get("multianimalproject"):
                df_2d_view2 = pd.read_hdf(dataname[1])
                individuals_order = [individuals[i] for i in list(voting.values())]
                df_2d_view2 = auxfun_multianimal.reorder_individuals_in_df(
                    df_2d_view2, individuals_order
                )
                df_2d_view2.to_hdf(
                    dataname[1],
                    key="tracks",
                    format="table",
                    mode="w",
                )

            auxiliaryfunctions_3d.SaveMetadata3d(
                str(output_filename) + "_meta.pickle", metadata
            )

            if save_as_csv:
                df_3d.to_csv(str(output_filename) + ".csv")

            print("Triangulated data for video", video)
            print("Results are saved under: ", destfolder)
            # have to make the dest folder none so that it can be updated for a new pair of videos
            if destfolder == str(Path(video).parents[0]):
                destfolder = None

    if len(video_list) > 0:
        print("All videos were analyzed...")
        print("Now you can create 3D video(s) using deeplabcut.create_labeled_video_3d")


def _undistort_points(points, mat, coeffs, p, r):
    pts = points.reshape((-1, 3))
    pts_undist = cv2.undistortPoints(
        src=pts[:, :2].astype(np.float32),
        cameraMatrix=mat,
        distCoeffs=coeffs,
        P=p,
        R=r,
    )
    pts[:, :2] = pts_undist.squeeze()
    return pts.reshape((points.shape[0], -1))


def _undistort_views(df_view_pairs, stereo_params):
    df_views_undist = []
    for df_view_pair, camera_pair in zip(df_view_pairs, stereo_params):
        params = stereo_params[camera_pair]
        dfs = []
        for i, df_view in enumerate(df_view_pair, start=1):
            pts_undist = _undistort_points(
                df_view.to_numpy(),
                params[f"cameraMatrix{i}"],
                params[f"distCoeffs{i}"],
                params[f"P{i}"],
                params[f"R{i}"],
            )
            df = pd.DataFrame(pts_undist, df_view.index, df_view.columns)
            dfs.append(df)
        df_views_undist.append(dfs)
    return df_views_undist


def undistort_points(config, dataframe, camera_pair):
    cfg_3d = auxiliaryfunctions.read_config(config)
    path_camera_matrix = auxiliaryfunctions_3d.Foldernames3Dproject(cfg_3d)[2]
    """
    path_undistort = destfolder
    filename_cam1 = Path(dataframe[0]).stem
    filename_cam2 = Path(dataframe[1]).stem

    #currently no intermediate saving of this due to high speed.
    # check if the undistorted files are already present
    if os.path.exists(os.path.join(path_undistort,filename_cam1 + '_undistort.h5')) and os.path.exists(os.path.join(path_undistort,filename_cam2 + '_undistort.h5')):
        print("The undistorted files are already present at %s" % os.path.join(path_undistort,filename_cam1))
        dataFrame_cam1_undistort = pd.read_hdf(os.path.join(path_undistort,filename_cam1 + '_undistort.h5'))
        dataFrame_cam2_undistort = pd.read_hdf(os.path.join(path_undistort,filename_cam2 + '_undistort.h5'))
    else:
    """
    if len(dataframe) != 2:
        raise ValueError(
            f"undistort_points(config, dataframe, camera_pair) needs filenames to two data frames, but got dataframe={dataframe}."
        )
    for filename in dataframe:
        if not os.path.exists(filename):
            raise FileNotFoundError(
                f"Dataframe path '{filename}' could not be found in the filesystem."
            )
    if not os.path.exists(path_camera_matrix):
        raise FileNotFoundError(
            f"Camera matrix file '{path_camera_matrix}' could not be found in the filesystem."
        )
    # Create an empty dataFrame to store the undistorted 2d coordinates and likelihood
    dataframe_cam1 = pd.read_hdf(dataframe[0])
    dataframe_cam2 = pd.read_hdf(dataframe[1])
    path_stereo_file = os.path.join(path_camera_matrix, "stereo_params.pickle")
    stereo_file = auxiliaryfunctions.read_pickle(path_stereo_file)
    dataFrame_cam1_undistort, dataFrame_cam2_undistort = _undistort_views(
        [(dataframe_cam1, dataframe_cam2)],
        stereo_file,
    )[0]

    return (
        dataFrame_cam1_undistort,
        dataFrame_cam2_undistort,
        stereo_file[camera_pair],
        path_stereo_file,
    )


--- File: deeplabcut/refine_training_dataset/tracklets.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import numpy as np
import pandas as pd
import pickle
import re
from deeplabcut.post_processing import columnwise_spline_interp
from deeplabcut.utils import auxiliaryfunctions
from tqdm import trange


class TrackletManager:
    def __init__(self, config, min_swap_len=2, min_tracklet_len=2, max_gap=0):
        """

        Parameters
        ----------
        config : str
            Path to a configuration file.
        min_swap_len : float, optional (default=2)
            Minimum swap length.
            Swaps shorter than 2 frames are discarded by default.
        min_tracklet_len : float, optional (default=2)
            Minimum tracklet length.
            Tracklets shorter than 2 frames are discarded by default.
        max_gap : int, optional (default = 0).
            Number of frames to consider when filling in missing data.

        Examples
        --------

        manager = TrackletManager(config_path, min_swap_frac=0, min_tracklet_frac=0)

        manager.load_tracklets_from_pickle(filename)
        # Alternatively
        manager.load_tracklets_from_h5(filename)

        manager.find_swapping_bodypart_pairs()
        """
        self.config = config
        self.cfg = auxiliaryfunctions.read_config(config)
        self.min_swap_len = min_swap_len
        self.min_tracklet_len = min_tracklet_len
        self.max_gap = max_gap

        self.filename = ""
        self.data = None
        self.xy = None
        self._xy = None
        self.prob = None
        self.nframes = 0
        self.times = []
        self.scorer = None
        self.bodyparts = []
        self.nindividuals = len(self.cfg["individuals"])
        self.individuals = []
        self.tracklet2id = []
        self.tracklet2bp = []
        self.swapping_pairs = []
        self.swapping_bodyparts = []
        self._label_pairs = None

    def _load_tracklets(self, tracklets, auto_fill):
        header = tracklets.pop("header")
        self.scorer = header.get_level_values("scorer").unique().to_list()
        bodyparts = header.get_level_values("bodyparts")
        bodyparts_multi = [
            bp for bp in self.cfg["multianimalbodyparts"] if bp in bodyparts
        ]
        bodyparts_single = self.cfg["uniquebodyparts"]
        mask_multi = bodyparts.isin(bodyparts_multi)
        mask_single = bodyparts.isin(bodyparts_single)
        self.bodyparts = list(bodyparts[mask_multi]) * self.nindividuals + list(
            bodyparts[mask_single]
        )

        # Sort tracklets by length to prioritize greater continuity
        temp = sorted(tracklets.values(), key=len)
        if not len(temp):
            raise IOError("Tracklets are empty.")

        get_frame_ind = lambda s: int(re.findall(r"\d+", s)[0])

        # Drop tracklets that are too short
        tracklets_sorted = []
        last_frames = []
        for tracklet in temp:
            last_frames.append(get_frame_ind(list(tracklet)[-1]))
            if len(tracklet) > self.min_tracklet_len:
                tracklets_sorted.append(tracklet)
        self.nframes = max(last_frames) + 1
        self.times = np.arange(self.nframes)

        if auto_fill:  # Recursively fill the data containers
            tracklets_multi = np.full(
                (self.nindividuals, self.nframes, len(bodyparts_multi) * 3),
                np.nan,
                np.float16,
            )
            tracklets_single = np.full(
                (self.nframes, len(bodyparts_single) * 3), np.nan, np.float16
            )
            for _ in trange(len(tracklets_sorted)):
                tracklet = tracklets_sorted.pop()
                inds, temp = zip(*[(get_frame_ind(k), v) for k, v in tracklet.items()])
                inds = np.asarray(inds)
                data = np.asarray(temp, dtype=np.float16)
                data_single = data[:, mask_single]
                is_multi = np.isnan(data_single).all()
                if not is_multi:
                    # Where slots are available, copy the data over
                    is_free = np.isnan(tracklets_single[inds])
                    has_data = ~np.isnan(data_single)
                    mask = has_data & is_free
                    rows, cols = np.nonzero(mask)
                    tracklets_single[inds[rows], cols] = data_single[mask]
                    # If about to overwrite data, keep tracklets with highest confidence
                    overwrite = has_data & ~is_free
                    if overwrite.any():
                        rows, cols = np.nonzero(overwrite)
                        more_confident = (
                            data_single[overwrite] > tracklets_single[inds[rows], cols]
                        )[2::3]
                        idx = np.flatnonzero(more_confident)
                        for i in idx:
                            sl = slice(i * 3, i * 3 + 3)
                            tracklets_single[inds[rows[sl]], cols[sl]] = data_single[
                                rows[sl], cols[sl]
                            ]
                else:
                    is_free = np.isnan(tracklets_multi[:, inds])
                    data_multi = data[:, mask_multi]
                    has_data = ~np.isnan(data_multi)
                    overwrite = has_data & ~is_free
                    overwrite_risk = np.any(overwrite, axis=(1, 2))
                    if overwrite_risk.all():
                        # Squeeze some data into empty slots
                        n_empty = is_free.all(axis=2).sum(axis=1)
                        for ind in np.argsort(n_empty)[::-1]:
                            mask = has_data & is_free
                            current_mask = mask[ind]
                            rows, cols = np.nonzero(current_mask)
                            if rows.size:
                                tracklets_multi[ind, inds[rows], cols] = data_multi[
                                    current_mask
                                ]
                                is_free[ind, current_mask] = False
                                has_data[current_mask] = False
                        if has_data.any():
                            # For the remaining data, overwrite where we are least confident
                            remaining = data_multi[has_data].reshape((-1, 3))
                            mask3d = np.broadcast_to(
                                has_data, (self.nindividuals,) + has_data.shape
                            )
                            dims, rows, cols = np.nonzero(mask3d)
                            temp = tracklets_multi[dims, inds[rows], cols].reshape(
                                (self.nindividuals, -1, 3)
                            )
                            diff = remaining - temp
                            # Find keypoints closest to the remaining data
                            # Use Manhattan distance to avoid overflow
                            dist = np.abs(diff[:, :, 0]) + np.abs(diff[:, :, 1])
                            closest = np.argmin(dist, axis=0)
                            # Only overwrite if improving confidence
                            prob = diff[closest, range(len(closest)), 2]
                            better = np.flatnonzero(prob > 0)
                            idx = closest[better]
                            rows, cols = np.nonzero(has_data)
                            for i, j in zip(idx, better):
                                sl = slice(j * 3, j * 3 + 3)
                                tracklets_multi[i, inds[rows[sl]], cols[sl]] = (
                                    remaining.flat[sl]
                                )
                    else:
                        rows, cols = np.nonzero(has_data)
                        n = np.argmin(overwrite_risk)
                        tracklets_multi[n, inds[rows], cols] = data_multi[has_data]

            multi = tracklets_multi.swapaxes(0, 1).reshape((self.nframes, -1))
            data = np.c_[multi, tracklets_single].reshape((self.nframes, -1, 3))
            xy = data[:, :, :2].reshape((self.nframes, -1))
            prob = data[:, :, 2].reshape((self.nframes, -1))

            # Fill existing gaps
            missing = np.isnan(xy)
            xy_filled = columnwise_spline_interp(xy, self.max_gap)
            filled = ~np.isnan(xy_filled)
            xy[filled] = xy_filled[filled]
            inds = np.argwhere(missing & filled)
            if inds.size:
                # Retrieve original individual label indices
                inds[:, 1] //= 2
                inds = np.unique(inds, axis=0)
                prob[inds[:, 0], inds[:, 1]] = 0.01
            data[:, :, :2] = xy.reshape((self.nframes, -1, 2))
            data[:, :, 2] = prob
            self.data = data.swapaxes(0, 1)
            self.xy = self.data[:, :, :2]
            self.prob = self.data[:, :, 2]

            # Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.
            self.individuals = self.cfg["individuals"] + (
                ["single"] if len(self.cfg["uniquebodyparts"]) else []
            )
            self.tracklet2id = [
                i for i in range(0, self.nindividuals) for _ in bodyparts_multi
            ] + [self.nindividuals] * len(bodyparts_single)
            bps = bodyparts_multi + bodyparts_single
            map_ = dict(zip(bps, range(len(bps))))
            self.tracklet2bp = [map_[bp] for bp in self.bodyparts[::3]]
            self._label_pairs = self.get_label_pairs()
        else:
            tracklets_raw = np.full(
                (len(tracklets_sorted), self.nframes, len(bodyparts)),
                np.nan,
                np.float16,
            )
            for n, tracklet in enumerate(tracklets_sorted[::-1]):
                for frame, data in tracklet.items():
                    i = get_frame_ind(frame)
                    tracklets_raw[n, i] = data
            self.data = (
                tracklets_raw.swapaxes(0, 1)
                .reshape((self.nframes, -1, 3))
                .swapaxes(0, 1)
            )
            self.xy = self.data[:, :, :2]
            self.prob = self.data[:, :, 2]
            self.tracklet2id = self.tracklet2bp = [0] * self.data.shape[0]

    def load_tracklets_from_pickle(self, filename, auto_fill=True):
        self.filename = filename
        with open(filename, "rb") as file:
            tracklets = pickle.load(file)
        self._load_tracklets(tracklets, auto_fill)
        self._xy = self.xy.copy()

    def load_tracklets_from_hdf(self, filename):
        self.filename = filename
        df = pd.read_hdf(filename)

        # Fill existing gaps
        data = df.to_numpy()
        mask = ~df.columns.get_level_values(level="coords").str.contains("likelihood")
        xy = data[:, mask]
        prob = data[:, ~mask]
        missing = np.isnan(xy)
        xy_filled = columnwise_spline_interp(xy, self.max_gap)
        filled = ~np.isnan(xy_filled)
        xy[filled] = xy_filled[filled]
        inds = np.argwhere(missing & filled)
        if inds.size:
            # Retrieve original individual label indices
            inds[:, 1] //= 2
            inds = np.unique(inds, axis=0)
            prob[inds[:, 0], inds[:, 1]] = 0.01
        data[:, mask] = xy
        data[:, ~mask] = prob
        df = pd.DataFrame(data, index=df.index, columns=df.columns)

        idx = df.columns
        self.scorer = idx.get_level_values("scorer").unique().to_list()
        self.bodyparts = idx.get_level_values("bodyparts")
        self.nframes = len(df)
        self.times = np.arange(self.nframes)
        self.data = df.values.reshape((self.nframes, -1, 3)).swapaxes(0, 1)
        self.xy = self.data[:, :, :2]
        self.prob = self.data[:, :, 2]
        individuals = idx.get_level_values("individuals")
        self.individuals = individuals.unique().to_list()
        self.tracklet2id = individuals.map(
            dict(zip(self.individuals, range(len(self.individuals))))
        ).tolist()[::3]
        bodyparts = self.bodyparts.unique()
        self.tracklet2bp = self.bodyparts.map(
            dict(zip(bodyparts, range(len(bodyparts))))
        ).tolist()[::3]
        self._label_pairs = list(idx.droplevel(["scorer", "coords"]).unique())
        self._xy = self.xy.copy()

    def calc_completeness(self, xy, by_individual=False):
        comp = np.sum(~np.isnan(xy).any(axis=2), axis=1)
        if by_individual:
            inds = np.insert(np.diff(self.tracklet2id), 0, 1)
            comp = np.add.reduceat(comp, np.flatnonzero(inds))
        return comp

    def to_num_bodypart(self, ind):
        return self.tracklet2bp[ind]

    def to_num_individual(self, ind):
        return self.tracklet2id[ind]

    def get_non_nan_elements(self, at):
        data = self.xy[:, at]
        mask = ~np.isnan(data).any(axis=1)
        return data[mask], mask, np.flatnonzero(mask)

    def swap_tracklets(self, track1, track2, inds):
        self.xy[np.ix_([track1, track2], inds)] = self.xy[
            np.ix_([track2, track1], inds)
        ]
        self.prob[np.ix_([track1, track2], inds)] = self.prob[
            np.ix_([track2, track1], inds)
        ]
        self.tracklet2bp[track1], self.tracklet2bp[track2] = (
            self.tracklet2bp[track2],
            self.tracklet2bp[track1],
        )

    def find_swapping_bodypart_pairs(self, force_find=False):
        if not self.swapping_pairs or force_find:
            sub = (
                self.xy[:, np.newaxis] - self.xy
            )  # Broadcasting for efficient subtraction of X and Y coordinates
            with np.errstate(
                invalid="ignore"
            ):  # Get rid of annoying warnings when comparing with NaNs
                pos = sub > 0
                neg = sub <= 0
                down = neg[:, :, 1:] & pos[:, :, :-1]
                up = pos[:, :, 1:] & neg[:, :, :-1]
                zero_crossings = down | up
            # ID swaps occur when X and Y simultaneously intersect each other.
            self.tracklet_swaps = zero_crossings.all(axis=3)
            cross = self.tracklet_swaps.sum(axis=2) > self.min_swap_len
            mat = np.tril(cross)
            temp_pairs = np.where(mat)
            # Get only those bodypart pairs that belong to different individuals
            pairs = []
            for a, b in zip(*temp_pairs):
                if self.tracklet2id[a] != self.tracklet2id[b]:
                    pairs.append((a, b))
            self.swapping_pairs = pairs
            self.swapping_bodyparts = np.unique(pairs).tolist()

    def get_swap_indices(self, tracklet1, tracklet2):
        return np.flatnonzero(self.tracklet_swaps[tracklet1, tracklet2])

    def get_nonoverlapping_segments(self, tracklet1, tracklet2):
        swap_inds = self.get_swap_indices(tracklet1, tracklet2)
        inds = np.insert(swap_inds, [0, len(swap_inds)], [0, self.nframes])
        mask = np.ones_like(self.times, dtype=bool)
        for i, j in zip(inds[::2], inds[1::2]):
            mask[i:j] = False
        return mask

    def flatten_data(self):
        data = np.concatenate((self.xy, np.expand_dims(self.prob, axis=2)), axis=2)
        return data.swapaxes(0, 1).reshape((self.nframes, -1))

    def format_multiindex(self):
        scorer = self.scorer * len(self.bodyparts)
        map_ = dict(zip(range(len(self.individuals)), self.individuals))
        individuals = [map_[ind] for ind in self.tracklet2id for _ in range(3)]
        coords = ["x", "y", "likelihood"] * len(self.tracklet2id)
        return pd.MultiIndex.from_arrays(
            [scorer, individuals, self.bodyparts, coords],
            names=["scorer", "individuals", "bodyparts", "coords"],
        )

    def get_label_pairs(self):
        return list(self.format_multiindex().droplevel(["scorer", "coords"]).unique())

    def format_data(self):
        columns = self.format_multiindex()
        return pd.DataFrame(self.flatten_data(), columns=columns, index=self.times)

    def find_edited_frames(self):
        mask = np.isclose(self.xy, self._xy, equal_nan=True).all(axis=(0, 2))
        return np.flatnonzero(~mask)

    def save(self, output_name="", *args):
        df = self.format_data()
        if not output_name:
            output_name = self.filename.replace("pickle", "h5")
        df.to_hdf(output_name, "df_with_missing", format="table", mode="w")


--- File: deeplabcut/refine_training_dataset/__init__.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


from deeplabcut.refine_training_dataset.tracklets import *
from deeplabcut.refine_training_dataset.outlier_frames import *


--- File: deeplabcut/refine_training_dataset/outlier_frames.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#


import argparse
import os
import pickle
import re
from pathlib import Path
from typing import List, Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import statsmodels.api as sm
from skimage.util import img_as_ubyte

from deeplabcut.core import inferenceutils
from deeplabcut.utils import (
    auxiliaryfunctions,
    auxfun_multianimal,
    conversioncode,
    visualization,
    frameselectiontools,
)
from deeplabcut.utils.auxfun_videos import VideoWriter


def find_outliers_in_raw_data(
    config,
    pickle_file,
    video_file,
    pcutoff=0.1,
    percentiles=(5, 95),
    with_annotations=True,
    extraction_algo="kmeans",
    copy_videos=False,
):
    """
    Extract outlier frames from either raw detections or assemblies of multiple animals.

    Parameter
    ----------
    config : str
        Absolute path to the project config.yaml.

    pickled_file : str
        Path to a *_full.pickle or *_assemblies.pickle.

    video_file : str
        Path to the corresponding video file for frame extraction.

    pcutoff : float, optional (default=0.1)
        Detection confidence threshold below which frames are flagged as
        containing outliers. Only considered if raw detections are passed in.

    percentiles : tuple, optional (default=(5, 95))
        Assemblies are considered outliers if their areas are beyond the 5th
        and 95th percentiles. Must contain a lower and upper bound.

    with_annotations : bool, optional (default=True)
        If true, extract frames and the corresponding network predictions.
        Otherwise, only the frames are extracted.

    extraction_algo : string, optional (default="kmeans")
        Outlier detection algorithm. Must be either ``uniform`` or ``kmeans``.

    copy_videos : bool, optional (default=False)
        If True, newly-added videos (from which outlier frames are extracted) are
        copied to the project folder. By default, symbolic links are created instead.

    """
    if extraction_algo not in ("kmeans", "uniform"):
        raise ValueError(f"Unsupported extraction algorithm {extraction_algo}.")

    video_name = Path(video_file).stem
    pickle_name = Path(pickle_file).stem
    if not pickle_name.startswith(video_name):
        raise ValueError("Video and pickle files do not match.")

    with open(pickle_file, "rb") as file:
        data = pickle.load(file)
    if pickle_file.endswith("_full.pickle"):
        inds, data = find_outliers_in_raw_detections(data, threshold=pcutoff)
        with_annotations = False
    elif pickle_file.endswith("_assemblies.pickle"):
        assemblies = dict()
        for k, lst in data.items():
            if k == "single":
                continue
            ass = []
            for vals in lst:
                a = inferenceutils.Assembly(len(vals))
                a.data = vals
                ass.append(a)
            assemblies[k] = ass
        inds = inferenceutils.find_outlier_assemblies(assemblies, qs=percentiles)
    else:
        raise IOError(f"Raw data file {pickle_file} could not be parsed.")

    cfg = auxiliaryfunctions.read_config(config)
    ExtractFramesbasedonPreselection(
        inds,
        extraction_algo,
        data,
        video=video_file,
        cfg=cfg,
        config=config,
        savelabeled=False,
        with_annotations=with_annotations,
        copy_videos=copy_videos,
    )


def find_outliers_in_raw_detections(
    pickled_data, algo="uncertain", threshold=0.1, kept_keypoints=None
):
    """
    Find outlier frames from the raw detections of multiple animals.

    Parameter
    ----------
    pickled_data : dict
        Data in the *_full.pickle file obtained after `analyze_videos`.

    algo : string, optional (default="uncertain")
        Outlier detection algorithm. Currently, only 'uncertain' is supported
        for multi-animal raw detections.

    threshold: float, optional (default=0.1)
        Detection confidence threshold below which frames are flagged as
        containing outliers. Only considered if `algo`==`uncertain`.

    kept_keypoints : list, optional (default=None)
        Indices in the list of labeled body parts to be kept of the analysis.
        By default, all keypoints are used for outlier search.

    Returns
    -------
    candidates : list
        Indices of video frames containing potential outliers
    """
    if algo != "uncertain":
        raise ValueError(f"Only method 'uncertain' is currently supported.")

    try:
        _ = pickled_data.pop("metadata")
    except KeyError:
        pass

    def get_frame_ind(s):
        return int(re.findall(r"\d+", s)[0])

    candidates = []
    data = dict()
    for frame_name, dict_ in pickled_data.items():
        frame_ind = get_frame_ind(frame_name)
        temp_coords = dict_["coordinates"][0]
        temp = dict_["confidence"]
        if kept_keypoints is not None:
            temp_coords = [temp_coords[i] for i in kept_keypoints]
            temp = [temp[i] for i in kept_keypoints]
        coords = np.concatenate(temp_coords)
        conf = np.concatenate(temp)
        data[frame_ind] = np.c_[coords, conf].squeeze()
        if np.any(conf < threshold):
            candidates.append(frame_ind)
    return candidates, data


def extract_outlier_frames(
    config,
    videos,
    videotype="",
    shuffle=1,
    trainingsetindex=0,
    outlieralgorithm="jump",
    frames2use=None,
    comparisonbodyparts="all",
    epsilon=20,
    p_bound=0.01,
    ARdegree=3,
    MAdegree=1,
    alpha=0.01,
    extractionalgorithm="kmeans",
    automatic=False,
    cluster_resizewidth=30,
    cluster_color=False,
    opencv=True,
    savelabeled=False,
    copy_videos=False,
    destfolder=None,
    modelprefix="",
    track_method="",
):
    """Extracts the outlier frames.

    Extracts the outlier frames if the predictions are not correct for a certain video
    from the cropped video running from start to stop as defined in config.yaml.

    Another crucial parameter in config.yaml is how many frames to extract
    ``numframes2extract``.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    videos : list[str]
        The full paths to videos for analysis or a path to the directory, where all the
        videos with same extension are stored.

    videotype: str, optional, default=""
        Checks for the extension of the video in case the input to the video is a
        directory. Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions
        ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle : int, optional, default=1
        The shuffle index of training dataset. The extracted frames will be stored in
        the labeled-dataset for the corresponding shuffle of training dataset.

    trainingsetindex: int, optional, default=0
        Integer specifying which TrainingsetFraction to use.
        Note that TrainingFraction is a list in config.yaml.

    outlieralgorithm: str, optional, default="jump".
        String specifying the algorithm used to detect the outliers.

        * ``'fitting'`` fits an Auto Regressive Integrated Moving Average model to the
          data and computes the distance to the estimated data. Larger distances than
          epsilon are then potentially identified as outliers
        * ``'jump'`` identifies larger jumps than 'epsilon' in any body part
        * ``'uncertain'`` looks for frames with confidence below p_bound
        * ``'manual'`` launches a GUI from which the user can choose the frames
        * ``'list'`` looks for user to provide a list of frame numbers to use, 'frames2use'. In this case, ``'extractionalgorithm'`` is forced to be ``'uniform.'``

    frames2use: list[str], optional, default=None
        If ``'outlieralgorithm'`` is ``'list'``, provide the list of frames here.

    comparisonbodyparts: list[str] or str, optional, default="all"
        This selects the body parts for which the comparisons with the outliers are
        carried out. If ``"all"``, then all body parts from config.yaml are used. If a
        list of strings that are a subset of the full list E.g. ['hand','Joystick'] for
        the demo Reaching-Mackenzie-2018-08-30/config.yaml to select only these body
        parts.

    p_bound: float between 0 and 1, optional, default=0.01
        For outlieralgorithm ``'uncertain'`` this parameter defines the likelihood
        below which a body part will be flagged as a putative outlier.

    epsilon: float, optional, default=20
        If ``'outlieralgorithm'`` is ``'fitting'``, this is the float bound according
        to which frames are picked when the (average) body part estimate deviates from
        model fit.

        If ``'outlieralgorithm'`` is ``'jump'``, this is the float bound specifying the
        distance by which body points jump from one frame to next (Euclidean distance).

    ARdegree: int, optional, default=3
        For outlieralgorithm ``'fitting'``: Autoregressive degree of ARIMA model degree.
        (Note we use SARIMAX without exogeneous and seasonal part)
        See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html

    MAdegree: int, optional, default=1
        For outlieralgorithm ``'fitting'``: MovingAvarage degree of ARIMA model degree.
        (Note we use SARIMAX without exogeneous and seasonal part)
        See https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html

    alpha: float, optional, default=0.01
        Significance level for detecting outliers based on confidence interval of
        fitted ARIMA model. Only the distance is used however.

    extractionalgorithm : str, optional, default="kmeans"
        String specifying the algorithm to use for selecting the frames from the
        identified putatative outlier frames. Currently, deeplabcut supports either
        ``kmeans`` or ``uniform`` based selection (same logic as for extract_frames).

    automatic : bool, optional, default=False
        If ``True``, extract outliers without being asked for user feedback.

    cluster_resizewidth: number, default=30
        If ``"extractionalgorithm"`` is ``"kmeans"``, one can change the width to which
        the images are downsampled (aspect ratio is fixed).

    cluster_color: bool, optional, default=False
        If ``False``, each downsampled image is treated as a grayscale vector
        (discarding color information). If ``True``, then the color channels are
        considered. This increases the computational complexity.

    opencv: bool, optional, default=True
        Uses openCV for loading & extractiong (otherwise moviepy (legacy)).

    savelabeled: bool, optional, default=False
        If ``True``, frame are saved with predicted labels in each folder.

    copy_videos: bool, optional, default=False
        If True, newly-added videos (from which outlier frames are extracted) are
        copied to the project folder. By default, symbolic links are created instead.

    destfolder: str or None, optional, default=None
        Specifies the destination folder that was used for storing analysis data. If
        ``None``, the path of the video is used.

    modelprefix: str, optional, default=""
        Directory containing the deeplabcut models to use when evaluating the network.
        By default, the models are assumed to exist in the project folder.

    track_method: str, optional, default=""
         Specifies the tracker used to generate the data.
         Empty by default (corresponding to a single animal project).
         For multiple animals, must be either 'box', 'skeleton', or 'ellipse' and will
         be taken from the config.yaml file if none is given.

    Returns
    -------
    None

    Examples
    --------

    Extract the frames with default settings on Windows.

    >>> deeplabcut.extract_outlier_frames(
            'C:\\myproject\\reaching-task\\config.yaml',
            ['C:\\yourusername\\rig-95\\Videos\\reachingvideo1.avi'],
        )

    Extract the frames with default settings on Linux/MacOS.

    >>> deeplabcut.extract_outlier_frames(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/video/reachinvideo1.avi'],
        )

    Extract the frames using the "kmeans" algorithm.

    >>> deeplabcut.extract_outlier_frames(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/video/reachinvideo1.avi'],
            extractionalgorithm='kmeans',
        )

    Extract the frames using the "kmeans" algorithm and ``"epsilon=5"`` pixels.

    >>> deeplabcut.extract_outlier_frames(
            '/analysis/project/reaching-task/config.yaml',
            ['/analysis/project/video/reachinvideo1.avi'],
            epsilon=5,
            extractionalgorithm='kmeans',
        )
    """

    cfg = auxiliaryfunctions.read_config(config)
    bodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
        cfg, comparisonbodyparts
    )
    if not len(bodyparts):
        raise ValueError("No valid bodyparts were selected.")

    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)

    DLCscorer, DLCscorerlegacy = auxiliaryfunctions.get_scorer_name(
        cfg,
        shuffle,
        trainFraction=cfg["TrainingFraction"][trainingsetindex],
        modelprefix=modelprefix,
    )

    Videos = auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if len(Videos) == 0:
        print("No suitable videos found in", videos)

    for video in Videos:
        if destfolder is None:
            videofolder = str(Path(video).parents[0])
        else:
            videofolder = destfolder
        vname = os.path.splitext(os.path.basename(video))[0]

        try:
            df, dataname, _, _ = auxiliaryfunctions.load_analyzed_data(
                videofolder, vname, DLCscorer, track_method=track_method
            )
            metadata = auxiliaryfunctions.load_video_metadata(
                videofolder, vname, DLCscorer
            )
            nframes = len(df)
            startindex = max([int(np.floor(nframes * cfg["start"])), 0])
            stopindex = min([int(np.ceil(nframes * cfg["stop"])), nframes])
            Index = np.arange(stopindex - startindex) + startindex

            # offset if the data was cropped
            if metadata.get("data", {}).get("cropping"):
                x1, _, y1, _ = metadata["data"]["cropping_parameters"]
                df.iloc[:, df.columns.get_level_values(level="coords") == "x"] += x1
                df.iloc[:, df.columns.get_level_values(level="coords") == "y"] += y1

            df = df.iloc[Index]
            mask = df.columns.get_level_values("bodyparts").isin(bodyparts)
            df_temp = df.loc[:, mask]
            Indices = []
            if outlieralgorithm == "uncertain":
                p = df_temp.xs("likelihood", level="coords", axis=1)
                ind = df_temp.index[(p < p_bound).any(axis=1)].tolist()
                Indices.extend(ind)
            elif outlieralgorithm == "jump":
                temp_dt = df_temp.diff(axis=0) ** 2
                temp_dt.drop("likelihood", axis=1, level="coords", inplace=True)
                sum_ = temp_dt.groupby(level="bodyparts", axis=1).sum()
                ind = df_temp.index[(sum_ > epsilon**2).any(axis=1)].tolist()
                Indices.extend(ind)
            elif outlieralgorithm == "fitting":
                d, o = compute_deviations(
                    df_temp, dataname, p_bound, alpha, ARdegree, MAdegree
                )
                # Some heuristics for extracting frames based on distance:
                ind = np.flatnonzero(
                    d > epsilon
                )  # time points with at least average difference of epsilon
                if (
                    len(ind) < cfg["numframes2pick"] * 2
                    and len(d) > cfg["numframes2pick"] * 2
                ):  # if too few points qualify, extract the most distant ones.
                    ind = np.argsort(d)[::-1][: cfg["numframes2pick"] * 2]
                Indices.extend(ind)
            elif outlieralgorithm == "manual":
                from deeplabcut.gui.widgets import launch_napari

                added_video = attempt_to_add_video(
                    config=config,
                    video=video,
                    copy_videos=copy_videos,
                    coords=None,
                )
                if added_video:
                    project_video_path = (
                        Path(cfg["project_path"]) / "videos" / Path(video).name
                    )
                    _ = launch_napari([project_video_path, dataname])
                return

            elif outlieralgorithm == "list":
                if frames2use is not None:
                    try:
                        frames2use = np.array(frames2use).astype("int")
                    except ValueError() as e:
                        print(
                            "Could not cast frames2use into np array, please check that frames2use is a simply a list of integers!"
                        )
                        raise
                    Indices.extend(frames2use)
                else:
                    raise ValueError(
                        'Expected list of frames2use for outlieralgorithm "list"!'
                    )
            else:
                raise ValueError(f"outlieralgorithm {outlieralgorithm} not recognized!")

            # Run always except when the outlieralgorithm == manual.
            if not outlieralgorithm == "manual":
                Indices = np.sort(list(set(Indices)))  # remove repetitions.
                print(
                    "Method ",
                    outlieralgorithm,
                    " found ",
                    len(Indices),
                    " putative outlier frames.",
                )
                print(
                    "Do you want to proceed with extracting ",
                    cfg["numframes2pick"],
                    " of those?",
                )
                if outlieralgorithm == "uncertain" or outlieralgorithm == "jump":
                    print(
                        "If this list is very large, perhaps consider changing the parameters "
                        "(start, stop, p_bound, comparisonbodyparts) or use a different method."
                    )
                elif outlieralgorithm == "fitting":
                    print(
                        "If this list is very large, perhaps consider changing the parameters "
                        "(start, stop, epsilon, ARdegree, MAdegree, alpha, comparisonbodyparts) "
                        "or use a different method."
                    )

                if not automatic:
                    askuser = input("yes/no")
                else:
                    askuser = "Ja"

                if (
                    askuser == "y"
                    or askuser == "yes"
                    or askuser == "Ja"
                    or askuser == "ha"
                ):  # multilanguage support :)
                    # Now extract from those Indices!
                    ExtractFramesbasedonPreselection(
                        Indices,
                        extractionalgorithm,
                        df,
                        video,
                        cfg,
                        config,
                        opencv,
                        cluster_resizewidth,
                        cluster_color,
                        savelabeled,
                        copy_videos=copy_videos,
                    )
                else:
                    print(
                        "Nothing extracted, please change the parameters and start again..."
                    )
        except FileNotFoundError as e:
            print(e)
            print(
                "It seems the video has not been analyzed yet, or the video is not found! "
                "You can only refine the labels after the a video is analyzed. Please run 'analyze_video' first. "
                "Or, please double check your video file path"
            )


def convertparms2start(pn):
    """Creating a start value for sarimax in case of an value error
    See: https://groups.google.com/forum/#!topic/pystatsmodels/S_Fo53F25Rk"""
    if "ar." in pn:
        return 0
    elif "ma." in pn:
        return 0
    elif "sigma" in pn:
        return 1
    else:
        return 0


def FitSARIMAXModel(x, p, pcutoff, alpha, ARdegree, MAdegree, nforecast=0, disp=False):
    # Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX)
    # see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax
    Y = x.copy()
    Y[p < pcutoff] = np.nan  # Set uncertain estimates to nan (modeled as missing data)
    if np.sum(np.isfinite(Y)) > 10:
        # SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!)
        mod = sm.tsa.statespace.SARIMAX(
            Y.flatten(),
            order=(ARdegree, 0, MAdegree),
            seasonal_order=(0, 0, 0, 0),
            simple_differencing=True,
        )
        # Autoregressive Moving Average ARMA(p,q) Model
        # mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)
        try:
            res = mod.fit(disp=disp)
        except (
            ValueError
        ):  # https://groups.google.com/forum/#!topic/pystatsmodels/S_Fo53F25Rk (let's update to statsmodels 0.10.0 soon...)
            startvalues = np.array([convertparms2start(pn) for pn in mod.param_names])
            res = mod.fit(start_params=startvalues, disp=disp)
        except np.linalg.LinAlgError:
            # The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...
            # Relaxing those constraints should do the job.
            mod = sm.tsa.statespace.SARIMAX(
                Y.flatten(),
                order=(ARdegree, 0, MAdegree),
                seasonal_order=(0, 0, 0, 0),
                simple_differencing=True,
                enforce_stationarity=False,
                enforce_invertibility=False,
                use_exact_diffuse=False,
            )
            res = mod.fit(disp=disp)

        predict = res.get_prediction(end=mod.nobs + nforecast - 1)
        return predict.predicted_mean, predict.conf_int(alpha=alpha)
    else:
        return np.nan * np.zeros(len(Y)), np.nan * np.zeros((len(Y), 2))


def compute_deviations(
    Dataframe, dataname, p_bound, alpha, ARdegree, MAdegree, storeoutput=None
):
    """Fits Seasonal AutoRegressive Integrated Moving Average with eXogenous regressors model to data and computes confidence interval
    as well as mean fit."""

    print("Fitting state-space models with parameters:", ARdegree, MAdegree)
    df_x, df_y, df_likelihood = Dataframe.values.reshape((Dataframe.shape[0], -1, 3)).T
    preds = []
    for row in range(len(df_x)):
        x = df_x[row]
        y = df_y[row]
        p = df_likelihood[row]
        meanx, CIx = FitSARIMAXModel(x, p, p_bound, alpha, ARdegree, MAdegree)
        meany, CIy = FitSARIMAXModel(y, p, p_bound, alpha, ARdegree, MAdegree)
        distance = np.sqrt((x - meanx) ** 2 + (y - meany) ** 2)
        significant = (
            (x < CIx[:, 0]) + (x > CIx[:, 1]) + (y < CIy[:, 0]) + (y > CIy[:, 1])
        )
        preds.append(np.c_[distance, significant, meanx, meany, CIx, CIy])

    columns = Dataframe.columns
    prod = []
    for i in range(columns.nlevels - 1):
        prod.append(columns.get_level_values(i).unique())
    prod.append(
        [
            "distance",
            "sig",
            "meanx",
            "meany",
            "lowerCIx",
            "higherCIx",
            "lowerCIy",
            "higherCIy",
        ]
    )
    pdindex = pd.MultiIndex.from_product(prod, names=columns.names)
    data = pd.DataFrame(np.concatenate(preds, axis=1), columns=pdindex)
    # average distance and average # significant differences avg. over comparisonbodyparts
    d = data.xs("distance", axis=1, level=-1).mean(axis=1).values
    o = data.xs("sig", axis=1, level=-1).mean(axis=1).values

    if storeoutput == "full":
        data.to_hdf(
            dataname.split(".h5")[0] + "filtered.h5",
            "df_with_missing",
            format="table",
            mode="w",
        )
        return d, o, data
    else:
        return d, o


def attempt_to_add_video(
    config: str,
    video: str,
    copy_videos: bool,
    coords: Optional[List],
) -> bool:
    """
    Add new videos to the config file at any stage of the project.

    Parameters
    ----------
    config : string
        Full path of the config file in the project.

    video : string
        Full path of the video to add to the project.

    copy_videos : bool, optional
        If this is set to True, the videos will be copied to the project/videos directory. If False, the symlink of the
        videos will be copied instead. The default is
        ``False``; if provided it must be either ``True`` or ``False``.

    coords: list, optional
        A list containing the list of cropping coordinates of the video. The default is set to None.

    Returns
    -------
    True iff the video was successfully added to the project
    """
    from deeplabcut.create_project import add

    # make sure coords and videos are a list
    videos = [video]
    if coords is not None:
        coords = [coords]

    try:
        add.add_new_videos(config, videos, coords=coords, copy_videos=copy_videos)
    except:
        # can we make a catch here? - in fact we should drop indices from DataCombined
        # if they are in CollectedData.. [ideal behavior; currently pretty unlikely]
        print(
            f"AUTOMATIC ADDING OF VIDEO TO CONFIG FILE FAILED! You need to "
            "do this manually for including it in the config.yaml file!"
        )
        print("Videopath:", video, "Coordinates for cropping:", coords)
        return False

    return True


def ExtractFramesbasedonPreselection(
    Index,
    extractionalgorithm,
    data,
    video,
    cfg,
    config,
    opencv=True,
    cluster_resizewidth=30,
    cluster_color=False,
    savelabeled=True,
    with_annotations=True,
    copy_videos=False,
):
    start = cfg["start"]
    stop = cfg["stop"]
    numframes2extract = cfg["numframes2pick"]
    bodyparts = auxiliaryfunctions.intersection_of_body_parts_and_ones_given_by_user(
        cfg, "all"
    )

    videofolder = str(Path(video).parents[0])
    vname = str(Path(video).stem)
    tmpfolder = os.path.join(cfg["project_path"], "labeled-data", vname)
    if os.path.isdir(tmpfolder):
        print("Frames from video", vname, " already extracted (more will be added)!")
    else:
        auxiliaryfunctions.attempt_to_make_folder(tmpfolder, recursive=True)

    nframes = len(data)
    print("Loading video...")
    if opencv:
        vid = VideoWriter(video)
        fps = vid.fps
        duration = vid.calc_duration()
    else:
        from moviepy.editor import VideoFileClip

        clip = VideoFileClip(video)
        fps = clip.fps
        duration = clip.duration

    if cfg["cropping"]:  # one might want to adjust
        coords = cfg["video_sets"].get(video, {}).get("crop")
        if coords is not None:
            coords = list(map(int, coords.split(", ")))
    else:
        coords = None

    print("Cropping coords:", coords)
    print("Duration of video [s]: ", duration, ", recorded @ ", fps, "fps!")
    print("Overall # of frames: ", nframes, "with (cropped) frame dimensions: ")
    if extractionalgorithm == "uniform":
        if opencv:
            if coords is not None:
                vid.set_bbox(*coords)
            frames2pick = frameselectiontools.UniformFramescv2(
                vid, numframes2extract, start, stop, Index
            )
        else:
            if coords is not None:
                clip = clip.crop(
                    y1=coords[2],
                    y2=coords[3],
                    x1=coords[0],
                    x2=coords[1],
                )
            frames2pick = frameselectiontools.UniformFrames(
                clip, numframes2extract, start, stop, Index
            )
    elif extractionalgorithm == "kmeans":
        if opencv:
            if coords is not None:
                vid.set_bbox(*coords)
            frames2pick = frameselectiontools.KmeansbasedFrameselectioncv2(
                vid,
                numframes2extract,
                start,
                stop,
                Index,
                resizewidth=cluster_resizewidth,
                color=cluster_color,
            )
        else:
            if coords is not None:
                clip = clip.crop(
                    y1=coords[2],
                    y2=coords[3],
                    x1=coords[0],
                    x2=coords[1],
                )
            frames2pick = frameselectiontools.KmeansbasedFrameselection(
                clip,
                numframes2extract,
                start,
                stop,
                Index,
                resizewidth=cluster_resizewidth,
                color=cluster_color,
            )

    else:
        print(
            "Please implement this method yourself! Currently the options are 'kmeans', 'jump', 'uniform'."
        )
        frames2pick = []

    # Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data
    print("Let's select frames indices:", frames2pick)
    colors = visualization.get_cmap(len(bodyparts), cfg["colormap"])
    strwidth = int(np.ceil(np.log10(nframes)))  # width for strings
    for index in frames2pick:  ##tqdm(range(0,nframes,10)):
        if opencv:
            PlottingSingleFramecv2(
                vid,
                data,
                bodyparts,
                tmpfolder,
                index,
                cfg["dotsize"],
                cfg["pcutoff"],
                cfg["alphavalue"],
                colors,
                strwidth,
                savelabeled,
            )
        else:
            PlottingSingleFrame(
                clip,
                data,
                bodyparts,
                tmpfolder,
                index,
                cfg["dotsize"],
                cfg["pcutoff"],
                cfg["alphavalue"],
                colors,
                strwidth,
                savelabeled,
            )
        plt.close("all")

    # close videos
    if opencv:
        vid.close()
    else:
        clip.close()
        del clip

    # Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data
    if len(frames2pick) > 0:
        added_video = attempt_to_add_video(
            config=config,
            video=video,
            copy_videos=copy_videos,
            coords=coords,
        )
        if not added_video:
            pass

        if with_annotations:
            machinefile = os.path.join(
                tmpfolder, "machinelabels-iter" + str(cfg["iteration"]) + ".h5"
            )
            if isinstance(data, pd.DataFrame):
                df = data.loc[frames2pick]
                df.index = pd.MultiIndex.from_tuples(
                    [
                        (
                            "labeled-data",
                            vname,
                            "img" + str(index).zfill(strwidth) + ".png",
                        )
                        for index in df.index
                    ]
                )  # exchange index number by file names.
            elif isinstance(data, dict):
                idx = pd.MultiIndex.from_tuples(
                    [
                        (
                            "labeled-data",
                            vname,
                            "img" + str(index).zfill(strwidth) + ".png",
                        )
                        for index in frames2pick
                    ]
                )
                filename = os.path.join(
                    str(tmpfolder), f"CollectedData_{cfg['scorer']}.h5"
                )
                try:
                    df_temp = pd.read_hdf(filename, "df_with_missing")
                    columns = df_temp.columns
                except FileNotFoundError:
                    columns = pd.MultiIndex.from_product(
                        [
                            [cfg["scorer"]],
                            cfg["individuals"],
                            cfg["multianimalbodyparts"],
                            ["x", "y"],
                        ],
                        names=["scorer", "individuals", "bodyparts", "coords"],
                    )
                    if cfg["uniquebodyparts"]:
                        columns2 = pd.MultiIndex.from_product(
                            [
                                [cfg["scorer"]],
                                ["single"],
                                cfg["uniquebodyparts"],
                                ["x", "y"],
                            ],
                            names=["scorer", "individuals", "bodyparts", "coords"],
                        )
                        df_temp = pd.concat(
                            (
                                pd.DataFrame(columns=columns),
                                pd.DataFrame(columns=columns2),
                            )
                        )
                        columns = df_temp.columns
                array = np.full((len(frames2pick), len(columns)), np.nan)
                for i, index in enumerate(frames2pick):
                    data_temp = data.get(index)
                    if data_temp is not None:
                        vals = np.concatenate(data_temp)[:, :2].flatten()
                        array[i, : len(vals)] = vals
                df = pd.DataFrame(array, index=idx, columns=columns)
            else:
                return
            if Path(machinefile).is_file():
                Data = pd.read_hdf(machinefile, "df_with_missing")
                conversioncode.guarantee_multiindex_rows(Data)
                DataCombined = pd.concat([Data, df])
                # drop duplicate labels:
                DataCombined = DataCombined[
                    ~DataCombined.index.duplicated(keep="first")
                ]

                DataCombined.to_hdf(machinefile, key="df_with_missing", mode="w")
                DataCombined.to_csv(
                    os.path.join(tmpfolder, "machinelabels.csv")
                )  # this is always the most current one (as reading is from h5)
            else:
                df.to_hdf(machinefile, key="df_with_missing", mode="w")
                df.to_csv(os.path.join(tmpfolder, "machinelabels.csv"))

        print(
            "The outlier frames are extracted. They are stored in the subdirectory labeled-data\%s."
            % vname
        )
        print(
            "Once you extracted frames for all videos, use 'refine_labels' to manually correct the labels."
        )
    else:
        print("No frames were extracted.")


def PlottingSingleFrame(
    clip,
    Dataframe,
    bodyparts2plot,
    tmpfolder,
    index,
    dotsize,
    pcutoff,
    alphavalue,
    colors,
    strwidth=4,
    savelabeled=True,
):
    """Label frame and save under imagename / this is already cropped (for clip)"""
    from skimage import io

    imagename1 = os.path.join(tmpfolder, "img" + str(index).zfill(strwidth) + ".png")
    imagename2 = os.path.join(
        tmpfolder, "img" + str(index).zfill(strwidth) + "labeled.png"
    )

    if not os.path.isfile(
        os.path.join(tmpfolder, "img" + str(index).zfill(strwidth) + ".png")
    ):
        plt.axis("off")
        image = img_as_ubyte(clip.get_frame(index * 1.0 / clip.fps))
        io.imsave(imagename1, image)

        if savelabeled:
            if np.ndim(image) > 2:
                h, w, nc = np.shape(image)
            else:
                h, w = np.shape(image)

            bpts = Dataframe.columns.get_level_values("bodyparts")
            all_bpts = bpts.values[::3]
            df_x, df_y, df_likelihood = Dataframe.values.reshape(
                (Dataframe.shape[0], -1, 3)
            ).T
            bplist = bpts.unique().to_list()
            if Dataframe.columns.nlevels == 3:
                map2bp = list(range(len(all_bpts)))
            else:
                map2bp = [bplist.index(bp) for bp in all_bpts]
            keep = np.flatnonzero(np.isin(all_bpts, bodyparts2plot))

            plt.figure(frameon=False, figsize=(w * 1.0 / 100, h * 1.0 / 100))
            plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.imshow(image)
            for i, ind in enumerate(keep):
                if df_likelihood[ind, index] > pcutoff:
                    plt.scatter(
                        df_x[ind, index],
                        df_y[ind, index],
                        s=dotsize**2,
                        color=colors(map2bp[i]),
                        alpha=alphavalue,
                    )
            plt.xlim(0, w)
            plt.ylim(0, h)
            plt.axis("off")
            plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.gca().invert_yaxis()
            plt.savefig(imagename2)
            plt.close("all")


def PlottingSingleFramecv2(
    cap,
    Dataframe,
    bodyparts2plot,
    tmpfolder,
    index,
    dotsize,
    pcutoff,
    alphavalue,
    colors,
    strwidth=4,
    savelabeled=True,
):
    """Label frame and save under imagename / cap is not already cropped."""
    from skimage import io

    imagename1 = os.path.join(tmpfolder, "img" + str(index).zfill(strwidth) + ".png")
    imagename2 = os.path.join(
        tmpfolder, "img" + str(index).zfill(strwidth) + "labeled.png"
    )

    if not os.path.isfile(
        os.path.join(tmpfolder, "img" + str(index).zfill(strwidth) + ".png")
    ):
        plt.axis("off")
        cap.set_to_frame(index)
        frame = cap.read_frame(crop=True)
        if frame is None:
            print("Frame could not be read.")
            return
        image = img_as_ubyte(frame)
        io.imsave(imagename1, image)

        if savelabeled:
            if np.ndim(image) > 2:
                h, w, nc = np.shape(image)
            else:
                h, w = np.shape(image)

            bpts = Dataframe.columns.get_level_values("bodyparts")
            all_bpts = bpts.values[::3]
            df_x, df_y, df_likelihood = Dataframe.values.reshape(
                (Dataframe.shape[0], -1, 3)
            ).T
            bplist = bpts.unique().to_list()
            if Dataframe.columns.nlevels == 3:
                map2bp = list(range(len(all_bpts)))
            else:
                map2bp = [bplist.index(bp) for bp in all_bpts]
            keep = np.flatnonzero(np.isin(all_bpts, bodyparts2plot))

            plt.figure(frameon=False, figsize=(w * 1.0 / 100, h * 1.0 / 100))
            plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.imshow(image)
            for i, ind in enumerate(keep):
                if df_likelihood[ind, index] > pcutoff:
                    plt.scatter(
                        df_x[ind, index],
                        df_y[ind, index],
                        s=dotsize**2,
                        color=colors(map2bp[i]),
                        alpha=alphavalue,
                    )
            plt.xlim(0, w)
            plt.ylim(0, h)
            plt.axis("off")
            plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)
            plt.gca().invert_yaxis()
            plt.savefig(imagename2)
            plt.close("all")


def merge_datasets(config, forceiterate=None):
    """Merge the original training dataset with the newly refined data.

    Checks if the original training dataset can be merged with the newly refined
    training dataset. To do so it will check if the frames in all extracted video sets
    were relabeled.

    If this is the case then the ``"iteration"`` variable is advanced by 1.

    Parameters
    ----------
    config: str
        Full path of the config.yaml file.

    forceiterate: int or None, optional, default=None
        If an integer is given the iteration variable is set to this value
        This is only done if all datasets were labeled or refined.

    Examples
    --------

    >>> deeplabcut.merge_datasets('/analysis/project/reaching-task/config.yaml')
    """

    cfg = auxiliaryfunctions.read_config(config)
    config_path = Path(config).parents[0]

    bf = Path(str(config_path / "labeled-data"))
    allfolders = [
        os.path.join(bf, fn)
        for fn in os.listdir(bf)
        if "_labeled" not in fn and not fn.startswith(".")
    ]  # exclude labeled data folders and temporary files
    flagged = False
    for findex, folder in enumerate(allfolders):
        if os.path.isfile(
            os.path.join(folder, "MachineLabelsRefine.h5")
        ):  # Folder that was manually refine...
            pass
        elif os.path.isfile(
            os.path.join(folder, "CollectedData_" + cfg["scorer"] + ".h5")
        ):  # Folder that contains human data set...
            pass
        else:
            print("The following folder was not manually refined,...", folder)
            flagged = True
            pass  # this folder does not contain a MachineLabelsRefine file (not updated...)

    if not flagged:
        # updates iteration by 1
        iter_prev = cfg["iteration"]
        if not forceiterate:
            cfg["iteration"] = int(iter_prev + 1)
        else:
            cfg["iteration"] = forceiterate

        auxiliaryfunctions.write_config(config, cfg)

        print(
            "Merged data sets and updated refinement iteration to "
            + str(cfg["iteration"])
            + "."
        )
        print(
            "Now you can create a new training set for the expanded annotated images (use create_training_dataset)."
        )
    else:
        print("Please label, or remove the un-corrected folders.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("config")
    parser.add_argument("videos")
    cli_args = parser.parse_args()


--- File: deeplabcut/refine_training_dataset/stitch.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from typing import List, Optional

import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import os
import pandas as pd
import pickle
import re
import scipy.linalg.interpolative as sli
import shelve
import warnings
from collections import defaultdict

import deeplabcut
from deeplabcut.utils.auxfun_videos import VideoWriter
from functools import partial
from deeplabcut.core.trackingutils import (
    calc_iou,
    TRACK_METHODS,
)
from deeplabcut.utils import auxiliaryfunctions, auxfun_multianimal
from itertools import combinations, cycle
from networkx.algorithms.flow import preflow_push
from pathlib import Path
from scipy.linalg import hankel
from scipy.spatial.distance import directed_hausdorff
from scipy.stats import mode
from tqdm import trange


class Tracklet:
    def __init__(self, data, inds):
        """
        Create a Tracklet object.

        Parameters
        ----------
        data : ndarray
            3D array of shape (nframes, nbodyparts, 3 or 4), where the last
            dimension is for x, y, likelihood and, optionally, identity.
        inds : array-like
            Corresponding time frame indices.
        """
        if data.ndim != 3 or data.shape[-1] not in (3, 4):
            raise ValueError("Data must of shape (nframes, nbodyparts, 3 or 4)")

        if data.shape[0] != len(inds):
            raise ValueError(
                "Data and corresponding indices must have the same length."
            )

        self.data = data.astype(np.float64)
        self.inds = np.array(inds)
        monotonically_increasing = all(a < b for a, b in zip(inds, inds[1:]))
        if not monotonically_increasing:
            idx = np.argsort(inds, kind="mergesort")  # For stable sort with duplicates
            self.inds = self.inds[idx]
            self.data = self.data[idx]
        self._centroid = None

    def __len__(self):
        return self.inds.size

    def __add__(self, other):
        """Join this tracklet to another one."""
        data = np.concatenate((self.data, other.data))
        inds = np.concatenate((self.inds, other.inds))
        return Tracklet(data, inds)

    def __radd__(self, other):
        if other == 0:
            return self
        return self.__add__(other)

    def __sub__(self, other):
        mask = np.isin(self.inds, other.inds, assume_unique=True)
        if mask.all():
            return None
        return Tracklet(self.data[~mask], self.inds[~mask])

    def __lt__(self, other):
        """Test whether this tracklet precedes the other one."""
        return self.end < other.start

    def __gt__(self, other):
        """Test whether this tracklet follows the other one."""
        return self.start > other.end

    def __contains__(self, other_tracklet):
        """Test whether tracklets temporally overlap."""
        return np.isin(self.inds, other_tracklet.inds, assume_unique=True).any()

    def __repr__(self):
        return (
            f"Tracklet of length {len(self)} from {self.start} to {self.end} "
            f"with reliability {self.likelihood:.3f}"
        )

    @property
    def xy(self):
        """Return the x and y coordinates."""
        return self.data[..., :2]

    @property
    def centroid(self):
        """
        Return the instantaneous 2D position of the Tracklet centroid.
        For Tracklets longer than 10 frames, the centroid is automatically
        smoothed using an exponential moving average.
        The result is cached for efficiency.
        """
        if self._centroid is None:
            self._update_centroid()
        return self._centroid

    def _update_centroid(self):
        like = self.data[..., 2:3] + 1e-10 # Avoid division by zero in very uncertain tracklets
        self._centroid = np.nansum(self.xy * like, axis=1) / np.nansum(like, axis=1)

    @property
    def likelihood(self):
        """Return the average likelihood of all Tracklet detections."""
        return np.nanmean(self.data[..., 2])

    @property
    def identity(self):
        """Return the average predicted identity of all Tracklet detections."""
        try:
            return mode(
                self.data[..., 3],
                axis=None,
                nan_policy="omit",
                keepdims=False,
            )[0]
        except IndexError:
            return -1

    @property
    def start(self):
        """Return the time at which the tracklet starts."""
        return self.inds[0]

    @property
    def end(self):
        """Return the time at which the tracklet ends."""
        return self.inds[-1]

    @property
    def flat_data(self):
        return self.data[..., :3].reshape((len(self)), -1)

    def get_data_at(self, ind):
        return self.data[np.searchsorted(self.inds, ind)]

    def set_data_at(self, ind, data):
        self.data[np.searchsorted(self.inds, ind)] = data

    def del_data_at(self, ind):
        idx = np.searchsorted(self.inds, ind)
        self.inds = np.delete(self.inds, idx)
        self.data = np.delete(self.data, idx, axis=0)
        self._update_centroid()

    def interpolate(self, max_gap=1):
        if max_gap < 1:
            raise ValueError("Gap should be a strictly positive integer.")

        gaps = np.diff(self.inds) - 1
        valid_gaps = (0 < gaps) & (gaps <= max_gap)
        fills = []
        for i in np.flatnonzero(valid_gaps):
            s, e = self.inds[[i, i + 1]]
            data1, data2 = self.data[[i, i + 1]]
            diff = (data2 - data1) / (e - s)
            diff[np.isnan(diff)] = 0
            interp = diff[..., np.newaxis] * np.arange(1, e - s)
            data = data1 + np.rollaxis(interp, axis=2)
            data[..., 2] = 0.5  # Chance detections
            if data.shape[1] == 4:
                data[:, 3] = self.identity
            fills.append(Tracklet(data, np.arange(s + 1, e)))
        if not fills:
            return self
        return self + sum(fills)

    def contains_duplicates(self, return_indices=False):
        """
        Evaluate whether the Tracklet contains duplicate time indices.
        If `return_indices`, also return the indices of the duplicates.
        """
        has_duplicates = len(set(self.inds)) != len(self.inds)
        if not return_indices:
            return has_duplicates
        return has_duplicates, np.flatnonzero(np.diff(self.inds) == 0)

    def calc_velocity(self, where="head", norm=True):
        """
        Calculate the linear velocity of either the `head`
        or `tail` of the Tracklet, computed over the last or first
        three frames, respectively. If `norm`, return the absolute
        speed rather than a 2D vector.
        """
        if where == "tail":
            vel = (
                np.diff(self.centroid[:3], axis=0)
                / np.diff(self.inds[:3])[:, np.newaxis]
            )
        elif where == "head":
            vel = (
                np.diff(self.centroid[-3:], axis=0)
                / np.diff(self.inds[-3:])[:, np.newaxis]
            )
        else:
            raise ValueError(f"Unknown where={where}")
        if norm:
            return np.sqrt(np.sum(vel**2, axis=1)).mean()
        return vel.mean(axis=0)

    @property
    def maximal_velocity(self):
        vel = np.diff(self.centroid, axis=0) / np.diff(self.inds)[:, np.newaxis]
        return np.sqrt(np.max(np.sum(vel**2, axis=1)))

    def calc_rate_of_turn(self, where="head"):
        """
        Calculate the rate of turn (or angular velocity) of
        either the `head` or `tail` of the Tracklet, computed over
        the last or first three frames, respectively.
        """
        if where == "tail":
            v = np.diff(self.centroid[:3], axis=0)
        else:
            v = np.diff(self.centroid[-3:], axis=0)
        theta = np.arctan2(v[:, 1], v[:, 0])
        return (theta[1] - theta[0]) / (self.inds[1] - self.inds[0])

    @property
    def is_continuous(self):
        """Test whether there are gaps in the time indices."""
        return self.end - self.start + 1 == len(self)

    def immediately_follows(self, other_tracklet, max_gap=1):
        """
        Test whether this Tracklet follows another within
        a tolerance of`max_gap` frames.
        """
        return 0 < self.start - other_tracklet.end <= max_gap

    def distance_to(self, other_tracklet):
        """
        Calculate the Euclidean distance between this Tracklet and another.
        If the Tracklets overlap in time, this is the mean distance over
        those frames. Otherwise, it is the distance between the head/tail
        of one to the tail/head of the other.
        """
        if self in other_tracklet:
            dist = (
                self.centroid[np.isin(self.inds, other_tracklet.inds)]
                - other_tracklet.centroid[np.isin(other_tracklet.inds, self.inds)]
            )
            return np.sqrt(np.sum(dist**2, axis=1)).mean()
        elif self < other_tracklet:
            return np.sqrt(
                np.sum((self.centroid[-1] - other_tracklet.centroid[0]) ** 2)
            )
        else:
            return np.sqrt(
                np.sum((self.centroid[0] - other_tracklet.centroid[-1]) ** 2)
            )

    def motion_affinity_with(self, other_tracklet):
        """
        Evaluate the motion affinity of this Tracklet' with another one.
        This evaluates whether the Tracklets could realistically be reached
        by one another, knowing the time separating them and their velocities.
        Return 0 if the Tracklets overlap.
        """
        time_gap = self.time_gap_to(other_tracklet)
        if time_gap > 0:
            if self < other_tracklet:
                d1 = self.centroid[-1] + time_gap * self.calc_velocity(norm=False)
                d2 = other_tracklet.centroid[
                    0
                ] - time_gap * other_tracklet.calc_velocity("tail", False)
                delta1 = other_tracklet.centroid[0] - d1
                delta2 = self.centroid[-1] - d2
            else:
                d1 = other_tracklet.centroid[
                    -1
                ] + time_gap * other_tracklet.calc_velocity(norm=False)
                d2 = self.centroid[0] - time_gap * self.calc_velocity("tail", False)
                delta1 = self.centroid[0] - d1
                delta2 = other_tracklet.centroid[-1] - d2
            return (np.sqrt(np.sum(delta1**2)) + np.sqrt(np.sum(delta2**2))) / 2
        return 0

    def time_gap_to(self, other_tracklet):
        """Return the time gap separating this Tracklet to another."""
        if self in other_tracklet:
            t = 0
        elif self < other_tracklet:
            t = other_tracklet.start - self.end
        else:
            t = self.start - other_tracklet.end
        return t

    def shape_dissimilarity_with(self, other_tracklet):
        """Calculate the dissimilarity in shape between this Tracklet and another."""
        if self in other_tracklet:
            dist = np.inf
        elif self < other_tracklet:
            dist = self.undirected_hausdorff(self.xy[-1], other_tracklet.xy[0])
        else:
            dist = self.undirected_hausdorff(self.xy[0], other_tracklet.xy[-1])
        return dist

    def box_overlap_with(self, other_tracklet):
        """Calculate the overlap between each Tracklet's bounding box."""
        if self in other_tracklet:
            overlap = 0
        else:
            if self < other_tracklet:
                bbox1 = self.calc_bbox(-1)
                bbox2 = other_tracklet.calc_bbox(0)
            else:
                bbox1 = self.calc_bbox(0)
                bbox2 = other_tracklet.calc_bbox(-1)
            overlap = calc_iou(bbox1, bbox2)
        return overlap

    @staticmethod
    def undirected_hausdorff(u, v):
        return max(directed_hausdorff(u, v)[0], directed_hausdorff(v, u)[0])

    def calc_bbox(self, ind):
        xy = self.xy[ind]
        bbox = np.empty(4)
        bbox[:2] = np.nanmin(xy, axis=0)
        bbox[2:] = np.nanmax(xy, axis=0)
        return bbox

    @staticmethod
    def hankelize(xy):
        ncols = int(np.ceil(len(xy) * 2 / 3))
        nrows = len(xy) - ncols + 1
        mat = np.empty((2 * nrows, ncols))
        mat[::2] = hankel(xy[:nrows, 0], xy[-ncols:, 0])
        mat[1::2] = hankel(xy[:nrows, 1], xy[-ncols:, 1])
        return mat

    def to_hankelet(self):
        # See Li et al., 2012. Cross-view Activity Recognition using Hankelets.
        # As proposed in the paper, the Hankel matrix can either be formed from
        # the tracklet's centroid or its normalized velocity.
        # vel = np.diff(self.centroid, axis=0)
        # vel /= np.linalg.norm(vel, axis=1, keepdims=True)
        # return self.hankelize(vel)
        return self.hankelize(self.centroid)

    def dynamic_dissimilarity_with(self, other_tracklet):
        """
        Compute a dissimilarity score between Hankelets.
        This metric efficiently captures the degree of alignment of
        the subspaces spanned by the columns of both matrices.

        See Li et al., 2012.
            Cross-view Activity Recognition using Hankelets.
        """
        hk1 = self.to_hankelet()
        hk1 /= np.linalg.norm(hk1)
        hk2 = other_tracklet.to_hankelet()
        hk2 /= np.linalg.norm(hk2)
        min_shape = min(hk1.shape + hk2.shape)
        temp1 = (hk1 @ hk1.T)[:min_shape, :min_shape]
        temp2 = (hk2 @ hk2.T)[:min_shape, :min_shape]
        return 2 - np.linalg.norm(temp1 + temp2)

    def dynamic_similarity_with(self, other_tracklet, tol=0.01):
        """
        Evaluate the complexity of the tracklets' underlying dynamics
        from the rank of their Hankel matrices, and assess whether
        they originate from the same track. The idea is that if two
        tracklets are part of the same track, they can be approximated
        by a low order regressor. Conversely, tracklets belonging to
        different tracks will require a higher order regressor.

        See Dicle et al., 2013.
            The Way They Move: Tracking Multiple Targets with Similar Appearance.
        """
        # TODO Add missing data imputation
        joint_tracklet = self + other_tracklet
        joint_rank = joint_tracklet.estimate_rank(tol)
        rank1 = self.estimate_rank(tol)
        rank2 = other_tracklet.estimate_rank(tol)
        return (rank1 + rank2) / joint_rank - 1

    def estimate_rank(self, tol):
        """
        Estimate the (low) rank of a noisy matrix via
        hard thresholding of singular values.

        See Gavish & Donoho, 2013.
            The optimal hard threshold for singular values is 4/sqrt(3)
        """
        mat = self.to_hankelet()
        if np.any(mat):  # check that the matrix contains non-zero entries
            # nrows, ncols = mat.shape
            # beta = nrows / ncols
            # omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43
            _, s, _ = sli.svd(mat, min(10, min(mat.shape)))
        else:
            s = np.zeros(min(10, min(mat.shape)))

        # return np.argmin(s > omega * np.median(s))
        eigen = s**2
        diff = np.abs(np.diff(eigen / eigen[0]))
        return np.argmin(diff > tol)

    def plot(self, centroid_only=True, color=None, ax=None, interactive=False):
        if ax is None:
            fig, ax = plt.subplots()
        centroid = np.full((self.end + 1, 2), np.nan)
        centroid[self.inds] = self.centroid
        lines = ax.plot(centroid, c=color, lw=2, picker=interactive)
        if not centroid_only:
            xy = np.full((self.end + 1, self.xy.shape[1], 2), np.nan)
            xy[self.inds] = self.xy
            ax.plot(xy[..., 0], c=color, lw=1)
            ax.plot(xy[..., 1], c=color, lw=1)
        return lines


class TrackletStitcher:
    def __init__(
        self,
        tracklets,
        n_tracks,
        min_length=10,
        split_tracklets=True,
        prestitch_residuals=True,
    ):
        if n_tracks < 1:
            raise ValueError("There must at least be one track to reconstruct.")

        if min_length < 3:
            raise ValueError("A tracklet must have a minimal length of 3.")

        self.min_length = min_length
        self.filename = ""
        self.header = None
        self.single = None
        self.n_tracks = n_tracks
        self.G = None
        self.paths = None
        self.tracks = None

        self.tracklets = []
        self.residuals = []
        for unpure_tracklet in tracklets:
            tracklet = self.purify_tracklet(unpure_tracklet)
            if tracklet is None:
                continue
            if not tracklet.is_continuous and split_tracklets:
                idx = np.flatnonzero(np.diff(tracklet.inds) != 1) + 1
                tracklet = self.split_tracklet(tracklet, tracklet.inds[idx])
            if not isinstance(tracklet, list):
                tracklet = [tracklet]
            for t in tracklet:
                if len(t) >= min_length:
                    self.tracklets.append(t)
                elif len(t) < min_length:
                    self.residuals.append(t)

        if not len(self.tracklets):
            raise IOError("Tracklets are empty.")

        if prestitch_residuals:
            self._prestitch_residuals(5)  # Hard-coded but found to work very well
        self.tracklets = sorted(self.tracklets, key=lambda t: t.start)
        self._first_frame = self.tracklets[0].start
        self._last_frame = max(self.tracklets, key=lambda t: t.end).end

        # Note that if tracklets are very short, some may actually be part of the same track
        # and thus incorrectly reflect separate track endpoints...
        self._first_tracklets = sorted(self, key=lambda t: t.start)[: self.n_tracks]
        self._last_tracklets = sorted(self, key=lambda t: t.end)[-self.n_tracks :]

        # Map each Tracklet to an entry and output nodes and vice versa,
        # which is convenient once the tracklets are stitched.
        self._mapping = {
            tracklet: {"in": f"{i}in", "out": f"{i}out"}
            for i, tracklet in enumerate(self)
        }
        self._mapping_inv = {
            label: k for k, v in self._mapping.items() for label in v.values()
        }

        # Store tracklets and corresponding negatives (those that overlap in time)
        self._lu_overlap = defaultdict(list)
        for tracklet1, tracklet2 in combinations(self, 2):
            if tracklet2 in tracklet1:
                self._lu_overlap[tracklet1].append(tracklet2)
                self._lu_overlap[tracklet2].append(tracklet1)

    def __getitem__(self, item):
        return self.tracklets[item]

    def __len__(self):
        return len(self.tracklets)

    @classmethod
    def from_pickle(
        cls,
        pickle_file,
        n_tracks,
        min_length=10,
        split_tracklets=True,
        prestitch_residuals=True,
    ):
        with open(pickle_file, "rb") as file:
            tracklets = pickle.load(file)
        class_ = cls.from_dict_of_dict(
            tracklets, n_tracks, min_length, split_tracklets, prestitch_residuals
        )
        class_.filename = pickle_file
        return class_

    @classmethod
    def from_dict_of_dict(
        cls,
        dict_of_dict,
        n_tracks,
        min_length=10,
        split_tracklets=True,
        prestitch_residuals=True,
    ):
        tracklets = []
        header = dict_of_dict.pop("header", None)
        single = None
        for k, dict_ in dict_of_dict.items():
            try:
                inds, data = zip(*[(cls.get_frame_ind(k), v) for k, v in dict_.items()])
            except ValueError:
                continue
            inds = np.asarray(inds)
            data = np.asarray(data)
            try:
                nrows, ncols = data.shape
                data = data.reshape((nrows, ncols // 3, 3))
            except ValueError:
                pass
            tracklet = Tracklet(data, inds)
            if k == "single":
                single = tracklet
            else:
                tracklets.append(Tracklet(data, inds))
        class_ = cls(
            tracklets, n_tracks, min_length, split_tracklets, prestitch_residuals
        )
        class_.header = header
        class_.single = single
        return class_

    @staticmethod
    def get_frame_ind(s):
        if isinstance(s, str):
            return int(re.findall(r"\d+", s)[0])
        return s

    @staticmethod
    def purify_tracklet(tracklet):
        valid = ~np.isnan(tracklet.xy).all(axis=(1, 2))
        if not np.any(valid):
            return None
        return Tracklet(tracklet.data[valid], tracklet.inds[valid])

    @staticmethod
    def split_tracklet(tracklet, inds):
        idx = sorted(set(np.searchsorted(tracklet.inds, inds)))
        inds_new = np.split(tracklet.inds, idx)
        data_new = np.split(tracklet.data, idx)
        return [Tracklet(data, inds) for data, inds in zip(data_new, inds_new)]

    @property
    def n_frames(self):
        return self._last_frame - self._first_frame + 1

    # TODO Avoid looping over all pairs of tracklets
    @staticmethod
    def compute_max_gap(tracklets):
        gap = defaultdict(list)
        for tracklet1, tracklet2 in combinations(tracklets, 2):
            gap[tracklet1].append(tracklet1.time_gap_to(tracklet2))
        max_gap = 0
        for vals in gap.values():
            for val in sorted(vals):
                if val > 0:
                    if val > max_gap:
                        max_gap = val
                    break
        return max_gap

    def mine(self, n_samples):
        if not self._lu_overlap:
            raise ValueError("No overlapping tracklets found.")

        p = np.asarray([t.likelihood for t in self])
        p /= p.sum()
        triplets = []
        while len(triplets) != n_samples:
            tracklet = np.random.choice(self, p=p)
            overlapping_tracklets = self._lu_overlap[tracklet]
            if not overlapping_tracklets:
                continue
            # Pick the closest (spatially) overlapping tracklet
            ind_min = np.argmin(
                [tracklet.distance_to(t) for t in overlapping_tracklets]
            )
            overlapping_tracklet = overlapping_tracklets[ind_min]
            common_inds = set(tracklet.inds).intersection(overlapping_tracklet.inds)
            ind_anchor = np.random.choice(list(common_inds))
            anchor = tracklet.get_data_at(ind_anchor)[:, :2].astype(int)
            neg = overlapping_tracklet.get_data_at(ind_anchor)[:, :2].astype(int)
            ind_pos = np.random.choice(tracklet.inds[tracklet.inds != ind_anchor])
            pos = tracklet.get_data_at(ind_pos)[:, :2].astype(int)
            triplet = ((anchor, ind_anchor), (pos, ind_pos), (neg, ind_anchor))
            triplets.append(triplet)
        return triplets

    def build_graph(
        self,
        nodes=None,
        max_gap=None,
        weight_func=None,
    ):
        if nodes is None:
            nodes = self.tracklets
        nodes = sorted(nodes, key=lambda t: t.start)
        n_nodes = len(nodes)

        if not max_gap:
            max_gap = int(1.5 * self.compute_max_gap(nodes))

        self.G = nx.DiGraph()
        self.G.add_node("source", demand=-self.n_tracks)
        self.G.add_node("sink", demand=self.n_tracks)
        nodes_in, nodes_out = zip(
            *[v.values() for k, v in self._mapping.items() if k in nodes]
        )
        self.G.add_nodes_from(nodes_in, demand=1)
        self.G.add_nodes_from(nodes_out, demand=-1)
        self.G.add_edges_from(zip(nodes_in, nodes_out), capacity=1)
        self.G.add_edges_from(zip(["source"] * n_nodes, nodes_in), capacity=1)
        self.G.add_edges_from(zip(nodes_out, ["sink"] * n_nodes), capacity=1)
        if weight_func is None:
            weight_func = self.calculate_edge_weight
        for i in trange(n_nodes):
            node_i = nodes[i]
            end = node_i.end
            for j in range(i + 1, n_nodes):
                node_j = nodes[j]
                start = node_j.start
                gap = start - end
                if gap > max_gap:
                    break
                elif gap > 0:
                    # The algorithm works better with integer weights
                    w = int(100 * weight_func(node_i, node_j))
                    self.G.add_edge(
                        self._mapping[node_i]["out"],
                        self._mapping[node_j]["in"],
                        weight=w,
                        capacity=1,
                    )

    def _update_edge_weights(self, weight_func):
        if self.G is None:
            raise ValueError("Inexistent graph. Call `build_graph` first")

        for node1, node2, weight in self.G.edges.data("weight"):
            if weight is not None:
                w = weight_func(self._mapping_inv[node1], self._mapping_inv[node2])
                self.G.edges[(node1, node2)]["weight"] = w

    def stitch(self, add_back_residuals=True):
        if self.G is None:
            raise ValueError("Inexistent graph. Call `build_graph` first")

        try:
            _, self.flow = nx.capacity_scaling(self.G)
            self.paths = self.reconstruct_paths()
        except nx.exception.NetworkXUnfeasible:
            warnings.warn("No optimal solution found. Employing black magic...")
            # Let us prune the graph by removing all source and sink edges
            # but those connecting the `n_tracks` first and last tracklets.
            in_to_keep = [
                self._mapping[first_tracklet]["in"]
                for first_tracklet in self._first_tracklets
            ]
            out_to_keep = [
                self._mapping[last_tracklet]["out"]
                for last_tracklet in self._last_tracklets
            ]
            in_to_remove = set(
                node for _, node in self.G.out_edges("source")
            ).difference(in_to_keep)
            out_to_remove = set(node for node, _ in self.G.in_edges("sink")).difference(
                out_to_keep
            )
            self.G.remove_edges_from(zip(["source"] * len(in_to_remove), in_to_remove))
            self.G.remove_edges_from(zip(out_to_remove, ["sink"] * len(out_to_remove)))
            # Preflow push seems to work slightly better than shortest
            # augmentation path..., and is more computationally efficient.
            paths = []
            for path in nx.node_disjoint_paths(
                self.G, "source", "sink", preflow_push, self.n_tracks
            ):
                temp = set()
                for node in path[1:-1]:
                    self.G.remove_node(node)
                    temp.add(self._mapping_inv[node])
                paths.append(list(temp))
            incomplete_tracks = self.n_tracks - len(paths)
            remaining_nodes = set(
                self._mapping_inv[node]
                for node in self.G
                if node not in ("source", "sink")
            )
            if len(remaining_nodes) > 0:
                if (
                    incomplete_tracks == 1
                ):  # All remaining nodes must belong to the same track
                    # Verify whether there are overlapping tracklets
                    for t1, t2 in combinations(remaining_nodes, 2):
                        if t1 in t2:
                            # Pick the segment that minimizes "smoothness", computed here
                            # with the coefficient of variation of the differences.
                            if t1 in remaining_nodes:
                                remaining_nodes.remove(t1)
                            if t2 in remaining_nodes:
                                remaining_nodes.remove(t2)
                            track = sum(remaining_nodes)
                            hyp1 = track + t1
                            hyp2 = track + t2
                            dx1 = np.diff(hyp1.centroid, axis=0)
                            cv1 = dx1.std() / np.abs(dx1).mean()
                            dx2 = np.diff(hyp2.centroid, axis=0)
                            cv2 = dx2.std() / np.abs(dx2).mean()
                            if cv1 < cv2:
                                remaining_nodes.add(t1)
                                self.residuals.append(t2)
                            else:
                                remaining_nodes.add(t2)
                                self.residuals.append(t1)
                    paths.append(list(remaining_nodes))
                elif incomplete_tracks > 1:
                    # Rebuild a full graph from the remaining nodes without
                    # temporal constraint on what tracklets can be stitched together.
                    self.build_graph(list(remaining_nodes), max_gap=np.inf)
                    self.G.nodes["source"]["demand"] = -incomplete_tracks
                    self.G.nodes["sink"]["demand"] = incomplete_tracks
                    _, self.flow = nx.capacity_scaling(self.G)
                    paths += self.reconstruct_paths()
            self.paths = paths
            if len(self.paths) != self.n_tracks:
                warnings.warn(f"Only {len(self.paths)} tracks could be reconstructed.")

        finally:
            if self.paths is None:
                raise ValueError(
                    f"Could not reconstruct {self.n_tracks} tracks from the tracklets given."
                )

            self.tracks = np.asarray([sum(path) for path in self.paths if path])
            if add_back_residuals:
                _ = self._finalize_tracks()

    def _finalize_tracks(self):
        residuals = [res for res in sorted(self.residuals, key=len) if len(res) > 1]
        # Cycle through the residuals and incorporate back those
        # that only fit in a single tracklet.
        n_attemps = 0
        n_max = len(residuals)
        while n_attemps < n_max and residuals:
            for res in residuals[::-1]:
                easy_fit = [
                    i for i, track in enumerate(self.tracks) if res not in track
                ]
                if not easy_fit:
                    residuals.remove(res)
                    continue
                if len(easy_fit) == 1:
                    self.tracks[easy_fit[0]] += res
                    residuals.remove(res)
                    n_attemps = 0
                else:
                    n_attemps += 1

        # Greedily add the remaining residuals
        for res in residuals[::-1]:
            c1 = res.centroid[[0, -1]]
            easy_fit = [i for i, track in enumerate(self.tracks) if res not in track]
            dists = []
            for n, track in enumerate(self.tracks[easy_fit]):
                e = np.searchsorted(track.inds, res.end)
                s = e - 1
                try:
                    t = track.inds[[s, e]]
                except IndexError:
                    continue
                left_gap = res.start - t[0]
                right_gap = t[1] - res.end
                if not left_gap > 0 and right_gap > 0:
                    continue
                if left_gap <= 3:
                    dist = np.linalg.norm(track.centroid[s] - c1[0])
                elif right_gap <= 3:
                    dist = np.linalg.norm(track.centroid[e] - c1[1])
                else:
                    dist = np.linalg.norm(track.centroid[s] - c1[0]) + np.linalg.norm(
                        track.centroid[e] - c1[1]
                    )
                dists.append((n, dist))
            if not dists:
                continue
            if len(dists) == 1:
                ind = easy_fit[dists[0][0]]
            else:
                ind = sorted(dists, key=lambda x: x[1])[0][0]
            self.tracks[ind] += res
            residuals.remove(res)
        return residuals

    def _prestitch_residuals(self, max_gap=5):
        G = nx.DiGraph()
        residuals = sorted(self.residuals, key=lambda x: x.start)
        for i in range(len(residuals)):
            e = residuals[i].end
            for j in range(i + 1, len(residuals)):
                s = residuals[j].start
                gap = s - e
                if gap < 1:
                    continue
                if gap < max_gap:
                    w = 1 - residuals[i].box_overlap_with(residuals[j])
                    G.add_edge(i, j, weight=w)
                else:
                    break
        mini_tracks = []
        to_remove = []
        for comp in nx.connected_components(G.to_undirected()):
            sub_ = G.subgraph(comp)
            inds = nx.dag_longest_path(sub_)
            to_remove.extend(inds)
            mini_tracks.append(sum(residuals[ind] for ind in inds))
        for ind in sorted(to_remove, reverse=True):
            self.residuals.pop(ind)
        self.residuals.extend(mini_tracks)

    def concatenate_data(self):
        if self.tracks is None:
            raise ValueError("No tracks were found. Call `stitch` first")

        # Refresh temporal bounds
        self._first_frame = min(self.tracks, key=lambda t: t.start).start
        self._last_frame = max(self.tracks, key=lambda t: t.end).end
        data = []
        # Guarantee track data are sorted in the order defined in the config
        for track in sorted(self.tracks, key=lambda t: t.identity):
            flat_data = track.flat_data
            temp = np.full((self.n_frames, flat_data.shape[1]), np.nan)
            temp[track.inds - self._first_frame] = flat_data
            data.append(temp)

        # If there isn't a track for each animal, fill in the dataframe with NaNs
        missing_tracks = self.n_tracks - len(self.tracks)
        if missing_tracks > 0:
            track_shape = self.tracks[0].flat_data.shape[1]
            data += missing_tracks * [np.full((self.n_frames, track_shape), np.nan)]

        return np.hstack(data)

    def format_df(self, animal_names=None):
        data = self.concatenate_data()
        if not animal_names or len(animal_names) < self.n_tracks:
            animal_names = [f"ind{i}" for i in range(1, self.n_tracks + 1)]
        elif len(animal_names) > self.n_tracks:
            animal_names = animal_names[:self.n_tracks]

        coords = ["x", "y", "likelihood"]
        n_multi_bpts = data.shape[1] // (len(animal_names) * len(coords))
        n_unique_bpts = 0 if self.single is None else self.single.data.shape[1]

        if self.header is not None:
            scorer = self.header.get_level_values("scorer").unique().to_list()
            bpts = self.header.get_level_values("bodyparts").unique().to_list()
        else:
            scorer = ["scorer"]
            bpts = [f"bpt{i}" for i in range(1, n_multi_bpts + 1)]
            bpts += [f"bpt_unique{i}" for i in range(1, n_unique_bpts + 1)]

        columns = pd.MultiIndex.from_product(
            [scorer, animal_names, bpts[:n_multi_bpts], coords],
            names=["scorer", "individuals", "bodyparts", "coords"],
        )
        inds = range(self._first_frame, self._last_frame + 1)
        df = pd.DataFrame(data, columns=columns, index=inds)
        df = df.reindex(range(self._last_frame + 1))
        if self.single is not None:
            columns = pd.MultiIndex.from_product(
                [scorer, ["single"], bpts[-n_unique_bpts:], coords],
                names=["scorer", "individuals", "bodyparts", "coords"],
            )
            df2 = pd.DataFrame(
                self.single.flat_data, columns=columns, index=self.single.inds
            )
            df = df.join(df2, how="outer")
        return df

    def write_tracks(
        self, output_name="", suffix="", animal_names=None, save_as_csv=False
    ):
        df = self.format_df(animal_names)
        if not output_name:
            if suffix:
                suffix = "_" + suffix
            output_name = self.filename.replace(".pickle", f"{suffix}.h5")
        df.to_hdf(output_name, "tracks", format="table", mode="w")
        if save_as_csv:
            df.to_csv(output_name.replace(".h5", ".csv"))

    @staticmethod
    def calculate_edge_weight(tracklet1, tracklet2):
        # Default to the distance cost function
        return tracklet1.distance_to(tracklet2)

    @property
    def weights(self):
        if self.G is None:
            raise ValueError("Inexistent graph. Call `build_graph` first")

        return nx.get_edge_attributes(self.G, "weight")

    def draw_graph(self, with_weights=False):
        if self.G is None:
            raise ValueError("Inexistent graph. Call `build_graph` first")

        pos = nx.spring_layout(self.G)
        nx.draw_networkx(self.G, pos)
        if with_weights:
            nx.draw_networkx_edge_labels(self.G, pos, edge_labels=self.weights)

    def plot_paths(self, colormap="Set2"):
        if self.paths is None:
            raise ValueError("No paths were found. Call `stitch` first")

        fig, ax = plt.subplots()
        ax.set_yticks([])
        for loc, spine in ax.spines.items():
            if loc != "bottom":
                spine.set_visible(False)
        for path in self.paths:
            length = len(path)
            colors = plt.get_cmap(colormap, length)(range(length))
            for tracklet, color in zip(path, colors):
                tracklet.plot(color=color, ax=ax)

    def plot_tracks(self, colormap="viridis"):
        if self.tracks is None:
            raise ValueError("No tracks were found. Call `stitch` first")

        fig, ax = plt.subplots()
        ax.set_yticks([])
        for loc, spine in ax.spines.items():
            if loc != "bottom":
                spine.set_visible(False)
        colors = plt.get_cmap(colormap, self.n_tracks)(range(self.n_tracks))
        for track, color in zip(self.tracks, colors):
            track.plot(color=color, ax=ax)

    def plot_tracklets(self, colormap="Paired"):
        fig, axes = plt.subplots(ncols=2, figsize=(14, 4))
        axes[0].set_yticks([])
        for loc, spine in axes[0].spines.items():
            if loc != "bottom":
                spine.set_visible(False)
        axes[1].axis("off")

        cmap = plt.get_cmap(colormap)
        colors = cycle(cmap.colors)
        line2tracklet = dict()
        tracklet2lines = dict()
        all_points = defaultdict(dict)
        for tracklet in self:
            color = next(colors)
            lines = tracklet.plot(ax=axes[0], color=color)
            tracklet2lines[tracklet] = lines
            for line in lines:
                line2tracklet[line] = tracklet
            for i, (x, y) in zip(tracklet.inds, tracklet.centroid):
                all_points[i][(x, y)] = color

    def reconstruct_paths(self):
        paths = []
        for node, flow in self.flow["source"].items():
            if flow == 1:
                path = self.reconstruct_path(node.replace("in", "out"))
                paths.append([self._mapping_inv[tracklet] for tracklet in path])
        return paths

    def reconstruct_path(self, source):
        path = [source]
        for node, flow in self.flow[source].items():
            if flow == 1:
                if node != "sink":
                    self.flow[source][node] -= 1
                    path.extend(self.reconstruct_path(node.replace("in", "out")))
                return path


def stitch_tracklets(
    config_path,
    videos,
    videotype="",
    shuffle=1,
    trainingsetindex=0,
    n_tracks=None,
    animal_names: Optional[List[str]] = None,
    min_length=10,
    split_tracklets=True,
    prestitch_residuals=True,
    max_gap=None,
    weight_func=None,
    destfolder=None,
    modelprefix="",
    track_method="",
    output_name="",
    transformer_checkpoint="",
    save_as_csv=False,
):
    """
    Stitch sparse tracklets into full tracks via a graph-based,
    minimum-cost flow optimization problem.

    Parameters
    ----------
    config_path : str
        Path to the main project config.yaml file.

    videos : list
        A list of strings containing the full paths to videos for analysis or a path to the directory, where all the videos with same extension are stored.

    videotype: string, optional
        Checks for the extension of the video in case the input to the video is a directory.\n Only videos with this extension are analyzed.
        If left unspecified, videos with common extensions ('avi', 'mp4', 'mov', 'mpeg', 'mkv') are kept.

    shuffle: int, optional
        An integer specifying the shuffle index of the training dataset used for training the network. The default is 1.

    trainingsetindex: int, optional
        Integer specifying which TrainingsetFraction to use. By default the first (note that TrainingFraction is a list in config.yaml).

    n_tracks : int, optional
        Number of tracks to reconstruct. By default, taken as the number
        of individuals defined in the config.yaml. Another number can be
        passed if the number of animals in the video is different from
        the number of animals the model was trained on.

    animal_names: list, optional
        If you want the names given to individuals in the labeled data file, you can
        specify those names as a list here. If given and `n_tracks` is None, `n_tracks`
        will be set to `len(animal_names)`. If `n_tracks` is not None, then it must be
        equal to `len(animal_names)`. If it is not given, then `animal_names` will
        be loaded from the `individuals` in the project config.yaml file.

    min_length : int, optional
        Tracklets less than `min_length` frames of length
        are considered to be residuals; i.e., they do not participate
        in building the graph and finding the solution to the
        optimization problem, but are rather added last after
        "almost-complete" tracks are formed. The higher the value,
        the lesser the computational cost, but the higher the chance of
        discarding relatively long and reliable tracklets that are
        essential to solving the stitching task.
        Default is 10, and must be 3 at least.

    split_tracklets : bool, optional
        By default, tracklets whose time indices are not consecutive integers
        are split in shorter tracklets whose time continuity is guaranteed.
        This is for example very powerful to get rid of tracking errors
        (e.g., identity switches) which are often signaled by a missing
        time frame at the moment they occur. Note though that for long
        occlusions where tracker re-identification capability can be trusted,
        setting `split_tracklets` to False is preferable.

    prestitch_residuals : bool, optional
        Residuals will by default be grouped together according to their
        temporal proximity prior to being added back to the tracks.
        This is done to improve robustness and simultaneously reduce complexity.

    max_gap : int, optional
        Maximal temporal gap to allow between a pair of tracklets.
        This is automatically determined by the TrackletStitcher by default.

    weight_func : callable, optional
        Function accepting two tracklets as arguments and returning a scalar
        that must be inversely proportional to the likelihood that the tracklets
        belong to the same track; i.e., the higher the confidence that the
        tracklets should be stitched together, the lower the returned value.

    destfolder: string, optional
        Specifies the destination folder for analysis data (default is the path of the
        video). Note that for subsequent analysis this folder also needs to be passed.

    track_method: string, optional
         Specifies the tracker used to generate the pose estimation data.
         For multiple animals, must be either 'box', 'skeleton', or 'ellipse'
         and will be taken from the config.yaml file if none is given.

    output_name : str, optional
        Name of the output h5 file.
        By default, tracks are automatically stored into the same directory
        as the pickle file and with its name.

    save_as_csv: bool, optional
        Whether to write the tracks to a CSV file too (False by default).

    Returns
    -------
    A TrackletStitcher object
    """
    vids = deeplabcut.utils.auxiliaryfunctions.get_list_of_videos(videos, videotype)
    if not vids:
        print("No video(s) found. Please check your path!")
        return

    cfg = auxiliaryfunctions.read_config(config_path)
    track_method = auxfun_multianimal.get_track_method(cfg, track_method=track_method)

    if animal_names is None:
        animal_names = cfg["individuals"]
    elif n_tracks is not None and n_tracks != len(animal_names):
        raise ValueError(
            "When setting both `n_tracks` and `animal_names`, `n_tracks` must be equal "
            f"to len(animal_names)`. Found `n_tracks`={n_tracks} and `animal_names`="
            f"{animal_names} of length {len(animal_names)}.`")

    if n_tracks is None:
        n_tracks = len(animal_names)

    DLCscorer, _ = deeplabcut.utils.auxiliaryfunctions.get_scorer_name(
        cfg,
        shuffle,
        cfg["TrainingFraction"][trainingsetindex],
        modelprefix=modelprefix,
    )

    if transformer_checkpoint:
        from deeplabcut.pose_tracking_pytorch import inference

        dlctrans = inference.DLCTrans(checkpoint=transformer_checkpoint)

    def trans_weight_func(tracklet1, tracklet2, nframe, feature_dict):
        zfill_width = int(np.ceil(np.log10(nframe)))
        if tracklet1 < tracklet2:
            ind_img1 = tracklet1.inds[-1]
            coord1 = tracklet1.data[-1][:, :2]
            ind_img2 = tracklet2.inds[0]
            coord2 = tracklet2.data[0][:, :2]
        else:
            ind_img2 = tracklet2.inds[-1]
            ind_img1 = tracklet1.inds[0]
            coord2 = tracklet2.data[-1][:, :2]
            coord1 = tracklet1.data[0][:, :2]
        t1 = (coord1, ind_img1)
        t2 = (coord2, ind_img2)

        dist = dlctrans(t1, t2, zfill_width, feature_dict)
        dist = (dist + 1) / 2

        return -dist

    for video in vids:
        print("Processing... ", video)
        nframe = len(VideoWriter(video))
        videofolder = str(Path(video).parents[0])
        dest = destfolder or videofolder
        deeplabcut.utils.auxiliaryfunctions.attempt_to_make_folder(dest)
        vname = Path(video).stem

        feature_dict_path = os.path.join(
            dest, vname + DLCscorer + "_bpt_features.pickle"
        )
        # should only exist one
        if transformer_checkpoint:
            import dbm

            try:
                feature_dict = shelve.open(feature_dict_path, flag="r")
            except dbm.error:
                raise FileNotFoundError(
                    f"{feature_dict_path} does not exist. Did you run transformer_reID()?"
                )

        dataname = os.path.join(dest, vname + DLCscorer + ".h5")

        method = TRACK_METHODS[track_method]
        pickle_file = dataname.split(".h5")[0] + f"{method}.pickle"
        try:
            stitcher = TrackletStitcher.from_pickle(
                pickle_file, n_tracks, min_length, split_tracklets, prestitch_residuals
            )
            with_id = any(tracklet.identity != -1 for tracklet in stitcher)
            if with_id and weight_func is None:
                # Add in identity weighing before building the graph
                def weight_func(t1, t2):
                    w = 0.01 if t1.identity == t2.identity else 1
                    return w * stitcher.calculate_edge_weight(t1, t2)

            if transformer_checkpoint:
                stitcher.build_graph(
                    max_gap=max_gap,
                    weight_func=partial(
                        trans_weight_func, nframe=nframe, feature_dict=feature_dict
                    ),
                )
            else:
                stitcher.build_graph(max_gap=max_gap, weight_func=weight_func)

            stitcher.stitch()
            if transformer_checkpoint:
                stitcher.write_tracks(
                    output_name=output_name,
                    animal_names=animal_names,
                    suffix="tr",
                    save_as_csv=save_as_csv,
                )
            else:
                stitcher.write_tracks(
                    output_name=output_name,
                    animal_names=animal_names,
                    suffix="",
                    save_as_csv=save_as_csv,
                )
        except FileNotFoundError as e:
            print(e, "\nSkipping...")


--- File: conda-environments/DEEPLABCUT.yaml ---
# DEEPLABCUT.yaml

#DeepLabCut Toolbox (deeplabcut.org)
#© A. & M.W. Mathis Labs
#https://github.com/DeepLabCut/DeepLabCut
#Please see AUTHORS for contributors.

#https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#Licensed under GNU Lesser General Public License v3.0
#
# DeepLabCut environment
#
# FIRST: If you have an NVIDIA GPU and want to use it, check that you have drivers installed!
# To check if your GPUs are visible to PyTorch (and thus DeepLabCut), run:
# >>> python -c "import torch; print(torch.cuda.is_available())"
#
# If "False" is printed, PyTorch (and thus DeepLabCut) cannot access your GPU. For
# more information, see: https://pytorch.org/get-started/locally/
#
# install: conda env create -f DEEPLABCUT.yaml
# update:  conda env update -f DEEPLABCUT.yaml
name: DEEPLABCUT
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.10
  - pip
  - ipython
  - jupyter
  - nb_conda
  - notebook<7.0.0
  - ffmpeg
  - pytables==3.8.0
  - pip:
    - torch
    - torchvision
    - "git+https://github.com/DeepLabCut/DeepLabCut.git@pytorch_dlc#egg=deeplabcut[gui,modelzoo,wandb]"


--- File: conda-environments/README.md ---
### Please head over to [Installation](/docs/installation.md) to see how to utilize our supplied conda envs!


--- File: examples/testscript_3d.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
DeepLabCut2.0 Toolbox (deeplabcut.org)
© A. & M. Mathis Labs
https://github.com/DeepLabCut/DeepLabCut
Please see AUTHORS for contributors.

https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
Licensed under GNU Lesser General Public License v3.0

This script tests various functionalities in an automatic way.
It produces nothing of interest scientifically.
"""
import os, deeplabcut
import zipfile, urllib.request, shutil
from datetime import datetime as dt
import glob
from pathlib import Path
import subprocess


if __name__ == "__main__":
    print("Imported DLC!")
    task = "TEST3D"  # Enter the name of your experiment Task
    scorer = "Alex"  # Enter the name of the experimenter/labeler
    num_cameras = 2  # Enter the number of cameras

    basepath = str(Path(os.path.realpath(__file__)).parents[0])
    videoname = "reachingvideo1"
    video = [
        os.path.join(
            basepath,
            "Reaching-Mackenzie-2018-08-30",
            "videos",
            videoname + ".avi",
        )
    ]

    folder = os.path.join(basepath, "3Dtestviews_videos")
    deeplabcut.auxiliaryfunctions.attempt_to_make_folder(folder)

    # copying demo video from reaching data set and create two "views":
    dst_videoname1 = "vid1_camera-1"
    dst_videoname2 = "vid1_camera-2"
    dst_videoname3 = "long_camera-2"
    output1 = os.path.join(folder, dst_videoname1 + ".avi")
    output2 = os.path.join(folder, dst_videoname2 + ".avi")
    output3 = os.path.join(folder, dst_videoname3 + ".avi")
    shutil.copyfile(video[0], output3)

    vname = "brief"
    try:  # you need ffmpeg command line interface
        subprocess.call(
            [
                "ffmpeg",
                "-i",
                video[0],
                "-ss",
                "00:00:00",
                "-to",
                "00:00:00.4",
                "-c",
                "copy",
                output1,
            ]
        )
        subprocess.call(
            [
                "ffmpeg",
                "-i",
                video[0],
                "-ss",
                "00:00:00",
                "-to",
                "00:00:00.4",
                "-c",
                "copy",
                output2,
            ]
        )
    except:
        pass

    """
    # copying demo video from reaching data set and create two "views":
    dst_videoname1 = 'vid1_camera-1'
    dst_videoname2 = 'vid1_camera-2'
    output1 = os.path.join(basepath,folder,dst_videoname1+'.avi')
    output2 = os.path.join(basepath,folder,dst_videoname2+'.avi')
    shutil.copyfile(video[0], output1)
    shutil.copyfile(video[0], output2)
    """
    # checking if 2d test project is available
    try:
        config = glob.glob(os.path.join(basepath, "TEST*", "config.yaml"))[-1]
    except:
        raise RuntimeError("Please run the testscript.py first before testing for 3d")

    dfolder = None

    print("CREATING 3-D PROJECT")
    path_config_file = deeplabcut.create_new_project_3d(task, scorer, num_cameras)

    try:
        cfg = deeplabcut.auxiliaryfunctions.read_config(path_config_file)
        cfg["config_file_camera-1"] = config
        cfg["shuffle_camera-1"] = 1

        cfg["config_file_camera-2"] = config
        cfg["shuffle_camera-2"] = 2

        cfg["skeleton"] = [["bodypart1", "bodypart2"], ["objectA", "bodypart3"]]
        deeplabcut.auxiliaryfunctions.write_config_3d(path_config_file, cfg)
    except:
        raise (
            "Please delete the project and re-try."
        )  # otherwise the cfg is an empty array!

    """
    # Creating the name of the project
    date = dt.today()
    month = date.strftime("%B")
    day = date.day
    d = str(month[0:3]+str(day))
    date = dt.today().strftime('%Y-%m-%d')
    project_name = '{pn}-{exp}-{date}-{triangulate}'.format(pn=task, exp=scorer, date=date,triangulate='3d')
    """
    project_name = path_config_file.split(os.sep)[-2]

    os.chdir(os.path.join(project_name, "calibration_images"))

    file_name = os.path.join(basepath,"stereo_example.zip")
    with zipfile.ZipFile(file_name) as zf:
        zf.extractall()

    # Deleting unnecessary images; the ones whose corners are not detected and .mat files
    cwd = os.getcwd()
    [os.remove(file) for file in os.listdir(cwd) if not file.endswith(".jpg")]

    # change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2
    cam1_images = glob.glob(os.path.join(cwd, "left*.jpg"))
    cam2_images = glob.glob(os.path.join(cwd, "right*.jpg"))
    # Sorting images
    cam1_images.sort(key=lambda f: int("".join(filter(str.isdigit, f))))
    cam2_images.sort(key=lambda f: int("".join(filter(str.isdigit, f))))
    for idx, name in enumerate(cam1_images):
        os.rename(
            name,
            os.path.join(cwd, str("camera-1_" + "{0:0=2d}".format(idx + 1) + ".jpg")),
        )

    for idx, name in enumerate(cam2_images):
        os.rename(
            name,
            os.path.join(cwd, str("camera-2_" + "{0:0=2d}".format(idx + 1) + ".jpg")),
        )

    # Removing some of the images where the corner was not detected
    [os.remove(file) for file in glob.glob(os.path.join(cwd, "*06.jpg"))]
    [os.remove(file) for file in glob.glob(os.path.join(cwd, "*01.jpg"))]

    print("CALIBRATING THE CAMERAS")
    deeplabcut.calibrate_cameras(path_config_file, calibrate=True)

    print("CHECKING FOR UNDISTORTION")
    deeplabcut.check_undistortion(path_config_file)

    print("TRIANGULATING")
    video_dir = os.path.join(os.path.dirname(basepath), folder)
    deeplabcut.auxiliaryfunctions.edit_config(
        path_config_file, edits={"pcutoff": 0.1}
    )  # otherwise get all-nan slices
    deeplabcut.triangulate(path_config_file, video_dir, save_as_csv=True)

    print("CREATING LABELED VIDEO 3-D")
    deeplabcut.create_labeled_video_3d(path_config_file, [video_dir], start=5, end=10, videotype=".avi")

    # output_path = [os.path.join(basepath,folder)]
    # deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)

    print("ALL DONE!!! - default 3D cases are functional.")


--- File: examples/testscript_mobilenets.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Created on Tue Oct  2 13:56:11 2018
@author: alex

DEVELOPERS:
This script tests various functionalities (creating project ,training, evaluating, outlierextraction, retraining...) in an automatic way.
For that purpose, it trains ResNet and MobileNet briefly on a "fake" dataset.

It should take about 4:15 minutes to run this in a CPU. (incl. downloading the ResNet + MobileNet weights)

It produces nothing of interest scientifically.
"""
import os

os.environ["DLClight"] = "True"
import deeplabcut
from pathlib import Path
import pandas as pd
import numpy as np


def Cuttrainingschedule(
    path_config_file, shuffle, trainingsetindex=0, initweights="imagenet", lastvalue=10
):
    cfg = deeplabcut.auxiliaryfunctions.read_config(path_config_file)
    posefile = os.path.join(
        cfg["project_path"],
        "dlc-models/iteration-"
        + str(cfg["iteration"])
        + "/"
        + cfg["Task"]
        + cfg["date"]
        + "-trainset"
        + str(int(cfg["TrainingFraction"][trainingsetindex] * 100))
        + "shuffle"
        + str(shuffle),
        "train/pose_cfg.yaml",
    )

    edits = {
        "save_iters": lastvalue,
        "display_iters": 1,
        "multi_step": [[0.001, lastvalue]],
        "intermediate_supervision": False,
    }

    if initweights == "previteration":
        edits["init_weights"] = os.path.join(
            cfg["project_path"],
            "dlc-models/iteration-"
            + str(cfg["iteration"] - 1)
            + "/"
            + cfg["Task"]
            + cfg["date"]
            + "-trainset"
            + str(int(cfg["TrainingFraction"][trainingsetindex] * 100))
            + "shuffle"
            + str(shuffle),
            "train/snapshot-" + str(lastvalue),
        )

    print("CHANGING training parameters to end quickly!")
    DLC_config = deeplabcut.auxiliaryfunctions.edit_config(posefile, edits)
    return


if __name__ == "__main__":
    task = "TEST-multipleNets"  # Enter the name of your experiment Task
    scorer = "Alex"  # Enter the name of the experimenter/labeler
    print("Imported DLC!")
    basepath = os.path.dirname(os.path.realpath(__file__))
    videoname = "reachingvideo1"
    video = [
        os.path.join(
            basepath, "Reaching-Mackenzie-2018-08-30", "videos", videoname + ".avi"
        )
    ]

    # to test destination folder:
    dfolder = os.path.join(basepath, "OUT")
    deeplabcut.auxiliaryfunctions.attempt_to_make_folder(dfolder)

    # dfolder=None
    augmenter_type = "tensorpack"  # imgaug'

    print("CREATING PROJECT")
    path_config_file = deeplabcut.create_new_project(
        task, scorer, video, copy_videos=True
    )

    cfg = deeplabcut.auxiliaryfunctions.read_config(path_config_file)
    cfg["numframes2pick"] = 5
    cfg["pcutoff"] = 0.01
    cfg["TrainingFraction"] = [0.8]
    cfg["skeleton"] = [["bodypart1", "bodypart2"], ["bodypart1", "bodypart3"]]

    deeplabcut.auxiliaryfunctions.write_config(path_config_file, cfg)

    print("EXTRACTING FRAMES")
    deeplabcut.extract_frames(path_config_file, mode="automatic", userfeedback=False)

    print("CREATING-SOME LABELS FOR THE FRAMES")
    frames = os.listdir(os.path.join(cfg["project_path"], "labeled-data", videoname))
    # As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)
    for index, bodypart in enumerate(cfg["bodyparts"]):
        columnindex = pd.MultiIndex.from_product(
            [[scorer], [bodypart], ["x", "y"]], names=["scorer", "bodyparts", "coords"]
        )
        frame = pd.DataFrame(
            100 + np.ones((len(frames), 2)) * 50 * index,
            columns=columnindex,
            index=[os.path.join("labeled-data", videoname, fn) for fn in frames],
        )
        if index == 0:
            dataFrame = frame
        else:
            dataFrame = pd.concat([dataFrame, frame], axis=1)

    dataFrame.to_csv(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            videoname,
            "CollectedData_" + scorer + ".csv",
        )
    )
    dataFrame.to_hdf(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            videoname,
            "CollectedData_" + scorer + ".h5",
        ),
        "df_with_missing",
        format="table",
        mode="w",
    )

    stoptrain = 5  # 0
    keepdeconvweights = True

    print("Plot labels...")
    deeplabcut.check_labels(path_config_file)
    for shuffle, net_type in enumerate(
        ["mobilenet_v2_0.35", "resnet_50"]
    ):  #'mobilenet_v2_1.0']): # 'resnet_50']):
        """
        if shuffle==0:
            keepdeconvweights=True
        else:
            keepdeconvweights=False
        """
        print("CREATING TRAININGSET", net_type)
        if "resnet_50" == net_type:  # this tests the default condition...
            deeplabcut.create_training_dataset(
                path_config_file, Shuffles=[shuffle], augmenter_type=augmenter_type
            )
        else:
            deeplabcut.create_training_dataset(
                path_config_file,
                Shuffles=[shuffle],
                net_type=net_type,
                augmenter_type=augmenter_type,
            )
        Cuttrainingschedule(path_config_file, shuffle, lastvalue=stoptrain)

        print("TRAIN")
        deeplabcut.train_network(path_config_file, shuffle=shuffle)

        print("EVALUATE")
        deeplabcut.evaluate_network(path_config_file, Shuffles=[shuffle], plotting=True)

        print("CREATE A SHORT VIDEO AND ANALYZE")
        if shuffle == 0:
            # Make super short video (so the analysis is quick!)
            newvideo = deeplabcut.ShortenVideo(
                video[0],
                start="00:00:00",
                stop="00:00:01",
                outsuffix="short",
                outpath=os.path.join(cfg["project_path"], "videos"),
            )
            vname = Path(newvideo).stem

        deeplabcut.analyze_videos(
            path_config_file,
            [newvideo],
            shuffle=shuffle,
            save_as_csv=True,
            destfolder=dfolder,
            videotype="avi",
        )

        print("CREATE VIDEO")
        deeplabcut.create_labeled_video(
            path_config_file,
            [newvideo],
            shuffle=shuffle,
            destfolder=dfolder,
            videotype="avi",
        )

        print("Making plots")
        deeplabcut.plot_trajectories(
            path_config_file,
            [newvideo],
            shuffle=shuffle,
            destfolder=dfolder,
            videotype="avi",
        )

        print("EXTRACT OUTLIERS")
        deeplabcut.extract_outlier_frames(
            path_config_file,
            [newvideo],
            shuffle=shuffle,
            outlieralgorithm="jump",
            epsilon=0,
            automatic=True,
            destfolder=dfolder,
            videotype="avi",
        )
        file = os.path.join(
            cfg["project_path"],
            "labeled-data",
            vname,
            "machinelabels-iter" + str(cfg["iteration"]) + ".h5",
        )

        print("RELABELING")
        DF = pd.read_hdf(file, "df_with_missing")
        DLCscorer = np.unique(DF.columns.get_level_values(0))[0]
        DF.columns.set_levels(
            [scorer.replace(DLCscorer, scorer)], level=0, inplace=True
        )
        DF = DF.drop("likelihood", axis=1, level=2)
        DF.to_csv(
            os.path.join(
                cfg["project_path"],
                "labeled-data",
                vname,
                "CollectedData_" + scorer + ".csv",
            )
        )
        DF.to_hdf(
            os.path.join(
                cfg["project_path"],
                "labeled-data",
                vname,
                "CollectedData_" + scorer + ".h5",
            ),
            "df_with_missing",
            format="table",
            mode="w",
        )

        print("MERGING")
        deeplabcut.merge_datasets(path_config_file)

        print("CREATING TRAININGSET")
        deeplabcut.create_training_dataset(
            path_config_file, Shuffles=[shuffle], net_type=net_type
        )
        Cuttrainingschedule(
            path_config_file, shuffle, lastvalue=stoptrain, initweights="previteration"
        )

        print("TRAINING from previous snapshot!!!!!")
        deeplabcut.train_network(
            path_config_file, shuffle=shuffle, keepdeconvweights=keepdeconvweights
        )

        print("ANALYZING some individual frames")
        deeplabcut.analyze_time_lapse_frames(
            path_config_file,
            os.path.join(cfg["project_path"], "labeled-data/reachingvideo1/"),
            shuffle=shuffle,
        )

    print("ALL DONE!!! - Mobilnets and ResNets are good!")


--- File: examples/testscript_transreid.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import deeplabcut
import numpy as np
import pandas as pd
import pickle
from deeplabcut.utils import auxfun_multianimal, auxiliaryfunctions
import random
from pathlib import Path

# MODELS = ["dlcrnet_ms5", "dlcr101_ms5", "efficientnet-b0", "mobilenet_v2_0.35"]
MODELS = [
    "dlcrnet_ms5",
]  # "efficientnet-b0", "mobilenet_v2_0.35"]

N_ITER = 5
TESTTRACKER = "ellipse"

USE_SHELVE = False  # random.choice([True, False])

if __name__ == "__main__":
    TASK = "multi_mouse"
    SCORER = "dlc_team"
    NUM_FRAMES = 5
    TRAIN_SIZE = 0.8

    # NET = "dlcr101_ms5"
    NET = "dlcrnet_ms5"

    # Always test a different model from list above
    NET = random.choice(MODELS)

    basepath = os.path.dirname(os.path.realpath(__file__))
    DESTFOLDER = basepath

    video = "m3v1mp4"
    video_path = os.path.join(
        basepath, "openfield-Pranav-2018-10-30", "videos", video + ".mp4"
    )

    print("Creating project...")
    config_path = deeplabcut.create_new_project(
        TASK, SCORER, [video_path], copy_videos=True, multianimal=True
    )

    print("Project created.")

    print("Editing config...")
    cfg = auxiliaryfunctions.edit_config(
        config_path,
        {
            "numframes2pick": NUM_FRAMES,
            "TrainingFraction": [TRAIN_SIZE],
            "identity": True,
            "uniquebodyparts": ["corner1", "corner2"],
        },
    )
    print("Config edited.")

    print("Extracting frames...")
    deeplabcut.extract_frames(config_path, mode="automatic", userfeedback=False)
    print("Frames extracted.")

    print("Creating artificial data...")
    rel_folder = os.path.join("labeled-data", os.path.splitext(video)[0])
    image_folder = os.path.join(cfg["project_path"], rel_folder)
    n_animals = len(cfg["individuals"])
    (
        animals,
        bodyparts_single,
        bodyparts_multi,
    ) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
    animals_id = [i for i in range(n_animals) for _ in bodyparts_multi] + [
        n_animals
    ] * len(bodyparts_single)
    map_ = dict(zip(range(len(animals)), animals))
    individuals = [map_[ind] for ind in animals_id for _ in range(2)]
    scorer = [SCORER] * len(individuals)
    coords = ["x", "y"] * len(animals_id)
    bodyparts = [
        bp for _ in range(n_animals) for bp in bodyparts_multi for _ in range(2)
    ]
    bodyparts += [bp for bp in bodyparts_single for _ in range(2)]
    columns = pd.MultiIndex.from_arrays(
        [scorer, individuals, bodyparts, coords],
        names=["scorer", "individuals", "bodyparts", "coords"],
    )
    index = [
        os.path.join(rel_folder, image)
        for image in auxiliaryfunctions.grab_files_in_folder(image_folder, "png")
    ]
    fake_data = np.tile(
        np.repeat(50 * np.arange(len(animals_id)) + 50, 2), (len(index), 1)
    )
    df = pd.DataFrame(fake_data, index=index, columns=columns)
    output_path = os.path.join(image_folder, f"CollectedData_{SCORER}.csv")
    df.to_csv(output_path)
    df.to_hdf(
        output_path.replace("csv", "h5"), "df_with_missing", format="table", mode="w"
    )
    print("Artificial data created.")

    print("Checking labels...")
    deeplabcut.check_labels(config_path, draw_skeleton=False)
    print("Labels checked.")

    print("Creating train dataset...")
    deeplabcut.create_multianimaltraining_dataset(
        config_path, net_type=NET, crop_size=(200, 200)
    )
    print("Train dataset created.")

    # Check the training image paths are correctly stored as arrays of strings
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    datafile, _ = auxiliaryfunctions.get_data_and_metadata_filenames(
        trainingsetfolder,
        0.8,
        1,
        cfg,
    )
    datafile = datafile.split(".mat")[0] + ".pickle"
    with open(os.path.join(cfg["project_path"], datafile), "rb") as f:
        pickledata = pickle.load(f)
    num_images = len(pickledata)
    assert all(len(pickledata[i]["joints"]) == 3 for i in range(num_images))

    print("Editing pose config...")
    model_folder = auxiliaryfunctions.get_model_folder(
        TRAIN_SIZE, 1, cfg, cfg["project_path"]
    )
    pose_config_path = os.path.join(model_folder, "train", "pose_cfg.yaml")
    edits = {
        "global_scale": 0.5,
        "batch_size": 1,
        "save_iters": N_ITER,
        "display_iters": N_ITER // 2,
        "crop_size": [200, 200],
        # "multi_step": [[0.001, N_ITER]],
    }
    deeplabcut.auxiliaryfunctions.edit_config(pose_config_path, edits)
    print("Pose config edited.")

    print("Training network...")
    deeplabcut.train_network(config_path, maxiters=N_ITER)
    print("Network trained.")

    print("Evaluating network...")
    deeplabcut.evaluate_network(config_path, plotting=True)

    print("Network evaluated....")

    print("Extracting maps...")
    deeplabcut.extract_save_all_maps(config_path, Indices=[0, 1, 2])

    new_video_path = deeplabcut.ShortenVideo(
        video_path,
        start="00:00:00",
        stop="00:00:01",
        outsuffix="short",
        outpath=os.path.join(cfg["project_path"], "videos"),
    )

    print("Analyzing video...")
    deeplabcut.analyze_videos(
        config_path,
        [new_video_path],
        "mp4",
        robust_nframes=True,
        allow_growth=True,
        use_shelve=USE_SHELVE,
    )

    print("Video analyzed.")

    print("Create video with all detections...")
    scorer, _ = auxiliaryfunctions.get_scorer_name(cfg, 1, TRAIN_SIZE)

    deeplabcut.create_video_with_all_detections(
        config_path, [new_video_path], shuffle=1, displayedbodyparts=["bodypart1"]
    )

    print("Video created.")

    print("Convert detections to tracklets...")
    deeplabcut.convert_detections2tracklets(
        config_path, [new_video_path], "mp4", track_method=TESTTRACKER
    )
    print("Tracklets created...")

    ### adding it here
    modelprefix = ""
    (
        trainposeconfigfile,
        testposeconfigfile,
        snapshotfolder,
    ) = deeplabcut.return_train_network_path(
        config_path, shuffle=1, modelprefix=modelprefix, trainingsetindex=0
    )

    print("Creating triplet dataset")

    deeplabcut.pose_estimation_tensorflow.create_tracking_dataset(
        config_path,
        [new_video_path],
        TESTTRACKER,
        videotype="mp4",
    )

    train_epochs = 10
    train_frac = 0.8

    print("Training transformer")

    deeplabcut.pose_tracking_pytorch.train_tracking_transformer(
        config_path,
        scorer,
        [new_video_path],
        train_frac=train_frac,
        modelprefix=modelprefix,
        train_epochs=train_epochs,
        ckpt_folder=snapshotfolder,
    )

    transformer_checkpoint = os.path.join(
        snapshotfolder, f"dlc_transreid_{train_epochs}.pth"
    )

    print("Stitching tracklets based on transformer")

    deeplabcut.stitch_tracklets(
        config_path,
        [new_video_path],
        "mp4",
        track_method=TESTTRACKER,
        transformer_checkpoint=transformer_checkpoint,
    )

    print("Plotting trajectories...")
    deeplabcut.plot_trajectories(
        config_path, [new_video_path], "mp4", track_method=TESTTRACKER
    )
    print("Trajectory plotted.")

    print("Creating labeled video...")
    deeplabcut.create_labeled_video(
        config_path,
        [new_video_path],
        "mp4",
        save_frames=False,
        color_by="individual",
        track_method="transformer",
    )
    print("Labeled video created.")

    print("Filtering predictions...")
    deeplabcut.filterpredictions(
        config_path, [new_video_path], "mp4", track_method=TESTTRACKER
    )
    print("Predictions filtered.")

    print("Extracting outlier frames...")
    deeplabcut.extract_outlier_frames(
        config_path, [new_video_path], "mp4", automatic=True, track_method=TESTTRACKER
    )
    print("Outlier frames extracted.")

    vname = Path(new_video_path).stem

    file = os.path.join(
        cfg["project_path"],
        "labeled-data",
        vname,
        "machinelabels-iter" + str(cfg["iteration"]) + ".h5",
    )

    """
    print("RELABELING")
    DF = pd.read_hdf(file, "df_with_missing")
    DLCscorer = np.unique(DF.columns.get_level_values(0))[0]
    DF.columns.set_levels([scorer.replace(DLCscorer, scorer)], level=0, inplace=True)
    DF = DF.drop("likelihood", axis=1, level=3)
    DF.to_csv(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            vname,
            "CollectedData_" + scorer + ".csv",
        )
    )
    DF.to_hdf(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            vname,
            "CollectedData_" + scorer + ".h5",
        ),
        "df_with_missing",
        format="table",
        mode="w",
    )
    """

    print("MERGING")
    deeplabcut.merge_datasets(config_path)  # iteration + 1

    print("CREATING TRAININGSET UPDATED TRAINING SET")
    deeplabcut.create_training_dataset(config_path, Shuffles=[3], net_type=NET)

    print("TRAINING NETWORK...")
    deeplabcut.train_network(config_path, shuffle=3, maxiters=N_ITER)
    print("NETWORK TRAINED!")

    print("EVALUATING NETWORK...")
    deeplabcut.evaluate_network(config_path, Shuffles=[3], plotting=True)

    print("NETWORK EVALUATED....")

    print("ANALYZING VIDEO WITH AUTO_TRACK....")
    deeplabcut.analyze_videos(
        config_path,
        [new_video_path],
        shuffle=3,
        videotype="mp4",
        save_as_csv=True,
        destfolder=DESTFOLDER,
        cropping=[0, 50, 0, 50],
        allow_growth=True,
        use_shelve=USE_SHELVE,
        auto_track=True,
    )

    n_tracks = 3

    print("TESTING THE UNIFIED API FOR TRANSFORMER")

    deeplabcut.transformer_reID(
        config_path,
        [new_video_path],
        videotype="mp4",
        shuffle=3,
        n_tracks=n_tracks,
        track_method=TESTTRACKER,
        train_epochs=10,
        n_triplets=10,
        destfolder=DESTFOLDER,
    )

    print("CREATING LABELED VIDEOS (FOR ELLIPSE AND TRANSFORMER)...")

    deeplabcut.create_labeled_video(
        config_path,
        [new_video_path],
        videotype="mp4",
        shuffle=3,
        track_method="ellipse",
        destfolder=DESTFOLDER,
    )

    deeplabcut.create_labeled_video(
        config_path,
        [new_video_path],
        videotype="mp4",
        shuffle=3,
        track_method="transformer",
        destfolder=DESTFOLDER,
    )

    print("ALL DONE!!! - default multianimal cases are functional.")


--- File: examples/testscript_superanimal_inference.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Testscript for super animal inference

"""
import deeplabcut
import os


if __name__ == "__main__":
    basepath = os.path.dirname(os.path.realpath(__file__))
    videoname = "reachingvideo1"
    video = [
        os.path.join(
            basepath, "Reaching-Mackenzie-2018-08-30", "videos", videoname + ".avi"
        )
    ]

    print("testing superanimal_topviewmouse")
    superanimal_name = "superanimal_topviewmouse"
    scale_list = [200, 300, 400]
    deeplabcut.video_inference_superanimal(
        video,
        superanimal_name,
        model_name="hrnet_w32",
        detector_name="fasterrcnn_resnet50_fpn_v2",
        videotype=".avi",
        scale_list=scale_list,
    )

    print("testing superanimal_quadruped")
    superanimal_name = "superanimal_quadruped"
    deeplabcut.video_inference_superanimal(
        video,
        superanimal_name,
        model_name="hrnet_w32",
        detector_name="fasterrcnn_resnet50_fpn_v2",
        videotype=".avi",
        scale_list=scale_list,
    )


--- File: examples/testscript_openfielddata_augmentationcomparison.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""

This is a test script to compare the loaders and models. 

This script creates one identical splits for the openfield test dataset and trains it with imgaug (default), scalecrop
and the tensorpack loader. We also compare 3 backbones (mobilenet, resnet, efficientnet)

My results were (Run with DLC *2.2.0.4* in Jan 6 2022) for 50 k iterations

DLC_mobnet_35_openfieldOct30shuffle0_50000 and Imgaug with # of training iterations: 50000
Results for 50000 training iterations: 95 0 train error: 3.06 pixels. Test error: 3.44 pixels.
With pcutoff of 0.4 train error: 3.06 pixels. Test error: 3.44 pixel

DLC_mobnet_35_openfieldOct30shuffle1_50000 and scalecrop with # of training iterations: 50000
Results for 50000 training iterations: 95 1 train error: 2.44 pixels. Test error: 3.84 pixels.
With pcutoff of 0.4 train error: 2.44 pixels. Test error: 3.84 pixels

DLC_mobnet_35_openfieldOct30shuffle2_50000 and tensorpack with # of training iterations: 50000
Results for 50000 training iterations: 95 2 train error: 2.41 pixels. Test error: 3.04 pixels.
With pcutoff of 0.4 train error: 2.41 pixels. Test error: 3.04 pixels

DLC_resnet50_openfieldOct30shuffle3_50000 and Imgaug with # of training iterations: 50000
Results for 50000 training iterations: 95 3 train error: 2.69 pixels. Test error: 2.97 pixels.
With pcutoff of 0.4 train error: 2.69 pixels. Test error: 2.97 pixels

DLC_resnet50_openfieldOct30shuffle4_50000 and scalecrop with # of training iterations: 50000
Results for 50000 training iterations: 95 4 train error: 2.0 pixels. Test error: 2.69 pixels.
With pcutoff of 0.4 train error: 2.0 pixels. Test error: 2.69 pixels

DLC_resnet50_openfieldOct30shuffle5_50000 and tensorpack with # of training iterations: 50000
Results for 50000 training iterations: 95 5 train error: 1.96 pixels. Test error: 2.65 pixels.
With pcutoff of 0.4 train error: 1.96 pixels. Test error: 2.65 pixels

DLC_effnet_b3_openfieldOct30shuffle6_50000 with Imgaug with # of training iterations: 50000
Results for 50000 training iterations: 95 6 train error: 2.63 pixels. Test error: 2.65 pixels.
With pcutoff of 0.4 train error: 2.63 pixels. Test error: 2.65 pixels

effnet with tensorpack and scalecrop didn't converge.


Notice: despite the higher RMSE for imgaug due to the augmentation,
the network performs much better on the testvideo (see Neuron Primer: https://www.cell.com/neuron/pdf/S0896-6273(20)30717-0.pdf)

"""


import os

os.environ["CUDA_VISIBLE_DEVICES"] = str(0)
import deeplabcut
import numpy as np

# Loading example data set
path_config_file = os.path.join(os.getcwd(), "openfield-Pranav-2018-10-30/config.yaml")
cfg = deeplabcut.auxiliaryfunctions.read_config(path_config_file)

maxiters = 50000
saveiters = 10000
displayiters = 500

deeplabcut.load_demo_data(path_config_file, createtrainingset=False)
## Create one identical splits for 3 networks and 3 augmentations

###Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:
Shuffles = deeplabcut.create_training_model_comparison(
    path_config_file,
    num_shuffles=1,
    net_types=["mobilenet_v2_0.35", "resnet_50", "efficientnet-b3"],
    augmenter_types=["imgaug", "scalecrop", "tensorpack"],
)

for idx, shuffle in enumerate(Shuffles):
    posefile, _, _ = deeplabcut.return_train_network_path(
        path_config_file, shuffle=shuffle
    )

    # Setting specific parameters for training
    if idx % 3 == 0:  # imgaug
        edits = {"rotation": 180, "motion_blur": True}
        DLC_config = deeplabcut.auxiliaryfunctions.edit_config(posefile, edits)
    elif idx % 3 == 2:  # Tensorpack
        edits = {"rotation": 180, "noise_sigma": 0.01}
        DLC_config = deeplabcut.auxiliaryfunctions.edit_config(posefile, edits)

    if idx > 5:  # EfficientNet
        print(posefile, "changing now!!")
        edits = {
            "decay_steps": maxiters,
            "lr_init": 0.0005,
        }
        DLC_config = deeplabcut.auxiliaryfunctions.edit_config(posefile, edits)

for shuffle in Shuffles:
    print("TRAIN NETWORK", shuffle)
    deeplabcut.train_network(
        path_config_file,
        shuffle=shuffle,
        saveiters=saveiters,
        displayiters=displayiters,
        maxiters=maxiters,
        max_snapshots_to_keep=11,
    )

    print("Analyze Video")

    videofile_path = os.path.join(
        os.getcwd(), "openfield-Pranav-2018-10-30", "videos", "m3v1mp4.mp4"
    )

    deeplabcut.analyze_videos(path_config_file, [videofile_path], shuffle=shuffle)

    print("Create Labeled Video and plot")
    deeplabcut.create_labeled_video(path_config_file, [videofile_path], shuffle=shuffle)
    deeplabcut.plot_trajectories(path_config_file, [videofile_path], shuffle=shuffle)

print("EVALUATE")
deeplabcut.evaluate_network(path_config_file, Shuffles=Shuffles, plotting=False)


--- File: examples/testscript.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Created on Tue Oct  2 13:56:11 2018
@author: alex

DEVELOPERS:
This script tests various functionalities in an automatic way.

It should take about 3:30 minutes to run this in a CPU.
It should take about 1:30 minutes on a GPU (incl. downloading the ResNet weights)

It produces nothing of interest scientifically.
"""
import os
import platform
import random
from pathlib import Path

import numpy as np
import pandas as pd
import scipy.io as sio

import deeplabcut
from deeplabcut.core.engine import Engine
from deeplabcut.utils import auxiliaryfunctions

USE_SHELVE = random.choice([True, False])
MODELS = ["resnet_50", "efficientnet-b0", "mobilenet_v2_0.35"]


if __name__ == "__main__":
    task = "TEST"  # Enter the name of your experiment Task
    scorer = "Alex"  # Enter the name of the experimenter/labeler
    engine = Engine.TF

    print("Imported DLC!")
    basepath = os.path.dirname(os.path.realpath(__file__))
    videoname = "reachingvideo1"
    video = [
        os.path.join(
            basepath, "Reaching-Mackenzie-2018-08-30", "videos", videoname + ".avi"
        )
    ]

    # For testing a color video:
    # videoname='baby4hin2min'
    # video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]
    # to test destination folder:
    DESTFOLDER = basepath

    DESTFOLDER = None
    NET = random.choice(MODELS)

    augmenter_type = "default"  # = imgaug!!
    augmenter_type2 = "scalecrop"

    if platform.system() == "Darwin" or platform.system() == "Windows":
        print("On Windows/OSX tensorpack is not tested by default.")
        augmenter_type3 = "imgaug"
    else:
        augmenter_type3 = "tensorpack"  # Does not work on WINDOWS

    N_ITER = 6
    SAVE_ITER = 3

    print("CREATING PROJECT")
    path_config_file = deeplabcut.create_new_project(
        task, scorer, video, copy_videos=True
    )

    cfg = deeplabcut.auxiliaryfunctions.read_config(path_config_file)
    cfg["numframes2pick"] = 5
    cfg["pcutoff"] = 0.01
    cfg["TrainingFraction"] = [0.8]
    cfg["skeleton"] = [["bodypart1", "bodypart2"], ["bodypart1", "bodypart3"]]

    deeplabcut.auxiliaryfunctions.write_config(path_config_file, cfg)

    print("EXTRACTING FRAMES")
    deeplabcut.extract_frames(path_config_file, mode="automatic", userfeedback=False)

    print("CREATING-SOME LABELS FOR THE FRAMES")
    frames = os.listdir(os.path.join(cfg["project_path"], "labeled-data", videoname))
    frames = [fn for fn in frames if fn.endswith(".png")]
    # As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)
    for index, bodypart in enumerate(cfg["bodyparts"]):
        columnindex = pd.MultiIndex.from_product(
            [[scorer], [bodypart], ["x", "y"]], names=["scorer", "bodyparts", "coords"]
        )
        frame = pd.DataFrame(
            100 + np.ones((len(frames), 2)) * 50 * index,
            columns=columnindex,
            index=[os.path.join("labeled-data", videoname, fn) for fn in frames],
        )
        if index == 0:
            dataFrame = frame
        else:
            dataFrame = pd.concat([dataFrame, frame], axis=1)

    dataFrame.to_csv(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            videoname,
            "CollectedData_" + scorer + ".csv",
        )
    )

    dataFrame.to_hdf(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            videoname,
            "CollectedData_" + scorer + ".h5",
        ),
        "df_with_missing",
        format="table",
        mode="w",
    )

    print("Plot labels...")

    deeplabcut.check_labels(path_config_file)

    print("CREATING TRAININGSET")
    deeplabcut.create_training_dataset(
        path_config_file, net_type=NET, augmenter_type=augmenter_type, engine=engine,
    )

    # Check the training image paths are correctly stored as arrays of strings
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    datafile, _ = auxiliaryfunctions.get_data_and_metadata_filenames(
        trainingsetfolder,
        0.8,
        1,
        cfg,
    )
    mlab = sio.loadmat(os.path.join(cfg["project_path"], datafile))["dataset"]
    num_images = mlab.shape[1]
    for i in range(num_images):
        imgpath = mlab[0, i][0][0]
        assert len(imgpath) == 3
        assert imgpath.dtype.char == "U"

    posefile = os.path.join(
        cfg["project_path"],
        "dlc-models/iteration-"
        + str(cfg["iteration"])
        + "/"
        + cfg["Task"]
        + cfg["date"]
        + "-trainset"
        + str(int(cfg["TrainingFraction"][0] * 100))
        + "shuffle"
        + str(1),
        "train/pose_cfg.yaml",
    )

    DLC_config = deeplabcut.auxiliaryfunctions.read_plainconfig(posefile)
    DLC_config["save_iters"] = SAVE_ITER
    DLC_config["display_iters"] = 2

    print("CHANGING training parameters to end quickly!")
    deeplabcut.auxiliaryfunctions.write_plainconfig(posefile, DLC_config)

    print("TRAIN")
    deeplabcut.train_network(path_config_file, maxiters=N_ITER)

    print("EVALUATE")
    deeplabcut.evaluate_network(
        path_config_file,
        plotting=True,
        per_keypoint_evaluation=True,
        snapshots_to_evaluate=[
            "snapshot-3",
            "snapshot-5",
            "snapshot-6",
        ],  # snapshot-5 intentionally missing :)
    )
    # deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)
    print("CUT SHORT VIDEO AND ANALYZE (with dynamic cropping!)")

    # Make super short video (so the analysis is quick!)

    try:  # you need ffmpeg command line interface
        # subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])
        newvideo = deeplabcut.ShortenVideo(
            video[0],
            start="00:00:00",
            stop="00:00:01",
            outsuffix="short",
            outpath=os.path.join(cfg["project_path"], "videos"),
        )
    except:  # if ffmpeg is broken/missing
        print("using alternative method")
        newvideo = os.path.join(cfg["project_path"], "videos", videoname + "short.mp4")
        from moviepy.editor import VideoClip, VideoFileClip

        clip = VideoFileClip(video[0])
        clip.reader.initialize()

        def make_frame(t):
            return clip.get_frame(1)

        newclip = VideoClip(make_frame, duration=1)
        newclip.write_videofile(newvideo, fps=30)

    vname = Path(newvideo).stem

    deeplabcut.analyze_videos(
        path_config_file,
        [newvideo],
        save_as_csv=True,
        destfolder=DESTFOLDER,
        dynamic=(True, 0.1, 5),
    )

    print("analyze again...")
    deeplabcut.analyze_videos(
        path_config_file, [newvideo], save_as_csv=True, destfolder=DESTFOLDER
    )

    print("CREATE VIDEO")
    successful = deeplabcut.create_labeled_video(
        path_config_file, [newvideo], destfolder=DESTFOLDER, save_frames=True
    )
    assert all(successful), f"Failed to create a labeled video!"

    print("Making plots")
    deeplabcut.plot_trajectories(path_config_file, [newvideo], destfolder=DESTFOLDER)

    print("EXTRACT OUTLIERS")
    deeplabcut.extract_outlier_frames(
        path_config_file,
        [newvideo],
        outlieralgorithm="jump",
        epsilon=0,
        automatic=True,
        destfolder=DESTFOLDER,
    )

    deeplabcut.extract_outlier_frames(
        path_config_file,
        [newvideo],
        outlieralgorithm="fitting",
        automatic=True,
        destfolder=DESTFOLDER,
    )

    file = os.path.join(
        cfg["project_path"],
        "labeled-data",
        vname,
        "machinelabels-iter" + str(cfg["iteration"]) + ".h5",
    )

    print("RELABELING")
    DF = pd.read_hdf(file, "df_with_missing")
    DLCscorer = np.unique(DF.columns.get_level_values(0))[0]
    DF.columns = DF.columns.set_levels([scorer.replace(DLCscorer, scorer)], level=0)
    DF = DF.drop("likelihood", axis=1, level=2)
    DF.to_csv(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            vname,
            "CollectedData_" + scorer + ".csv",
        )
    )
    DF.to_hdf(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            vname,
            "CollectedData_" + scorer + ".h5",
        ),
        "df_with_missing",
    )

    print("MERGING")
    deeplabcut.merge_datasets(path_config_file)  # iteration + 1

    print("CREATING TRAININGSET")
    deeplabcut.create_training_dataset(
        path_config_file, net_type=NET, augmenter_type=augmenter_type2, engine=engine
    )

    cfg = deeplabcut.auxiliaryfunctions.read_config(path_config_file)
    posefile = os.path.join(
        cfg["project_path"],
        "dlc-models/iteration-"
        + str(cfg["iteration"])
        + "/"
        + cfg["Task"]
        + cfg["date"]
        + "-trainset"
        + str(int(cfg["TrainingFraction"][0] * 100))
        + "shuffle"
        + str(1),
        "train/pose_cfg.yaml",
    )
    DLC_config = deeplabcut.auxiliaryfunctions.read_plainconfig(posefile)
    DLC_config["save_iters"] = SAVE_ITER
    DLC_config["display_iters"] = 1

    print("CHANGING training parameters to end quickly!")
    deeplabcut.auxiliaryfunctions.write_config(posefile, DLC_config)

    print("TRAIN")
    deeplabcut.train_network(path_config_file, maxiters=N_ITER)

    try:  # you need ffmpeg command line interface
        # subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])
        newvideo2 = deeplabcut.ShortenVideo(
            video[0],
            start="00:00:00",
            stop="00:00:01",
            outsuffix="short2",
            outpath=os.path.join(cfg["project_path"], "videos"),
        )

    except:  # if ffmpeg is broken
        newvideo2 = os.path.join(
            cfg["project_path"], "videos", videoname + "short2.mp4"
        )
        from moviepy.editor import VideoClip, VideoFileClip

        clip = VideoFileClip(video[0])
        clip.reader.initialize()

        def make_frame(t):
            return clip.get_frame(1)

        newclip = VideoClip(make_frame, duration=1)
        newclip.write_videofile(newvideo2, fps=30)

    vname = Path(newvideo2).stem

    print("Inference with direct cropping")
    deeplabcut.analyze_videos(
        path_config_file,
        [newvideo2],
        save_as_csv=True,
        destfolder=DESTFOLDER,
        cropping=[0, 50, 0, 50],
        allow_growth=True,
        use_shelve=USE_SHELVE,
    )

    print("Extracting skeleton distances, filter and plot filtered output")
    deeplabcut.analyzeskeleton(
        path_config_file, [newvideo2], save_as_csv=True, destfolder=DESTFOLDER
    )
    deeplabcut.filterpredictions(path_config_file, [newvideo2])

    successful = deeplabcut.create_labeled_video(
        path_config_file,
        [newvideo2],
        destfolder=DESTFOLDER,
        displaycropped=True,
        filtered=True,
    )
    assert all(successful), f"Failed to create a labeled video!"

    print("Creating a Johansson video!")
    successful = deeplabcut.create_labeled_video(
        path_config_file, [newvideo2], destfolder=DESTFOLDER, keypoints_only=True
    )
    assert all(successful), f"Failed to create a labeled video!"

    deeplabcut.plot_trajectories(
        path_config_file, [newvideo2], destfolder=DESTFOLDER, filtered=True
    )

    print("ALL DONE!!! - default cases without Tensorpack loader are functional.")

    print("CREATING TRAININGSET for shuffle 2")
    print("will be used for 3D testscript...")
    # TENSORPACK could fail in WINDOWS...
    deeplabcut.create_training_dataset(
        path_config_file,
        Shuffles=[2],
        net_type=NET,
        augmenter_type=augmenter_type3,
        engine=engine,
    )

    posefile = os.path.join(
        cfg["project_path"],
        "dlc-models/iteration-"
        + str(cfg["iteration"])
        + "/"
        + cfg["Task"]
        + cfg["date"]
        + "-trainset"
        + str(int(cfg["TrainingFraction"][0] * 100))
        + "shuffle"
        + str(2),
        "train/pose_cfg.yaml",
    )

    DLC_config = deeplabcut.auxiliaryfunctions.read_plainconfig(posefile)
    updated_max_iters = 10
    DLC_config["save_iters"] = updated_max_iters
    DLC_config["display_iters"] = 2

    print("CHANGING training parameters to end quickly!")
    deeplabcut.auxiliaryfunctions.write_plainconfig(posefile, DLC_config)

    print("TRAINING shuffle 2, with smaller allocated memory")
    deeplabcut.train_network(
        path_config_file, shuffle=2, allow_growth=True, maxiters=updated_max_iters
    )

    print("ANALYZING some individual frames")
    deeplabcut.analyze_time_lapse_frames(
        path_config_file,
        os.path.join(cfg["project_path"], "labeled-data/reachingvideo1/"),
    )

    print("Export model...")
    deeplabcut.export_model(path_config_file, shuffle=2, make_tar=False)

    print("Merging datasets...")
    trainIndices, testIndices = deeplabcut.mergeandsplit(
        path_config_file, trainindex=0, uniform=True
    )

    print("Creating two identical splits...")
    deeplabcut.create_training_dataset(
        path_config_file,
        Shuffles=[4, 5],
        trainIndices=[trainIndices, trainIndices],
        testIndices=[testIndices, testIndices],
        engine=engine,
    )

    print("ALL DONE!!! - default cases are functional.")


--- File: examples/testscript_deterministicwithResNet152.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Created on Tue Oct  2 13:56:11 2018
@author: alex

DEVELOPERS:
This script tests various functionalities in an automatic way.

Note that the same ResNet 152 is trained 4 times (twice with the standard loader).
The sequence of losses is different....

Then twice with the 'deterministic' loader. The losses are identical when this is done twice.

I.e. I get twice: ;)
iteration: 1 loss: 1.6505 lr: 0.001
iteration: 2 loss: 0.6929 lr: 0.001
iteration: 3 loss: 0.6420 lr: 0.001
iteration: 4 loss: 0.5579 lr: 0.001
iteration: 5 loss: 0.4746 lr: 0.001
iteration: 6 loss: 0.3366 lr: 0.001
iteration: 7 loss: 0.3194 lr: 0.001
iteration: 8 loss: 0.2561 lr: 0.001
iteration: 9 loss: 0.1964 lr: 0.001
iteration: 10 loss: 0.1220 lr: 0.001


It produces nothing of interest scientifically.
"""

task = "TEST-deterministic"  # Enter the name of your experiment Task
scorer = "Alex"  # Enter the name of the experimenter/labeler


import os, subprocess, deeplabcut
from pathlib import Path
import pandas as pd
import numpy as np

print("Imported DLC!")
basepath = os.path.dirname(os.path.abspath("testscript.py"))
videoname = "reachingvideo1"
video = [
    os.path.join(
        basepath, "Reaching-Mackenzie-2018-08-30", "videos", videoname + ".avi"
    )
]

# to test destination folder:
# dfolder=basepath
dfolder = None

print("CREATING PROJECT")
path_config_file = deeplabcut.create_new_project(task, scorer, video, copy_videos=True)

cfg = deeplabcut.auxiliaryfunctions.read_config(path_config_file)
cfg["numframes2pick"] = 5
cfg["pcutoff"] = 0.01
cfg["TrainingFraction"] = [0.8]
cfg["default_net_type"] = "resnet_152"  #'mobilenet_v2_0.35'

deeplabcut.auxiliaryfunctions.write_config(path_config_file, cfg)

print("EXTRACTING FRAMES")
deeplabcut.extract_frames(path_config_file, mode="automatic", userfeedback=False)

print("CREATING-SOME LABELS FOR THE FRAMES")
frames = os.listdir(os.path.join(cfg["project_path"], "labeled-data", videoname))
# As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)
for index, bodypart in enumerate(cfg["bodyparts"]):
    columnindex = pd.MultiIndex.from_product(
        [[scorer], [bodypart], ["x", "y"]], names=["scorer", "bodyparts", "coords"]
    )
    frame = pd.DataFrame(
        100 + np.ones((len(frames), 2)) * 50 * index,
        columns=columnindex,
        index=[os.path.join("labeled-data", videoname, fn) for fn in frames],
    )
    if index == 0:
        dataFrame = frame
    else:
        dataFrame = pd.concat([dataFrame, frame], axis=1)

dataFrame.to_csv(
    os.path.join(
        cfg["project_path"],
        "labeled-data",
        videoname,
        "CollectedData_" + scorer + ".csv",
    )
)
dataFrame.to_hdf(
    os.path.join(
        cfg["project_path"],
        "labeled-data",
        videoname,
        "CollectedData_" + scorer + ".h5",
    ),
    "df_with_missing",
    format="table",
    mode="w",
)

print("Plot labels...")

deeplabcut.check_labels(path_config_file)

print("CREATING TRAININGSET")
deeplabcut.create_training_dataset(path_config_file)

# posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')

shuffle = 1
posefile, _, _ = deeplabcut.return_train_network_path(path_config_file, shuffle=shuffle)
print("CHANGING training parameters to end quickly!")
edits = {"save_iters": 4, "display_iters": 1, "multi_step": [[0.001, 10]]}
DLC_config = deeplabcut.auxiliaryfunctions.edit_config(posefile, edits)

print("TRAIN")
deeplabcut.train_network(path_config_file)
print("TRAIN again... different loss?")
deeplabcut.train_network(path_config_file)

DLC_config = deeplabcut.auxiliaryfunctions.edit_config(
    posefile, {"dataset_type": "deterministic", "deterministic": True}
)

print("TRAIN")
deeplabcut.train_network(path_config_file)

print("TRAIN again... the same losses!")
deeplabcut.train_network(path_config_file)

print("ALL DONE!!! - deterministic at least runs... were the losses identical?")


--- File: examples/testscript_pretrained_models.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Testscript human network

"""
import os, subprocess, deeplabcut
from pathlib import Path
import pandas as pd
import numpy as np

Task = "human_dancing"
YourName = "teamDLC"
MODEL_NAME = "horse_sideview"  # full_human"

basepath = os.path.dirname(os.path.abspath("testscript.py"))
videoname = "reachingvideo1"
video = [
    os.path.join(
        basepath, "Reaching-Mackenzie-2018-08-30", "videos", videoname + ".avi"
    )
]

# legacy mode:
"""
configfile, path_train_config=deeplabcut.create_pretrained_human_project(Task, YourName,video,
                                                                        videotype='avi', analyzevideo=True,
                                                                        createlabeledvideo=True, copy_videos=False) #must leave copy_videos=True
"""
# new way:
configfile, path_train_config = deeplabcut.create_pretrained_project(
    Task,
    YourName,
    video,
    model=MODEL_NAME,
    videotype="avi",
    analyzevideo=True,
    createlabeledvideo=True,
    copy_videos=False,
    engine=deeplabcut.Engine.TF,
)  # must leave copy_videos=True


"""
lastvalue = 5
DLC_config = deeplabcut.auxiliaryfunctions.read_plainconfig(path_train_config)
pretrainedDeeperCutweights = DLC_config["init_weights"]

print("EXTRACTING FRAMES")
deeplabcut.extract_frames(configfile, mode="automatic", userfeedback=False)

print("CREATING-SOME LABELS FOR THE FRAMES")
cfg = deeplabcut.auxiliaryfunctions.read_config(configfile)
frames = os.listdir(os.path.join(cfg["project_path"], "labeled-data", videoname))
# As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)
for index, bodypart in enumerate(cfg["bodyparts"]):
    columnindex = pd.MultiIndex.from_product(
        [[cfg["scorer"]], [bodypart], ["x", "y"]],
        names=["scorer", "bodyparts", "coords"],
    )
    frame = pd.DataFrame(
        100 + np.ones((len(frames), 2)) * 50 * index,
        columns=columnindex,
        index=[os.path.join("labeled-data", videoname, fn) for fn in frames],
    )
    if index == 0:
        dataFrame = frame
    else:
        dataFrame = pd.concat([dataFrame, frame], axis=1)

dataFrame.to_csv(
    os.path.join(
        cfg["project_path"],
        "labeled-data",
        videoname,
        "CollectedData_" + cfg["scorer"] + ".csv",
    )
)
dataFrame.to_hdf(
    os.path.join(
        cfg["project_path"],
        "labeled-data",
        videoname,
        "CollectedData_" + cfg["scorer"] + ".h5",
    ),
    "df_with_missing",
    format="table",
    mode="w",
)

deeplabcut.create_training_dataset(configfile, Shuffles=[1])
edits = {
    "save_iters": lastvalue,
    "display_iters": 1,
    "multi_step": [[0.001, lastvalue]],
    "init_weights": pretrainedDeeperCutweights.split(".index")[0],
}
DLC_config = deeplabcut.auxiliaryfunctions.edit_config(path_train_config, edits)

deeplabcut.train_network(configfile, shuffle=1)

print("Adding bodypart!")
cfg = deeplabcut.auxiliaryfunctions.read_config(configfile)
cfg["bodyparts"] = [
    "ankle1",
    "knee1",
    "hip1",
    "hip2",
    "knee2",
    "ankle2",
    "wrist1",
    "elbow1",
    "shoulder1",
    "shoulder2",
    "elbow2",
    "wrist2",
    "chin",
    "forehead",
    "plus1more",
]
deeplabcut.auxiliaryfunctions.write_config(configfile, cfg)

print("CREATING-SOME More LABELS FOR THE FRAMES (including the new bodypart!)")
cfg = deeplabcut.auxiliaryfunctions.read_config(configfile)
frames = [
    f
    for f in os.listdir(os.path.join(cfg["project_path"], "labeled-data", videoname))
    if ".png" in f
]

# As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)
for index, bodypart in enumerate(cfg["bodyparts"]):
    columnindex = pd.MultiIndex.from_product(
        [[cfg["scorer"]], [bodypart], ["x", "y"]],
        names=["scorer", "bodyparts", "coords"],
    )
    frame = pd.DataFrame(
        100 + np.ones((len(frames), 2)) * 50 * index,
        columns=columnindex,
        index=[os.path.join("labeled-data", videoname, fn) for fn in frames],
    )
    if index == 0:
        dataFrame = frame
    else:
        dataFrame = pd.concat([dataFrame, frame], axis=1)

dataFrame.to_csv(
    os.path.join(
        cfg["project_path"],
        "labeled-data",
        videoname,
        "CollectedData_" + cfg["scorer"] + ".csv",
    )
)
dataFrame.to_hdf(
    os.path.join(
        cfg["project_path"],
        "labeled-data",
        videoname,
        "CollectedData_" + cfg["scorer"] + ".h5",
    ),
    "df_with_missing",
    format="table",
    mode="w",
)

edits = {
    "save_iters": lastvalue,
    "display_iters": 1,
    "multi_step": [[0.001, lastvalue]],
    "init_weights": pretrainedDeeperCutweights.split(".index")[0],
}
DLC_config = deeplabcut.auxiliaryfunctions.edit_config(path_train_config, edits)

# deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!
deeplabcut.train_network(configfile, shuffle=1, keepdeconvweights=False)
"""


--- File: examples/testscript_multianimal.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
import os
import pickle
import random
from pathlib import Path

import numpy as np
import pandas as pd

import deeplabcut
from deeplabcut.core.engine import Engine
from deeplabcut.utils import auxfun_multianimal, auxiliaryfunctions
from deeplabcut.utils.auxfun_videos import VideoReader

MODELS = ["dlcrnet_ms5", "dlcr101_ms5", "efficientnet-b0", "mobilenet_v2_0.35"]


N_ITER = 5
TESTTRACKER = "ellipse"

USE_SHELVE = False  # random.choice([True, False])

if __name__ == "__main__":
    TASK = "multi_mouse"
    SCORER = "dlc_team"
    NUM_FRAMES = 5
    TRAIN_SIZE = 0.8
    ENGINE = Engine.TF

    # NET = "dlcr101_ms5"
    NET = "dlcrnet_ms5"

    # Always test a different model from list above
    NET = random.choice(MODELS)

    basepath = os.path.dirname(os.path.realpath(__file__))
    DESTFOLDER = basepath

    video = "m3v1mp4"
    video_path = os.path.join(
        basepath, "openfield-Pranav-2018-10-30", "videos", video + ".mp4"
    )

    print("Creating project...")
    config_path = deeplabcut.create_new_project(
        TASK, SCORER, [video_path], copy_videos=True, multianimal=True
    )

    print("Project created.")

    print("Editing config...")
    cfg = auxiliaryfunctions.edit_config(
        config_path,
        {
            "numframes2pick": NUM_FRAMES,
            "TrainingFraction": [TRAIN_SIZE],
            "identity": True,
            "uniquebodyparts": ["corner1", "corner2"],
        },
    )
    print("Config edited.")

    print("Extracting frames...")
    deeplabcut.extract_frames(config_path, mode="automatic", userfeedback=False)
    print("Frames extracted.")

    print("Creating artificial data...")
    rel_folder = os.path.join("labeled-data", os.path.splitext(video)[0])
    image_folder = os.path.join(cfg["project_path"], rel_folder)
    n_animals = len(cfg["individuals"])
    (
        animals,
        bodyparts_single,
        bodyparts_multi,
    ) = auxfun_multianimal.extractindividualsandbodyparts(cfg)
    animals_id = [i for i in range(n_animals) for _ in bodyparts_multi] + [
        n_animals
    ] * len(bodyparts_single)
    map_ = dict(zip(range(len(animals)), animals))
    individuals = [map_[ind] for ind in animals_id for _ in range(2)]
    scorer = [SCORER] * len(individuals)
    coords = ["x", "y"] * len(animals_id)
    bodyparts = [
        bp for _ in range(n_animals) for bp in bodyparts_multi for _ in range(2)
    ]
    bodyparts += [bp for bp in bodyparts_single for _ in range(2)]
    columns = pd.MultiIndex.from_arrays(
        [scorer, individuals, bodyparts, coords],
        names=["scorer", "individuals", "bodyparts", "coords"],
    )
    index = [
        os.path.join(rel_folder, image)
        for image in auxiliaryfunctions.grab_files_in_folder(image_folder, "png")
    ]
    fake_data = np.tile(
        np.repeat(50 * np.arange(len(animals_id)) + 50, 2), (len(index), 1)
    )
    df = pd.DataFrame(fake_data, index=index, columns=columns)
    output_path = os.path.join(image_folder, f"CollectedData_{SCORER}.csv")
    df.to_csv(output_path)
    df.to_hdf(
        output_path.replace("csv", "h5"), "df_with_missing", format="table", mode="w"
    )
    print("Artificial data created.")

    print("Checking labels...")
    deeplabcut.check_labels(config_path, draw_skeleton=False)
    print("Labels checked.")

    print("Creating train dataset...")
    deeplabcut.create_multianimaltraining_dataset(
        config_path,
        net_type=NET,
        crop_size=(200, 200),
        engine=ENGINE,
    )
    print("Train dataset created.")

    # Check the training image paths are correctly stored as arrays of strings
    trainingsetfolder = auxiliaryfunctions.get_training_set_folder(cfg)
    datafile, _ = auxiliaryfunctions.get_data_and_metadata_filenames(
        trainingsetfolder,
        0.8,
        1,
        cfg,
    )
    datafile = datafile.split(".mat")[0] + ".pickle"
    with open(os.path.join(cfg["project_path"], datafile), "rb") as f:
        pickledata = pickle.load(f)
    num_images = len(pickledata)
    assert all(len(pickledata[i]["image"]) == 3 for i in range(num_images))

    print("Editing pose config...")
    model_folder = auxiliaryfunctions.get_model_folder(
        TRAIN_SIZE, 1, cfg, engine=ENGINE, modelprefix=cfg["project_path"]
    )
    pose_config_path = os.path.join(model_folder, "train", "pose_cfg.yaml")
    edits = {
        "global_scale": 0.5,
        "batch_size": 1,
        "save_iters": N_ITER,
        "display_iters": N_ITER // 2,
        "crop_size": [200, 200],
        # "multi_step": [[0.001, N_ITER]],
    }
    deeplabcut.auxiliaryfunctions.edit_config(pose_config_path, edits)
    print("Pose config edited.")

    print("Training network...")
    deeplabcut.train_network(config_path, maxiters=N_ITER)
    print("Network trained.")

    print("Evaluating network...")
    deeplabcut.evaluate_network(
        config_path, plotting=True, per_keypoint_evaluation=True
    )

    print("Network evaluated....")

    print("Extracting maps...")
    deeplabcut.extract_save_all_maps(config_path, Indices=[0, 1, 2])

    new_video_path = deeplabcut.ShortenVideo(
        video_path,
        start="00:00:00",
        stop="00:00:01",
        outsuffix="short",
        outpath=os.path.join(cfg["project_path"], "videos"),
    )

    print("Analyzing video...")
    try:
        deeplabcut.analyze_videos(
            config_path,
            [new_video_path],
            "mp4",
            robust_nframes=True,
            allow_growth=True,
            use_shelve=USE_SHELVE,
        )
        print("Video analyzed.")
    except ValueError:
        pass

    print("Create video with all detections...")
    scorer, _ = auxiliaryfunctions.get_scorer_name(cfg, 1, TRAIN_SIZE)

    deeplabcut.create_video_with_all_detections(
        config_path, [new_video_path], shuffle=1, displayedbodyparts=["bodypart1"]
    )

    print("Video created.")

    print("Convert detections to tracklets...")
    deeplabcut.convert_detections2tracklets(
        config_path, [new_video_path], "mp4", track_method=TESTTRACKER
    )
    print("Tracklets created...")
    h5path = os.path.splitext(new_video_path)[0] + scorer + "_el.h5"
    try:
        deeplabcut.stitch_tracklets(
            config_path,
            [new_video_path],
            "mp4",
            output_name=h5path,
            track_method=TESTTRACKER,
        )
    except ValueError:
        # Sometimes tracks cannot be reconstructed as test data are randomly
        # created; when this happens, we generate a fake h5 data file.
        individuals = [map_[ind] for ind in animals_id for _ in range(3)]
        scorer = [SCORER] * len(individuals)
        coords = ["x", "y", "likelihood"] * len(animals_id)
        bodyparts = [
            bp for _ in range(n_animals) for bp in bodyparts_multi for _ in range(3)
        ]
        bodyparts += [bp for bp in bodyparts_single for _ in range(3)]
        columns = pd.MultiIndex.from_arrays(
            [scorer, individuals, bodyparts, coords],
            names=["scorer", "individuals", "bodyparts", "coords"],
        )
        vid = VideoReader(new_video_path)
        fake_data = np.ones((len(vid), columns.shape[0]))
        df = pd.DataFrame(fake_data, columns=columns)
        df.to_hdf(h5path, key="data")

    print("Plotting trajectories...")
    deeplabcut.plot_trajectories(
        config_path, [new_video_path], "mp4", track_method=TESTTRACKER
    )
    print("Trajectory plotted.")

    print("Creating labeled video...")
    deeplabcut.create_labeled_video(
        config_path,
        [new_video_path],
        "mp4",
        save_frames=False,
        color_by="individual",
        track_method=TESTTRACKER,
    )
    print("Labeled video created.")

    print("Filtering predictions...")
    deeplabcut.filterpredictions(
        config_path, [new_video_path], "mp4", track_method=TESTTRACKER
    )
    print("Predictions filtered.")

    print("Extracting outlier frames...")
    deeplabcut.extract_outlier_frames(
        config_path, [new_video_path], "mp4", automatic=True, track_method=TESTTRACKER
    )
    print("Outlier frames extracted.")

    vname = Path(new_video_path).stem

    file = os.path.join(
        cfg["project_path"],
        "labeled-data",
        vname,
        "machinelabels-iter" + str(cfg["iteration"]) + ".h5",
    )

    print("RELABELING")
    DF = pd.read_hdf(file, "df_with_missing")
    DLCscorer = np.unique(DF.columns.get_level_values(0))[0]
    DF.columns = DF.columns.set_levels([scorer.replace(DLCscorer, SCORER)], level=0)
    DF = DF.drop("likelihood", axis=1, level=3)
    DF.to_csv(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            vname,
            "CollectedData_" + SCORER + ".csv",
        )
    )
    DF.to_hdf(
        os.path.join(
            cfg["project_path"],
            "labeled-data",
            vname,
            "CollectedData_" + SCORER + ".h5",
        ),
        "df_with_missing",
    )

    print("MERGING")
    deeplabcut.merge_datasets(config_path)  # iteration + 1

    print("CREATING TRAININGSET updated training set")
    deeplabcut.create_training_dataset(config_path, net_type=NET, engine=ENGINE)

    print("Training network...")
    deeplabcut.train_network(config_path, maxiters=N_ITER)
    print("Network trained.")

    print("Evaluating network...")
    deeplabcut.evaluate_network(
        config_path, plotting=True, per_keypoint_evaluation=True
    )

    print("Network evaluated....")

    print("Analyzing video with auto_track....")
    deeplabcut.analyze_videos(
        config_path,
        [new_video_path],
        save_as_csv=True,
        destfolder=DESTFOLDER,
        cropping=[0, 50, 0, 50],
        allow_growth=True,
        use_shelve=USE_SHELVE,
        auto_track=False,
    )

    print("Export model...")
    deeplabcut.export_model(config_path, shuffle=1, make_tar=False)

    print("Merging datasets...")
    trainIndices, testIndices = deeplabcut.mergeandsplit(
        config_path, trainindex=0, uniform=True
    )

    print("Creating two identical splits...")
    deeplabcut.create_multianimaltraining_dataset(
        config_path,
        Shuffles=[4, 5],
        net_type=NET,
        trainIndices=[trainIndices, trainIndices],
        testIndices=[testIndices, testIndices],
        engine=ENGINE,
    )

    print("ALL DONE!!! - default multianimal cases are functional.")


--- File: examples/testscript_pytorch_multi_animal.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Testscript for single animal PyTorch projects"""
from __future__ import annotations

from pathlib import Path

import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.compat import Engine

from utils import (
    cleanup,
    create_fake_project,
    log_step,
    run,
    SyntheticProjectParameters,
)


def main(
    net_types: list[str],
    params: SyntheticProjectParameters,
    epochs: int = 1,
    top_down_epochs: int = 1,
    detector_epochs: int = 1,
    save_epochs: int = 1,
    batch_size: int = 1,
    detector_batch_size: int = 1,
    max_snapshots_to_keep: int = 5,
    device: str = "cpu",
    logger: dict | None = None,
    create_labeled_videos: bool = False,
    delete_after_test_run: bool = False,
) -> None:
    project_path = Path("synthetic-data-niels-multi-animal").resolve()
    config_path = project_path / "config.yaml"
    create_fake_project(path=project_path, params=params)

    engine = Engine.PYTORCH
    cfg = af.read_config(config_path)
    trainset_index = 0
    train_frac = cfg["TrainingFraction"][trainset_index]
    try:
        for net_type in net_types:
            epochs_ = epochs
            if "top_down" in net_type:
                epochs_ = top_down_epochs
            try:
                run(
                    config_path=config_path,
                    train_fraction=train_frac,
                    trainset_index=trainset_index,
                    net_type=net_type,
                    videos=[str(project_path / "videos" / "video.mp4")],
                    device=device,
                    engine=engine,
                    pytorch_cfg_updates={
                        "train_settings.display_iters": 50,
                        "train_settings.epochs": epochs_,
                        "train_settings.batch_size": batch_size,
                        "runner.device": device,
                        "runner.snapshots.save_epochs": save_epochs,
                        "runner.snapshots.max_snapshots": max_snapshots_to_keep,
                        "detector.train_settings.display_iters": 1,
                        "detector.train_settings.epochs": detector_epochs,
                        "detector.train_settings.batch_size": detector_batch_size,
                        "detector.runner.snapshots.save_epochs": save_epochs,
                        "detector.runner.snapshots.max_snapshots": max_snapshots_to_keep,
                        "logger": logger,
                    },
                    create_labeled_videos=create_labeled_videos,
                )
            except Exception as err:
                log_step(f"FAILED TO RUN {net_type}")
                log_step(str(err))
                log_step("Continuing to next model")
                raise err

    finally:
        if delete_after_test_run:
            cleanup(project_path)


if __name__ == "__main__":
    wandb_logger = {
        "type": "WandbLogger",
        "project_name": "testscript-dev",
        "run_name": "test-logging",
    }
    main(
        net_types=["top_down_resnet_50", "resnet_50", "dekr_w32"],
        params=SyntheticProjectParameters(
            multianimal=True,
            num_bodyparts=4,
            num_individuals=3,
            num_unique=0,
            num_frames=25,
            frame_shape=(256, 256),
        ),
        batch_size=2,
        detector_batch_size=2,
        epochs=8,
        top_down_epochs=2,
        detector_epochs=10,
        save_epochs=4,
        max_snapshots_to_keep=2,
        device="cpu",  # "cpu", "cuda:0", "mps"
        logger=None,
        create_labeled_videos=True,
        delete_after_test_run=True,
    )


--- File: examples/testscript_superanimal_adaptation.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Test script for super animal adaptation
"""
import deeplabcut
import os


if __name__ == "__main__":
    basepath = os.path.dirname(os.path.realpath(__file__))
    videoname = "m3v1mp4"
    video = os.path.join(
        basepath, "openfield-Pranav-2018-10-30", "videos", videoname + ".mp4"
    )
    video = deeplabcut.ShortenVideo(
        video,
        start="00:00:00",
        stop="00:00:01",
        outsuffix="short",
    )

    print("adaptation training for superanimal_topviewmouse")

    superanimal_name = "superanimal_topviewmouse"
    videotype = ".mp4"
    scale_list = [200, 300, 400]
    deeplabcut.video_inference_superanimal(
        [video],
        superanimal_name,
        model_name="hrnet_w32",
        detector_name="fasterrcnn_resnet50_fpn_v2",
        videotype=".mp4",
        video_adapt=True,
        scale_list=scale_list,
        pcutoff=0.1,
        adapt_iterations=50,
    )


--- File: examples/README.md ---
# Demo Jupyter & Colaboratory Notebooks:

<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572293604382-W6BWA63LZ9J8R7N0QEA5/ke17ZwdGBToddI8pDm48kIw6YkRUEyoge4858uAJfaMUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYwL8IeDg6_3B-BRuF4nNrNcQkVuAT7tdErd0wQFEGFSnH9wUPiI8bGoX-EQadkbLIJwhzjIpw393-uEwSKO7VZIL9gN_Sb5I_dLwvWryjeCJg/dlc_overview-01.png?format=1000w" width="550" title="DLC" alt="DLC" align="right" vspace = "70">

We provide a Project Manager GUI that will walk you through the major steps and options of the DeepLabCut Toolbox. However, there are more options and features that can be accessed by running the code in an interactive environment, such as Jupyter*. Moreover, if you don't have a GPU, you can create your project on any computer, then move your project to the cloud to use GPUs. To do this, we provide you with Google Colaboratory Notebooks (see [Demo using Google Colaboratory below](/examples#demo-deeplabcut-training-and-analysis-on-google-colaboratory-with-googles-gpus)).


## Demo 1: run DeepLabCut [on our open-field data](JUPYTER/Demo_labeledexample_Openfield.ipynb)
 - This will give you a feel of the workflow for DeepLabCut. Follow the instructions inside the notebook!

Note, the notebooks with labeled data: [reaching data](JUPYTER/Demo_labeledexample_MouseReaching.ipynb), or [open-field data](JUPYTER/Demo_labeledexample_Openfield.ipynb) can be run on a CPU, GPU, etc. The one with the open-field data even achieves good/okay results, when trained for half an hour on a GPU! (To note, this is NOT the full dataset that was used in Mathis et al, 2018)

## Demo 2: Set up DeepLabCut on [your own data](JUPYTER/Demo_yourowndata.ipynb)
- Now that you're a master of the demos, this Notebook walks you through how to build your own pipeline:
  - Create a new project
  - Label new data
  - Then, either use your CPU, or your GPU (the Notebook will guide you at this junction), to train, analyze and perform some basic analysis of your data.

For GPU-based training and analysis you will need to switch to either our [supplied Docker container](https://deeplabcut.github.io/DeepLabCut/docs/docker.html), or you need to [install your local GPU](https://deeplabcut.github.io/DeepLabCut/docs/recipes/installTips.html?highlight=gpu#how-to-confirm-that-your-gpu-is-being-used-by-deeplabcut) in an Anaconda Env, or use Google Colab, more below: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb)

## Demo 3: DeepLabCut training and analysis on Google Colaboratory (with Google's GPUs!):

We suggest making a "Fork" of this repo, git clone or download the folder into your google drive, then linking your google account to your GitHub (you'll see how to do this in the Notebook below). Then you can edit the Notebooks for your own data too (just put https://colab.research.google.com/ in front of the web address of your own repo).

- You can use Google [Colaboratory](https://colab.research.google.com) to demo running DeepLabCut on our data. Here is an example colab-ready Jupyter Notebook for the open field data, which you can launch by clicking the badge below: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_DEMO_mouse_openfield.ipynb)

- Using Colab on your data for the training and analysis of new videos, i.e. the parts that need a GPU!
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb)

1. Click Open in Colab to launch the notebook.
2. Make the notebook live by clicking 'Connect' in the Colab toolbar, and then click "Runtime > Change Runtime Type > and select Python3 and GPU as your hardware. Follow the instructions in the Notebook.
3. Be aware, they often don't let you run on their GPUs for very long (>6 hrs) without a Pro account, so make sure your ``save_inters`` variable is lower for this setting.

Here is a demo of us using the Colab Notebooks: https://www.youtube.com/watch?v=qJGs8nxx80A & https://www.youtube.com/watch?v=j13aXxysI2E


*Warning: Colab updates their CUDA/TensorFlow likely faster than we can keep up, so this may not work at all future points in time (and, as a reminder, this whole package is released with a [LICENSE](/LICENSE) that implies no Liability and no Warranty).*

## Using 3D DeepLabCut:

Ready to take your pose estimation to a new dimension? As of 2.0.7+ we support 3D within the package. Please check out the dedicated 3D_Demo_DeepLabCut.ipynb above for more details!

## Using the DLC Model Zoo:

We provide a COLAB notebook to use the growing number of networks that are trained on specific animals/scenarios. Read more here: http://www.mousemotorlab.org/dlc-modelzoo. This code will also create a new project folder so you can refine, add new bodyparts or label other objects, and re-train. Launch COLAB here: [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_DLC_ModelZoo.ipynb)

## Using Python/iPython:

All of DeepLabCut can be run from an ipython console in the program **terminal**! Go [here](/docs/UseOverviewGuide.md) for detailed instructions!

We also have some video tutorials to demonstrate how we use Anaconda and Docker via the terminal:

 https://www.youtube.com/watch?v=7xwOhUcIGio &  https://www.youtube.com/watch?v=bgfnz1wtlpo


## * You can download DeepLabCut & associated files:

To have a copy of DeepLabCut on your own computer, we recommend using **Anaconda to install Python and Jupyter Notebooks, see the [Installation](/docs/installation.md) page**. Then, on your local machine using these notebooks to guide you, you can (1) demo our labeled data (or create your own), (2) create a project, extract frames to label, use the GUI to label, and create a training set for the neural network(s).

We suggest making a "Fork" of this repo and/or then place DeepLabCut files in a folder:
``git clone https://github.com/DeepLabCut/DeepLabCut``
so you can access it locally with **Anaconda.** You can also click the "download" button, rather than using ``git``. Then you can edit the Notebooks as you like!


--- File: examples/testscript_superanimal_create_pretrained_project.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Testscript for creating a pretrained project from a super animal model

"""
import glob
import shutil
from pathlib import Path

import deeplabcut

if __name__ == "__main__":
    superanimal_name = "superanimal_quadruped"
    working_dir = Path(__file__).resolve().parent
    video_dir = working_dir / "openfield-Pranav-2018-10-30/videos/m3v1mp4.mp4"
    project_name = "pretrained"

    deeplabcut.create_pretrained_project(
        project_name,
        "max",
        [str(video_dir)],
        engine=deeplabcut.Engine.PYTORCH,
    )

    dirs_to_delete = glob.glob(f"{working_dir}/{project_name}*")

    # Delete directories
    for directory in dirs_to_delete:
        shutil.rmtree(directory)

    print("Test passed!")


--- File: examples/testscript_openfielddata.py ---
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Created on Mon Nov 5 18:06:13 2018

@author: alex

This is a test script that should achieve a certain performance on your system.

See values on our system, the DLC 2.0 docker with TF 1.8 on a NVIDIA GTX 1080Ti.
https://github.com/MMathisLab/Docker4DeepLabCut2.0

This test trains on the open field data set for about 30 minutes (15k iterations).

The results will be something like this:

Results for 15001  training iterations: 95 1 train error: 2.89 pixels. Test error: 2.81  pixels.
With pcutoff of 0.1  train error: 2.89 pixels. Test error: 2.81 pixels

The analysis of the video takes 41 seconds (batch size 32) and creating the frames 8 seconds (+ a few seconds for ffmpeg) to create the video.
"""
import deeplabcut
import os


if __name__ == "__main__":
    # Loading example data set
    path_config_file = os.path.join(
        os.getcwd(), "openfield-Pranav-2018-10-30/config.yaml"
    )
    deeplabcut.load_demo_data(path_config_file)
    shuffle = 13

    deeplabcut.create_training_dataset(path_config_file, Shuffles=[shuffle])
    cfg = deeplabcut.auxiliaryfunctions.read_config(path_config_file)

    # example how to set pose config variables:
    posefile, _, _ = deeplabcut.return_train_network_path(
        path_config_file, shuffle=shuffle
    )
    edits = {"save_iters": 15000, "display_iters": 1000, "multi_step": [[0.005, 15001]]}
    DLC_config = deeplabcut.auxiliaryfunctions.edit_config(posefile, edits)

    print("TRAIN NETWORK")
    deeplabcut.train_network(path_config_file, shuffle=shuffle, max_snapshots_to_keep=3)

    print("EVALUATE")
    deeplabcut.evaluate_network(path_config_file, Shuffles=[shuffle], plotting=True)

    print("Analyze Video")
    videofile_path = os.path.join(
        os.getcwd(), "openfield-Pranav-2018-10-30", "videos", "m3v1mp4.mp4"
    )
    deeplabcut.analyze_videos(
        path_config_file, [videofile_path], shuffle=shuffle
    )  # ,videotype='.mp4')

    print("Create Labeled Video")
    deeplabcut.create_labeled_video(
        path_config_file, [videofile_path], save_frames=False, shuffle=shuffle
    )  # ,videotype='.mp4')


--- File: examples/utils.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from __future__ import annotations

import shutil
import string
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import cv2
import deeplabcut
import deeplabcut.utils.auxiliaryfunctions as af
import numpy as np
import pandas as pd
from deeplabcut.compat import Engine
from deeplabcut.generate_training_dataset import get_existing_shuffle_indices
from PIL import Image


def log_step(message: Any) -> None:
    print(100 * "-")
    print(str(message))
    print(100 * "-")


def cleanup(test_path: Path) -> None:
    if test_path.exists():
        shutil.rmtree(test_path)


@dataclass(frozen=True)
class SyntheticProjectParameters:
    multianimal: bool
    num_bodyparts: int
    num_frames: int = 10
    num_individuals: int = 1
    num_unique: int = 0
    identity: bool = False
    frame_shape: tuple[int, int] = (480, 640)

    def bodyparts(self) -> list[str]:
        return [i for i in string.ascii_lowercase[: self.num_bodyparts]]

    def unique(self) -> list[str]:
        return [f"unique_{i}" for i in string.ascii_lowercase[: self.num_unique]]

    def individuals(self) -> list[str]:
        return [f"animal_{i}" for i in range(self.num_individuals)]


def sample_pose_random(
    gen: np.random.Generator,
    num_individuals: int,
    num_bodyparts: int,
    num_unique: int,
    img_h: int,
    img_w: int,
) -> np.ndarray:
    """Fully random pose sampling"""
    xs = gen.choice(img_w, size=(num_individuals, num_bodyparts), replace=False)
    ys = gen.choice(img_h, size=(num_individuals, num_bodyparts), replace=False)
    pose = np.stack([xs, ys], axis=-1)

    image_data = pose.reshape(-1)
    if num_unique > 0:
        unique_pose = np.stack(
            [
                gen.choice(img_w, size=(1, num_unique), replace=False),
                gen.choice(img_h, size=(1, num_unique), replace=False),
            ],
            axis=-1,
        )
        image_data = np.concatenate([image_data, unique_pose.reshape(-1)])
    return image_data


def sample_pose_from_center(
    center_xs: np.ndarray,
    center_ys: np.ndarray,
    num_individuals: int,
    num_bodyparts: int,
    num_unique: int,
    radius: int = 25,
) -> np.ndarray:
    """Sample keypoints from the center of each individual"""
    pose = np.zeros((num_individuals, num_bodyparts, 2))
    for i, (xc, yc) in enumerate(zip(center_xs, center_ys)):
        if i < num_individuals:
            x_start, x_end = xc - radius + 1, xc + radius - 1
            y_start, y_end = yc - radius + 1, yc + radius - 1
            pose[i, :, 0] = np.linspace(start=x_start, stop=x_end, num=num_bodyparts)
            pose[i, :, 1] = np.linspace(start=y_start, stop=y_end, num=num_bodyparts)

    image_data = pose.reshape(-1)
    if num_unique > 0:
        xc, yc = center_xs[-1], center_ys[-1]
        x_start, x_end = xc - radius + 1, xc + radius - 1
        y_start, y_end = yc - radius + 1, yc + radius - 1
        unique_pose = np.zeros((1, num_unique, 2))
        unique_pose[0, :, 0] = np.linspace(start=x_start, stop=x_end, num=num_unique)
        unique_pose[0, :, 1] = np.linspace(start=y_start, stop=y_end, num=num_unique)
        image_data = np.concatenate([image_data, unique_pose.reshape(-1)])
    return image_data


def gen_fake_data(
    scorer: str,
    video_name: str,
    params: SyntheticProjectParameters,
) -> pd.DataFrame:
    kpt_entries = ["x", "y"]
    col_names = ["scorer", "individuals", "bodyparts", "coords"]
    col_values = []
    for i in params.individuals():
        for b in params.bodyparts():
            col_values += [(scorer, i, b, entry) for entry in kpt_entries]

    for unique_bpt in params.unique():
        col_values += [(scorer, "single", unique_bpt, entry) for entry in kpt_entries]

    index_data = []
    pose_data = []
    gen = np.random.default_rng(seed=0)

    # sample starting points for each individual
    img_h, img_w = params.frame_shape[:2]
    radius = 8
    center_xs = gen.choice(
        np.arange(radius, img_w - radius),
        size=params.num_individuals + 1,  # in case unique bodyparts
        replace=False,
    )
    center_ys = gen.choice(
        np.arange(radius, img_h - radius),
        size=params.num_individuals + 1,  # in case unique bodyparts
        replace=False,
    )

    for frame_index in range(params.num_frames):
        index_data.append(("labeled-data", video_name, f"img{frame_index:04}.png"))
        pose_data.append(
            sample_pose_from_center(
                center_xs,
                center_ys,
                num_individuals=params.num_individuals,
                num_bodyparts=params.num_bodyparts,
                num_unique=params.num_unique,
                radius=radius,
            )
        )
        mvt_x = gen.integers(low=-1, high=4, size=center_xs.size)
        mvt_y = gen.integers(low=-1, high=4, size=center_ys.size)
        center_xs = np.clip(center_xs + mvt_x, radius, img_w - radius)
        center_ys = np.clip(center_ys + mvt_y, radius, img_h - radius)

    pose = np.stack(pose_data)
    pose[params.num_frames // 2, :] = np.nan  # add missing row in a frame
    for idv in range(params.num_individuals):
        idv_start = 2 * params.num_bodyparts * idv
        idv_end = 2 * params.num_bodyparts * (idv + 1)
        if params.num_frames > idv + 1:
            pose[idv + 1, idv_start:idv_end] = np.nan

    for bpt in range(params.num_bodyparts):
        frame_idx = 1 + params.num_individuals + bpt
        idv_idx = bpt % params.num_individuals
        offset = 2 * params.num_bodyparts * idv_idx
        bpt_start, bpt_end = 2 * bpt + offset, 2 * (bpt + 1) + offset
        if params.num_frames + 1 > frame_idx:
            pose[frame_idx, bpt_start:bpt_end] = np.nan

    return pd.DataFrame(
        pose,
        index=pd.MultiIndex.from_tuples(index_data),
        columns=pd.MultiIndex.from_tuples(col_values, names=col_names),
    )


def gen_fake_image(
    project_root: Path,
    row: pd.Series,
    params: SyntheticProjectParameters,
    radius: int = 5,
):
    img_h, img_w = params.frame_shape
    image_array = np.zeros((*params.frame_shape, 3), dtype=np.uint8)
    for i, idv in enumerate(params.individuals()):
        r = int(255 * (i + 1) / params.num_individuals)
        if "individuals" in row.index.names:
            idv_data = row.droplevel("scorer").loc[idv]
        else:
            idv_data = row.droplevel("scorer")

        keypoints = idv_data.to_numpy().reshape((-1, 2))
        if not np.all(np.isnan(keypoints)):
            idv_center = np.nanmean(keypoints, axis=0)
            x, y = int(idv_center[0]), int(idv_center[1])
            xmin, xmax = max(0, x - radius), min(img_w - 1, x + radius)
            ymin, ymax = max(0, y - radius), min(img_h - 1, y + radius)
            image_array[ymin:ymax, xmin:xmax, 0] = r

            for j, bpt in enumerate(params.bodyparts()):
                g = int(255 * (j + 1) / params.num_bodyparts)

                bpt_data = idv_data.loc[bpt]
                if np.all(~pd.isnull(bpt_data)):
                    x, y = int(bpt_data.x), int(bpt_data.y)
                    xmin, xmax = max(0, x - radius), min(img_w - 1, x + radius)
                    ymin, ymax = max(0, y - radius), min(img_h - 1, y + radius)
                    image_array[ymin:ymax, xmin:xmax, 0] = r
                    image_array[ymin:ymax, xmin:xmax, 1] = g

    if params.num_unique > 0:
        unique_data = row.droplevel("scorer").loc["single"]
        for i, unique_bpt in enumerate(params.unique()):
            bpt_data = unique_data.loc[unique_bpt]
            if np.all(~pd.isnull(bpt_data)):
                x, y = int(bpt_data.x), int(bpt_data.y)
                xmin, xmax = max(0, x - radius), min(img_w - 1, x + radius)
                ymin, ymax = max(0, y - radius), min(img_h - 1, y + radius)
                image_array[ymin:ymax, xmin:xmax, 2] = int(
                    255 * (i + 1) / params.num_unique
                )

    img = Image.fromarray(image_array)
    img.save(project_root / Path(*row.name))


def generate_video_from_images(image_dir: Path, output_video: Path) -> None:
    images = [p for p in image_dir.iterdir() if p.is_file() and p.suffix == ".png"]
    images = sorted(images, key=lambda f: f.stem)
    if len(images) == 0:
        return

    height, width, channels = cv2.imread(str(images[0])).shape
    fourcc = cv2.VideoWriter_fourcc(*"MJPG")
    out = cv2.VideoWriter(str(output_video), fourcc, 10, (width, height))
    for img_path in images:
        img = cv2.imread(str(img_path))
        out.write(img)
    out.release()


def create_fake_project(path: Path, params: SyntheticProjectParameters) -> None:
    if path.exists():
        raise ValueError(f"Cannot create a fake project at an existing path")

    scorer = "synthetic"
    video_name = "cat"
    path.mkdir(parents=True, exist_ok=False)
    config = {
        "Task": "synthetic",
        "scorer": scorer,
        "date": "Nov11",
        "multianimalproject": params.multianimal,
        "identity": params.identity,
        "project_path": str(path / "config.yaml"),
        "TrainingFraction": [0.8],
        "iteration": 0,
        "default_net_type": "resnet_50",
        "default_augmenter": "default",
        "default_track_method": "ellipse",
        "snapshotindex": "all",
        "batch_size": 8,
        "pcutoff": 0.6,
        "video_sets": {
            str(path / "videos" / video_name): {
                "crop": (0, params.frame_shape[1], 0, params.frame_shape[0]),
            },
        },
        "start": 0,
        "stop": 1,
        "numframes2pick": 10,
        "dotsize": 4,
        "alphavalue": 1.0,
        "colormap": "rainbow",
    }
    if not params.multianimal:
        config["bodyparts"] = params.bodyparts()
        assert params.num_individuals == 1
        assert params.num_unique == 0
    else:
        config["bodyparts"] = "MULTI!"
        config["multianimalbodyparts"] = params.bodyparts()
        config["uniquebodyparts"] = params.unique()
        config["individuals"] = params.individuals()

    af.write_config(str(path / "config.yaml"), config)
    image_dir = path / "labeled-data" / video_name
    image_dir.mkdir(parents=True, exist_ok=False)

    df = gen_fake_data(
        scorer=scorer,
        video_name=video_name,
        params=params,
    )
    print("SYNTHETIC DATA:")
    print(df)
    print("\n")
    if not params.multianimal:
        df.columns = df.columns.droplevel("individuals")

    df.to_hdf(image_dir / f"CollectedData_{scorer}.h5", key="df_with_missing")
    df.to_csv(image_dir / f"CollectedData_{scorer}.csv")

    for idx in range(params.num_frames):
        gen_fake_image(path, df.iloc[idx], params=params, radius=5)

    output_video = path / "videos" / "video.mp4"
    output_video.parent.mkdir(exist_ok=True)
    generate_video_from_images(image_dir, output_video)


def copy_project_for_test() -> Path:
    data_path = Path.cwd() / "openfield-Pranav-2018-10-30"
    test_path = Path.cwd() / "pytorch-testscript1234-openfield-Pranav-2018-10-30"
    if not test_path.exists():
        shutil.copytree(data_path, test_path)

    project_config = af.read_config(str(test_path / "config.yaml"))
    videos = list(project_config["video_sets"].keys())
    video = videos[0]
    crop = project_config["video_sets"][video]
    project_config["video_sets"] = {str(test_path / "videos" / "m3v1mp4.mp4"): crop}
    af.write_config(str(test_path / "config.yaml"), project_config)
    return test_path


def run(
    config_path: Path,
    train_fraction: float,
    trainset_index: int,
    net_type: str,
    videos: list[str],
    device: str,
    engine: Engine = Engine.PYTORCH,
    pytorch_cfg_updates: dict | None = None,
    create_labeled_videos: bool = False,
) -> None:
    times = [time.time()]
    log_step(f"Testing with net type {net_type}")
    log_step("Creating the training dataset")
    deeplabcut.create_training_dataset(
        str(config_path), net_type=net_type, engine=engine
    )
    existing_shuffles = get_existing_shuffle_indices(
        config_path, train_fraction=train_fraction, engine=engine
    )
    shuffle_index = existing_shuffles[-1]

    log_step(
        f"Starting training for train_frac {train_fraction}, shuffle {shuffle_index}"
    )
    deeplabcut.train_network(
        config=str(config_path),
        shuffle=shuffle_index,
        trainingsetindex=trainset_index,
        device=device,
        pytorch_cfg_updates=pytorch_cfg_updates,
    )
    times.append(time.time())
    log_step(f"Train time: {times[-1] - times[-2]} seconds")

    log_step(
        f"Starting evaluation for train_frac {train_fraction}, shuffle {shuffle_index}"
    )
    deeplabcut.evaluate_network(
        config=str(config_path),
        Shuffles=[shuffle_index],
        trainingsetindex=trainset_index,
        device=device,
        plotting=True,
        per_keypoint_evaluation=True,
    )
    times.append(time.time())
    log_step(f"Evaluation time: {times[-1] - times[-2]} seconds")

    if len(videos) > 0:
        log_step(f"Analyzing videos for {train_fraction}, shuffle {shuffle_index}")
        video_kwargs = dict(
            videos=videos, shuffle=shuffle_index, trainingsetindex=trainset_index
        )
        deeplabcut.analyze_videos(
            str(config_path), **video_kwargs, device=device, auto_track=False
        )
        times.append(time.time())
        log_step(f"Video analysis time: {times[-1] - times[-2]} seconds")
        log_step(f"Total test time: {times[-1] - times[0]} seconds")

        cfg = af.read_config(config_path)
        if cfg.get("multianimalproject"):
            if create_labeled_videos:
                deeplabcut.create_video_with_all_detections(
                    str(config_path), **video_kwargs
                )

            # relaxed tracking parameters
            deeplabcut.convert_detections2tracklets(
                str(config_path),
                **video_kwargs,
                inferencecfg=dict(
                    boundingboxslack=10,
                    iou_threshold=0.2,
                    max_age=5,
                    method="m1",
                    min_hits=1,
                    minimalnumberofconnections=2,
                    pafthreshold=0.1,
                    pcutoff=0.1,
                    topktoretain=3,
                    variant=0,
                    withid=False,
                ),
            )
            deeplabcut.stitch_tracklets(str(config_path), **video_kwargs, min_length=3)

        if create_labeled_videos:
            log_step(f"Making labeled video, {train_fraction}, shuffle={shuffle_index}")
            results = deeplabcut.create_labeled_video(
                config=str(config_path),
                videos=videos,
                shuffle=shuffle_index,
                trainingsetindex=trainset_index,
            )
            assert all(results), f"Failed to create some labeled video for {videos}"


if __name__ == "__main__":
    create_fake_project(
        path=Path("synthetic-data-niels"),
        params=SyntheticProjectParameters(
            multianimal=True,
            num_bodyparts=4,
            num_individuals=3,
            num_unique=1,
            num_frames=50,
            frame_shape=(128, 256),
        ),
    )


--- File: examples/testscript_pytorch_single_animal.py ---
"""Testscript for single animal PyTorch projects"""

from __future__ import annotations

from pathlib import Path

import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.compat import Engine

from utils import (
    cleanup,
    copy_project_for_test,
    create_fake_project,
    log_step,
    run,
    SyntheticProjectParameters,
)


def main(
    synthetic_data: bool,
    net_types: list[str],
    epochs: int = 1,
    save_epochs: int = 1,
    max_snapshots_to_keep: int = 5,
    batch_size: int = 1,
    device: str = "cpu",
    logger: dict | None = None,
    synthetic_data_params: SyntheticProjectParameters = SyntheticProjectParameters(
        multianimal=False,
        num_bodyparts=6,
    ),
    create_labeled_videos: bool = False,
    delete_after_test_run: bool = False,
) -> None:
    engine = Engine.PYTORCH
    if synthetic_data:
        project_path = Path("synthetic-data-niels-single-animal").resolve()
        videos = [str(project_path / "videos" / "video.mp4")]
        create_fake_project(path=project_path, params=synthetic_data_params)

    else:
        project_path = copy_project_for_test()
        videos = [str(project_path / "videos" / "m3v1mp4.mp4")]

    config_path = project_path / "config.yaml"
    cfg = af.read_config(config_path)
    trainset_index = 0
    train_frac = cfg["TrainingFraction"][trainset_index]
    try:
        for net_type in net_types:
            try:
                run(
                    config_path=config_path,
                    train_fraction=train_frac,
                    trainset_index=trainset_index,
                    net_type=net_type,
                    videos=videos,
                    device=device,
                    engine=engine,
                    pytorch_cfg_updates={
                        "train_settings.display_iters": 50,
                        "train_settings.epochs": epochs,
                        "train_settings.batch_size": batch_size,
                        "runner.device": device,
                        "runner.snapshots.save_epochs": save_epochs,
                        "runner.snapshots.max_snapshots": max_snapshots_to_keep,
                        "logger": logger,
                    },
                    create_labeled_videos=create_labeled_videos,
                )

            except Exception as err:
                log_step(f"FAILED TO RUN {net_type}")
                log_step(str(err))
                log_step("Continuing to next model")
                raise err
    finally:
        if delete_after_test_run:
            cleanup(project_path)


if __name__ == "__main__":
    wandb_logger = {
        "type": "WandbLogger",
        "project_name": "testscript-dev",
        "run_name": "test-logging",
    }
    main(
        synthetic_data=True,
        net_types=["cspnext_m", "resnet_50", "hrnet_w32"],
        batch_size=4,
        epochs=8,
        save_epochs=2,
        max_snapshots_to_keep=2,
        device="cpu",  # "cpu", "cuda:0", "mps"
        logger=None,
        synthetic_data_params=SyntheticProjectParameters(
            multianimal=False,
            num_bodyparts=4,
            num_individuals=1,
            num_unique=0,
            num_frames=12,
            frame_shape=(128, 128),
        ),
        create_labeled_videos=True,
        delete_after_test_run=True,
    )


--- File: examples/test.sh ---
rm -r TEST*
rm -r multi_mouse*
rm -r 3D*
rm -r OUT

cd ..
pip uninstall deeplabcut
python3 setup.py sdist bdist_wheel
pip install dist/deeplabcut-3.0.0rc7-none-any.whl

cd examples

python3 testscript.py
python3 testscript_3d.py #does not work in container
#python3 testscript_mobilenets.py
python3 testscript_multianimal.py

#python3 testscript_openfielddata_netcomparison.py
#python3 testscript_openfielddata_augmentationcomparison.py


--- File: examples/testscript_superanimal_transfer_learning.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""
Test script for super animal adaptation
"""
import os

import deeplabcut
from deeplabcut.modelzoo.weight_initialization import build_weight_init

print(deeplabcut.__file__)
if __name__ == "__main__":

    superanimal_name = "superanimal_topviewmouse"
    basepath = os.path.dirname(os.path.realpath(__file__))
    config_path = os.path.join(basepath, "openfield-Pranav-2018-10-30", "config.yaml")
    model_name = "hrnet_w32"
    detector_name = "fasterrcnn_resnet50_fpn_v2"

    weight_init = build_weight_init(
        cfg=config_path,
        super_animal=superanimal_name,
        model_name=model_name,
        detector_name=detector_name,
        with_decoder=False,
    )
    deeplabcut.create_training_dataset(config_path, weight_init=weight_init)

    deeplabcut.train_network(
        config_path,
        epochs=1,
        superanimal_name=superanimal_name,
        superanimal_transfer_learning=True,
    )


--- File: examples/Reaching-Mackenzie-2018-08-30/config.yaml ---
# Project definitions (do not edit)
Task: Reaching
scorer: Mackenzie
date: Aug30

# Project path (change when moving around)
project_path: WILL BE AUTOMATICALLY UPDATED BY DEMO CODE

# Annotation data set configuration (and individual video cropping parameters)
video_sets:
  WILL BE AUTOMATICALLY UPDATED BY DEMO CODE:
    crop: 0, 832, 0, 747
bodyparts:
- Hand
- Finger1
- Tongue
- Joystick1
- Joystick2
start: 0
stop: 1
numframes2pick: 40

# Plotting configuration
skeleton: [['Hand', 'Finger1'], ['Joystick1', 'Joystick2']]
skeleton_color: blue
pcutoff: 0.4
dotsize: 12
alphavalue: 0.7
colormap: jet


# Training,Evaluation and Analysis configuration
TrainingFraction:
- 0.95
iteration: 0
default_net_type: resnet_50
snapshotindex: -1
batch_size: 4

# Cropping Parameters (for analysis and outlier frame detection)
cropping: false
#if cropping is true for analysis, then set the values here:
x1: 0
x2: 640
y1: 277
y2: 624

# Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
- 50
- 50
move2corner: true


--- File: examples/Reaching-Mackenzie-2018-08-30/labeled-data/reachingvideo1/CollectedData_Mackenzie.csv ---
scorer,Mackenzie,Mackenzie,Mackenzie,Mackenzie,Mackenzie,Mackenzie,Mackenzie,Mackenzie,Mackenzie,Mackenzie
bodyparts,Hand,Hand,Finger1,Finger1,Tongue,Tongue,Joystick1,Joystick1,Joystick2,Joystick2
coords,x,y,x,y,x,y,x,y,x,y
labeled-data/reachingvideo1/img005.png,185.25173502386923,610.3622760638572,208.25876751706778,638.481982444433,,,368.02982649761293,463.3729018017556,364.1953210820799,623.1439607823007
labeled-data/reachingvideo1/img020.png,191.64257738309107,615.4749499512345,204.42426210153468,641.0383193881219,586.5966351829987,294.6546635182999,364.1953210820799,462.0947333299112,364.1953210820799,623.1439607823007
labeled-data/reachingvideo1/img023.png,189.08624043940233,610.3622760638572,208.25876751706778,638.481982444433,,,368.02982649761293,462.0947333299112,362.91715261023546,630.8129716133669
labeled-data/reachingvideo1/img028.png,208.25876751706778,556.679200246394,199.3115882141572,597.5805913454135,,,366.7516580257685,463.3729018017556,360.3608156665467,615.4749499512345
labeled-data/reachingvideo1/img031.png,284.94887582772947,476.1545865201992,320.7375930393715,495.3271135978646,,,368.02982649761293,458.2602279143781,366.7516580257685,625.7002977259895
labeled-data/reachingvideo1/img033.png,396.95213682968875,178.16362399056345,409.96003085019504,196.8624716450413,,,364.432401778423,192.79750476363307,365.2453951547046,383.8509481898194
labeled-data/reachingvideo1/img036.png,411.4875545403212,463.3729018017556,467.7269673014731,459.5383963862225,,,359.0826471947024,469.76374416097735,366.7516580257685,624.4221292541451
labeled-data/reachingvideo1/img037.png,444.7199348082746,444.2003747240901,486.89949437913856,446.75671166777886,,,365.4734895539242,465.9292387454443,361.63898413839115,626.9784661978338
labeled-data/reachingvideo1/img038.png,453.04867979312223,152.9608293258325,504.26726249886576,160.27776971236727,,,369.31036203611285,191.9845113873514,366.0583885309863,360.2741402776517
labeled-data/reachingvideo1/img040.png,486.38140822066964,139.13994192904454,553.8598584520461,142.3919154341711,,,365.2453951547046,192.79750476363307,364.432401778423,357.0221667725251
labeled-data/reachingvideo1/img042.png,508.62835840049263,402.02081515322624,576.3712874082438,405.85532056875934,,,365.4734895539242,467.2074072172887,364.1953210820799,630.8129716133669
labeled-data/reachingvideo1/img043.png,529.0790539500025,405.85532056875934,582.7621297674656,400.74264668138187,,,368.02982649761293,467.2074072172887,365.4734895539242,619.3094553667677
labeled-data/reachingvideo1/img046.png,504.26726249886576,128.5710280373832,570.119725977679,139.9529353053262,,,362.8064150258597,191.9845113873514,364.432401778423,343.20127937573716
labeled-data/reachingvideo1/img048.png,485.62132590729414,433.97502694933524,523.966380062625,458.2602279143781,,,361.63898413839115,464.65107027359994,364.1953210820799,624.4221292541451
labeled-data/reachingvideo1/img052.png,438.32909244905284,472.3200811046661,467.7269673014731,495.3271135978646,,,355.24814177916926,469.76374416097735,359.0826471947024,634.6474770289
labeled-data/reachingvideo1/img060.png,403.818543709255,490.21443971048717,407.65304912478814,519.6123145629075,,,346.30096247625875,465.9292387454443,352.69180483548064,637.2038139725887
labeled-data/reachingvideo1/img068.png,373.1425003849904,496.60528206970895,370.58616344130166,520.8904830347518,,,336.0756147015039,468.48557568913304,339.91012011703697,623.1439607823007
labeled-data/reachingvideo1/img071.png,355.24814177916926,496.60528206970895,355.24814177916926,518.3341460910631,,,313.06858220830543,467.2074072172887,320.7375930393715,666.601688825009
labeled-data/reachingvideo1/img075.png,319.4594245675272,496.60528206970895,316.9030876238385,511.9433037318413,,,276.00169652481895,469.76374416097735,284.94887582772947,635.9256455007444
labeled-data/reachingvideo1/img077.png,307.95590832092796,495.3271135978646,302.8432344335505,517.0559776192188,,,265.776348750064,460.81656485806684,279.836201940352,643.5946563318105
labeled-data/reachingvideo1/img080.png,276.00169652481895,496.60528206970895,276.00169652481895,515.7778091473745,592.9874775422205,306.15817976489916,241.49114778502116,463.3729018017556,250.43832708793173,647.4291617473436
labeled-data/reachingvideo1/img087.png,228.70946306657754,497.8834505415533,233.82213695395495,509.38696678815256,603.2128253169755,313.82719059596525,185.25173502386923,473.59824957651045,200.58975668600158,658.9326779939429
labeled-data/reachingvideo1/img090.png,217.28060067144543,213.9353325469558,228.66250793938843,230.1952000725887,604.265447781508,38.3287632701207,179.06991198620813,194.42349151619635,197.76875964068591,374.0950276744397
labeled-data/reachingvideo1/img100.png,231.26580001026628,492.7707766541759,217.2059468199783,500.43978748524205,599.3783199014423,317.6616960114984,182.69539808018055,468.48557568913304,192.92074585493543,661.4890149376315
labeled-data/reachingvideo1/img103.png,224.87495765104444,494.04894512602027,215.92777834813393,502.9961244289308,595.5438144859093,321.49620142703145,182.69539808018055,468.48557568913304,201.86792515784595,664.0453518813204
labeled-data/reachingvideo1/img108.png,238.93481084133242,488.9362712386428,217.2059468199783,496.60528206970895,548.2515810276678,304.88001129305474,187.80807196755796,474.8764180483548,206.9805990452234,684.4960474308301
labeled-data/reachingvideo1/img116.png,239.23142183104977,183.04158424825334,230.2884946919517,195.23648489247796,,,183.13487886761635,193.61049813991468,209.15066690862898,391.98088195263585
labeled-data/reachingvideo1/img118.png,235.10030542579932,458.2602279143781,229.9876315384219,476.1545865201992,,,186.5299034957136,467.2074072172887,203.1460936296903,647.4291617473436
labeled-data/reachingvideo1/img119.png,224.5975410579802,180.6026041194084,236.79244170220488,193.61049813991468,,,188.82583250158785,192.79750476363307,205.08570002722075,368.4040740404682
labeled-data/reachingvideo1/img126.png,237.65664236948805,453.1475540270007,231.26580001026628,472.3200811046661,580.2057928237768,312.54902212412094,198.03341974231284,468.48557568913304,212.09327293260083,666.601688825009
labeled-data/reachingvideo1/img141.png,225.41053443426182,169.2206968514654,234.35346157335994,185.48056437709823,558.737818709736,23.69488249705114,196.14277288812264,180.6026041194084,210.77665366119226,389.5419018237909
labeled-data/reachingvideo1/img142.png,219.76228376366703,441.64403778040145,235.10030542579932,463.3729018017556,,,194.1989143267798,474.8764180483548,210.81510446075646,676.8270365997639
labeled-data/reachingvideo1/img145.png,223.78454768169854,166.78171672262044,235.97944832592322,189.54553125850646,,,194.51678613555936,194.42349151619635,208.3376735323473,398.48482896288897
labeled-data/reachingvideo1/img151.png,229.9876315384219,448.0348801396232,240.2129793131768,468.48557568913304,,,200.58975668600158,479.9890919357323,213.3714414044452,669.1580257686977
labeled-data/reachingvideo1/img152.png,224.5975410579802,168.40770347518372,240.8574085836131,187.91954450594318,,,203.45971327465747,198.48845839760457,210.77665366119226,397.67183558660736
labeled-data/reachingvideo1/img157.png,247.88199014424293,446.75671166777886,251.71649555977604,465.9292387454443,,,212.09327293260083,468.48557568913304,228.70946306657754,669.1580257686977
labeled-data/reachingvideo1/img167.png,239.23142183104977,165.15572997005717,253.8653026041194,187.10655112966157,606.7044279103529,83.85639234189273,211.58964703747387,197.6754650213229,223.78454768169854,408.24074947826875
labeled-data/reachingvideo1/img168.png,241.49114778502116,442.9222062522458,259.38550639084224,460.81656485806684,609.6036676761972,354.7285816949849,208.25876751706778,474.8764180483548,224.87495765104444,681.9397104871414
labeled-data/reachingvideo1/img177.png,244.0474847287099,442.9222062522458,254.27283250346477,460.81656485806684,,,219.76228376366703,485.1017658231097,235.10030542579932,674.2706996560752
labeled-data/reachingvideo1/img179.png,240.8574085836131,165.15572997005717,260.36924961437256,189.54553125850646,,,219.71958080029032,209.87036566554758,235.16645494964155,404.98877597314214
labeled-data/reachingvideo1/img180.png,237.65664236948805,442.9222062522458,259.38550639084224,458.2602279143781,,,218.48411529182266,478.7109234638879,231.26580001026628,670.4361942405421
labeled-data/reachingvideo1/img194.png,269.61085416559706,440.3658693085571,267.0545172219083,464.65107027359994,,,233.82213695395495,468.48557568913304,254.27283250346477,669.1580257686977
labeled-data/reachingvideo1/img201.png,259.38550639084224,437.80953236486835,268.33268569375275,467.2074072172887,605.7691622606641,341.94689697654127,232.54396848211064,468.48557568913304,247.88199014424293,683.2178789589857
labeled-data/reachingvideo1/img211.png,255.55100097530908,435.2531954211796,265.776348750064,455.70389097068943,591.7093090703762,301.0455058775217,235.10030542579932,472.3200811046661,242.76931625686552,687.0523843745189
labeled-data/reachingvideo1/img213.png,252.2393158515561,163.5297432174939,266.06020324834407,186.2935577533799,584.7536067507485,3.370048090009959,227.84951456310677,195.23648489247796,241.67040195989472,400.1108157154523
labeled-data/reachingvideo1/img214.png,244.0474847287099,440.3658693085571,261.941843334531,465.9292387454443,,,235.10030542579932,482.54542887942097,245.32565320055426,680.661542015297
labeled-data/reachingvideo1/img225.png,292.61788665879556,439.0877008367127,281.1143704121963,467.2074072172887,,,254.27283250346477,478.7109234638879,267.0545172219083,676.8270365997639
labeled-data/reachingvideo1/img227.png,,,,,,,292.61788665879556,469.76374416097735,295.1742236024843,662.7671834094759
labeled-data/reachingvideo1/img228.png,245.32565320055426,524.7249884502849,265.776348750064,545.1756839997947,,,310.5122452646167,469.76374416097735,314.34675068014974,664.0453518813204
labeled-data/reachingvideo1/img230.png,204.42426210153468,582.2425696832812,231.26580001026628,606.527770648324,,,365.4734895539242,467.2074072172887,364.1953210820799,666.601688825009
labeled-data/reachingvideo1/img231.png,187.19984574902458,330.19338535523093,218.9065874240087,348.8922330097087,,,395.3261500771255,187.91954450594318,389.635196443154,407.4277561019871
labeled-data/reachingvideo1/img234.png,173.74821877726998,629.5348031415226,192.92074585493543,655.0981725784097,,,417.87839689954296,473.59824957651045,412.7657230121656,667.8798572968534
labeled-data/reachingvideo1/img237.png,162.81004446057523,343.20127937573716,184.76086562017963,371.65604754559473,,,355.4894746393249,196.8624716450413,357.1154613918882,380.59897468469285
labeled-data/reachingvideo1/img240.png,159.688365586982,615.4749499512345,178.86089266464745,649.9854986910323,,,337.35378317334823,472.3200811046661,334.7974462296595,671.7143627123864
labeled-data/reachingvideo1/img245.png,162.24470253067074,620.587623838612,189.08624043940233,649.9854986910323,594.2656460140648,290.82015810276675,371.864331913146,472.3200811046661,369.30799496945724,664.0453518813204


--- File: examples/SUPERANIMAL/video_adapt_example.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Script to run video adaptation"""
import deeplabcut.modelzoo.video_inference as modelzoo


def main():
    modelzoo.video_inference_superanimal(
        videos=["/mnt/md0/shaokai/tom_video.mp4"],
        superanimal_name="superanimal_topviewmouse",
        model_name="hrnet_w32",
        detector_name="fasterrcnn_resnet50_fpn_v2",
        video_adapt=True,
        max_individuals=3,
        pseudo_threshold=0.1,
        bbox_threshold=0.9,
        detector_epochs=1,
        pose_epochs=1,
    )


if __name__ == "__main__":
    main()


--- File: examples/SUPERANIMAL/keypoint_space_conversion.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Script to convert a dataset for its keypoint space to match the SuperAnimal space"""
from pathlib import Path

from deeplabcut.modelzoo.generalized_data_converter.datasets import COCOPoseDataset
from deeplabcut.utils.auxiliaryfunctions import get_deeplabcut_path


def main():
    src_proj_root = Path("/media/data/trimouse_coco_original_shuffle0")
    conversion_table_path = (
        Path(get_deeplabcut_path())
        / "modelzoo"
        / "conversion_tables"
        / "conversion_table_topview.csv"
    )
    dataset = COCOPoseDataset(str(src_proj_root), "trimouse")
    dataset.project_with_conversion_table(conversion_table_path)
    dataset.materialize(
        src_proj_root.with_name("trimouse_coco_superanimal_shuffle0_shallow_copy"),
        deepcopy=False,
        framework="coco",
    )


if __name__ == "__main__":
    main()


--- File: examples/SUPERANIMAL/eval_zeroshot.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""SuperAnimal model zero-shot evaluation"""
from __future__ import annotations

from pathlib import Path

import torch

import deeplabcut.utils.auxiliaryfunctions as af
from deeplabcut.generate_training_dataset import TrainingDatasetMetadata
from deeplabcut.modelzoo import build_weight_init
from deeplabcut.pose_estimation_pytorch import DLCLoader
from deeplabcut.pose_estimation_pytorch.apis.evaluation import evaluate_snapshot
from deeplabcut.pose_estimation_pytorch.models import PoseModel
from deeplabcut.pose_estimation_pytorch.runners.snapshots import Snapshot


def main(
    config_path: Path,
    super_animal: str,
    shuffle_index: int,
    device: str,
    super_animal_model: str = "hrnet_w32",
    super_animal_detector: str = "fasterrcnn_resnet50_fpn_v2",
):
    metadata = TrainingDatasetMetadata.load(config_path, load_splits=True)
    shuffles = [s for s in metadata.shuffles if s.index == shuffle_index]
    if len(shuffles) != 1:
        raise ValueError(
            "Found multiple shuffles with different train indices but the same index "
            f"({shuffles}). To run this benchmark, there should only be one such "
            "shuffle."
        )

    shuffle = shuffles[0]
    print(f"Training shuffle: {shuffle.name}")
    print(f"  index: {shuffle.index}")
    print(f"  train fraction: {shuffle.train_fraction}")
    print(f"  train indices: {shuffle.split.train_indices}")
    print(f"  test indices: {shuffle.split.test_indices}")
    print()

    # edit config to have the desired training fraction
    af.edit_config(str(config_path), {"TrainingFraction": [shuffle.train_fraction]})

    # Load the config and create a data loader
    cfg = af.read_config(str(config_path))
    loader = DLCLoader(
        config=Path(cfg["project_path"]) / "config.yaml",
        shuffle=shuffle.index,
        trainset_index=0,
        modelprefix="",
    )
    loader.evaluation_folder.mkdir(exist_ok=True, parents=True)
    loader.model_cfg["device"] = device

    # Build the pose model
    model = PoseModel.build(
        loader.model_cfg["model"],
        weight_init=build_weight_init(
            cfg=cfg,
            super_animal=super_animal,
            model_name=super_animal_model,
            detector_name=super_animal_detector,
            with_decoder=True,
        ),
    )

    # Save the zero-shot snapshot
    state_dict = {
        "model": model.state_dict(),
        "metadata": {
            "epoch": 0,
            "metrics": {},
            "losses": {},
        },
    }
    snapshot_path = loader.model_folder / "zero-shot.pt"
    torch.save(state_dict, snapshot_path)

    # Evaluate the snapshot
    evaluate_snapshot(
        loader=loader,
        cfg=cfg,
        scorer=f"{super_animal}-zero-shot",
        snapshot=Snapshot(best=False, epochs=0, path=snapshot_path),
        transform=None,
        plotting=True,
        show_errors=True,
        detector_snapshot=None,
    )


if __name__ == "__main__":
    DATA = Path("/home/niels/datasets/superanimal")
    CONFIG_PATH = DATA / "openfield-Pranav-2018-08-20" / "config.yaml"
    SUPER_ANIMAL = "superanimal_topviewmouse"
    main(
        config_path=CONFIG_PATH,
        super_animal=SUPER_ANIMAL,
        shuffle_index=1001,
        device="cuda",
    )


--- File: examples/SUPERANIMAL/superanimal_image_inference.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
from deeplabcut.pose_estimation_pytorch.apis.analyze_images import (
    superanimal_analyze_images,
)


if __name__ == "__main__":
    superanimal_name = "superanimal_quadruped"
    model_name = "hrnet_w32"
    detector_name = "fasterrcnn_resnet50_fpn_v2"
    device = "cuda"
    max_individuals = 3

    ret = superanimal_analyze_images(
        superanimal_name,
        model_name,
        detector_name,
        "test_rodent_images",
        max_individuals,
        "vis_test_rodent_images",
    )


--- File: examples/SUPERANIMAL/memory_replay_example.py ---
#
# DeepLabCut Toolbox (deeplabcut.org)
# © A. & M.W. Mathis Labs
# https://github.com/DeepLabCut/DeepLabCut
#
# Please see AUTHORS for contributors.
# https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS
#
# Licensed under GNU Lesser General Public License v3.0
#
"""Script to fine-tune a SuperAnimal model with memory replay"""
from pathlib import Path

import deeplabcut
from deeplabcut.core.engine import Engine
from deeplabcut.core.weight_init import WeightInitialization
from deeplabcut.modelzoo.utils import (
    create_conversion_table,
    read_conversion_table_from_csv,
)
from deeplabcut.pose_estimation_pytorch.modelzoo.utils import (
    get_super_animal_snapshot_path,
)
from deeplabcut.utils.pseudo_label import keypoint_matching


def main(
    dlc_proj_root: Path,
    super_animal_name: str,
    super_animal_model: str = "hrnet_w32",
    super_animal_detector: str = "fasterrcnn_resnet50_fpn_v2",
):
    config_path = str(dlc_proj_root / "config.yaml")
    shuffle = 0
    device = "cuda"

    # keypoint matching before create training dataset
    # keypoint matching creates pseudo prediction and a conversion table
    keypoint_matching(
        config_path,
        super_animal_name,
        super_animal_model,
        super_animal_detector,
    )

    # keypoint matching creates a memory_replay folder in the root. The conversion table
    # can be read from there
    conversion_table_path = dlc_proj_root / "memory_replay" / "conversion_table.csv"

    table = create_conversion_table(
        config=config_path,
        super_animal=super_animal_name,
        project_to_super_animal=read_conversion_table_from_csv(conversion_table_path),
    )

    weight_init = WeightInitialization(
        dataset=super_animal_name,
        snapshot_path=get_super_animal_snapshot_path(
            dataset=super_animal_name,
            model_name=super_animal_model,
            download=True,
        ),
        detector_snapshot_path=get_super_animal_snapshot_path(
            dataset=super_animal_name,
            model_name=super_animal_detector,
            download=True,
        ),
        conversion_array=table.to_array(),
        with_decoder=True,
    )

    deeplabcut.create_training_dataset(
        config_path,
        Shuffles=[shuffle],
        net_type="top_down_hrnet_w32",
        weight_init=weight_init,
        engine=Engine.PYTORCH,
        userfeedback=False,
    )

    # passing pose_threshold controls the behavior of memory replay. We discard
    # predictions that are lower than the threshold
    deeplabcut.train_network(
        config_path, shuffle=shuffle, device=device, pose_threshold=0.1
    )


if __name__ == "__main__":
    main(
        dlc_proj_root=Path("/media/data/myproject"),
        super_animal_name="superanimal_topviewmouse",
    )


--- File: examples/openfield-Pranav-2018-10-30/config.yaml ---
# Project definitions (do not edit)
Task: openfield
scorer: Pranav
date: Oct30
multianimalproject:
identity:


# Project path (change when moving around)
project_path: WILL BE AUTOMATICALLY UPDATED BY DEMO CODE

# Default DeepLabCut engine to use for shuffle creation (either pytorch or tensorflow)
engine: pytorch


# Annotation data set configuration (and individual video cropping parameters)
video_sets:
  WILL BE AUTOMATICALLY UPDATED BY DEMO CODE:
    crop: 0, 640, 0, 480
bodyparts:
- snout
- leftear
- rightear
- tailbase


# Fraction of video to start/stop when extracting frames for labeling/refinement
start: 0
stop: 1
numframes2pick: 20


# Plotting configuration
skeleton: []
skeleton_color: black
pcutoff: 0.4
dotsize: 8
alphavalue: 0.7
colormap: jet


# Training,Evaluation and Analysis configuration
TrainingFraction:
- 0.95
iteration: 0
default_net_type: resnet_50
default_augmenter: imgaug
snapshotindex: -1
detector_snapshotindex: -1
batch_size: 4
detector_batch_size: 1


# Cropping Parameters (for analysis and outlier frame detection)
cropping: false
#if cropping is true for analysis, then set the values here:
x1: 0
x2: 640
y1: 277
y2: 624


# Refinement configuration (parameters from annotation dataset configuration also relevant in this stage)
corner2move2:
- 50
- 50
move2corner: true


# Conversion tables to fine-tune SuperAnimal weights
SuperAnimalConversionTables:
  superanimal_topviewmouse:
    snout: nose
    leftear: left_ear
    rightear: right_ear
    tailbase: tail_base


--- File: examples/openfield-Pranav-2018-10-30/labeled-data/m4s1/CollectedData_Pranav.csv ---
scorer,Pranav,Pranav,Pranav,Pranav,Pranav,Pranav,Pranav,Pranav
bodyparts,snout,snout,leftear,leftear,rightear,rightear,tailbase,tailbase
coords,x,y,x,y,x,y,x,y
labeled-data/m4s1/img0000.png,21.521,265.428,33.819,265.941,19.984,250.05599999999998,87.11,152.69799999999998
labeled-data/m4s1/img0001.png,10.248,288.487,19.984,297.198,12.298,281.313,95.821,221.361
labeled-data/m4s1/img0002.png,24.596,354.075,38.431,354.075,23.058000000000003,337.166,99.40799999999999,256.205
labeled-data/m4s1/img0003.png,73.78699999999999,374.57199999999995,78.911,366.37300000000005,57.39,361.76099999999997,106.581,270.04
labeled-data/m4s1/img0004.png,38.431,333.066,50.729,341.777,39.968,323.33099999999996,131.17700000000002,273.627
labeled-data/m4s1/img0005.png,23.570999999999998,327.43,30.745,339.215,28.183000000000003,321.793,131.17700000000002,318.719
labeled-data/m4s1/img0006.png,42.53,332.042,48.167,337.67800000000005,46.629,323.33099999999996,147.57399999999998,316.157
labeled-data/m4s1/img0007.png,63.026,333.066,70.71300000000001,341.265,67.638,324.868,173.707,316.66900000000004
labeled-data/m4s1/img0008.png,95.821,334.60400000000004,104.01899999999999,343.827,100.432,326.405,209.063,317.694
labeled-data/m4s1/img0009.png,113.755,318.207,120.416,332.042,122.978,313.082,235.709,320.256
labeled-data/m4s1/img0010.png,54.828,339.728,64.051,348.439,59.44,329.48,177.294,326.405
labeled-data/m4s1/img0011.png,32.282,383.795,40.993,388.407,30.745,370.472,134.764,331.017
labeled-data/m4s1/img0012.png,28.183000000000003,440.16,39.968,441.69699999999995,27.67,426.837,109.14299999999999,358.175
labeled-data/m4s1/img0013.png,92.74600000000001,460.657,93.771,449.384,73.78699999999999,445.284,125.54,358.68699999999995
labeled-data/m4s1/img0014.png,24.083000000000002,423.251,35.356,431.449,28.695,412.49,130.665,390.45599999999996
labeled-data/m4s1/img0015.png,24.083000000000002,395.06800000000004,32.282,406.341,29.206999999999997,386.35699999999997,136.30100000000002,405.316
labeled-data/m4s1/img0016.png,28.183000000000003,430.42400000000004,37.918,439.648,30.232,422.226,137.838,418.639
labeled-data/m4s1/img0017.png,49.191,430.93699999999995,57.39,440.673,50.216,425.81300000000005,156.797,413.515
labeled-data/m4s1/img0018.png,66.101,433.499,74.29899999999999,444.772,68.15100000000001,425.81300000000005,176.78099999999998,415.05199999999996
labeled-data/m4s1/img0019.png,86.085,435.036,95.821,444.772,89.15899999999999,426.837,200.352,417.102
labeled-data/m4s1/img0020.png,103.507,446.82099999999997,115.292,451.94599999999997,106.581,436.061,218.287,417.61400000000003
labeled-data/m4s1/img0021.png,53.803000000000004,437.086,64.564,448.35900000000004,58.927,428.375,175.24400000000003,425.3
labeled-data/m4s1/img0022.png,26.645,435.548,36.381,448.871,32.794000000000004,428.375,147.062,441.69699999999995
labeled-data/m4s1/img0023.png,29.206999999999997,407.87800000000004,34.330999999999996,424.275,39.968,405.829,146.037,448.35900000000004
labeled-data/m4s1/img0024.png,22.546,419.151,29.206999999999997,427.35,26.645,410.44,136.813,437.086
labeled-data/m4s1/img0025.png,18.959,425.3,26.645,437.086,23.570999999999998,416.077,135.276,431.962
labeled-data/m4s1/img0026.png,25.108,443.235,31.769000000000002,448.871,28.695,430.93699999999995,135.789,438.62300000000005
labeled-data/m4s1/img0027.png,41.505,413.515,45.092,432.474,49.70399999999999,415.05199999999996,159.359,442.722
labeled-data/m4s1/img0028.png,17.422,399.167,26.645,415.05199999999996,29.72,398.655,133.739,437.086
labeled-data/m4s1/img0029.png,30.745,428.88699999999994,37.406,438.62300000000005,34.844,419.664,142.96200000000002,430.42400000000004
labeled-data/m4s1/img0030.png,55.853,434.524,61.489,444.259,57.39,428.88699999999994,166.533,430.42400000000004
labeled-data/m4s1/img0031.png,81.986,437.598,89.15899999999999,445.284,84.035,429.912,195.22799999999998,430.93699999999995
labeled-data/m4s1/img0032.png,98.895,448.35900000000004,109.656,452.97,100.945,437.598,217.262,429.912
labeled-data/m4s1/img0033.png,43.555,442.21,51.753,449.384,47.653999999999996,432.474,162.434,442.722
labeled-data/m4s1/img0034.png,25.62,437.086,33.306999999999995,446.309,30.745,428.88699999999994,144.5,438.62300000000005
labeled-data/m4s1/img0035.png,40.993,419.664,38.943000000000005,445.79699999999997,41.505,425.81300000000005,149.624,446.309
labeled-data/m4s1/img0036.png,57.902,421.20099999999996,57.902,446.309,57.39,428.375,172.17,447.846
labeled-data/m4s1/img0037.png,78.911,420.176,79.936,446.82099999999997,79.936,428.375,196.765,447.334
labeled-data/m4s1/img0038.png,105.044,424.275,109.656,448.871,115.805,427.35,229.047,449.89599999999996
labeled-data/m4s1/img0039.png,77.88600000000001,428.88699999999994,87.62200000000001,445.284,87.62200000000001,427.86199999999997,209.576,450.408
labeled-data/m4s1/img0040.png,50.216,381.233,50.216,405.316,64.051,384.30699999999996,159.872,448.35900000000004
labeled-data/m4s1/img0041.png,71.737,307.95799999999997,51.753,333.066,71.225,329.48,119.904,426.325
labeled-data/m4s1/img0042.png,73.78699999999999,203.42700000000002,56.878,223.923,74.29899999999999,220.33599999999998,92.74600000000001,341.265
labeled-data/m4s1/img0043.png,96.333,104.01899999999999,74.29899999999999,118.367,95.30799999999999,116.829,100.432,234.171
labeled-data/m4s1/img0044.png,71.737,89.15899999999999,70.71300000000001,102.48200000000001,83.523,88.135,111.705,198.815
labeled-data/m4s1/img0045.png,38.431,72.25,41.505,81.986,48.167,66.101,129.127,140.4
labeled-data/m4s1/img0046.png,17.422,66.613,22.034000000000002,80.44800000000001,28.183000000000003,60.977,120.929,106.581
labeled-data/m4s1/img0047.png,24.083000000000002,64.051,29.206999999999997,78.911,34.844,59.44,128.102,97.35799999999999
labeled-data/m4s1/img0048.png,19.984,73.78699999999999,21.521,85.572,31.256999999999998,65.07600000000001,125.02799999999999,89.67200000000001
labeled-data/m4s1/img0049.png,23.058000000000003,78.399,28.183000000000003,89.67200000000001,34.330999999999996,71.225,134.251,90.697
labeled-data/m4s1/img0050.png,24.596,92.234,31.256999999999998,102.994,32.794000000000004,79.936,146.549,83.01
labeled-data/m4s1/img0051.png,19.472,79.42399999999999,27.158,91.721,29.206999999999997,72.25,132.714,82.49799999999999
labeled-data/m4s1/img0052.png,18.959,73.275,26.133000000000003,84.035,33.306999999999995,63.538999999999994,134.764,84.035
labeled-data/m4s1/img0053.png,27.158,73.78699999999999,29.206999999999997,85.06,34.844,69.175,143.475,88.135
labeled-data/m4s1/img0054.png,57.902,73.78699999999999,59.952,83.523,65.07600000000001,67.638,177.294,84.54799999999999
labeled-data/m4s1/img0055.png,84.54799999999999,70.71300000000001,88.135,81.473,93.771,63.026,207.014,84.54799999999999
labeled-data/m4s1/img0056.png,92.234,73.275,97.87,80.44800000000001,100.945,63.538999999999994,210.6,85.06
labeled-data/m4s1/img0057.png,58.415,72.762,63.026,83.01,68.15100000000001,63.026,174.732,76.861
labeled-data/m4s1/img0058.png,81.473,116.829,90.697,112.21799999999999,77.88600000000001,93.771,182.93,83.01
labeled-data/m4s1/img0059.png,40.993,140.91299999999998,52.778,143.475,45.092,126.565,153.72299999999998,83.523
labeled-data/m4s1/img0060.png,32.282,207.014,43.555,202.91400000000002,27.158,187.542,109.14299999999999,112.73
labeled-data/m4s1/img0061.png,30.745,293.611,41.505,286.437,24.083000000000002,274.652,75.837,168.583
labeled-data/m4s1/img0062.png,51.753,354.075,65.58800000000001,349.464,45.092,336.653,87.11,228.02200000000002
labeled-data/m4s1/img0063.png,43.555,387.38199999999995,54.828,386.869,38.431,376.621,94.796,281.825
labeled-data/m4s1/img0064.png,117.34200000000001,407.87800000000004,111.705,398.655,91.721,405.829,100.945,294.123
labeled-data/m4s1/img0065.png,87.11,429.912,97.35799999999999,427.86199999999997,81.986,418.639,125.02799999999999,306.934
labeled-data/m4s1/img0066.png,29.206999999999997,422.226,39.455999999999996,426.837,31.769000000000002,411.465,130.665,373.54699999999997
labeled-data/m4s1/img0067.png,50.216,318.207,37.918,344.339,55.853,330.504,135.276,408.903
labeled-data/m4s1/img0068.png,59.44,217.262,47.141999999999996,238.271,62.001999999999995,228.535,92.74600000000001,348.439
labeled-data/m4s1/img0069.png,52.778,107.094,39.455999999999996,126.565,54.315,116.829,78.399,234.171
labeled-data/m4s1/img0070.png,76.34899999999999,71.225,62.001999999999995,83.523,80.961,76.34899999999999,100.945,180.368
labeled-data/m4s1/img0071.png,91.721,65.58800000000001,82.49799999999999,75.837,99.40799999999999,70.71300000000001,117.854,176.269
labeled-data/m4s1/img0072.png,108.631,64.564,100.432,75.837,119.39200000000001,69.688,136.30100000000002,175.757
labeled-data/m4s1/img0073.png,132.714,61.489,121.954,72.25,143.987,66.613,161.409,175.24400000000003
labeled-data/m4s1/img0074.png,163.459,62.513999999999996,150.136,71.225,171.145,67.638,186.517,174.732
labeled-data/m4s1/img0075.png,152.69799999999998,62.513999999999996,149.111,76.861,167.046,64.564,214.7,164.484
labeled-data/m4s1/img0076.png,113.243,60.977,116.829,76.34899999999999,128.615,56.365,215.725,121.441
labeled-data/m4s1/img0077.png,54.828,74.812,60.977,83.01,67.126,66.101,171.145,91.721
labeled-data/m4s1/img0078.png,23.058000000000003,94.796,29.72,102.48200000000001,30.232,84.035,136.813,85.572
labeled-data/m4s1/img0079.png,18.447,80.961,23.570999999999998,91.721,27.158,71.225,126.053,85.06
labeled-data/m4s1/img0080.png,26.133000000000003,143.475,36.894,139.376,26.645,120.416,125.02799999999999,80.961
labeled-data/m4s1/img0081.png,25.108,194.71599999999998,36.894,195.74099999999999,25.62,176.269,112.21799999999999,104.01899999999999
labeled-data/m4s1/img0082.png,15.372,177.294,29.206999999999997,186.005,23.058000000000003,163.459,120.929,131.689
labeled-data/m4s1/img0083.png,35.869,177.294,43.555,183.955,39.968,166.021,143.475,133.739
labeled-data/m4s1/img0084.png,57.902,175.757,68.15100000000001,183.955,64.564,162.946,169.095,134.764
labeled-data/m4s1/img0085.png,81.986,176.269,90.184,182.418,87.62200000000001,163.459,196.765,134.251
labeled-data/m4s1/img0086.png,126.053,193.179,134.251,189.592,119.904,173.707,223.41099999999997,135.276
labeled-data/m4s1/img0087.png,95.821,184.468,105.044,192.15400000000002,102.48200000000001,171.657,221.361,153.211
labeled-data/m4s1/img0088.png,49.191,166.533,52.778,175.24400000000003,56.878,157.31,173.195,167.046
labeled-data/m4s1/img0089.png,23.570999999999998,113.755,29.206999999999997,122.978,33.819,106.581,133.227,158.335
labeled-data/m4s1/img0090.png,23.570999999999998,161.922,30.232,165.50799999999998,29.206999999999997,145.524,134.764,137.326
labeled-data/m4s1/img0091.png,21.009,259.79200000000003,35.869,255.69299999999998,16.91,241.345,87.62200000000001,151.161
labeled-data/m4s1/img0092.png,24.596,346.902,36.894,343.827,19.472,331.017,72.25,229.56
labeled-data/m4s1/img0093.png,19.472,390.969,32.282,393.01800000000003,19.472,380.208,81.473,295.661
labeled-data/m4s1/img0094.png,28.183000000000003,410.44,36.381,412.49,27.67,400.705,105.556,339.215
labeled-data/m4s1/img0095.png,22.546,426.837,35.869,429.912,23.570999999999998,418.639,105.556,356.125
labeled-data/m4s1/img0096.png,47.141999999999996,446.82099999999997,57.902,447.334,45.604,435.036,129.127,385.845
labeled-data/m4s1/img0097.png,49.191,406.341,55.853,436.57300000000004,56.365,413.00199999999995,150.649,394.556
labeled-data/m4s1/img0098.png,23.058000000000003,386.869,29.72,411.465,33.819,389.43199999999996,134.764,424.275
labeled-data/m4s1/img0099.png,42.018,388.91900000000004,44.58,406.853,49.191,387.38199999999995,149.624,425.81300000000005
labeled-data/m4s1/img0100.png,72.25,391.481,72.25,409.416,86.085,387.894,179.343,424.788
labeled-data/m4s1/img0101.png,94.796,388.91900000000004,96.333,408.903,103.507,391.994,204.96400000000003,425.81300000000005
labeled-data/m4s1/img0102.png,95.30799999999999,406.341,104.01899999999999,399.167,105.044,419.151,219.31099999999998,426.837
labeled-data/m4s1/img0103.png,86.59700000000001,333.579,75.837,362.274,88.135,346.902,176.269,413.515
labeled-data/m4s1/img0104.png,44.067,259.79200000000003,42.018,278.751,53.291000000000004,261.329,122.978,360.73699999999997
labeled-data/m4s1/img0105.png,59.44,170.12,41.505,191.641,57.902,180.368,96.333,293.098
labeled-data/m4s1/img0106.png,79.42399999999999,90.184,58.415,100.945,79.42399999999999,97.35799999999999,80.44800000000001,212.65
labeled-data/m4s1/img0107.png,145.524,90.184,117.854,94.28299999999999,139.376,99.40799999999999,108.118,208.551
labeled-data/m4s1/img0108.png,232.63400000000001,75.324,204.96400000000003,81.473,225.46,97.87,143.987,187.542
labeled-data/m4s1/img0109.png,139.376,84.035,138.863,97.87,154.748,85.572,180.88099999999997,192.15400000000002
labeled-data/m4s1/img0110.png,76.34899999999999,75.837,81.473,91.721,91.721,73.78699999999999,189.592,132.714
labeled-data/m4s1/img0111.png,30.745,65.58800000000001,35.356,79.42399999999999,42.53,60.977,138.863,97.87
labeled-data/m4s1/img0112.png,29.206999999999997,104.53200000000001,36.894,109.14299999999999,36.894,89.67200000000001,143.987,89.15899999999999
labeled-data/m4s1/img0113.png,30.232,188.567,39.968,181.393,27.67,167.046,112.21799999999999,96.845
labeled-data/m4s1/img0114.png,26.645,262.35400000000004,38.431,262.35400000000004,24.596,248.519,84.54799999999999,155.773
labeled-data/m4s1/img0115.png,65.58800000000001,321.281,72.25,312.058,52.778,306.934,92.74600000000001,192.15400000000002


--- File: .github/FUNDING.yml ---
# These are supported funding model platforms

github: DeepLabCut


--- File: .github/workflows/publish-book.yml ---
name: publish-book

on:
  push:
    branches:
    - main

jobs:
  deploy-book:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install .[docs]
        pip install jupyter-book sphinxcontrib-mermaid

    - name: Build the book
      run: |
        jupyter-book build .

    - name: GitHub Pages action
      uses: peaceiris/actions-gh-pages@v3.9.3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./_build/html


--- File: .github/workflows/codespell.yml ---
---
name: Codespell

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  codespell:
    name: Check for spelling errors
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v3
      - name: Annotate locations with typos
        uses: codespell-project/codespell-problem-matcher@v1
      - name: Codespell
        uses: codespell-project/actions-codespell@v2


--- File: .github/workflows/python-package.yml ---
name: Python package

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ${{ matrix.os }}

    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-13, windows-latest]
        python-version: ["3.10"]
        include:
          - os: ubuntu-latest
            path: ~/.cache/pip
          - os: macos-13
            path: ~/Library/Caches/pip
          - os: windows-latest
            path: ~\AppData\Local\pip\Cache
        exclude:
          - os: macos-13
            python-version: 3.7
          - os: windows-latest
            python-version: 3.7

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      - name: Install ffmpeg
        run: |
          if [ "$RUNNER_OS" == "Linux" ]; then
            sudo apt-get update
            sudo apt-get install ffmpeg
          elif [ "$RUNNER_OS" == "macOS" ]; then
            brew install ffmpeg || true
          else
            choco install ffmpeg
          fi
        shell: bash

      - name: Run pytest tests
        run: |
          pip install pytest
          python -m pytest

      - name: Run functional tests
        run: |
          pip install git+https://github.com/${{ github.repository }}.git@${{ github.sha }}
          python examples/testscript.py
          python examples/testscript_multianimal.py
          python examples/testscript_pytorch_single_animal.py
          python examples/testscript_pytorch_multi_animal.py


--- File: .github/ISSUE_TEMPLATE/bug_report.yml ---
name: Bug Report
description: File a bug report to help us improve
assignees:
  - n-poulsen
body:
  - type: markdown
    attributes:
      value: |
        Thanks for opening this issue, and thanks for using DeepLabCut (we hope you are enjoying it ☺️)
  - type: checkboxes
    attributes:
      label: Is there an existing issue for this?
      description: Please search to see if an issue already exists for the bug you encountered. Remove `is:open` from the search field to search closed (solved) issues.
      options:
        - label: I have searched the existing issues
          required: true
  - type: textarea
    id: what-happened
    attributes:
      label: Bug description
      description: Also tell us concisely what you expected to happen
      placeholder: What happened?
    validations:
      required: true
  - type: textarea
    attributes:
      label: Operating System
      description: What operating system are you using?
      placeholder: macOS Big Sur
      value: operating system
    validations:
      required: true
  - type: textarea
    attributes:
      label: DeepLabCut version
      description: What version of DLC are you using? Please check with `import deeplabcut`, `deeplabcut.__version__`
      placeholder: 2.2rc3
      value: dlc version
    validations:
      required: true
  - type: dropdown
    id: dlcmode
    attributes:
      label: DeepLabCut mode
      options:
        - single animal
        - multi animal
        - 3d
    validations:
      required: true
  - type: textarea
    attributes:
      label: Device type
      description: What GPU/CPU are you using?
      placeholder: GeForce 2080 RTX
      value: gpu
    validations:
      required: true
  - type: textarea
    attributes:
      label: Steps To Reproduce
      description: Steps to reproduce the behavior.
      placeholder: |
        1. In this environment...
        2. With this config...
        3. Run '...'
        4. See error...
    validations:
      required: false
  - type: textarea
    id: logs
    attributes:
      label: Relevant log output
      description: Please copy and paste any relevant log output. This will be automatically formatted into code, so no need for backticks.
      render: shell
  - type: textarea
    attributes:
      label: Anything else?
      description: |
        Links? References? Anything that will give us more context about the issue you are encountering!

        Tip: You can attach images and other files by clicking this area to highlight it and then dragging files in.
    validations:
      required: false
  - type: checkboxes
    attributes:
      label: Code of Conduct
      description: The Code of Conduct helps create a safe space for everyone. We require
        that everyone agrees to it.
      options:
        - label: I agree to follow this project's [Code of Conduct](https://github.com/DeepLabCut/DeepLabCut/blob/master/CODE_OF_CONDUCT.md)
          required: true
  - type: markdown
    attributes:
      value: "Thanks for completing our bug report!"


--- File: .github/ISSUE_TEMPLATE/feature_request.md ---
---
name: Feature request
about: Suggest an idea for this project

---

**Is your feature request related to a problem? Please describe.**
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]

**Describe the solution you'd like**
A clear and concise description of what you want to happen.

**Describe alternatives you've considered**
A clear and concise description of any alternative solutions or features you've considered.

**Additional context**
Add any other context or screenshots about the feature request here.


--- File: .github/ISSUE_TEMPLATE/config.yml ---
blank_issues_enabled: false
contact_links:
- name: deeplabcut forum
  url: https://forum.image.sc/tag/deeplabcut
  about: Please ask general questions here
- name: deeplabcut gitter
  url: https://gitter.im/DeepLabCut/community
  about: Chat with the community


--- File: .circleci/config.yml ---
version: 2.1

orbs:
  python: circleci/python@0.2.1

jobs:
  build-and-test:
    working_directory: ~/circleci-demo-python-django
    docker:
      - image: circleci/python:3.10  # primary container for the build job
        auth:
          username: mydockerhub-user
          password: $DOCKERHUB_PASSWORD  # context / project UI env-var reference
    steps:
      - checkout
      - python/load-cache
      - python/install-deps
      - python/save-cache
      - run:
          command: python testscript_cli.py
          name: TestDLC

workflows:
  main:
    jobs:
      - build-and-test


