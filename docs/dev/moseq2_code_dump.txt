--- File: .flake8 ---
[flake8]
max-line-length = 110


--- File: LICENSE.md ---
# Non-Commercial Research and Academic Use Software License and Terms of Use


MoSeq is a software package that includes original code created by the Harvard researchers listed below (the “Software”), and third-party code that may be obtained by End Users separately. The Software is designed to label behaviors identified from 3D pose dynamics data through the use of computational modeling and fitting approaches. The Software was developed by Alexander Wiltschko, Ph.D., Sandeep Robert Datta, Ph.D., and Matthew J. Johnson, Ph.D. at Harvard University. It is distributed for free academic and non-commercial research use by the President and Fellows of Harvard College (“Harvard”).

Using the Software indicates your agreement to be bound by the terms of this Software Use Agreement (“Agreement”). Absent your agreement to the terms below, you (the “End User”) have no rights to hold or use the Software whatsoever.

Harvard agrees to grant hereunder a limited non-exclusive license to End User for the use of the Software in the performance of End User’s internal, non-commercial research and academic use at End User’s academic or not-for-profit research institution (“Institution”) on the following terms and conditions:

1. **NO COMMERCIAL USE.** End User shall not use the Software for Commercial use and any such use of the Software is expressly prohibited. “Commercial use” includes, but is not limited to, (i) use of the Software in fee-for-service arrangements, (ii) use of the Software by core facilities or laboratories to provide research services to (or in collaboration with) for-profit third parties for a fee, and (iii) use of the Software in industry-sponsored and/or collaborative research projects in which any commercial rights are granted to the sponsor or collaborator. If End User wishes to use the Software for Commercial use, End User must execute a separate license agreement with Harvard.

_Requests for use of the Software for Commercial use, please contact:_

Office of Technology Development
Harvard University
Smith Campus Center, Suite 727E
1350 Massachusetts Avenue Cambridge, MA 02138 USA Telephone: (617) 495-3067
Facsimile: (617) 495-9568
E-mail: otd@harvard.edu

2. **OWNERSHIP AND COPYRIGHT NOTICE.** Harvard owns all intellectual property in the Software. End User shall gain no ownership to the Software.  End User shall not remove or delete, and shall retain in the Software (including in any modifications to the Software and in any Derivative Works), the copyright, trademark, or other notices pertaining to Software as provided with the Software.

3. **DERIVATIVE WORKS.** End User may create use and distribute Derivative Works, as such term is defined under U.S. copyright laws, provided that any such Derivative Works shall be restricted to non-commercial research and academic use. End User may not distribute Derivative Works to any for-profit third parties for Commercial use.

4. **FEEDBACK.** In order to improve the Software, comments from End Users may be useful. End User agrees to provide Harvard with feedback on the End User’s use of the Software (e.g., any bugs in the Software, the user experience, etc.).  Harvard is permitted to use such information provided by End User in making changes and improvements to the Software without compensation or accounting to End User.

5. **NON ASSERT.** End User acknowledges that Harvard may develop modifications to the Software that may be based on the feedback provided by End User under Section 5 above. Harvard shall not be restricted in any way by End User regarding its use of such information.  End User acknowledges the right of Harvard to prepare, publish, display, reproduce, transmit and or use modifications to the Software that may be substantially similar or functionally equivalent to End User’s modifications and/or improvements if any.  In the event that End User obtains patent protection for any modification or improvement to Software, End User agrees not to allege or enjoin infringement of End User’s patent against Harvard, or any of the researchers, medical or research staff, officers, directors and employees of those institutions.

6. **PUBLICATION & ATTRIBUTION.** End User has the right to publish, present, or share results from the use of the Software.  In accordance with customary academic practice, End User will acknowledge Harvard as the provider of the Software and may cite the relevant reference(s) from the following list of publications:

Wiltschko AB, Johnson MJ, Iurilli G, Peterson RE, Katon JE, Pashkovski SL, Abraira VE, Adams RP and Datta SR. (2015). Mapping sub-second Structure in Behavior. Neuron, 88:1121.

Markowitz JE, Gillis WF, Beron CC, Neufeld SQ, Robertson K, Bhagat ND, Peterson RE, Peterson E, Hyun M, Linderman SW, Sabatini BL and Datta SR. (2018). The striatum organizes 3D behavior via moment-to-moment action selection. Cell, in press.

7.** NO WARRANTIES.** THE SOFTWARE IS PROVIDED "AS IS." TO THE FULLEST EXTENT PERMITTED BY LAW, HARVARD HEREBY DISCLAIMS ALL WARRANTIES OF ANY KIND (EXPRESS, IMPLIED OR OTHERWISE) REGARDING THE SOFTWARE, INCLUDING BUT NOT LIMITED TO ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, OWNERSHIP, AND NON-INFRINGEMENT.  HARVARD MAKES NO WARRANTY ABOUT THE ACCURACY, RELIABILITY, COMPLETENESS, TIMELINESS, SUFFICIENCY OR QUALITY OF THE SOFTWARE.  HARVARD DOES NOT WARRANT THAT THE SOFTWARE WILL OPERATE WITHOUT ERROR OR INTERRUPTION.

8. **LIMITATIONS OF LIABILITY AND REMEDIES.** USE OF THE SOFTWARE IS AT END USER’S OWN RISK. IF END USER IS DISSATISFIED WITH THE SOFTWARE, ITS EXCLUSIVE REMEDY IS TO STOP USING IT.  IN NO EVENT SHALL HARVARD BE LIABLE TO END USER OR ITS INSTITUTION, IN CONTRACT, TORT OR OTHERWISE, FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR OTHER DAMAGES OF ANY KIND WHATSOEVER ARISING OUT OF OR IN CONNECTION WITH THE SOFTWARE, EVEN IF HARVARD IS NEGLIGENT OR OTHERWISE AT FAULT, AND REGARDLESS OF WHETHER HARVARD IS ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.

9. **INDEMNIFICATION.** To the extent permitted by law, End User shall indemnify, defend and hold harmless Harvard, their corporate affiliates, current or future directors, trustees, officers, faculty, medical and professional staff, employees, students and agents and their respective successors, heirs and assigns (the "Indemnitees"), against any liability, damage, loss or expense (including reasonable attorney's fees and expenses of litigation) incurred by or imposed upon the Indemnitees or any one of them in connection with any claims, suits, actions, demands or judgments arising from End User’s breach of this Agreement or its Institution’s use of the Software except to the extent caused by the gross negligence or willful misconduct of Harvard. This indemnification provision shall survive expiration or termination of this Agreement.

10. **GOVERNING LAW.** This Agreement shall be construed and governed by the laws of the Commonwealth of Massachusetts regardless of otherwise applicable choice of law standards.

11. **NON-USE OF NAME.**  Nothing in this License and Terms of Use shall be construed as granting End Users or their Institutions any rights or licenses to use any trademarks, service marks or logos associated with the Software.  End User may not use the terms “Harvard” (or a substantially similar term) in any way that is inconsistent with the permitted uses described herein. End Users may not use any name or emblem of Harvard or any of its schools or subdivisions for any purpose, or to falsely suggest any relationship between End User (or its Institution) and Harvard, or in any manner that would infringe or violate any of its rights.

12. End User represents and warrants that it has the legal authority to enter into this License and Terms of Use on behalf of itself and its Institution.

                                                                  ***      




--- File: pytest.ini ---
[pytest]
log_cli=true
log_level=INFO
addopts =
    --cov=moseq2_extract/
    --cov-branch
    --pyargs
testpaths =
    tests/

--- File: README.md ---
# MoSeq2-Extract: Depth Video Rodent-Tracking Toolkit
 
[![Build Status](https://travis-ci.com/dattalab/moseq2-extract.svg?token=gvoikVySDHEmvHT7Dbed&branch=test-suite)](https://travis-ci.com/dattalab/moseq2-extract)
[![codecov](https://codecov.io/gh/dattalab/moseq2-extract/branch/test-suite/graph/badge.svg?token=ICPjpMMwYZ)](https://codecov.io/gh/dattalab/moseq2-extract)
[![DOI](https://zenodo.org/badge/120829989.svg)](https://zenodo.org/badge/latestdoi/120829989)

# [Documentation: MoSeq2 Wiki](https://github.com/dattalab/moseq2-app/wiki)
You can find more information about MoSeq Pipeline, installation, step-by-step instructions, documentation for Command Line Interface(CLI), tutorials etc in [MoSeq2 Wiki](https://github.com/dattalab/moseq2-app/wiki).

You can run `moseq2-extract --version` to check the current version and `moseq2-extract --help` to see all the commands.
```bash
Usage: moseq2-extract [OPTIONS] COMMAND [ARGS]...

Options:
  --version  Show the version and exit.  [default: False]
  --help     Show this message and exit.  [default: False]

Commands:
  aggregate-results   Copies all extracted results (h5, yaml, avi) files...
  batch-extract       Batch processes all the raw depth recordings located...
  convert-raw-to-avi  Converts/Compresses a raw depth file into an avi file...
  copy-slice          Copies a segment of an input depth recording into a...
  download-flip-file  Downloads Flip-correction model that helps with...
  extract             Processes raw input depth recordings to output a...
  find-roi            Finds the ROI and background distance to subtract
                      from...
  generate-config     Generates a configuration file that holds editable...
  generate-index      Generates an index YAML file containing all extracted...
```


# Community Support and Contributing
- Please join [![MoSeq Slack Channel](https://img.shields.io/badge/slack-MoSeq-blue.svg?logo=slack)](https://moseqworkspace.slack.com) to post questions and interactive with MoSeq developers and users.
- If you encounter bugs, errors or issues, please submit a Bug report [here](https://github.com/dattalab/moseq2-app/issues/new/choose). We encourage you to check out the [troubleshooting and tips section](https://github.com/dattalab/moseq2-app/wiki/Troubleshooting-and-Tips) and search your issues in [the existing issues](https://github.com/dattalab/moseq2-app/issues) first.   
- If you want to see certain features in MoSeq or you have new ideas, please submit a Feature request [here](https://github.com/dattalab/moseq2-app/issues/new/choose).
- If you want to contribute to our codebases, please check out our [Developer Guidelines](https://github.com/dattalab/moseq2-app/wiki/MoSeq-Developer-Guidelines).
- Please tell us what you think by filling out [this user survey](https://forms.gle/FbtEN8E382y8jF3p6).

# License

MoSeq is freely available for academic use under a license provided by Harvard University. Please refer to the license file for details. If you are interested in using MoSeq for commercial purposes please contact Bob Datta directly at srdatta@hms.harvard.edu, who will put you in touch with the appropriate people in the Harvard Technology Transfer office.


--- File: Pipfile ---
[[source]]
name = "pypi"
url = "https://pypi.org/simple"
verify_ssl = true

[dev-packages]
pytest = "*"
pytest-cov = "*"

[packages]
h5py = '*'
tqdm = "*"
scipy = "*"
numpy = "==1.18.3"
click = "*"
awscli = "*"
joblib = "*"
cytoolz = "*"
matplotlib = '*'
statsmodels = "*"
ruamel_yaml = '==0.16.5'
scikit-image = "*"
scikit-learn = "==0.20.3"
opencv-python = '*'

--- File: setup.py ---
from setuptools import setup, find_packages
import subprocess
import codecs
import sys
import os


def install(package):
    subprocess.call([sys.executable, "-m", "pip", "install", package])


try:
    import cv2
except ImportError:
    install('opencv-python')

try:
    import cython
except ImportError:
    install('cython')

def read(rel_path):
    here = os.path.abspath(os.path.dirname(__file__))
    with codecs.open(os.path.join(here, rel_path), 'r') as fp:
        return fp.read()

def get_version(rel_path):
    for line in read(rel_path).splitlines():
        if line.startswith('__version__'):
            delim = '"' if '"' in line else "'"
            return line.split(delim)[1]
    else:
        raise RuntimeError("Unable to find version string.")

# note that scikit-learn must == 0.19 since flip classifiers were trained using this version
setup(
    name='moseq2-extract',
    author='Datta Lab',
    description='To boldly go where no mouse has gone before',
    version=get_version('moseq2_extract/__init__.py'),
    platforms=['mac', 'unix'],
    packages=find_packages(),
    install_requires=[
                      # 'h5py==2.10.0', # Relaxed for ARM compatibility
                      'h5py',
                      'tqdm>=4.48.0',
                      # 'scipy==1.3.2', # Relaxed for ARM compatibility
                      'scipy',
                      # 'numpy==1.18.3', # Relaxed for ARM compatibility
                      'numpy',
                      'click==7.0',
                      'joblib==0.15.1',
                      'cytoolz==0.10.1',
                      # 'matplotlib==3.1.2', # Potentially relax later if needed
                      'matplotlib',
                      'statsmodels==0.10.2', # Keep pinned for now
                      # 'scikit-image==0.16.2', # Potentially relax later if needed
                      'scikit-image',
                      'scikit-learn==0.20.3', # Keep pinned for now (see note above setup())
                      # 'opencv-python==4.1.2.30', # Potentially relax later if needed
                      'opencv-python',
                      'ruamel.yaml==0.16.5'], # Keep pinned for now
    # python_requires='>=3.6,<3.8', # Relaxed for Python 3.8+
    entry_points={'console_scripts': ['moseq2-extract = moseq2_extract.cli:cli']},
    extras_require={
        "docs": [
            "sphinx",
            "sphinx-click",
            "sphinx-rtd-theme",
        ],
    },
)


--- File: .gitignore ---
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
*.swp

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.coverage
.coverage.*
.cache
nosetests.xml
*.xml
*.cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
.static_storage/
.media/
local_settings.py

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/

# it just works
.DS_Store

.vscode
tests/test_flip_classifiers/flip_classifier_k2_largemicewithfiber.pkl

tests/test_flip_classifiers/flip_classifier_k2_inscopix.pkl

tests/test_flip_classifiers/flip_test1.pkl
data/
.doctrees/


--- File: .travis.yml ---
#specs for travis ci
language: python
dist: xenial
os: linux
cache: false

stages:
  - latest-pythons

jobs:
  include:
    - env: ISOLATED=true
      dist: xenial
      python: '3.7'
      stage: latest-pythons
      # get the dependencies
      before_install:
        - pip install -U pip
        - pip install pytest codecov
        - pip install "pytest-cov>=2.4.0,<2.6"
        - export PYTHONPATH=$PYTHONPATH:$(pwd)
        - if [ "$TRAVIS_OS_NAME" == "linux" ]; then sudo add-apt-repository -y ppa:mc3man/xerus-media; fi
        - if [ "$TRAVIS_OS_NAME" == "linux" ]; then sudo apt-get update; fi
        - if [ "$TRAVIS_OS_NAME" == "linux" ]; then sudo apt-get install -y ffmpeg --allow-unauthenticated; fi
      install:
        - pip install --upgrade importlib_metadata
        - pip install -e .
      before_script:
        - ./scripts/download_test_dataset.sh
      script:
        - pytest --cov=./tests/
      after_success:
        - codecov -t $CC_TEST_REPORTER_ID

--- File: tests/__init__.py ---


--- File: tests/unit_tests/__init__.py ---


--- File: tests/unit_tests/test_util.py ---
import os
import cv2
import h5py
import json
import shutil
import numpy as np
import ruamel.yaml as yaml
import numpy.testing as npt
from unittest import TestCase
from os.path import exists, dirname
from moseq2_extract.cli import find_roi
from moseq2_extract.io.image import read_image
from ..integration_tests.test_cli import write_fake_movie
from moseq2_extract.util import (
    gen_batch_sequence,
    load_metadata,
    load_timestamps,
    recursive_find_unextracted_dirs,
    select_strel,
    scalar_attributes,
    dict_to_h5,
    click_param_annot,
    strided_app,
    get_strels,
    get_bucket_center,
    make_gradient,
    graduate_dilated_wall_area,
    convert_raw_to_avi_function,
    command_with_config,
    recursive_find_h5s,
    clean_file_str,
    load_textdata,
    time_str_for_filename,
    build_path,
    read_yaml,
    detect_and_set_camera_parameters,
)


class TestExtractUtils(TestCase):

    def test_build_path(self):
        out = build_path({"test1": "value", "test2": "value2"}, "{test1}_{test2}")
        assert out == "value_value2"

    def test_detect_and_set_camera_parameters(self):
        test_config_data = {"camera_type": "kinect", "bg_roi_weights": (1, 0.1, 1)}

        new_config_data = detect_and_set_camera_parameters(test_config_data)

        assert new_config_data["bg_roi_weights"] == (1, 0.1, 1)

        test_config_data["camera_type"] = "azure"
        new_config_data = detect_and_set_camera_parameters(test_config_data)

        assert new_config_data["bg_roi_weights"] == (10, 0.1, 1)

        test_config_data["camera_type"] = "realsense"
        new_config_data = detect_and_set_camera_parameters(test_config_data)

        assert new_config_data["bg_roi_weights"] == (10, 0.1, 1)

        test_config_data["camera_type"] = None
        new_config_data = detect_and_set_camera_parameters(test_config_data)

        assert new_config_data["bg_roi_weights"] == (10, 0.1, 1)

        test_config_data["camera_type"] = "auto"
        new_config_data = detect_and_set_camera_parameters(
            test_config_data, "data/test-out.avi"
        )

        assert new_config_data["bg_roi_weights"] == (1, 0.1, 1)

        test_config_data["camera_type"] = "auto"
        new_config_data = detect_and_set_camera_parameters(
            test_config_data, "data/azure_test/nfov_test.mkv"
        )

        assert new_config_data["bg_roi_weights"] == (10, 0.1, 1)

    def test_get_strels(self):

        test_input_dict = {
            "bg_roi_shape": "ellipse",
            "tail_filter_shape": "rect",
            "cable_filter_shape": "rect",
            "tail_filter_size": (7, 7),
            "cable_filter_size": (7, 7),
            "bg_roi_dilate": (3, 3),
            "bg_roi_erode": (4, 4),
        }

        test_strel_out = get_strels(test_input_dict)

        out_keys = ["strel_dilate", "strel_erode", "strel_tail", "strel_min"]

        assert list(test_strel_out.keys()) == out_keys

    def test_read_yaml(self):

        test_file = "data/config.yaml"
        test_dict = read_yaml(test_file)

        with open(test_file, "r") as f:
            truth_dict = yaml.safe_load(f)

        assert truth_dict == test_dict

    def test_clean_file_str(self):
        test_name = 'd<a:t\\t"a'
        truth_out = "d-a-t-t-a"

        test_out = clean_file_str(test_name)
        assert truth_out == test_out

    def test_load_textdata(self):
        data_file = "data/depth_ts.txt"

        data, timestamps = load_textdata(data_file, np.uint8)
        assert data.all() is not None
        assert timestamps.all() is not None
        assert len(data) == len(timestamps)

    def test_time_str_for_filename(self):

        test_out = time_str_for_filename("12:12:12")
        truth_out = "12-12-12"
        assert test_out == truth_out

    def test_recursive_find_h5s(self):

        h5s, dicts, yamls = recursive_find_h5s("data/")
        assert len(h5s) == len(dicts) == len(yamls) > 0

    def test_gen_batch_sequence(self):

        tmp_list = [range(0, 10), range(5, 15), range(10, 20), range(15, 25)]

        gen_list = list(gen_batch_sequence(25, 10, 5))

        assert gen_list == tmp_list

    def test_load_timestamps(self):

        txt_path = "data/tmp_timestamps.txt"

        tmp_timestamps = np.arange(0, 5, 0.05)
        with open(txt_path, "w") as f:
            for timestamp in tmp_timestamps:
                print("{}".format(str(timestamp)), file=f)

        loaded_timestamps = load_timestamps(txt_path)
        npt.assert_almost_equal(loaded_timestamps, tmp_timestamps, 10)
        os.remove(txt_path)

    def test_load_metadata(self):

        tmp_dict = {"test": "test2"}

        json_file = "data/test_metadata.json"
        with open(json_file, "w") as f:
            json.dump(tmp_dict, f)

        loaded_dict = load_metadata(json_file)

        assert loaded_dict == tmp_dict
        os.remove(json_file)

    def test_convert_raw_to_avi(self):

        # writing a file to test following pipeline
        data_path = "data/fake_movie_to_convert.dat"

        write_fake_movie(data_path)

        convert_raw_to_avi_function(data_path)
        assert os.path.isfile(data_path.replace(".dat", ".avi"))
        os.remove(data_path)
        os.remove(data_path.replace(".dat", ".avi"))

    def test_select_strel(self):

        strel = select_strel("ellipse", size=(9, 9))
        npt.assert_equal(strel, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9)))

        strel = select_strel("rectangle", size=(9, 9))
        npt.assert_equal(strel, cv2.getStructuringElement(cv2.MORPH_RECT, (9, 9)))

        strel = select_strel("sdfdfsf", size=(9, 9))
        npt.assert_equal(strel, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9)))

    def test_scalar_attributes(self):

        dct = scalar_attributes()

        assert dct is not None

    def test_dict_to_h5(self):

        tmp_dic = {
            "subdict": {
                "sd_tuple": (0, 1),
                "sd_string": "quick brown fox",
                "sd_integer": 1,
                "sd_float": 1.0,
                "sd_bool": False,
                "sd_list": [1, 2, 3],
            },
            "tuple": (0, 1),
            "string": "quick brown fox",
            "integer": 1,
            "float": 1.0,
            "bool": False,
            "list": [1, 2, 3],
        }

        fpath = "data/fake_results.h5"
        with h5py.File(fpath, "w") as f:
            dict_to_h5(f, tmp_dic, "data/")

        def h5_to_dict(h5file, path):
            ans = {}
            if not path.endswith("/"):
                path = path + "/"
            for key, item in h5file[path].items():
                if type(item) is h5py.Dataset:
                    ans[key] = item[()]
                elif type(item) is h5py.Group:
                    ans[key] = h5_to_dict(h5file, path + key + "/")
            return ans

        with h5py.File(fpath, "r") as f:
            result = h5_to_dict(f, "data/")
        npt.assert_equal(result, tmp_dic)
        os.remove(fpath)

    def test_get_bucket_center(self):
        img = read_image("data/tiffs/bground_bucket.tiff")
        roi = read_image("data/tiffs/roi_bucket_01.tiff")
        true_depth = np.median(img[roi > 0])

        x, y = get_bucket_center(img, true_depth)

        assert isinstance(x, int)
        assert isinstance(y, int)

        assert x > 0 and x < img.shape[1]
        assert y > 0 and y < img.shape[0]

    def test_make_gradient(self):
        img = read_image("data/tiffs/bground_bucket.tiff")
        width = img.shape[1]
        height = img.shape[0]
        xc = int(img.shape[1] / 2)
        yc = int(img.shape[0] / 2)
        radx = int(img.shape[1] / 2)
        rady = int(img.shape[0] / 2)
        theta = 0

        grad = make_gradient(width, height, xc, yc, radx, rady, theta)
        assert grad[grad >= 0.08].all() == True

    def test_graduate_dilated_wall_area(self):
        img = read_image("data/tiffs/bground_bucket.tiff")
        roi = read_image("data/tiffs/roi_bucket_01.tiff")
        true_depth = np.median(img[roi > 0])

        config_data = {"true_depth": true_depth}
        strel_dilate = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9))
        output_dir = "data/tiffs/"

        new_bg = graduate_dilated_wall_area(img, config_data, strel_dilate, output_dir)

        assert np.median(new_bg) > np.median(img)
        assert os.path.exists("data/tiffs/new_bg.tiff")
        os.remove("data/tiffs/new_bg.tiff")

    def test_strided_app(self):
        test_in = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])

        test_out = strided_app(test_in, L=5, S=3)
        expected_out = np.array([[1, 2, 3, 4, 5], [4, 5, 6, 7, 8], [7, 8, 9, 10, 11]])

        npt.assert_array_equal(test_out, expected_out)

    def test_recursive_find_unextracted_dirs(self):

        # writing a file to test following pipeline
        data_path = "data/test/fake_movie_to_convert.dat"
        if not exists(dirname(data_path)):
            os.makedirs(dirname(data_path))

        write_fake_movie(data_path)

        root_dir = "data/"
        proc_dirs = recursive_find_unextracted_dirs(root_dir, skip_checks=True)
        print(proc_dirs)
        assert len(proc_dirs) == 1
        os.remove(data_path)

    def test_command_with_config(self):
        command_with_config(find_roi)


--- File: tests/unit_tests/test_extract_roi.py ---
import numpy as np
import numpy.testing as npt
from unittest import TestCase
from moseq2_extract.extract.roi import plane_fit3, plane_ransac


class TestExtractROI(TestCase):
    def test_plane_fit3(self):
        # let's create two planes from simple equations and ensure
        # that everything looks kosher ya?

        # x,y plus intercept

        def plane_equation1(xy):
            return 2 * xy[:, 0] + 2 * xy[:, 1] + 2

        def plane_equation2(xy):
            return 1 * xy[:, 0] + 50 * xy[:, 1] + 3

        xy = np.random.randint(low=0, high=100, size=(3, 2))
        xyz = np.hstack((xy, plane_equation1(xy)[:, np.newaxis])).astype("float64")
        a = plane_fit3(xyz)
        norma = -a / a[2]

        npt.assert_almost_equal(norma[[0, 1, 3]], np.array([2, 2, 2]), 3)

        xyz = np.hstack((xy, plane_equation2(xy)[:, np.newaxis])).astype("float64")
        a = plane_fit3(xyz)
        norma = -a / a[2]

        npt.assert_almost_equal(norma[[0, 1, 3]], np.array([1, 50, 3]), 3)

    def test_plane_ransac(self):

        # make a plane where we can test adding noise...

        def plane_equation_noisy(xy, noise_scale=0):
            return (
                2 * xy[:, 0]
                + 5 * xy[:, 1]
                + np.random.normal(0, noise_scale, size=(xy.shape[0],))
            )

        xx, yy = np.meshgrid(np.arange(0, 50), np.arange(0, 50))
        xy = np.vstack((xx.ravel(), yy.ravel()))

        # low noise regime

        z = plane_equation_noisy(xy.T, noise_scale=0.1)
        tmp_img = z.reshape(xx.shape)

        a = plane_ransac(
            tmp_img, bg_roi_depth_range=(0, 1000), iters=1000, noise_tolerance=10
        )
        norma = -a[0] / a[0][2]

        npt.assert_almost_equal(np.round(norma[[0, 1]]), np.array([2, 5]), 1)


--- File: tests/unit_tests/test_io_video.py ---
import os
import numpy as np
import numpy.testing as npt
from unittest import TestCase
from moseq2_extract.io.video import (
    read_frames_raw,
    get_raw_info,
    read_frames,
    write_frames,
    get_video_info,
    write_frames_preview,
    get_movie_info,
    load_movie_data,
    load_timestamps_from_movie,
    read_mkv,
)


class TestVideoIO(TestCase):
    def test_read_frames_raw(self):

        data_path = "data/fake_depth.dat"

        test_data = np.random.randint(0, 256, size=(300, 424, 512), dtype="int16")
        test_data.tofile(data_path)

        read_data = read_frames_raw(data_path)
        npt.assert_array_equal(test_data, read_data)
        os.remove(data_path)

    def test_get_raw_info(self):

        data_path = "data/fake_raw_depth.dat"

        test_data = np.random.randint(0, 256, size=(100, 424, 512), dtype="int16")
        test_data.tofile(data_path)

        vid_info = get_raw_info(data_path)

        npt.assert_equal(vid_info["bytes"], test_data.nbytes)
        npt.assert_equal(vid_info["nframes"], test_data.shape[0])
        npt.assert_equal(vid_info["dims"], (test_data.shape[2], test_data.shape[1]))
        npt.assert_equal(
            vid_info["bytes_per_frame"], test_data.nbytes / test_data.shape[0]
        )
        os.remove(data_path)

    def test_ffv1(self):

        data_path = "data/fake_ffv1_depth.avi"

        test_data = np.random.randint(0, 256, size=(300, 424, 512), dtype="int16")
        test_data.tofile(data_path)

        write_frames(data_path, test_data, fps=30)
        read_data = read_frames(data_path)

        vid_info = get_video_info(data_path)

        npt.assert_equal(test_data, read_data)
        npt.assert_equal(vid_info["fps"], 30)
        npt.assert_equal((vid_info["dims"]), (test_data.shape[2], test_data.shape[1]))
        npt.assert_equal(vid_info["nframes"], test_data.shape[0])
        os.remove(data_path)

    def test_write_frames_preview(self):

        data_path = "data/fake_preview_depth.avi"

        test_data = np.random.randint(0, 256, size=(300, 424, 512), dtype="int16")
        write_frames_preview(
            data_path, test_data, fps=30, frame_range=range(len(test_data))
        )
        os.remove(data_path)

    def test_get_movie_info(self):

        avi_path = "data/fake_movie_info.avi"
        dat_path = "data/fake_movie_info.dat"

        test_data = np.random.randint(0, 256, size=(300, 424, 512), dtype="int16")

        write_frames(avi_path, test_data, fps=30)
        test_data.tofile(dat_path)

        vid_info = get_movie_info(avi_path)
        raw_info = get_movie_info(dat_path)

        npt.assert_equal(vid_info["dims"], raw_info["dims"])
        npt.assert_equal(vid_info["nframes"], raw_info["nframes"])
        os.remove(avi_path)
        os.remove(dat_path)

    def test_load_movie_data(self):
        avi_path = "data/fake_movie_data.avi"
        dat_path = "data/fake_movie_data.dat"
        mkv_path = "data/azure_test/nfov_test.mkv"

        test_data = np.random.randint(0, 256, size=(60, 424, 512), dtype="int16")

        write_frames(avi_path, test_data, fps=30)
        test_data.tofile(dat_path)

        read_data_vid = load_movie_data(avi_path)
        read_data_raw = load_movie_data(dat_path)
        read_data_mkv = load_movie_data(
            mkv_path, frames=range(60), frame_size=(640, 576)
        )

        npt.assert_array_equal(read_data_vid, read_data_raw)
        assert len(read_data_raw) == len(read_data_mkv)
        os.remove(avi_path)
        os.remove(dat_path)

    def test_load_mkv_timestamps(self):

        mkv_path = "data/azure_test/nfov_test.mkv"

        test_timestamps = load_timestamps_from_movie(mkv_path)

        # length assertion
        assert len(test_timestamps) == 66

        avi_test = "data/test-out.avi"
        avi_ts = load_timestamps_from_movie(avi_test)
        assert len(avi_ts) == 100

    def test_read_mkv(self):

        mkv_path = "data/azure_test/nfov_test.mkv"
        test_out = read_mkv(mkv_path, frames=[0])

        assert len(test_out) == 1

        test_full = read_mkv(mkv_path, frames=range(65), frames_is_timestamp=True)

        assert len(test_full) == 65
        assert len(test_full) != 66


--- File: tests/unit_tests/test_io_image.py ---
import os
import numpy as np
import numpy.testing as npt
from unittest import TestCase
from skimage.external import tifffile
from moseq2_extract.io.image import write_image, read_image, read_tiff_files


class TestImageIO(TestCase):
    def test_write_image(self):

        data_path = "data/temp_w_image.tiff"

        # make some random ints, don't exceed 16 bit limits
        rnd_img = np.random.randint(low=0, high=100, size=(50, 50)).astype("uint16")
        write_image(data_path, rnd_img, scale=False)

        with tifffile.TiffFile(data_path) as tif:
            tmp = tif

        image = tmp.asarray().astype("uint16")
        npt.assert_almost_equal(rnd_img, image, 3)
        assert os.path.isfile(data_path)
        os.remove(data_path)

    def test_read_image(self):

        data_path = "data/temp_r_image.tiff"

        rnd_img = np.random.randint(low=0, high=100, size=(50, 50)).astype("uint16")

        write_image(data_path, rnd_img, scale=True)
        image = read_image(data_path, scale=True)

        npt.assert_almost_equal(rnd_img, image, 3)

        write_image(data_path, rnd_img, scale=True, scale_factor=(0, 100))
        image = read_image(data_path, scale=True)

        npt.assert_almost_equal(rnd_img, image, 3)

        write_image(data_path, rnd_img, scale=False)
        image = read_image(data_path, scale=False)

        npt.assert_almost_equal(rnd_img, image, 3)
        os.remove(data_path)

    def test_read_tiff_files(self):
        tiff_dir = "data/tiffs/"

        images, paths = read_tiff_files(tiff_dir)
        assert len(images) == len(paths) == 10


--- File: tests/unit_tests/test_helper_extract.py ---
import os
import cv2
import h5py
import uuid
import shutil
import numpy as np
from copy import deepcopy
import ruamel.yaml as yaml
from unittest import TestCase
from moseq2_extract.io.image import read_image
from moseq2_extract.helpers.data import create_extract_h5
from ..integration_tests.test_cli import write_fake_movie
from moseq2_extract.gui import generate_config_command, download_flip_command
from moseq2_extract.util import scalar_attributes, gen_batch_sequence, load_metadata
from moseq2_extract.helpers.extract import (
    run_local_extract,
    process_extract_batches,
    write_extracted_chunk_to_h5,
)


class TestHelperExtract(TestCase):

    def test_write_extracted_chunk_to_h5(self):

        output_dir = "data/"
        output_filename = "test_out"
        offset = 0
        frame_range = range(0, 100)
        scalars = ["speed"]
        config_data = {"flip_classifier": False}
        results = {
            "depth_frames": np.zeros((100, 10, 10)),
            "mask_frames": np.zeros((100, 10, 10)),
            "scalars": {"speed": np.ones((100, 1))},
        }

        out_file = os.path.join(output_dir, f"{output_filename}.h5")

        with h5py.File(out_file, "w") as f:
            f.create_dataset(f"scalars/speed", (100, 1), "float32", compression="gzip")
            f.create_dataset(f"frames", (100, 10, 10), "float32", compression="gzip")
            f.create_dataset(
                f"frames_mask", (100, 10, 10), "float32", compression="gzip"
            )

            write_extracted_chunk_to_h5(
                f, results, config_data, scalars, frame_range, offset
            )

        assert os.path.exists(out_file)
        os.remove(out_file)

    def test_process_extract_batches(self):

        output_dir = "data/"
        config_file = "data/config.yaml"
        metadata_path = "data/metadata.json"
        output_filename = "test_out"

        bground_im = read_image(
            os.path.join(output_dir, "tiffs/", "bground_bucket.tiff"), scale=True
        )
        roi = read_image(
            os.path.join(output_dir, "tiffs/", "roi_bucket_01.tiff"), scale=True
        )
        first_frame = np.zeros(roi.shape)
        true_depth = 773.0
        first_frame_idx = 0
        last_frame_idx = 20
        nframes = 20

        flip_file = "data/flip/flip_classifier_k2_c57_10to13weeks.pkl"
        if not os.path.isfile(flip_file):
            download_flip_command("data/flip/")

        assert os.path.isfile(flip_file), "flip file was not correctly downloaded"

        acquisition_metadata = load_metadata(metadata_path)

        with open(config_file, "r") as f:
            config_data = yaml.safe_load(f)

        config_data["flip_classifier"] = flip_file
        config_data["true_depth"] = true_depth
        config_data["tar"] = False

        status_dict = {
            "parameters": deepcopy(config_data),
            "complete": False,
            "skip": False,
            "uuid": str(uuid.uuid4()),
            "metadata": "",
        }

        frame_batches = list(
            gen_batch_sequence(
                nframes, config_data["chunk_size"], config_data["chunk_overlap"]
            )
        )

        str_els = {
            "strel_tail": cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9)),
            "strel_min": cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5)),
        }

        scalars_attrs = scalar_attributes()
        scalars = list(scalars_attrs.keys())

        # writing a file to test following pipeline
        data_path = "data/unit_test/"
        data_file = f"{data_path}depth.dat"
        if not os.path.isdir("data/unit_test/"):
            os.makedirs("data/unit_test/")

        write_fake_movie(data_file)
        assert os.path.isfile(data_file)

        with h5py.File(os.path.join(output_dir, f"{output_filename}.h5"), "w") as g:
            create_extract_h5(
                g,
                acquisition_metadata,
                config_data,
                status_dict,
                scalars_attrs,
                nframes,
                roi,
                bground_im,
                first_frame,
                first_frame_idx,
                last_frame_idx,
            )

            process_extract_batches(
                data_file,
                config_data,
                bground_im,
                roi,
                frame_batches,
                str_els,
                os.path.join(output_dir, output_filename + ".mp4"),
                scalars=scalars,
                h5_file=g,
            )

        assert os.path.exists(os.path.join(output_dir, f"{output_filename}.h5"))
        os.remove(os.path.join(output_dir, f"{output_filename}.h5"))
        assert os.path.exists(os.path.join(output_dir, f"{output_filename}.mp4"))
        os.remove(os.path.join(output_dir, f"{output_filename}.mp4"))
        shutil.rmtree(data_path)
        shutil.rmtree("data/flip/")

    def test_run_local_extract(self):

        config_path = "data/test_local_ex_config.yaml"
        generate_config_command(config_path)

        # writing a file to test following pipeline
        data_dir = "data/test_local_session/"
        data_path = f"{data_dir}depth.dat"
        if not os.path.isdir(data_dir):
            os.makedirs(data_dir)

        write_fake_movie(data_path)
        assert os.path.isfile(data_path), "fake movie was not written correctly"

        with open(config_path, "r") as f:
            params = yaml.safe_load(f)

        prefix = ""

        run_local_extract([str(data_path)], params, prefix)
        os.remove(config_path)
        shutil.rmtree(data_dir)


--- File: tests/unit_tests/test_extract_track.py ---
import numpy as np
import numpy.testing as npt
from unittest import TestCase
import statsmodels.stats.correlation_tools as stats_tools
from moseq2_extract.extract.track import em_get_ll, em_tracking


def make_fake_movie():
    edge_size = 40
    points = np.arange(-edge_size, edge_size)
    sig1 = 10
    sig2 = 20
    height = 50

    kernel = np.exp(-(points**2.0) / (2.0 * sig1**2.0))
    kernel2 = np.exp(-(points**2.0) / (2.0 * sig2**2.0))

    kernel_full = np.outer(kernel, kernel2)
    kernel_full /= np.max(kernel_full)
    kernel_full *= height

    fake_mouse = kernel_full
    fake_mouse[fake_mouse < 5] = 0
    im_size = (424, 512)
    tmp_image = np.zeros(im_size, dtype="int16")
    center = np.array(tmp_image.shape) // 2

    mouse_dims = np.array(fake_mouse.shape) // 2
    tmp_image[
        center[0] - mouse_dims[0] : center[0] + mouse_dims[0],
        center[1] - mouse_dims[1] : center[1] + mouse_dims[1],
    ] = fake_mouse

    xx, yy = np.meshgrid(np.arange(tmp_image.shape[1]), np.arange(tmp_image.shape[0]))
    coords = np.vstack((xx.ravel(), yy.ravel(), tmp_image.ravel()))
    tmp_cov = stats_tools.cov_nearest(np.cov(coords[:, coords[2, :] > 10]))

    nframes = 100
    fake_movie = np.tile(tmp_image, (nframes, 1, 1))

    return fake_movie, tmp_cov, center, im_size, nframes


class TestEMTracking(TestCase):

    def test_em_get_ll(self):

        # draw some random samples
        fake_movie, tmp_cov, center, im_size, nframes = make_fake_movie()

        ll = em_get_ll(
            frames=fake_movie,
            mean=np.tile(np.array([center[1], center[0], 50]), (nframes, 1)),
            cov=np.tile(tmp_cov, (nframes, 1, 1)),
        )

        for ll_im in ll:
            npt.assert_almost_equal(np.unravel_index(np.argmax(ll_im), im_size), center)

    def test_em_tracking(self):

        # draw some random samples
        fake_movie, tmp_cov, center, im_size, nframes = make_fake_movie()

        for init in ["raw", "min", "med"]:
            parameters = em_tracking(
                frames=fake_movie, raw_frames=fake_movie, init_method=init
            )

            # this is very loose atm, need to figure out what's going on here...
            for mu in parameters["mean"]:
                npt.assert_allclose(mu[:2], center[::-1], atol=5, rtol=0)


--- File: tests/unit_tests/test_extract_proc.py ---
import os
import re
import cv2
import glob
import numpy as np
import numpy.testing as npt
from unittest import TestCase
from moseq2_extract.io.image import read_image
from moseq2_extract.extract.proc import (
    get_roi,
    crop_and_rotate_frames,
    model_smoother,
    get_frame_features,
    compute_scalars,
    clean_frames,
    get_largest_cc,
    feature_hampel_filter,
)


class TestExtractProc(TestCase):

    def test_get_roi(self):
        # load in a bunch of ROIs where we have some ground truth
        bground_list = glob.glob("data/tiffs/bground*.tiff")

        for bground in bground_list:
            tmp = read_image(bground, scale=True)

            if re.search(r"gradient", bground) is not None:
                roi = get_roi(
                    tmp.astype("float32"),
                    depth_range=(750, 900),
                    iters=5000,
                    noise_tolerance=30,
                    bg_roi_gradient_filter=True,
                )
            else:
                roi = get_roi(
                    tmp.astype("float32"),
                    depth_range=(650, 750),
                    iters=5000,
                    noise_tolerance=30,
                )

            fname = os.path.basename(bground)
            dirname = os.path.dirname(bground)
            roi_file = "roi{}_01.tiff".format(re.search(r"\_[a-z|A-Z]*", fname).group())

            ground_truth = read_image(os.path.join(dirname, roi_file), scale=True)

            frac_nonoverlap_roi1 = np.empty((2,))
            frac_nonoverlap_roi2 = np.empty((2,))

            frac_nonoverlap_roi1[0] = np.mean(np.logical_xor(ground_truth, roi[0][0]))

            roi_file2 = "roi{}_02.tiff".format(
                re.search(r"\_[a-z|A-Z]*", fname).group()
            )

            if os.path.exists(os.path.join(dirname, roi_file2)):
                ground_truth = read_image(os.path.join(dirname, roi_file2), scale=True)
                frac_nonoverlap_roi2[0] = np.mean(
                    np.logical_xor(ground_truth, roi[0][1])
                )
                frac_nonoverlap_roi2[1] = np.mean(
                    np.logical_xor(ground_truth, roi[0][0])
                )
                frac_nonoverlap_roi1[1] = np.mean(
                    np.logical_xor(ground_truth, roi[0][1])
                )

                assert np.min(frac_nonoverlap_roi2) < 0.2

            assert np.min(frac_nonoverlap_roi1) < 0.2

    def test_crop_and_rotate(self):

        fake_mouse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (30, 20))

        tmp_image = np.zeros((80, 80), dtype="int8")
        center = np.array(tmp_image.shape) // 2

        mouse_dims = np.array(fake_mouse.shape) // 2

        tmp_image[
            center[0] - mouse_dims[0] : center[0] + mouse_dims[0],
            center[1] - mouse_dims[1] : center[1] + mouse_dims[1],
        ] = fake_mouse

        rotations = np.random.rand(100) * 180 - 90
        features = {}

        features["centroid"] = np.tile(center, (len(rotations), 1))
        features["orientation"] = np.deg2rad(rotations)

        fake_movie = np.zeros(
            (len(rotations), tmp_image.shape[0], tmp_image.shape[1]), dtype="float32"
        )

        for i, rotation in enumerate(rotations):

            rot_mat = cv2.getRotationMatrix2D(tuple(center), rotation, 1)
            fake_movie[i] = cv2.warpAffine(
                tmp_image.astype("float32"), rot_mat, (80, 80)
            )

        cropped_and_rotated = (
            crop_and_rotate_frames(fake_movie, features=features) > 0.4
        ).astype(tmp_image.dtype)
        percent_pixels_diff = np.mean(np.abs(cropped_and_rotated - tmp_image)) * 1e2

        assert percent_pixels_diff < 0.1

    def test_get_frame_features(self):

        fake_mouse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (30, 20))

        tmp_image = np.zeros((80, 80), dtype="int8")
        center = np.array(tmp_image.shape) // 2

        mouse_dims = np.array(fake_mouse.shape) // 2

        tmp_image[
            center[0] - mouse_dims[0] : center[0] + mouse_dims[0],
            center[1] - mouse_dims[1] : center[1] + mouse_dims[1],
        ] = fake_mouse

        fake_movie = np.tile(tmp_image, (100, 1, 1))
        fake_features, mask = get_frame_features(fake_movie, frame_threshold=0.01)

        npt.assert_almost_equal(fake_features["orientation"], 0, 2)
        npt.assert_almost_equal(
            fake_features["centroid"], np.tile(center, (fake_movie.shape[0], 1)), 0.1
        )
        npt.assert_array_almost_equal(
            fake_features["axis_length"],
            np.tile([30, 20], (fake_movie.shape[0], 1)),
            0.1,
        )

    def test_compute_scalars(self):

        fake_mouse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (30, 20))

        tmp_image = np.zeros((80, 80), dtype="int8")
        center = np.array(tmp_image.shape) // 2

        mouse_dims = np.array(fake_mouse.shape) // 2

        tmp_image[
            center[0] - mouse_dims[0] : center[0] + mouse_dims[0],
            center[1] - mouse_dims[1] : center[1] + mouse_dims[1],
        ] = fake_mouse

        fake_movie = np.tile(tmp_image, (100, 1, 1))
        fake_features, mask = get_frame_features(fake_movie, frame_threshold=0.01)
        fake_scalars = compute_scalars(fake_movie, fake_features, min_height=0.01)

        npt.assert_almost_equal(fake_scalars["centroid_x_px"], center[1], 0.1)
        npt.assert_almost_equal(fake_scalars["centroid_y_px"], center[0], 0.1)
        npt.assert_almost_equal(fake_scalars["angle"], 0, 2)
        npt.assert_almost_equal(fake_scalars["width_px"], mouse_dims[0] * 2, 0.1)
        npt.assert_almost_equal(fake_scalars["length_px"], mouse_dims[1] * 2, 0.1)
        npt.assert_almost_equal(fake_scalars["height_ave_mm"], 1, 1)
        npt.assert_almost_equal(fake_scalars["velocity_2d_px"], 0, 1)
        npt.assert_almost_equal(fake_scalars["velocity_3d_px"], 0, 1)
        npt.assert_almost_equal(fake_scalars["velocity_theta"], 0, 1)
        npt.assert_almost_equal(fake_scalars["area_px"], np.sum(tmp_image), 0.1)

    def test_get_largest_cc(self):

        fake_mouse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (30, 20))

        tmp_image = np.zeros((80, 80), dtype="int8")
        center = np.array(tmp_image.shape) // 2

        mouse_dims = np.array(fake_mouse.shape) // 2

        tmp_image[
            center[0] - mouse_dims[0] : center[0] + mouse_dims[0],
            center[1] - mouse_dims[1] : center[1] + mouse_dims[1],
        ] = fake_mouse

        fake_movie = np.tile(tmp_image, (100, 1, 1))
        largest_cc_movie = get_largest_cc(fake_movie)

        npt.assert_array_almost_equal(fake_movie, largest_cc_movie, 3)

    def test_clean_frames(self):

        edge_size = 40
        points = np.arange(-edge_size, edge_size)
        sig1 = 10
        sig2 = 20

        kernel = np.exp(-(points**2.0) / (2.0 * sig1**2.0))
        kernel2 = np.exp(-(points**2.0) / (2.0 * sig2**2.0))

        kernel_full = np.outer(kernel, kernel2)
        kernel_full /= np.max(kernel_full)
        kernel_full *= 50

        fake_mouse = kernel_full
        fake_mouse[fake_mouse < 5] = 0

        fake_movie = np.tile(fake_mouse, (100, 1, 1))
        cleaned_fake_movie = clean_frames(fake_movie, prefilter_time=(3,))

    def test_feature_hampel_filter(self):

        fake_mouse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (30, 20))

        tmp_image = np.zeros((80, 80), dtype="int8")
        center = np.array(tmp_image.shape) // 2

        mouse_dims = np.array(fake_mouse.shape) // 2

        tmp_image[
            center[0] - mouse_dims[0] : center[0] + mouse_dims[0],
            center[1] - mouse_dims[1] : center[1] + mouse_dims[1],
        ] = fake_mouse

        fake_movie = np.tile(tmp_image, (100, 1, 1))
        fake_features, mask = get_frame_features(fake_movie, frame_threshold=0.01)

        smoothed_features = feature_hampel_filter(
            fake_features,
            centroid_hampel_span=5,
            centroid_hampel_sig=5,
            angle_hampel_span=5,
            angle_hampel_sig=3,
        )

        assert list(smoothed_features.keys()) == list(fake_features.keys())
        for k in smoothed_features.keys():
            assert smoothed_features[k].all() == fake_features[k].all()

    def test_model_smoother(self):

        fake_mouse = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (30, 20))

        tmp_image = np.zeros((80, 80), dtype="int8")
        center = np.array(tmp_image.shape) // 2

        mouse_dims = np.array(fake_mouse.shape) // 2

        tmp_image[
            center[0] - mouse_dims[0] : center[0] + mouse_dims[0],
            center[1] - mouse_dims[1] : center[1] + mouse_dims[1],
        ] = fake_mouse

        fake_movie = np.tile(tmp_image, (100, 1, 1))
        fake_features, mask = get_frame_features(fake_movie, frame_threshold=0.01)

        smoothed_feats = model_smoother(fake_features, ll=None, clips=(-300, -125))
        assert smoothed_feats == fake_features

        test_ll = np.random.randint(-25, 256, size=(100, 30, 20), dtype="int16")
        fake_features["axis_length"][0] = [np.nan, np.nan]

        smoothed_feats = model_smoother(fake_features, ll=test_ll, clips=(-300, -125))
        assert smoothed_feats == fake_features

        test_ll = np.random.randint(-25, 256, size=(100, 30, 20), dtype="int16")
        fake_features["axis_length"] = np.array(
            [x[0] for x in fake_features["axis_length"]]
        )
        print(fake_features["axis_length"])
        fake_features["axis_length"][0] = np.nan

        smoothed_feats = model_smoother(fake_features, ll=test_ll, clips=(-300, -125))
        assert smoothed_feats == fake_features


--- File: tests/unit_tests/test_helper_data.py ---
import os
import sys
import shutil
import tarfile
import ruamel.yaml as yaml
from unittest import TestCase
from moseq2_extract.util import load_metadata
from ..integration_tests.test_cli import write_fake_movie
from moseq2_extract.helpers.data import (
    load_extraction_meta_from_h5s,
    check_completion_status,
    build_manifest,
    copy_manifest_results,
    handle_extract_metadata,
    build_index_dict,
)


class TestHelperData(TestCase):

    def test_check_completion_status(self):
        test_file = "data/proc/results_00.yaml"
        assert check_completion_status(test_file) == True

        tmp_file = "data/test_file.yaml"  # non-existent
        assert check_completion_status(tmp_file) == False

    def test_build_index_dict(self):

        test_file = "data/proc/results_00.yaml"

        with open(test_file, "r") as f:
            dict = yaml.safe_load(f)

        test_files = [("data/proc/results_00.h5", "data/proc/results_00.yaml", dict)]

        index = build_index_dict(test_files)

        assert len(index["files"]) == 1
        assert index["pca_path"] == ""
        assert index["files"][0]["uuid"] == dict["uuid"]

    def test_load_h5s(self):

        test_fb_file = "data/feedback_ts.txt"
        with open("data/depth_ts.txt", "r") as f:
            with open(test_fb_file, "w") as g:
                g.write(f.read())

        assert os.path.exists(test_fb_file)
        metadata = load_metadata("data/metadata.json")
        to_load = [(metadata, "data/proc/results_00.h5")]
        loaded = load_extraction_meta_from_h5s(to_load)

        assert len(loaded) > 0
        os.remove(test_fb_file)

    def test_build_manifest(self):
        metadata = load_metadata("data/metadata.json")
        to_load = [(metadata, "data/proc/results_00.h5")]

        test_fb_file = "data/feedback_ts.txt"
        with open("data/depth_ts.txt", "r") as f:
            with open(test_fb_file, "w") as g:
                g.write(f.read())

        assert os.path.exists(test_fb_file)
        loaded = load_extraction_meta_from_h5s(to_load)

        manifest = build_manifest(loaded, "results_00")
        assert isinstance(manifest, dict)
        os.remove(test_fb_file)

    def test_copy_manifest_results(self):
        metadata = load_metadata("data/metadata.json")
        to_load = [(metadata, "data/proc/results_00.h5")]
        output_dir = "data/tmp/"
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        test_fb_file = "data/feedback_ts.txt"
        with open("data/depth_ts.txt", "r") as f:
            with open(test_fb_file, "w") as g:
                g.write(f.read())

        assert os.path.exists(test_fb_file)
        loaded = load_extraction_meta_from_h5s(to_load)

        manifest = build_manifest(loaded, "results_00")

        copy_manifest_results(manifest, output_dir)

        for p in os.listdir(output_dir):
            assert os.path.isfile(os.path.join(output_dir, p))

        shutil.rmtree(output_dir)

    def test_handle_extract_metadata(self):
        dirname = "data/"
        tmp_file = "data/test_vid.tar.gz"
        write_fake_movie(tmp_file)

        with tarfile.open(tmp_file, "w:gz") as tar:
            tar.add(dirname, arcname="test_vid.dat")
            tar.add(os.path.join(dirname, "metadata.json"), arcname="metadata.json")
            tar.add(os.path.join(dirname, "depth_ts.txt"), arcname="depth_ts.txt")

        acq_metadata, timestamps, tar = handle_extract_metadata(tmp_file, dirname)

        assert isinstance(acq_metadata, dict)
        assert len(timestamps.shape) == 1
        assert tar is not None

        os.remove(tmp_file)

        tmp_file = "data/test_vid.dat"
        write_fake_movie(tmp_file)

        acq_metadata, timestamps, tar = handle_extract_metadata(tmp_file, dirname)

        assert isinstance(acq_metadata, dict)
        assert len(timestamps.shape) == 1
        assert tar is None

        os.remove(tmp_file)


--- File: tests/integration_tests/__init__.py ---


--- File: tests/integration_tests/test_gui.py ---
import os
import sys
import h5py
import shutil
from copy import deepcopy
import ruamel.yaml as yaml
from os.path import exists
from unittest import TestCase
from .test_cli import write_fake_movie
from moseq2_extract.helpers.wrappers import copy_h5_metadata_to_yaml_wrapper
from moseq2_extract.gui import (
    generate_config_command,
    generate_index_command,
    aggregate_extract_results_command,
    download_flip_command,
    find_roi_command,
    extract_command,
    extract_found_sessions,
    get_selected_sessions,
)


class GUITests(TestCase):

    @classmethod
    def setUpClass(cls):
        if exists("data/data/"):
            shutil.rmtree("data/data/")

    def test_get_selected_sessions(self):

        to_extract = ["a", "b", "c"]
        extract_all = False

        stdin = "data/tmp_stdin.txt"

        with open(stdin, "w") as f:
            f.write("1,2,3")

        sys.stdin = open(stdin)

        test_ret = get_selected_sessions(to_extract, extract_all)
        assert test_ret == to_extract

        with open(stdin, "w") as f:
            f.write("1-3")

        sys.stdin = open(stdin)
        test_ret = get_selected_sessions(to_extract, extract_all)
        assert sorted(test_ret) == sorted(to_extract)

        with open(stdin, "w") as f:
            f.write("1")

        sys.stdin = open(stdin)
        test_ret = get_selected_sessions(to_extract, extract_all)
        assert test_ret == [to_extract[0]]

        with open(stdin, "w") as f:
            f.write("e 2-3")

        sys.stdin = open(stdin)
        test_ret = get_selected_sessions(to_extract, extract_all)
        assert test_ret == [to_extract[0]]

    def test_generate_config_command(self):

        config_path = "data/test_config.yaml"

        if os.path.isfile(config_path):
            os.remove(config_path)

        # file does not exist yet
        ret = generate_config_command(config_path)
        assert "success" in ret, "config file was not generated sucessfully"
        assert os.path.isfile(
            config_path
        ), "config file does not exist in specified path"

        # file exists
        stdin = "data/tmp_stdin.txt"

        # retain old version
        with open(stdin, "w") as f:
            f.write("N")

        sys.stdin = open(stdin)

        ret = generate_config_command(config_path)
        assert "retained" in ret, "old config file was not retained"

        # overwrite old version
        with open(stdin, "w") as f:
            f.write("Y")

        sys.stdin = open(stdin)
        ret = generate_config_command(config_path)
        assert "success" in ret, "overwriting failed"
        os.remove(config_path)
        os.remove(stdin)

    def test_generate_index_command(self):

        input_dir = "data/"
        outfile = os.path.join(input_dir, "moseq2-index.yaml")

        # minimal test case - more use cases to come
        generate_index_command(input_dir, outfile)
        assert os.path.isfile(outfile), "index file was not generated correctly"
        os.remove(outfile)

    def test_download_flip_file_command(self):
        test_outdir = "data/flip/"
        download_flip_command(test_outdir)
        assert True in [
            x.endswith("pkl") for x in os.listdir(test_outdir)
        ], "flip file does not exist in correct directory"
        shutil.rmtree(test_outdir)

    def test_find_roi_command(self):

        config_path = "data/test_roi_config.yaml"

        if os.path.isfile(config_path):
            os.remove(config_path)

        generate_config_command(config_path)

        out = find_roi_command(
            "data/", config_path, select_session=True, exts=["dat", "avi"]
        )
        assert out is None, "roi function did not find any rois to extract"

        # writing a file to test following pipeline
        input_dir = "data/"
        data_path = "data/test_session/"
        data_filepath = os.path.join(data_path, "test_roi_depth.dat")

        if not os.path.exists(data_path):
            os.makedirs(data_path)

        write_fake_movie(data_filepath)
        assert os.path.isfile(data_filepath)

        stdin = "data/stdin.txt"
        # select test file
        with open(stdin, "w") as f:
            f.write("1")

        sys.stdin = open(stdin)

        images, filenames = find_roi_command(
            input_dir, config_path, select_session=True, exts=["dat", "avi"]
        )
        assert len(filenames) == 3, "incorrect number of rois were computed"
        assert len(images) == 3, "incorrect number of rois images were computed"

        os.remove(config_path)
        shutil.rmtree(data_path)
        os.remove(stdin)

    def test_extract_command(self):
        configfile = "data/test_ex_config.yaml"

        # writing a file to test following pipeline
        data_path = "data/test_extract/"
        data_filepath = os.path.join(data_path, "test_extract_depth.dat")
        if not os.path.isdir(data_path):
            os.makedirs(data_path)

        write_fake_movie(data_filepath)
        assert os.path.isfile(data_filepath), "fake movie was not written correctly"

        if os.path.isfile(configfile):
            os.remove(configfile)

        generate_config_command(configfile)
        assert os.path.isfile(configfile)

        flip_file = "data/flip/flip_classifier_k2_c57_10to13weeks.pkl"
        if not os.path.isfile(flip_file):
            download_flip_command("data/flip/")

        assert os.path.isfile(flip_file), "flip file was not correctly downloaded"

        with open(configfile, "r") as f:
            config_data = yaml.safe_load(f)

        config_data["compress"] = True
        config_data["camera_type"] = "auto"
        config_data["flip_classifier"] = flip_file
        config_data["use_plane_bground"] = True
        config_data["bg_roi_index"] = 0
        config_data["bg_sort_roi_by_position"] = True

        with open(configfile, "w") as f:
            yaml.safe_dump(config_data, f)

        stdin = "data/stdin.txt"
        with open(stdin, "w") as f:
            f.write("Y")

        sys.stdin = open(stdin)

        ret = extract_command(data_filepath, None, configfile, skip=False)

        out_dir = "data/test_extract/proc/"

        assert os.path.isdir(out_dir), "proc directory was not created"
        assert "completed" in ret, "GUI command failed"

        shutil.rmtree(data_path)
        os.remove(stdin)

        with open(configfile, "r") as f:
            config_data = yaml.safe_load(f)

        config_data["camera_type"] = "auto"
        config_data["bg_roi_index"] = [0]
        config_data["flip_classifier"] = flip_file
        config_data["use_plane_bground"] = False
        config_data["bg_roi_depth_range"] = [500, 700]
        config_data["session_config_path"] = "data/session_config.yaml"

        with open(configfile, "w") as f:
            yaml.safe_dump(config_data, f)

        session_config = {"azure_test": deepcopy(config_data)}

        with open(config_data["session_config_path"], "w") as f:
            yaml.safe_dump(session_config, f)

        mkv_path = "data/azure_test/nfov_test.mkv"
        ret = extract_command(mkv_path, None, configfile, skip=False, num_frames=60)

        out_dir = "data/azure_test/proc/"
        h5file = os.path.join(out_dir, "results_00.h5")

        # ensure extraction is completed
        assert os.path.isdir(out_dir), "proc directory was not created"
        assert "completed" in ret, "GUI command failed"
        assert os.path.exists(h5file)
        assert os.path.exists("data/azure_test/metadata.json")

        # check extraction contents
        h5_test = h5py.File(h5file)
        assert "timestamps" in h5_test.keys()
        assert len(h5_test["frames"][()]) == len(h5_test["timestamps"][()]) == 60

        shutil.rmtree("data/flip/")
        shutil.rmtree(out_dir)
        os.remove(configfile)
        os.remove(config_data["session_config_path"])
        os.remove("data/azure_test/metadata.json")

    def test_aggregate_results_command(self):

        input_dir = "data/"
        ret = aggregate_extract_results_command(input_dir, "", "aggregate_results")

        assert ret == os.path.join(
            input_dir, "moseq2-index.yaml"
        ), "index file was not generated in correct directory"
        assert os.path.isfile(os.path.join(input_dir, "moseq2-index.yaml"))
        assert os.path.isdir(
            os.path.join(input_dir, "aggregate_results")
        ), "aggregate results directory was not created"

        shutil.rmtree(os.path.join(input_dir, "aggregate_results"))
        os.remove(os.path.join(input_dir, "moseq2-index.yaml"))

    def test_extract_found_sessions(self):

        config_path = "data/test_config.yaml"
        generate_config_command(config_path)

        # writing a file to test following pipeline
        data_filepath = "data/test_extract_found.dat"

        write_fake_movie(data_filepath)
        assert os.path.isfile(data_filepath), "fake movie was not written correctly"

        extract_found_sessions(data_filepath, config_path, ".dat", skip_extracted=True)
        os.remove(data_filepath)
        os.remove(config_path)

    def test_copy_h5_metadata_to_yaml(self):
        input_dir = "data/proc/"
        h5_metadata_path = "/metadata/acquisition/"

        # Functionality check
        copy_h5_metadata_to_yaml_wrapper(input_dir, h5_metadata_path)


--- File: tests/integration_tests/test_cli.py ---
import os
import cv2
import glob
import click
import shutil
import numpy as np
import ruamel.yaml as yaml
from os.path import exists
import numpy.testing as npt
from unittest import TestCase
from click.testing import CliRunner
from moseq2_extract.util import read_yaml
from moseq2_extract.cli import (
    find_roi,
    extract,
    download_flip_file,
    generate_config,
    convert_raw_to_avi,
    copy_slice,
    generate_index,
    aggregate_extract_results,
)


def write_fake_movie(data_path):
    edge_size = 40
    points = np.arange(-edge_size, edge_size)
    sig1 = 10
    sig2 = 20

    kernel = np.exp(-(points**2.0) / (2.0 * sig1**2.0))
    kernel2 = np.exp(-(points**2.0) / (2.0 * sig2**2.0))

    kernel_full = np.outer(kernel, kernel2)
    kernel_full /= np.max(kernel_full)
    kernel_full *= 50

    fake_mouse = kernel_full
    fake_mouse[fake_mouse < 5] = 0

    tmp_image = np.ones((424, 512), dtype="int16") * 1000
    center = np.array(tmp_image.shape) // 2

    mouse_dims = np.array(fake_mouse.shape) // 2

    # put a mouse on top of a disk

    roi = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (300, 300)).astype("int16") * 300
    roi_dims = np.array(roi.shape) // 2

    tmp_image[
        center[0] - roi_dims[0] : center[0] + roi_dims[0],
        center[1] - roi_dims[1] : center[1] + roi_dims[1],
    ] = (
        tmp_image[
            center[0] - roi_dims[0] : center[0] + roi_dims[0],
            center[1] - roi_dims[1] : center[1] + roi_dims[1],
        ]
        - roi
    )

    tmp_image[
        center[0] - mouse_dims[0] : center[0] + mouse_dims[0],
        center[1] - mouse_dims[1] : center[1] + mouse_dims[1],
    ] = (
        tmp_image[
            center[0] - mouse_dims[0] : center[0] + mouse_dims[0],
            center[1] - mouse_dims[1] : center[1] + mouse_dims[1],
        ]
        - fake_mouse
    )

    fake_movie = np.tile(tmp_image, (20, 1, 1))
    fake_movie.tofile(data_path)


class CLITests(TestCase):

    def test_aggregate_extract_results(self):

        input_dir = "data/"
        output_dir = "data/aggregate_results"

        params = ["-i", input_dir, "-o", output_dir]

        runner = CliRunner()
        result = runner.invoke(
            aggregate_extract_results, params, catch_exceptions=False
        )

        assert result.exit_code == 0, "CLI command did not successfully complete"
        assert os.path.isdir(output_dir), "aggregate results directory was not created"
        assert len(os.listdir(output_dir)) == 2
        shutil.rmtree(output_dir)

    def test_generate_index(self):
        input_dir = "data/"
        output_file = "data/moseq2-index.yaml"

        params = ["-i", input_dir, "-o", output_file]

        runner = CliRunner()
        result = runner.invoke(generate_index, params, catch_exceptions=False)

        assert result.exit_code == 0, "CLI command did not successfully complete"
        assert os.path.isfile(output_file)
        os.remove(output_file)

    def test_extract(self):

        data_path = "data/extract_test_depth.dat"
        config_file = "data/config.yaml"

        config_data = read_yaml(config_file)
        config_data["flip_classifier"] = None

        with open(config_file, "w+") as f:
            yaml.safe_dump(config_data, f)

        write_fake_movie(data_path)
        assert os.path.isfile(data_path), "fake movie was not written"

        runner = CliRunner()
        result = runner.invoke(
            extract,
            [
                data_path,
                "--output-dir",
                "test_out/",
                "--compute-raw-scalars",
                "--config-file",
                config_file,
                "--use-tracking-model",
                True,
                "--bg-roi-depth-range",
                "auto",
            ],
            catch_exceptions=False,
        )

        assert (
            result.exit_code == 2
        ), "CLI command did not successfully complete, bg-roi-depth-range must be a float tuple"
        assert not exists("data/test_out/")

        result_2 = runner.invoke(
            extract,
            [
                data_path,
                "--output-dir",
                "test_out/",
                "--config-file",
                config_file,
                "--bg-roi-depth-range",
                650,
                750,
                "--manual-set-depth-range",
            ],
            catch_exceptions=False,
        )

        assert result_2.exit_code == 0, "CLI command did not successfully complete"
        assert exists("data/test_out/")
        shutil.rmtree("data/test_out/")
        os.remove(data_path)

    def test_find_roi(self):

        data_path = "data/roi_test_depth.dat"
        out_path = "out/"
        output_dir = "data/out/"

        write_fake_movie(data_path)

        runner = CliRunner()
        result = runner.invoke(
            find_roi,
            [data_path, "--output-dir", out_path, "--bg-roi-depth-range", "auto"],
        )

        assert result.exit_code == 2, "CLI command did not successfully complete"
        assert (
            len(glob.glob(output_dir + "*.tiff")) < 3
        ), "ROI files were not generated in the correct directory"

        # using default cli bg-roi-depth-range values
        result = runner.invoke(
            find_roi, [data_path, "--output-dir", out_path, "--manual-set-depth-range"]
        )

        assert result.exit_code == 0, "CLI command did not successfully complete"
        assert (
            len(glob.glob(output_dir + "*.tiff")) == 3
        ), "ROI files were not generated in the correct directory"

        shutil.rmtree(output_dir)
        os.remove(data_path)

    def test_download_flip_file(self):

        data_path = "data/config.yaml"
        out_path = "data/flip/"

        runner = CliRunner()
        result = runner.invoke(
            download_flip_file, [data_path, "--output-dir", out_path], input="0\n"
        )
        assert result.exit_code == 0, "CLI command did not complete successfully"
        assert (
            len(glob.glob("data/flip/*.pkl")) > 0
        ), "Flip file was not downloaded correctly"

        shutil.rmtree(out_path)

    def test_generate_config(self):

        data_path = "data/test_config.yaml"

        runner = CliRunner()
        result = runner.invoke(generate_config, ["--output-file", data_path])
        yaml_data = yaml.load("data/", Loader=yaml.RoundTripLoader)
        temp_p = extract.params
        params = [param for param in temp_p if type(temp_p) is click.core.Option]

        for param in params:
            npt.assert_equal(yaml_data[param.human_readable_name], param.default)

        assert result.exit_code == 0, "CLI Command did not complete successfully"
        assert os.path.isfile(data_path), "Config file does not exist"
        os.remove(data_path)

    def test_convert_raw_to_avi(self):

        data_path = "data/convert_test_depth.dat"
        outfile = data_path.replace(".dat", ".avi")

        write_fake_movie(data_path)

        assert os.path.isfile(data_path), "temp depth file not created"

        runner = CliRunner()
        result = runner.invoke(
            convert_raw_to_avi, [data_path, "-o", outfile, "-b", 1000, "--delete"]
        )

        assert result.exit_code == 0, "CLI command did not complete successfully"
        assert os.path.isfile(outfile), "avi file not created"
        assert not os.path.exists(data_path), "raw file was not deleted"

        write_fake_movie(data_path)

        assert os.path.isfile(data_path), "temp depth file not created"

        result = runner.invoke(
            convert_raw_to_avi,
            [
                data_path,
                "-o",
                outfile,
                "-b",
                1000,
            ],
        )

        assert result.exit_code == 0, "CLI command did not complete successfully"
        assert os.path.isfile(outfile), "avi file not created"
        os.remove(data_path)
        os.remove(outfile)

    def test_copy_slice(self):

        data_path = "data/copy_slice_test_depth.dat"

        outfile = data_path.replace(".dat", ".avi")

        write_fake_movie(data_path)

        runner = CliRunner()
        result = runner.invoke(
            copy_slice, [data_path, "-o", outfile, "-b", 1000, "--delete"]
        )

        assert os.path.isfile(outfile), "slice was not copied correctly"
        assert not os.path.isfile(data_path), "input data was not deleted"
        assert result.exit_code == 0, "CLI command did not complete successfully"
        os.remove(outfile)


--- File: moseq2_extract/util.py ---
"""
General utility functions throughout the extract package.
"""
import os
import re
import cv2
import math
import json
import h5py
import click
import tarfile
import warnings
import numpy as np
from glob import glob
from copy import deepcopy
import ruamel.yaml as yaml
from typing import Pattern
from cytoolz import valmap
from moseq2_extract.io.image import write_image
from moseq2_extract.io.video import get_movie_info
from os.path import join, exists, splitext, basename, abspath, dirname


def filter_warnings(func):
    """
    Applies warnings.simplefilter() to ignore warnings when
     running the main gui functionaity in a Jupyter Notebook.
     The function will filter out: yaml.error.UnsafeLoaderWarning, FutureWarning and UserWarning.

    Args:
    func (function): function to silence enclosed warnings.

    Returns:
    apply_warning_filters (func): Returns passed function after warnings filtering is completed.
    """
    def apply_warning_filters(*args, **kwargs):
        with warnings.catch_warnings():
            warnings.simplefilter('ignore', yaml.error.UnsafeLoaderWarning)
            warnings.simplefilter(action='ignore', category=FutureWarning)
            warnings.simplefilter(action='ignore', category=UserWarning)
            return func(*args, **kwargs)
    return apply_warning_filters


# from https://stackoverflow.com/questions/46358797/
# python-click-supply-arguments-and-options-from-a-configuration-file
def command_with_config(config_file_param_name):
    """
    Override default CLI variables with the values contained within the config.yaml being passed.

    Args:
    config_file_param_name (str): path to config file.

    Returns:
    custom_command_class (function): decorator function to update click.Command parameters with the config_file
    parameter values.
    """

    class custom_command_class(click.Command):

        def invoke(self, ctx):
            # grab the config file
            config_file = ctx.params[config_file_param_name]
            param_defaults = {p.human_readable_name: p.default for p in self.params
                              if isinstance(p, click.core.Option)}
            param_defaults = {k: tuple(v) if type(v) is list else v for k, v in param_defaults.items()}
            param_cli = {k: tuple(v) if type(v) is list else v for k, v in ctx.params.items()}

            if config_file is not None:

                config_data = read_yaml(config_file)
                # set config_data['output_file'] ['output_dir'] ['input_dir'] to None to avoid overwriting previous files
                # assuming users would either input their own paths or use the default path
                config_data['input_dir'] = None
                config_data['output_dir'] = None
                config_data['output_file'] = None

                # modified to only use keys that are actually defined in options and the value is not not none
                config_data = {k: tuple(v) if isinstance(v, yaml.comments.CommentedSeq) else v
                               for k, v in config_data.items() if k in param_defaults.keys() and v is not None}

                # find differences btw config and param defaults
                diffs = set(param_defaults.items()) ^ set(param_cli.items())

                # combine defaults w/ config data
                combined = {**param_defaults, **config_data}

                # update cli params that are non-default
                keys = [d[0] for d in diffs]
                for k in set(keys):
                    combined[k] = ctx.params[k]

                ctx.params = combined
                
                # add new parameters to the original config file
                config_data = read_yaml(config_file)
                
                # remove flags from combined so the flag values in config.yaml won't get overwritten
                flag_list = ['manual_set_depth_range', 'use_plane_bground', 'progress_bar', 'delete', 'compute_raw_scalars', 'skip_completed', 'skip_checks', 'get_cmd', 'run_cmd']
                combined = {k:v for k, v in combined.items() if k not in flag_list}
                # combine original config data and the combined params prioritizing the combined
                config_data = {**config_data, **combined}
                # with open(config_file, 'w') as f:
                #     yaml.safe_dump(config_data, f)

            return super().invoke(ctx)

    return custom_command_class

def set_bground_to_plane_fit(bground_im, plane, output_dir):
    """
    Replaces median-computed background image with plane fit.
    Only occurs if config_data['use_plane_bground'] == True.

    Args:
    bground_im (numpy.ndarray): Background image computed via median value in each pixel of depth video.
    plane (numpy.ndarray): Computed ROI Plane using RANSAC.
    output_dir (str): Path to write updated background image to.

    Returns:
    bground_im (numpy.ndarray): The background image.
    """

    xx, yy = np.meshgrid(np.arange(bground_im.shape[1]), np.arange(bground_im.shape[0]))
    coords = np.vstack((xx.ravel(), yy.ravel()))

    plane_im = (np.dot(coords.T, plane[:2]) + plane[3]) / -plane[2]
    plane_im = plane_im.reshape(bground_im.shape)

    write_image(join(output_dir, 'bground.tiff'), plane_im, scale=True)

    return plane_im

def get_frame_range_indices(trim_beginning, trim_ending, nframes):
    """
    Compute the total number of frames to be extracted, and find the start and end indices.

    Args:
    trim_beginning (int): number of frames to remove from beginning of recording
    trim_ending (int): number of frames to remove from ending of recording
    nframes (int): total number of requested frames to extract

    Returns:
    nframes (int): total number of frames to extract
    first_frame_idx (int): index of the frame to begin extraction from
    last_frame_idx (int): index of the last frame in the extraction
    """
    assert all((trim_ending >= 0, trim_beginning >= 0)) , "frame_trim arguments must be greater than or equal to 0!"

    first_frame_idx = 0
    if trim_beginning > 0 and trim_beginning < nframes:
        first_frame_idx = trim_beginning

    last_frame_idx = nframes
    if first_frame_idx < (nframes - trim_ending) and trim_ending > 0:
        last_frame_idx = nframes - trim_ending

    total_frames = last_frame_idx - first_frame_idx

    return total_frames, first_frame_idx, last_frame_idx

def gen_batch_sequence(nframes, chunk_size, overlap, offset=0):
    """
    Generates batches used to chunk videos prior to extraction.

    Args:
    nframes (int): total number of frames
    chunk_size (int): the number of desired chunk size
    overlap (int): number of overlapping frames
    offset (int): frame offset

    Returns:
    out (list): the list of batches
    """

    seq = range(offset, nframes)
    out = []
    for i in range(0, len(seq) - overlap, chunk_size - overlap):
        out.append(seq[i:i + chunk_size])
    return out

def load_timestamps(timestamp_file, col=0, alternate=False):
    """
    Read timestamps from space delimited text file for timestamps.

    Args:
    timestamp_file (str): path to timestamp file
    col (int): column in ts file read.
    alternate (boolean): specified if timestamps were saved in a csv file. False means txt file and True means csv file.

    Returns:
    ts (1D array): list of timestamps
    """

    ts = []
    try:
        with open(timestamp_file, 'r') as f:
            for line in f:
                cols = line.split()
                ts.append(float(cols[col]))
        ts = np.array(ts)
    except TypeError as e:
        # try iterating directly
        for line in timestamp_file:
            cols = line.split()
            ts.append(float(cols[col]))
        ts = np.array(ts)
    except FileNotFoundError as e:
        ts = None
        warnings.warn('Timestamp file was not found! Make sure the timestamp file exists is named '
            '"depth_ts.txt" or "timestamps.csv".')
        warnings.warn('This could cause issues for large number of dropped frames during the PCA step while '
            'imputing missing data.')

    # if timestamps were saved in a csv file
    if alternate:
        ts = ts * 1000

    return ts

def detect_avi_file(finfo):
    """
    Detect the camera type by comparing the read video resolution with known
     outputted dimensions of different camera types.

    Args:
    finfo (dict): dictionary containing the file metadata,

    Returns:
    detected (str): name of the detected camera type.
    """

    detected = 'azure'
    potential_camera_dims = {
        'kinect': [[512, 424]],
        'realsense': [[640, 480]],
        'azure': [[640, 576],
                  [320, 288],
                  [512, 512],
                  [1024, 1024]]
    }

    # Check dimensions
    if finfo is not None:
        if list(finfo['dims']) in potential_camera_dims['azure']:
            # Default Azure dimensions
            detected = 'azure'
        elif list(finfo['dims']) in potential_camera_dims['realsense']:
            # Realsense D415 output dimensions
            detected = 'realsense'
        elif list(finfo['dims']) in potential_camera_dims['kinect']:
            # Kinect output dimensions
            detected = 'kinect'
        else:
            warnings.warn('Could not infer camera type, using default Azure parameters.')

    return detected

def detect_and_set_camera_parameters(config_data, input_file=None):
    """
    Read the camera type and info and set the bg_roi_weights to the precomputed values.
    If camera_type is None, function will assume kinect is used.

    Args:
    config_data (dict): dictionary containing all input parameters.
    input_file (str): path to raw depth file

    Returns:
    config_data (dict): updated dictionary with bg-roi-weights to use for extraction.
    """

    # Auto-setting background weights
    camera_type = config_data.get('camera_type')
    finfo = config_data.get('finfo')

    default_parameters = {
        'kinect': {
            'bg_roi_weights': (1, .1, 1),
            'pixel_format': 'gray16le',
            'movie_dtype': '<u2'
        },
        'azure': {
            'bg_roi_weights': (10, 0.1, 1),
            'pixel_format': 'gray16be',
            'movie_dtype': '>u2'
        },
        'realsense': {
            'bg_roi_weights': (10, 0.1, 1),
            'pixel_format': 'gray16le',
            'movie_dtype': '<u2',
        },
    }

    if type(input_file) is tarfile.TarFile:
        detected = 'kinect'
    elif camera_type == 'auto' and input_file is not None:
        if input_file.endswith('.dat'):
            detected = 'kinect'
        elif input_file.endswith('.mkv'):
            detected = 'azure'
        elif input_file.endswith('.avi'):
            if finfo is None:
                finfo = get_movie_info(input_file,
                                       mapping=config_data.get('mapping', 0),
                                       threads=config_data.get('threads', 8)
                                       )
            detected = detect_avi_file(finfo)
        else:
            warnings.warn('Extension not recognized, trying default Kinect v2 parameters.')
            detected = 'kinect'

        # set the params
        config_data.update(**default_parameters[detected])
    elif camera_type in default_parameters:
        # update the config with the corresponding param set
        config_data.update(**default_parameters[camera_type])
    else:
        warnings.warn('Warning, make sure the following parameters are set to best handle your camera type: '
                      '"bg_roi_weights", "pixel_format", "movie_dtype"')

    return config_data

def check_filter_sizes(config_data):
    """
    Ensure spatial and temporal filter kernel sizes are odd numbers.

    Args:
    config_data (dict): a dictionary holding all extraction parameters

    Returns:
    config_data (dict): Updated configuration dict

    """

    # Ensure filter kernel sizes are odd
    if config_data['spatial_filter_size'][0] % 2 == 0 and config_data['spatial_filter_size'][0] > 0:
        warnings.warn("Spatial Filter Size must be an odd number. Incrementing value by 1.")
        config_data['spatial_filter_size'][0] += 1
    if config_data['temporal_filter_size'][0] % 2 == 0 and config_data['temporal_filter_size'][0] > 0:
        config_data['temporal_filter_size'][0] += 1
        warnings.warn("Spatial Filter Size must be an odd number. Incrementing value by 1.")

    return config_data

def generate_missing_metadata(sess_dir, sess_name):
    """
    Generate metadata.json with default avlues for session that does not already include one.

    Args:
    sess_dir (str): Path to session directory to create metadata.json file in.
    sess_name (str): Session Name to set the metadata SessionName.

    Returns:
    """

    # generate sample metadata json for each session that is missing one
    sample_meta = {'SubjectName': '', f'SessionName': f'{sess_name}',
                   'NidaqChannels': 0, 'NidaqSamplingRate': 0.0, 'DepthResolution': [512, 424],
                   'ColorDataType': "Byte[]", "StartTime": ""}

    with open(join(sess_dir, 'metadata.json'), 'w') as fp:
        json.dump(sample_meta, fp)

def load_metadata(metadata_file):
    """
    Load metadata from session metadata.json file.

    Args:
    metadata_file (str): path to metadata file

    Returns:
    metadata (dict): metadata dictionary of JSON contents
    """

    try:
        if not exists(metadata_file):
            generate_missing_metadata(dirname(metadata_file), basename(dirname(metadata_file)))

        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
    except TypeError:
        # try loading directly
        metadata = json.load(metadata_file)

    return metadata

def load_found_session_paths(input_dir, exts):
    """
    Find all depth files with the specified extension recursively in input directory.

    Args:
    input_dir (str): path to project base directory holding all the session sub-folders.
    exts (list or str): list of extensions to search for, or a single extension in string form.

    Returns:
    files (list): sorted list of all paths to found depth files
    """

    if not isinstance(exts, (tuple, list)):
        exts = [exts]

    files = []
    for ext in exts:
        files.extend(glob(join(input_dir, '*/*' + ext), recursive=True))

    return sorted(files)

def get_strels(config_data):
    """
    Get dictionary object of cv2 StructuringElements for image filtering given
    a dict of configurations parameters.

    Args:
    config_data (dict): dict containing cv2 Structuring Element parameters

    Returns:
    str_els (dict): dict containing cv2 StructuringElements used for image filtering
    """

    str_els = {
        'strel_dilate': select_strel(config_data['bg_roi_shape'], tuple(config_data['bg_roi_dilate'])),
        'strel_erode': select_strel(config_data['bg_roi_shape'], tuple(config_data['bg_roi_erode'])),
        'strel_tail': select_strel(config_data['tail_filter_shape'], tuple(config_data['tail_filter_size'])),
        'strel_min': select_strel(config_data['cable_filter_shape'], tuple(config_data['cable_filter_size']))
    }

    return str_els

def select_strel(string='e', size=(10, 10)):
    """
    Returns structuring element of specified shape.

    Args:
    string (str): string to indicate whether to use ellipse or rectangle
    size (tuple): size of structuring element

    Returns:
    strel (cv2.StructuringElement): selected cv2 StructuringElement to use in video filtering or ROI dilation/erosion.
    """

    if string[0].lower() == 'e':
        strel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, size)
    elif string[0].lower() == 'r':
        strel = cv2.getStructuringElement(cv2.MORPH_RECT, size)
    else:
        strel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, size)

    return strel

def convert_pxs_to_mm(coords, resolution=(512, 424), field_of_view=(70.6, 60), true_depth=673.1):
    """
    Converts x, y coordinates in pixel space to mm.

    Args:
    coords (list): list of x,y pixel coordinates
    resolution (tuple): image dimensions
    field_of_view (tuple): width and height scaling params
    true_depth (float): detected true depth

    Returns:
    new_coords (list): x,y coordinates in mm
    """

    # http://stackoverflow.com/questions/17832238/kinect-intrinsic-parameters-from-field-of-view/18199938#18199938
    # http://www.imaginativeuniversal.com/blog/post/2014/03/05/quick-reference-kinect-1-vs-kinect-2.aspx
    # http://smeenk.com/kinect-field-of-view-comparison/

    cx = resolution[0] // 2
    cy = resolution[1] // 2

    xhat = coords[:, 0] - cx
    yhat = coords[:, 1] - cy

    fw = resolution[0] / (2 * np.deg2rad(field_of_view[0] / 2))
    fh = resolution[1] / (2 * np.deg2rad(field_of_view[1] / 2))

    new_coords = np.zeros_like(coords)
    new_coords[:, 0] = true_depth * xhat / fw
    new_coords[:, 1] = true_depth * yhat / fh

    return new_coords


def scalar_attributes():
    """
    Gets scalar attributes dict with names paired with descriptions.

    Returns:
    attributes (dict): a dictionary of metadata keys and descriptions.
    """

    attributes = {
        'centroid_x_px': 'X centroid (pixels)',
        'centroid_y_px': 'Y centroid (pixels)',
        'velocity_2d_px': '2D velocity (pixels / frame), note that missing frames are not accounted for',
        'velocity_3d_px': '3D velocity (pixels / frame), note that missing frames are not accounted for, also height is in mm, not pixels for calculation',
        'width_px': 'Mouse width (pixels)',
        'length_px': 'Mouse length (pixels)',
        'area_px': 'Mouse area (pixels)',
        'centroid_x_mm': 'X centroid (mm)',
        'centroid_y_mm': 'Y centroid (mm)',
        'velocity_2d_mm': '2D velocity (mm / frame), note that missing frames are not accounted for',
        'velocity_3d_mm': '3D velocity (mm / frame), note that missing frames are not accounted for',
        'width_mm': 'Mouse width (mm)',
        'length_mm': 'Mouse length (mm)',
        'area_mm': 'Mouse area (mm)',
        'height_ave_mm': 'Mouse average height (mm)',
        'angle': 'Angle (radians, unwrapped)',
        'velocity_theta': 'Angular component of velocity (arctan(vel_x, vel_y))'
    }

    return attributes


def convert_raw_to_avi_function(input_file, chunk_size=2000, fps=30, delete=False, threads=3):
    """
    Compress depth file (.dat, '.mkv') to avi file.

    Args:
    input_file (str): path to depth file
    chunk_size (int): size of chunks to process at a time
    fps (int): frames per second
    delete (bool): flag for deleting original depth file
    threads (int): number of threads to write video.

    """

    new_file = f'{splitext(input_file)[0]}.avi'
    print(f'Converting {input_file} to {new_file}')
    # turn into os system call...
    use_kwargs = {
        'output-file': new_file,
        'chunk-size': chunk_size,
        'fps': fps,
        'threads': threads
    }
    use_flags = {
        'delete': delete
    }
    base_command = f'moseq2-extract convert-raw-to-avi {input_file}'
    for k, v in use_kwargs.items():
        base_command += f' --{k} {v}'
    for k, v in use_flags.items():
        if v:
            base_command += f' --{k}'

    print(base_command)
    print()

    os.system(base_command)

def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S
    """
    Create subarrays of an array with a given stride and window length.

    Args:
    a (np.ndarray) - original array
    L (int) - Window Length
    S (int) - Stride size

    Returns:
    (np.ndarray) - array of subarrays
    """

    # https://stackoverflow.com/questions/40084931/taking-subarrays-from-numpy-array-with-given-stride-stepsize/40085052#40085052

    nrows = ((a.size-L)//S)+1
    n = a.strides[0]
    return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S*n, n))


def dict_to_h5(h5, dic, root='/', annotations=None):
    """
    Save an dict to an h5 file, mounting at root.
    Keys are mapped to group names recursively.

    Args:
    h5 (h5py.File instance): h5py.file object to operate on
    dic (dict): dictionary of data to write
    root (string): group on which to add additional groups and datasets
    annotations (dict): annotation data to add to corresponding h5 datasets. Should contain same keys as dic.
    
    """

    if not root.endswith('/'):
        root = root + '/'

    if annotations is None:
        annotations = {} #empty dict is better than None, but dicts shouldn't be default parameters

    for key, item in dic.items():
        dest = root + key
        try:
            if isinstance(item, (np.ndarray, np.int64, np.float64, str, bytes)):
                h5[dest] = item
            elif isinstance(item, (tuple, list)):
                h5[dest] = np.asarray(item)
            elif isinstance(item, (int, float)):
                h5[dest] = np.asarray([item])[0]
            elif item is None:
                h5.create_dataset(dest, data=h5py.Empty(dtype=h5py.special_dtype(vlen=str)))
            elif isinstance(item, dict):
                dict_to_h5(h5, item, dest)
            else:
                raise ValueError('Cannot save {} type to key {}'.format(type(item), dest))
        except Exception as e:
            print(e)
            if key != 'inputs':
                print('h5py could not encode key:', key)

        if key in annotations:
            if annotations[key] is None:
                h5[dest].attrs['description'] = ""
            else:
                h5[dest].attrs['description'] = annotations[key]


def recursive_find_h5s(root_dir=os.getcwd(),
                       ext='.h5',
                       yaml_string='{}.yaml'):
    """
    Recursively find h5 files, along with yaml files with the same basename

    Args:
    root_dir (str): path to base directory to begin recursive search in.
    ext (str): extension to search for
    yaml_string (str): string for filename formatting when saving data

    Returns:
    h5s (list): list of found h5 files
    dicts (list): list of found metadata files
    yamls (list): list of found yaml files
    """
    if not ext.startswith('.'):
        ext = '.' + ext

    def has_frames(f):
        try:
            with h5py.File(f, 'r') as h5f:
                return 'frames' in h5f
        except OSError:
            warnings.warn(f'Error reading {f}, skipping...')
            return False

    h5s = glob(join(abspath(root_dir), '**', f'*{ext}'), recursive=True)
    h5s = filter(lambda f: exists(yaml_string.format(f.replace(ext, ''))), h5s)
    h5s = list(filter(has_frames, h5s))
    yamls = list(map(lambda f: yaml_string.format(f.replace(ext, '')), h5s))
    dicts = list(map(read_yaml, yamls))

    return h5s, dicts, yamls


def escape_path(path):
    """
    Return a path to return to original base directory.

    Args:
    path (str): path to current working dir

    Returns:
    path (str): path to original base_dir
    """

    return re.sub(r'\s', '\ ', path)


def clean_file_str(file_str: str, replace_with: str = '-') -> str:
    """
    Removes invalid characters for a file name from a string.

    Args:
    file_str (str): filename substring to replace
    replace_with (str): value to replace str with

    Returns:
    out (str): cleaned file string
    """

    out = re.sub(r'[ <>:"/\\|?*\']', replace_with, file_str)
    # find any occurrences of `replace_with`, i.e. (--)
    return re.sub(replace_with * 2, replace_with, out)


def load_textdata(data_file, dtype=np.float32):
    """
    Loads timestamp from txt/csv file.

    Args:
    data_file (str): path to timestamp file
    dtype (dtype): data type of timestamps

    Returns:
    data (np.ndarray): timestamp data
    timestamps (numpy.array): the array for the timestamps
    """

    data = []
    timestamps = []
    with open(data_file, "r") as f:
        for line in f.readlines():
            tmp = line.split(' ', 1)
            # appending timestamp value
            timestamps.append(int(float(tmp[0])))

            # append data indicator value
            clean_data = np.fromstring(tmp[1].replace(" ", "").strip(), sep=',', dtype=dtype)
            data.append(clean_data)

    data = np.stack(data, axis=0).squeeze()
    timestamps = np.array(timestamps, dtype=np.int)

    return data, timestamps


def time_str_for_filename(time_str: str) -> str:
    """
    Process the timestamp to be used in the filename.

    Args:
    time_str (str): time str to format

    Returns:
    out (str): formatted timestamp str
    """

    out = time_str.split('.')[0]
    out = out.replace(':', '-').replace('T', '_')
    return out

def build_path(keys: dict, format_string: str, snake_case=True) -> str:
    """
    Produce a new file name using keys collected from extraction h5 files.

    Args:
    keys (dict): dictionary specifying which keys used to produce the new file name
    format_string (str): the string to reformat using the `keys` dictionary i.e. '{subject_name}_{session_name}'.
    snake_case (bool): flag to save the files with snake_case

    Returns:
    out (str): a newly formatted filename useable with any operating system
    """

    if 'start_time' in keys:
        # process the time value
        keys['start_time'] = time_str_for_filename(keys['start_time'])

    if snake_case:
        keys = valmap(camel_to_snake, keys)

    return clean_file_str(format_string.format(**keys))

def read_yaml(yaml_file):
    """
    Read yaml file into a dictionary

    Args:
    yaml_file (str): path to yaml file

    Returns:
    return_dict (dict): dict of yaml contents
    """

    with open(yaml_file, 'r') as f:
        return yaml.safe_load(f)

def mouse_threshold_filter(h5file, thresh=0):
    """
    Filter frames in h5 files by threshold value.

    Args:
    h5file (str): path to h5 file
    thresh (int): threshold at which to apply filter

    Returns:
    (3d-np boolean array): array of regions to include after threshold filter.
    """

    with h5py.File(h5file, 'r') as f:
        # select 1st 1000 frames
        frames = f['frames'][:min(f['frames'].shape[0], 1000)]
    return np.nanmean(frames) > thresh

def _load_h5_to_dict(file: h5py.File, path) -> dict:
    """
    Loads h5 contents to dictionary object.

    Args:
    h5file (h5py.File): file path to the given h5 file or the h5 file handle
    path (str): path to the base dataset within the h5 file

    Returns:
    ans (dict): a dict with h5 file contents with the same path structure
    """

    ans = {}
    for key, item in file[path].items():
        if isinstance(item, h5py._hl.dataset.Dataset):
            ans[key] = item[()]
        elif isinstance(item, h5py._hl.group.Group):
            ans[key] = _load_h5_to_dict(file, '/'.join([path, key]))
    return ans


def h5_to_dict(h5file, path) -> dict:
    """
    Load h5 contents to dictionary object.

    Args:
    h5file (str or h5py.File): file path to the given h5 file or the h5 file handle
    path (str): path to the base dataset within the h5 file

    Returns:
    out (dict): a dict with h5 file contents with the same path structure
    """

    if isinstance(h5file, str):
        with h5py.File(h5file, 'r') as f:
            out = _load_h5_to_dict(f, path)
    elif isinstance(h5file, h5py.File):
        out = _load_h5_to_dict(h5file, path)
    else:
        raise Exception('file input not understood - need h5 file path or file object')
    return out

def clean_dict(dct):
    """
    Standardize types of dict value.

    Args:
    dct (dict): dict object with mixed type value objects.

    Returns:
    out (dict): dict object with list value objects.
    """

    def clean_entry(e):
        if isinstance(e, dict):
            out = clean_dict(e)
        elif isinstance(e, np.ndarray):
            out = e.tolist()
        elif isinstance(e, np.generic):
            out = np.asscalar(e)
        else:
            out = e
        return out

    return valmap(clean_entry, dct)

_underscorer1: Pattern[str] = re.compile(r'(.)([A-Z][a-z]+)')
_underscorer2 = re.compile('([a-z0-9])([A-Z])')

def camel_to_snake(s):
    """
    Convert CamelCase to snake_case

    Args:
    s (str): CamelCase string to convert to snake_case.

    Returns:
    (str): string in snake_case
    """

    subbed = _underscorer1.sub(r'\1_\2', s)
    return _underscorer2.sub(r'\1_\2', subbed).lower()


def recursive_find_unextracted_dirs(root_dir=os.getcwd(),
                                    session_pattern=r'session_\d+\.(?:tgz|tar\.gz)',
                                    extension='.dat',
                                    yaml_path='proc/results_00.yaml',
                                    metadata_path='metadata.json',
                                    skip_checks=False):
    """
    Recursively find unextracted (or incompletely extracted) directories

    Args:
    root_dir (str): path to base directory to start recursive search for unextracted folders.
    session_pattern (str): folder name pattern to search for
    extension (str): file extension to search for
    yaml_path (str): path to respective extracted metadata
    metadata_path (str): path to relative metadata.json files
    skip_checks (bool): indicates whether to check if the files exist at the given relative paths

    Returns:
    proc_dirs (1d-list): list of paths to each unextracted session's proc/ directory
    """

    from moseq2_extract.helpers.data import check_completion_status

    session_archive_pattern = re.compile(session_pattern)

    proc_dirs = []
    for root, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(extension) and not file.startswith("ir"):  # test for uncompressed session
                status_file = join(root, yaml_path)
                metadata_file = join(root, metadata_path)
            elif session_archive_pattern.fullmatch(file):  # test for compressed session
                session_name = basename(file).replace('.tar.gz', '').replace('.tgz', '')
                status_file = join(root, session_name, yaml_path)
                metadata_file = join(root, '{}.json'.format(session_name))
            else:
                continue  # skip this current file as it does not look like session data

            # perform checks, append depth file to list if extraction is missing or incomplete
            if skip_checks or (not check_completion_status(status_file) and exists(metadata_file)):
                proc_dirs.append(join(root, file))

    return proc_dirs

def click_param_annot(click_cmd):
    """
    Return a dict that maps option names to help strings from a click.Command instance.

    Args:
    click_cmd (click.Command): command to annotate

    Returns:
    annotations (dict): dictionary of options and their help messages
    """

    annotations = {}
    for p in click_cmd.params:
        if isinstance(p, click.Option):
            annotations[p.human_readable_name] = p.help
    return annotations

def get_bucket_center(img, true_depth, threshold=650):
    """
    Find Centroid coordinates of circular bucket.

    Args:
    img (np.ndaarray): original background image.
    true_depth (float): distance value from camera to bucket floor (automatically pre-computed)
    threshold (float): distance values to accept region into detected circle. (used to reduce fall noise interference)

    Returns:
    cX (int): x-coordinate of circle centroid
    cY (int): y-coordinate of circle centroid
    """

    # https://stackoverflow.com/questions/19768508/python-opencv-finding-circle-sun-coordinates-of-center-the-circle-from-pictu
    # convert the grayscale image to binary image
    ret, thresh = cv2.threshold(img, threshold, true_depth, 0)

    # calculate moments of binary image
    M = cv2.moments(thresh)

    # calculate x,y coordinate of center
    cX = int(M["m10"] / M["m00"])
    cY = int(M["m01"] / M["m00"])

    return cX, cY

def make_gradient(width, height, h, k, a, b, theta=0):
    """
    Create gradient around bucket floor representing slanted wall values.

    Args:
    width (int): bounding box width
    height (int) bounding box height
    h (int): centroid x coordinate
    k (int): centroid y coordinate
    a (int): x-radius of drawn ellipse
    b (int): y-radius of drawn ellipse
    theta (float): degree to rotate ellipse in radians. (has no effect if drawing a circle)

    Returns:
    np.ndarray: numpy array with weighted values from 0.08 -> 0.8 representing the proportion of values
    to create a gradient from. 0.8 being the proportioned values closest to the circle wall.
    """

    # https://stackoverflow.com/questions/49829783/draw-a-gradual-change-ellipse-in-skimage/49848093#49848093
    # Precalculate constants
    st, ct = math.sin(theta), math.cos(theta)
    aa, bb = a ** 2, b ** 2

    # Generate (x,y) coordinate arrays
    y, x = np.mgrid[-k:height - k, -h:width - h]

    # Calculate the weight for each pixel
    weights = (((x * ct + y * st) ** 2) / aa) + (((x * st - y * ct) ** 2) / bb)

    return np.clip(0.98 - weights, 0, 0.81)


def graduate_dilated_wall_area(bground_im, config_data, strel_dilate, output_dir):
    """
    Creates a gradient to represent the dilated (now visible) bucket wall regions.
    Only is used if background is dilated to capture larger rodents in convex shaped buckets (\_/).
    
    Args:
    bground_im (np.ndarray): the computed background image.
    config_data (dict): dictionary containing helper user configuration parameters.
    strel_dilate (cv2.structuringElement): dilation structuring element used to dilate background image.
    output_dir (str): path to save newly computed background to use.

    Returns:
    bground_im (np.ndarray): the new background image with a gradient around the floor from high to low depth values.
    """

    # store old and new backgrounds
    old_bg = deepcopy(bground_im)

    # dilate background size to match ROI size and attribute wall noise to cancel out
    bground_im = cv2.dilate(old_bg, strel_dilate, iterations=config_data.get('dilate_iterations', 5))

    # determine center of bground roi
    width, height = bground_im.shape[1], bground_im.shape[0]  # shape of bounding box

    # getting helper user parameters
    true_depth = config_data['true_depth']
    xoffset = config_data.get('x_bg_offset', -2)
    yoffset = config_data.get('y_bg_offset', 2)
    widen_radius = config_data.get('widen_radius', 0)
    bg_threshold = config_data.get('bg_threshold', np.median(bground_im))

    # getting bground centroid
    cx, cy = get_bucket_center(deepcopy(old_bg), true_depth, threshold=bg_threshold)

    # set up gradient
    h, k = cx + xoffset, cy + yoffset   # centroid of gradient circle
    a, b = cx + widen_radius + 67, cy + widen_radius + 67 # x,y radii of gradient circle
    theta = math.pi/24 # gradient angle; arbitrary - used to rotate ellipses.

    # create slant gradient
    bground_im = np.uint16((make_gradient(width, height, h, k, a, b, theta)) * 255)

    # scale it back to depth
    bground_im = np.uint16((bground_im/bground_im.max())*true_depth)

    # overlay with actual bucket floor distance
    if config_data.get('floor_slant', False):
        ret, thresh = cv2.threshold(old_bg, np.median(old_bg), true_depth, 0)
        contours, _ = cv2.findContours(thresh.copy().astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        mask = np.zeros(bground_im.shape, np.uint8)

        cv2.drawContours(mask, contours, -1, (255), -1)

        tmp = np.where(mask == True, bground_im, old_bg)
        bground_im = np.where(tmp == 0, bground_im, tmp)
    else:
        mask = np.ma.equal(old_bg, old_bg.max())
        bground_im = np.where(mask == True, old_bg, bground_im)

    bground_im = cv2.GaussianBlur(bground_im, (7, 7), 7)

    write_image(join(output_dir, 'new_bg.tiff'), bground_im, scale=True)

    return bground_im


--- File: moseq2_extract/__init__.py ---
__version__ = "1.2.0"


--- File: moseq2_extract/cli.py ---
"""
CLI for extracting the depth data.
"""

import os
import click
import ruamel.yaml as yaml
from tqdm.auto import tqdm
from copy import deepcopy
from moseq2_extract.util import (
    command_with_config,
    read_yaml,
    recursive_find_unextracted_dirs,
)
from moseq2_extract.helpers.wrappers import (
    get_roi_wrapper,
    extract_wrapper,
    flip_file_wrapper,
    generate_index_wrapper,
    aggregate_extract_results_wrapper,
    generate_index_from_agg_res_wrapper,
    convert_raw_to_avi_wrapper,
    copy_slice_wrapper,
)
from moseq2_extract.helpers.extract import run_slurm_extract, run_local_extract

orig_init = click.core.Option.__init__


def new_init(self, *args, **kwargs):
    orig_init(self, *args, **kwargs)
    self.show_default = True


click.core.Option.__init__ = new_init


@click.group()
@click.version_option()
def cli():
    pass


def common_roi_options(function):
    """
    Decorator function for grouping shared ROI related parameters.

    Args:
    function: Function to add enclosed parameters to as click options.

    Returns:
    function: Updated function including shared parameters.
    """

    function = click.option(
        "--bg-roi-dilate",
        default=(10, 10),
        type=(int, int),
        help="Size of StructuringElement to dilate roi",
    )(function)
    function = click.option(
        "--bg-roi-shape",
        default="ellipse",
        type=str,
        help="Shape to use to detect roi (ellipse or rect)",
    )(function)
    function = click.option(
        "--bg-roi-index",
        default=0,
        type=int,
        help="Index of which detected ROI mask to use",
    )(function)
    function = click.option(
        "--bg-roi-weights",
        default=(1, 0.1, 1),
        type=(float, float, float),
        help="ROI feature weighting (area, extent, dist to center)",
    )(function)
    function = click.option(
        "--camera-type",
        default="auto",
        type=click.Choice(["auto", "kinect", "azure", "realsense"]),
        help='Camera type used for recording for auto-sets bg-roi-weights to precomputed values for different camera types. \
                             Possible types: ["kinect", "azure", "realsense"]',
    )(function)
    function = click.option(
        "--manual-set-depth-range",
        is_flag=True,
        help="Flag to deactivate auto depth range setting.",
    )(function)
    function = click.option(
        "--bg-roi-depth-range",
        default=(650, 750),
        type=(float, float),
        help="Range to search for floor of arena (in mm)",
    )(function)
    function = click.option(
        "--bg-roi-gradient-filter",
        default=False,
        type=bool,
        help="Use graident filter to exclude walls for detected ROI",
    )(function)
    function = click.option(
        "--bg-roi-gradient-threshold",
        default=3000,
        type=float,
        help="Gradient must be less than threshold to include points",
    )(function)
    function = click.option(
        "--bg-roi-gradient-kernel",
        default=7,
        type=int,
        help="Kernel size for Sobel gradient filtering",
    )(function)
    function = click.option(
        "--bg-roi-fill-holes", default=True, type=bool, help="Fill holes in ROI"
    )(function)
    function = click.option(
        "--bg-sort-roi-by-position",
        default=False,
        type=bool,
        help="Sort ROIs by position",
    )(function)
    function = click.option(
        "--bg-sort-roi-by-position-max-rois",
        default=2,
        type=int,
        help="The number of maximum ROIs sorted by area",
    )(function)
    function = click.option(
        "--dilate-iterations",
        default=1,
        type=int,
        help="Number of dilation iterations to increase bucket floor size.",
    )(function)
    function = click.option(
        "--bg-roi-erode",
        default=(1, 1),
        type=(int, int),
        help="Size of cv2 Structure Element to erode roi.",
    )(function)
    function = click.option(
        "--erode-iterations",
        default=0,
        type=int,
        help="Number of erosion iterations to decrease bucket floor size.",
    )(function)
    function = click.option(
        "--noise-tolerance",
        default=30,
        type=int,
        help="Extent of noise to accept during RANSAC Plane ROI computation.",
    )(function)
    function = click.option(
        "--output-dir",
        default="proc",
        help="Output directory to save the results h5 file",
    )(function)
    function = click.option(
        "--use-plane-bground",
        is_flag=True,
        help="Use a plane fit for the background. Useful when mice don't move much",
    )(function)
    function = click.option(
        "--recompute-bg",
        default=False,
        help="Overwrite previously computed background image",
    )(function)
    function = click.option("--config-file", type=click.Path())(function)
    function = click.option(
        "--progress-bar", "-p", is_flag=True, help="Show verbose progress bars."
    )(function)
    return function


def common_avi_options(function):
    """
    Decorator function for grouping shared video processing parameters.

    Args:
    function: Function to add enclosed parameters to as click options.

    Returns:
    function: Updated function including shared parameters.
    """

    function = click.option(
        "-o",
        "--output-file",
        type=click.Path(),
        default=None,
        help="Path to output file",
    )(function)
    function = click.option(
        "-b", "--chunk-size", type=int, default=3000, help="Chunk size"
    )(function)
    function = click.option("--fps", type=float, default=30, help="Video FPS")(function)
    function = click.option(
        "--delete", is_flag=True, help="Delete raw file if encoding is sucessful"
    )(function)
    function = click.option(
        "-t", "--threads", type=int, default=8, help="Number of threads for encoding"
    )(function)
    function = click.option(
        "-m",
        "--mapping",
        type=str,
        default="DEPTH",
        help="Ffprobe stream selection variable. Default: DEPTH",
    )(function)

    return function


def extract_options(function):
    """
    Decorator function for grouping shared extraction prameters.

    Args:
    function : Function to add enclosed parameters to as click options.

    Returns:
    function: Updated function including shared parameters.
    """

    function = click.option(
        "--crop-size",
        "-c",
        default=(80, 80),
        type=(int, int),
        help="Width and height of cropped mouse image",
    )(function)
    function = click.option(
        "--num-frames",
        "-n",
        default=None,
        type=int,
        help="Number of frames to extract. Will extract full session if set to None.",
    )(function)
    function = click.option(
        "--min-height",
        default=10,
        type=int,
        help="Min mouse height threshold from floor (mm)",
    )(function)
    function = click.option(
        "--max-height",
        default=120,
        type=int,
        help="Max mouse height threshold from floor (mm)",
    )(function)
    function = click.option(
        "--detected-true-depth",
        default="auto",
        type=str,
        help='Option to override automatic depth estimation during extraction. \
This is only a debugging parameter, for cases where dilate_iterations > 1, otherwise has no effect. Either "auto" or an int value.',
    )(function)
    function = click.option(
        "--compute-raw-scalars",
        is_flag=True,
        help="Compute scalar values from raw cropped frames.",
    )(function)
    function = click.option(
        "--flip-classifier",
        default=None,
        help="path to the flip classifier used to properly orient the mouse (.pkl file)",
    )(function)
    function = click.option(
        "--flip-classifier-smoothing",
        default=51,
        type=int,
        help="Number of frames to smooth flip classifier probabilities",
    )(function)
    function = click.option(
        "--graduate-walls",
        default=False,
        type=bool,
        help="Graduates and dilates the background image to compensate for slanted bucket walls. \\_/",
    )(function)
    function = click.option(
        "--widen-radius",
        default=0,
        type=int,
        help="Number of pixels to increase/decrease radius by when graduating bucket walls.",
    )(function)
    function = click.option(
        "--use-cc",
        default=True,
        type=bool,
        help="Extract features using largest connected components.",
    )(function)
    function = click.option(
        "--use-tracking-model",
        default=False,
        type=bool,
        help="Use an expectation-maximization style model to aid mouse tracking. Useful for data with cables",
    )(function)
    function = click.option(
        "--tracking-model-ll-threshold",
        default=-100,
        type=float,
        help="Threshold on log-likelihood for pixels to use for update during tracking",
    )(function)
    function = click.option(
        "--tracking-model-mask-threshold",
        default=-16,
        type=float,
        help="Threshold on log-likelihood to include pixels for centroid and angle calculation",
    )(function)
    function = click.option(
        "--tracking-model-ll-clip",
        default=-100,
        type=float,
        help="Clip log-likelihoods below this value",
    )(function)
    function = click.option(
        "--tracking-model-segment",
        default=True,
        type=bool,
        help="Segment likelihood mask from tracking model",
    )(function)
    function = click.option(
        "--tracking-model-init",
        default="raw",
        type=str,
        help="Method for tracking model initialization",
    )(function)
    function = click.option(
        "--cable-filter-iters",
        default=0,
        type=int,
        help="Number of cable filter iterations",
    )(function)
    function = click.option(
        "--cable-filter-shape",
        default="rectangle",
        type=str,
        help="Cable filter shape (rectangle or ellipse)",
    )(function)
    function = click.option(
        "--cable-filter-size",
        default=(5, 5),
        type=(int, int),
        help="Cable filter size (in pixels)",
    )(function)
    function = click.option(
        "--tail-filter-iters",
        default=1,
        type=int,
        help="Number of tail filter iterations",
    )(function)
    function = click.option(
        "--tail-filter-size", default=(9, 9), type=(int, int), help="Tail filter size"
    )(function)
    function = click.option(
        "--tail-filter-shape", default="ellipse", type=str, help="Tail filter shape"
    )(function)
    function = click.option(
        "--spatial-filter-size",
        "-s",
        default=[3],
        type=int,
        help="Space prefilter kernel (median filter, must be odd)",
        multiple=True,
    )(function)
    function = click.option(
        "--temporal-filter-size",
        "-t",
        default=[0],
        type=int,
        help="Time prefilter kernel (median filter, must be odd)",
        multiple=True,
    )(function)
    function = click.option(
        "--chunk-overlap",
        default=0,
        type=int,
        help="Frames overlapped in each chunk. Useful for cable tracking",
    )(function)
    function = click.option(
        "--write-movie",
        default=True,
        type=bool,
        help="Write a results output movie including an extracted mouse",
    )(function)
    function = click.option(
        "--frame-dtype",
        default="uint8",
        type=click.Choice(["uint8", "uint16"]),
        help="Data type for processed frames",
    )(function)
    function = click.option(
        "--movie-dtype",
        default="<i2",
        help="Data type for raw frames read in for extraction",
    )(function)
    function = click.option(
        "--pixel-format",
        default="gray16le",
        type=str,
        help="Pixel format for reading in .avi and .mkv videos",
    )(function)
    function = click.option(
        "--centroid-hampel-span", default=0, type=int, help="Hampel filter span"
    )(function)
    function = click.option(
        "--centroid-hampel-sig", default=3, type=float, help="Hampel filter sig"
    )(function)
    function = click.option(
        "--angle-hampel-span", default=0, type=int, help="Angle filter span"
    )(function)
    function = click.option(
        "--angle-hampel-sig", default=3, type=float, help="Angle filter sig"
    )(function)
    function = click.option(
        "--model-smoothing-clips",
        default=(0, 0),
        type=(float, float),
        help="Model smoothing clips",
    )(function)
    function = click.option(
        "--frame-trim",
        default=(0, 0),
        type=(int, int),
        help="Frames to trim from beginning and end of data",
    )(function)
    function = click.option(
        "--compress",
        default=False,
        type=bool,
        help="Convert .dat to .avi after successful extraction",
    )(function)
    function = click.option(
        "--compress-chunk-size",
        type=int,
        default=3000,
        help="Chunk size for .avi compression",
    )(function)
    function = click.option(
        "--compress-threads", type=int, default=3, help="Number of threads for encoding"
    )(function)
    function = click.option(
        "--skip-completed",
        is_flag=True,
        help="Will skip the extraction if it is already completed.",
    )(function)

    return function


@cli.command(
    name="find-roi",
    cls=command_with_config("config_file"),
    help="Finds the ROI (the arena) and background to subtract from frames when extracting.",
)
@click.argument("input-file", type=click.Path(exists=True))
@common_roi_options
def find_roi(input_file, output_dir, **config_data):

    get_roi_wrapper(input_file, config_data, output_dir)


@cli.command(
    name="extract",
    cls=command_with_config("config_file"),
    help="Processes raw input depth recordings to output a cropped and oriented"
    "video of the mouse and saves the output+metadata to h5 files in the given output directory.",
)
@click.argument("input-file", type=click.Path(exists=True, resolve_path=False))
@click.option(
    "--cluster-type",
    type=click.Choice(["local", "slurm"]),
    default="local",
    help="Platform to train models on",
)
@common_roi_options
@common_avi_options
@extract_options
def extract(input_file, output_dir, num_frames, skip_completed, **config_data):

    extract_wrapper(
        input_file, output_dir, config_data, num_frames=num_frames, skip=skip_completed
    )


@cli.command(
    name="batch-extract",
    cls=command_with_config("config_file"),
    help="Batch processes " "all the raw depth recordings located in the input folder.",
)
@click.argument("input-folder", type=click.Path(exists=True, resolve_path=False))
@common_roi_options
@common_avi_options
@extract_options
@click.option(
    "--extensions",
    default=[".dat"],
    type=str,
    help="File extension of raw data",
    multiple=True,
)
@click.option(
    "--skip-checks",
    is_flag=True,
    help="Flag: skip checks for the existance of a metadata file",
)
@click.option(
    "--extract-out-script",
    type=click.Path(),
    default="extract_out.sh",
    help="Name of bash script file to save extract commands.",
)
@click.option(
    "--cluster-type",
    type=click.Choice(["local", "slurm"]),
    default="local",
    help="Platform to train models on",
)
@click.option(
    "--prefix",
    type=str,
    default="",
    help="Batch command string to prefix model training command (slurm only).",
)
@click.option(
    "--ncpus", "-c", type=int, default=1, help="Number of cores to use in extraction"
)
@click.option("--memory", type=str, default="5GB", help="RAM (slurm only)")
@click.option("--wall-time", type=str, default="3:00:00", help="Wall time (slurm only)")
@click.option(
    "--partition", type=str, default="short", help="Partition name (slurm only)"
)
@click.option(
    "--get-cmd", is_flag=True, default=True, help="Print scan command strings."
)
@click.option("--run-cmd", is_flag=True, help="Run scan command strings.")
def batch_extract(
    input_folder,
    output_dir,
    skip_completed,
    num_frames,
    extensions,
    skip_checks,
    **config_data,
):

    # check if there is a config file
    config_file = config_data.get("config_file")
    if not config_file:
        # Add message to tell the users to specify a config file
        print(
            "Command not run. Please specified a config file using --config-file flag."
        )
        return

    # Add message to tell the users to specify a config file
    to_extract = []
    for ex in extensions:
        to_extract.extend(
            recursive_find_unextracted_dirs(
                input_folder,
                extension=ex,
                skip_checks=True if ex in (".tgz", ".tar.gz") else skip_checks,
                yaml_path=os.path.join(output_dir, "results_00.yaml"),
            )
        )

    # Add message when all sessions are extracted
    if len(to_extract) == 0:
        print(
            'No session to be extracted. If you want to re-extract the data, please add "--skip-checks"'
        )
        return

    if config_data["cluster_type"] == "local":
        # the session specific config doesn't get generated in session proc file
        # session specific config direct used in config_data dictionary in extraction from extract_command function
        run_local_extract(to_extract, config_file, skip_completed)
    else:
        # add paramters to config
        config_data["session_config_path"] = (
            read_yaml(config_file).get("session_config_path", "")
            if config_file is not None
            else ""
        )
        config_data["config_file"] = os.path.abspath(config_file)
        config_data["output_dir"] = output_dir
        config_data["skip_completed"] = skip_completed
        config_data["num_frames"] = num_frames
        config_data["extensions"] = extensions
        config_data["skip_checks"] = skip_checks
        # run slurm extract will generate a config.yaml in session proc file for slurm
        run_slurm_extract(input_folder, to_extract, config_data, skip_completed)


@cli.command(
    name="download-flip-file",
    help="Downloads Flip-correction model that helps with orienting the mouse during extraction.",
)
@click.argument(
    "config-file",
    type=click.Path(exists=True, resolve_path=False),
    default="config.yaml",
)
@click.option(
    "--output-dir",
    type=click.Path(),
    default=os.getcwd(),
    help="Output directory for downloaded flip flie",
)
def download_flip_file(config_file, output_dir):

    flip_file_wrapper(config_file, output_dir)


@cli.command(
    name="generate-config",
    help="Generates a configuration file (config.yaml) that holds editable options for extraction parameters.",
)
@click.option("--output-file", "-o", type=click.Path(), default="config.yaml")
@click.option(
    "--camera-type",
    default="k2",
    type=str,
    help="specify the camera type (k2 or azure), default is k2",
)
def generate_config(output_file, camera_type):

    objs = extract.params
    params = {tmp.name: tmp.default for tmp in objs if not tmp.required}
    if camera_type == "azure":
        params["bg_roi_depth_range"] = [550, 650]
        params["spatial_filter_size"] = [5]
        params["tail_filter_size"] = [15, 15]
        params["crop_size"] = [120, 120]
        params["camera_type"] = "azure"

    with open(output_file, "w") as f:
        yaml.safe_dump(params, f)

    print("Successfully generated config file in base directory.")


@cli.command(
    name="generate-index",
    help="Generates an index file (moseq2-index.yaml) that contains all extracted session metadata.",
)
@click.option(
    "--input-dir",
    "-i",
    type=click.Path(),
    default=os.getcwd(),
    help="Directory to find h5 files",
)
@click.option(
    "--output-file",
    "-o",
    type=click.Path(),
    default=os.path.join(os.getcwd(), "moseq2-index.yaml"),
    help="Location for storing index",
)
def generate_index(input_dir, output_file):

    output_file = generate_index_wrapper(input_dir, output_file)

    if output_file is not None:
        print(f"Index file: {output_file} was successfully generated.")


@cli.command(
    name="aggregate-results",
    help="Copies all extracted results (h5, yaml, mp4) files from all extracted sessions to a new directory for modeling and analysis",
)
@click.option(
    "--input-dir",
    "-i",
    type=click.Path(),
    default=os.getcwd(),
    help="Directory to find h5 files",
)
@click.option(
    "--format",
    "-f",
    type=str,
    default="{start_time}_{session_name}_{subject_name}",
    help="New file name formats from resepective metadata",
)
@click.option(
    "--output-dir",
    "-o",
    type=click.Path(),
    default=os.path.join(os.getcwd(), "aggregate_results/"),
    help="Location for storing all results together",
)
@click.option(
    "--mouse-threshold",
    default=0,
    type=float,
    help="Threshold value for mean depth to include frames in aggregated results",
)
def aggregate_extract_results(input_dir, format, output_dir, mouse_threshold):

    aggregate_extract_results_wrapper(input_dir, format, output_dir, mouse_threshold)


@cli.command(
    name="agg-to-index",
    help="Generate an index file from aggregated results with default as group names",
)
@click.option(
    "--input-dir",
    "-i",
    type=click.Path(),
    default=os.path.join(os.getcwd(), "aggregate_results"),
    help="Directory for aggregated results folder",
)
def agg_to_index(input_dir):

    generate_index_from_agg_res_wrapper(input_dir)


@cli.command(
    name="convert-raw-to-avi",
    help="Loss less compresses a raw depth file (dat) into an avi file that is 8x smaller.",
)
@click.argument("input-file", type=click.Path(exists=True, resolve_path=False))
@common_avi_options
def convert_raw_to_avi(
    input_file, output_file, chunk_size, fps, delete, threads, mapping
):

    convert_raw_to_avi_wrapper(
        input_file, output_file, chunk_size, fps, delete, threads, mapping
    )


@cli.command(
    name="copy-slice",
    help="Copies a segment of an input depth recording into a new video file.",
)
@click.argument("input-file", type=click.Path(exists=True, resolve_path=False))
@common_avi_options
@click.option(
    "-c",
    "--copy-slice",
    type=(int, int),
    default=(0, 1000),
    help="Slice indices used for copy",
)
def copy_slice(
    input_file, output_file, copy_slice, chunk_size, fps, delete, threads, mapping
):

    copy_slice_wrapper(
        input_file, output_file, copy_slice, chunk_size, fps, delete, threads, mapping
    )


if __name__ == "__main__":
    cli()


--- File: moseq2_extract/gui.py ---
"""
GUI front-end operations accessible from a jupyter notebook.
"""

import os
import ruamel.yaml as yaml
from ast import literal_eval
from os.path import dirname, basename, exists, join
from moseq2_extract.util import read_yaml
from moseq2_extract.io.image import read_tiff_files
from moseq2_extract.helpers.extract import run_local_extract, run_slurm_extract
from moseq2_extract.helpers.wrappers import (
    get_roi_wrapper,
    extract_wrapper,
    flip_file_wrapper,
    generate_index_wrapper,
    aggregate_extract_results_wrapper,
)
from moseq2_extract.util import (
    recursive_find_unextracted_dirs,
    load_found_session_paths,
    filter_warnings,
)
from moseq2_extract.cli import batch_extract


def get_selected_sessions(to_extract, extract_all):
    """
    Return either selected sessions to extract, or all the sessions given user input, the function will

    Args:
    to_extract (list): list of paths to sessions to extract
    extract_all (bool): boolean to include all sessions and skip user-input prompt.

    Returns:
    to_extract (list): new list of selected sessions to extract.
    """

    selected_sess_idx, excluded_sess_idx, ret_extract = [], [], []

    def parse_input(s):
        """
        Parse user input for specifically numbered sessions, ranges of sessions,
        and/or sessions to exclude.

        Args:
        s (str): User input session indices.
        """
        if "e" not in s and "-" not in s:
            if isinstance(literal_eval(s), int):
                selected_sess_idx.append(int(s))
        elif "e" not in s and "-" in s:
            ss = s.split("-")
            if isinstance(literal_eval(ss[0]), int) and isinstance(
                literal_eval(ss[1]), int
            ):
                for i in range(int(ss[0]), int(ss[1]) + 1):
                    selected_sess_idx.append(i)
        elif "e" in s:
            ss = s.strip("e ")
            if "-" not in ss:
                if isinstance(literal_eval(ss), int):
                    excluded_sess_idx.append(int(ss))
            else:
                ssd = ss.split("-")
                if isinstance(literal_eval(ssd[0]), int) and isinstance(
                    literal_eval(ssd[1]), int
                ):
                    for i in range(int(ssd[0]), int(ssd[1]) + 1):
                        excluded_sess_idx.append(i)

    if len(to_extract) > 1 and not extract_all:
        for i, sess in enumerate(to_extract):
            print(f"[{str(i + 1)}] {sess}")

        print("You may input comma separated values for individual sessions")
        print(
            'Or you can input a hyphen separated range. E.g. "1-10" selects 10 sessions, including sessions 1 and 10'
        )
        print(
            'You can also exclude a range by prefixing the range selection with the letter "e"; e.g.: "e1-5".'
        )
        print("Press q to quit.")
        while len(ret_extract) == 0:
            sessions = input("Input your selected sessions to extract: ")
            if "q" in sessions.lower():
                return []
            if "," in sessions:
                selection = sessions.split(",")
                for s in selection:
                    s = s.strip()
                    parse_input(s)
                for i in selected_sess_idx:
                    if i not in excluded_sess_idx:
                        ret_extract.append(to_extract[i - 1])
            elif len(sessions) > 0:
                parse_input(sessions)
                if len(selected_sess_idx) > 0:
                    iters = selected_sess_idx
                else:
                    iters = range(1, len(to_extract) + 1)
                for i in iters:
                    if i not in excluded_sess_idx:
                        if i - 1 < len(to_extract):
                            ret_extract.append(to_extract[i - 1])
            else:
                print("Invalid input. Try again or press q to quit.")
    else:
        return to_extract

    return ret_extract


@filter_warnings
def generate_config_command(output_file, camera_type="k2"):
    """
    Generate configuration file (config.yaml) to use throughout pipeline.

    Args:
    output_file (str): path to saved config file.

    Returns:
    (str): status message.
    """

    from .cli import extract

    objs = extract.params

    params = {tmp.name: tmp.default for tmp in objs if not tmp.required}
    if camera_type == "azure":
        params["bg_roi_depth_range"] = [550, 650]
        params["spatial_filter_size"] = [5]
        params["tail_filter_size"] = [15, 15]

    # Check if the file already exists, and prompt user if they would like to overwrite pre-existing file
    if exists(output_file):
        ow = input(
            "This file already exists, would you like to overwrite it? [y -> yes, n -> no] "
        )
        if ow.lower() == "y":
            # Updating config file
            with open(output_file, "w") as f:
                yaml.safe_dump(params, f)
        else:
            return "Configuration file has been retained"
    else:
        print("Creating configuration file.")
        with open(output_file, "w") as f:
            yaml.safe_dump(params, f)

    return "Configuration file has been successfully generated."


@filter_warnings
def extract_found_sessions(
    input_dir, config_file, ext, extract_all=True, skip_extracted=False
):
    """
    Find and extract all depth files with specified extensions within input_dir

    Args:
    input_dir (str): path to directory containing all session folders
    config_file (str): path to config file
    ext (str): file extension for depth files to search for
    extract_all (bool): if True, auto searches for all sessions, else, prompts user to select sessions individually.
    skip_extracted (bool): indicates whether to skip already extracted session.

    """
    # error out early
    if not exists(config_file):
        raise IOError(f"Config file {config_file} does not exist")

    to_extract = []

    # find directories with .dat files that either have incomplete or no extractions
    if isinstance(ext, str):
        ext = [ext]
    for ex in ext:
        tmp = recursive_find_unextracted_dirs(input_dir, extension=ex, skip_checks=True)
        to_extract += [e for e in tmp if e.endswith(ex)]

    # filter out any incorrectly returned sessions
    temp = sorted([sess_dir for sess_dir in to_extract if "/tmp/" not in sess_dir])
    to_extract = get_selected_sessions(temp, extract_all)

    # read in the config file
    config_data = read_yaml(config_file)

    if config_data["cluster_type"] == "local":
        run_local_extract(to_extract, config_file, skip_extracted)
        print("Extractions Complete.")
    else:
        # Get default CLI params
        params = {
            tmp.name: tmp.default for tmp in batch_extract.params if not tmp.required
        }
        # merge default params and config data, preferring values in config data
        config_data = {**params, **config_data}

        # function call to run_slurm_extract to be implemented
        run_slurm_extract(input_dir, to_extract, config_data, skip_extracted)


def generate_index_command(input_dir, output_file):
    """
    Generate Index File (moseq2-index.yaml) based on aggregated sessions

    Args:
    input_dir (str): path to folder with aggregated results
    output_file (str): path to index file

    Returns:
    output_file (str): path to index file.
    """

    output_file = generate_index_wrapper(input_dir, output_file)
    print("Index file successfully generated.")
    return output_file


@filter_warnings
def aggregate_extract_results_command(
    input_dir, format, output_dir, mouse_threshold=0.0
):
    """
    Find all extracted h5, yaml and mp4 files and copies them all to a
    new directory relabeled with their respective session names.
    Also generates the index file (moseq2-index.yaml).

    Args:
    input_dir (str): path to base directory to recursively search for extracted files
    format (str): filename format for info to include in filenames
    output_dir (str): path to directory to save all aggregated results
    mouse_threshold (float): min threshold of extracted mouse height to include a session
    in the aggregated sesssions to ensure sessions with no extracted mouse is not included in the aggregated sessions.


    Returns:
    indexpath (str): path to generated index file (moseq2-index.yaml).
    """

    output_dir = join(input_dir, output_dir)

    if not exists(output_dir):
        os.makedirs(output_dir)

    indexpath = aggregate_extract_results_wrapper(
        input_dir, format, output_dir, mouse_threshold
    )

    return indexpath


def download_flip_command(output_dir, config_file="", selection=1):
    """
    Download flip classifier and saves its path to config file (config.yaml)

    Args:
    output_dir (str): path to output directory to save flip classifier
    config_file (str): path to config file (config.yaml)
    selection (int): index of which flip file to download (default is Adult male C57 classifer)

    Returns:
    """

    flip_file_wrapper(config_file, output_dir, selected_flip=selection)


@filter_warnings
def find_roi_command(
    input_dir,
    config_file,
    exts=["dat", "mkv", "avi"],
    select_session=False,
    default_session=0,
):
    """
    Compute ROI files given depth file.

    Args:
    input_dir (str): path to directory containing depth file
    config_file (str): path to config file
    exts (list): list of supported extensions
    select_session (bool): list all found sessions and allow user to select specific session to analyze via user-prompt
    default_session (int): index of the default session to find ROI for

    Returns:
    images (list of 2d arrays): list of 2d array images to graph in Notebook.
    filenames (list): list of paths to respective image paths
    """

    files = load_found_session_paths(input_dir, exts)

    if len(files) == 0:
        print("No recordings found")
        return

    if select_session:
        input_file = get_selected_sessions(files, False)
        if isinstance(input_file, list):
            input_file = input_file[0]
    else:
        input_file = files[default_session]

    print(f"Processing session: {input_file}")
    config_data = read_yaml(config_file)

    output_dir = join(dirname(input_file), "proc")
    get_roi_wrapper(input_file, config_data, output_dir)

    with open(config_file, "w") as g:
        yaml.safe_dump(config_data, g)

    images, filenames = read_tiff_files(output_dir)

    print(f"ROIs were successfully computed in {output_dir}")
    return images, filenames


@filter_warnings
def extract_command(input_file, output_dir, config_file, num_frames=None, skip=False):
    """
    Extract depth file

    Args:
    input_file (str): path to depth file to extract.
    output_dir (str): path to output directory.
    config_file (str): path to config file (config.yaml).
    num_frames (int): number of frames to extract. If None, all frames are extracted.
    skip (bool): skip already extracted file.

    Returns:
    (str): String indicating that the extracted is completed.
    """

    config_data = read_yaml(config_file)

    # Loading individual session config parameters if it exists
    if exists(config_data.get("session_config_path", "")):
        session_configs = read_yaml(config_data["session_config_path"])
        session_key = basename(dirname(input_file))

        # If key is found, update config_data, otherwise, use default dict
        config_data = session_configs.get(session_key, config_data)

    if output_dir is None:
        output_dir = config_data.get("output_dir", "proc")

    extract_wrapper(
        input_file, output_dir, config_data, num_frames=num_frames, skip=skip
    )

    return "Extraction completed."


--- File: moseq2_extract/io/__init__.py ---


--- File: moseq2_extract/io/video.py ---
"""
Video and video-metadata read/write functions.
"""

import os
import cv2
import tarfile
import datetime
import subprocess
import numpy as np
from os.path import exists
from tqdm.auto import tqdm
import matplotlib.pyplot as plt


def get_raw_info(filename, bit_depth=16, frame_size=(512, 424)):
    """
    Get info from a raw data file with specified frame dimensions and bit depth.

    Args:
    filename (str): name of raw data file
    bit_depth (int): bits per pixel (default: 16)
    frame_dims (tuple): wxh or hxw of each frame

    Returns:
    file_info (dict): dictionary containing depth file metadata
    """

    bytes_per_frame = (frame_size[0] * frame_size[1] * bit_depth) / 8

    if type(filename) is not tarfile.TarFile:
        file_info = {
            "bytes": os.stat(filename).st_size,
            "nframes": int(os.stat(filename).st_size / bytes_per_frame),
            "dims": frame_size,
            "bytes_per_frame": bytes_per_frame,
        }
    else:
        tar_members = filename.getmembers()
        tar_names = [_.name for _ in tar_members]
        input_file = tar_members[tar_names.index("depth.dat")]
        file_info = {
            "bytes": input_file.size,
            "nframes": int(input_file.size / bytes_per_frame),
            "dims": frame_size,
            "bytes_per_frame": bytes_per_frame,
        }
    return file_info


def read_frames_raw(
    filename,
    frames=None,
    frame_size=(512, 424),
    bit_depth=16,
    movie_dtype="<u2",
    **kwargs,
):
    """
    Reads in data from raw binary file.

    Args:
    filename (string): name of raw data file
    frames (list or range): frames to extract
    frame_dims (tuple): wxh of frames in pixels
    bit_depth (int): bits per pixel (default: 16)
    movie_dtype (str): An indicator for numpy to store the piped ffmpeg-read video in memory for processing.

    Returns:
    chunk (numpy ndarray): nframes x h x w
    """

    vid_info = get_raw_info(filename, frame_size=frame_size, bit_depth=bit_depth)

    if vid_info["dims"] != frame_size:
        frame_size = vid_info["dims"]

    if type(frames) is int:
        frames = [frames]
    elif not frames or (type(frames) is range) and len(frames) == 0:
        frames = range(0, vid_info["nframes"])

    seek_point = np.maximum(0, frames[0] * vid_info["bytes_per_frame"])
    read_points = len(frames) * frame_size[0] * frame_size[1]

    dims = (len(frames), frame_size[1], frame_size[0])

    if type(filename) is tarfile.TarFile:
        tar_members = filename.getmembers()
        tar_names = [_.name for _ in tar_members]
        input_file = tar_members[tar_names.index("depth.dat")]
        with filename.extractfile(input_file) as f:
            f.seek(int(seek_point))
            chunk = f.read(int(len(frames) * vid_info["bytes_per_frame"]))
            chunk = np.frombuffer(chunk, dtype=np.dtype(movie_dtype)).reshape(dims)
    else:
        with open(filename, "rb") as f:
            f.seek(int(seek_point))
            chunk = np.fromfile(
                file=f, dtype=np.dtype(movie_dtype), count=read_points
            ).reshape(dims)

    return chunk


# https://gist.github.com/hiwonjoon/035a1ead72a767add4b87afe03d0dd7b
def get_video_info(filename, mapping="DEPTH", threads=8, count_frames=False, **kwargs):
    """
    Get file metadata from videos.

    Args:
    filename (str): name of file to read video metadata from.
    mapping (str): chooses the stream to read from files.
    threads (int): number of threads to simultanoues run the ffprobe command
    count_frames (bool): indicates whether to count the frames individually.

    Returns:
    out_dict (dict): dictionary containing video file metadata
    """

    mapping_dict = get_stream_names(filename)
    if isinstance(mapping, str):
        mapping = mapping_dict.get(mapping, 0)

    stream_str = "stream=width,height,r_frame_rate,"
    if count_frames:
        stream_str += "nb_read_frames"
    else:
        stream_str += "nb_frames"

    command = [
        "ffprobe",
        "-v",
        "fatal",
        "-select_streams",
        f"v:{mapping}",
        "-show_entries",
        stream_str,
        "-of",
        "default=noprint_wrappers=1:nokey=1",
        "-threads",
        str(threads),
        filename,
        "-sexagesimal",
    ]

    if count_frames:
        command += ["-count_frames"]

    ffmpeg = subprocess.Popen(command, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
    out, err = ffmpeg.communicate()

    if err:
        print(err)

    out = out.decode().split("\n")
    out_dict = {
        "file": filename,
        "dims": (int(float(out[0])), int(float(out[1]))),
        "fps": float(out[2].split("/")[0]) / float(out[2].split("/")[1]),
    }

    try:
        out_dict["nframes"] = int(out[3])
    except ValueError:
        out_dict["nframes"] = None

    return out_dict


# simple command to pipe frames to an ffv1 file
def write_frames(
    filename,
    frames,
    threads=6,
    fps=30,
    pixel_format="gray16le",
    codec="ffv1",
    close_pipe=True,
    pipe=None,
    frame_dtype="uint16",
    slices=24,
    slicecrc=1,
    frame_size=None,
    get_cmd=False,
):
    """
    Write frames to avi file using the ffv1 lossless encoder

    Args:
    filename (str): path to file to write to.
    frames (np.ndarray): frames to write
    threads (int): number of threads to write video
    fps (int): frames per second
    pixel_format (str): format video color scheme
    codec (str): ffmpeg encoding-writer method to use
    close_pipe (bool): indicates to close the open pipe to video when done writing.
    pipe (subProcess.Pipe): pipe to currently open video file.
    frame_dtype (str): indicates the data type to use when writing the videos
    slices (int): number of frame slices to write at a time.
    slicecrc (int): check integrity of slices
    frame_size (tuple): shape/dimensions of image.
    get_cmd (bool): indicates whether function should return ffmpeg command (instead of executing)

    Returns:
    pipe (subProcess.Pipe): indicates whether video writing is complete.
    """

    # we probably want to include a warning about multiples of 32 for videos
    # (then we can use pyav and some speedier tools)
    if not frame_size and type(frames) is np.ndarray:
        frame_size = "{0:d}x{1:d}".format(frames.shape[2], frames.shape[1])
    elif not frame_size and type(frames) is tuple:
        frame_size = "{0:d}x{1:d}".format(frames[0], frames[1])

    command = [
        "ffmpeg",
        "-y",
        "-loglevel",
        "fatal",
        "-framerate",
        str(fps),
        "-f",
        "rawvideo",
        "-s",
        frame_size,
        "-pix_fmt",
        pixel_format,
        "-i",
        "-",
        "-an",
        "-vcodec",
        codec,
        "-threads",
        str(threads),
        "-slices",
        str(slices),
        "-slicecrc",
        str(slicecrc),
        "-r",
        str(fps),
        filename,
    ]

    if get_cmd:
        return command

    if not pipe:
        pipe = subprocess.Popen(command, stdin=subprocess.PIPE, stderr=subprocess.PIPE)

    for i in tqdm(
        range(frames.shape[0]), disable=True, desc=f"Writing frames to {filename}"
    ):
        pipe.stdin.write(frames[i].astype(frame_dtype).tostring())

    if close_pipe:
        pipe.communicate()
        return None
    else:
        return pipe


def get_stream_names(filename, stream_tag="title"):
    """
    Run an FFProbe command to determine whether an input video file contains multiple streams, and
    returns a stream_name to paired int values to extract the desired stream.

    Args:
    filename (str): path to video file to get streams from.
    stream_tag (str): value of the stream tags for ffprobe command to return

    Returns:
    out (dict): Dictionary of string to int pairs for the included streams in the mkv file.
    Dict will be used to choose the correct mapping number to choose which stream to read in read_frames().
    """

    command = [
        "ffprobe",
        "-v",
        "fatal",
        "-show_entries",
        "stream_tags={}".format(stream_tag),
        "-of",
        "default=noprint_wrappers=1:nokey=1",
        filename,
    ]

    ffmpeg = subprocess.Popen(command, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
    out, err = ffmpeg.communicate()

    if err or len(out) == 0:
        return {"DEPTH": 0}

    out = out.decode("utf-8").rstrip("\n").split("\n")

    return {o: i for i, o in enumerate(out)}


def read_frames(
    filename,
    frames=range(
        0,
    ),
    threads=6,
    fps=30,
    frames_is_timestamp=False,
    pixel_format="gray16le",
    movie_dtype="uint16",
    frame_size=None,
    slices=24,
    slicecrc=1,
    mapping="DEPTH",
    get_cmd=False,
    finfo=None,
    **kwargs,
):
    """
    Read in frames from the .mp4/.avi file using a pipe from ffmpeg.

    Args:
    filename (str): filename to get frames from
    frames (list or numpy.ndarray): list of frames to grab
    threads (int): number of threads to use for decode
    fps (int): frame rate of camera
    frames_is_timestamp (bool): if False, indicates timestamps represent kinect v2 absolute machine timestamps,
    pixel_format (str): ffmpeg pixel format of data
    movie_dtype (str): An indicator for numpy to store the piped ffmpeg-read video in memory for processing.
    frame_size (str): wxh frame size in pixels
    slices (int): number of slices to use for decode
    slicecrc (int): check integrity of slices
    mapping (str): the stream to read from mkv files.
    get_cmd (bool): indicates whether function should return ffmpeg command (instead of executing).
    finfo (dict): dictionary containing video file metadata

    Returns:
    video (numpy.ndarray):  frames x rows x columns
    """

    if finfo is None:
        finfo = get_video_info(filename, threads=threads, **kwargs)

    if frames is None or len(frames) == 0:
        frames = np.arange(finfo["nframes"], dtype="int64")

    if not frame_size:
        frame_size = finfo["dims"]

    # Compute starting time point to retrieve frames from
    if frames_is_timestamp:
        start_time = str(datetime.timedelta(seconds=frames[0]))
    else:
        start_time = str(datetime.timedelta(seconds=frames[0] / fps))

    command = [
        "ffmpeg",
        "-loglevel",
        "fatal",
        "-ss",
        start_time,
        "-i",
        filename,
        "-vframes",
        str(len(frames)),
        "-f",
        "image2pipe",
        "-s",
        "{:d}x{:d}".format(frame_size[0], frame_size[1]),
        "-pix_fmt",
        pixel_format,
        "-threads",
        str(threads),
        "-slices",
        str(slices),
        "-slicecrc",
        str(slicecrc),
        "-vcodec",
        "rawvideo",
    ]

    if isinstance(mapping, str):
        mapping_dict = get_stream_names(filename)
        mapping = mapping_dict.get(mapping, 0)

    if filename.endswith((".mkv", ".avi")):
        command += ["-map", f"0:{mapping}"]
        command += ["-vsync", "0"]

    command += ["-"]

    if get_cmd:
        return command

    pipe = subprocess.Popen(command, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
    out, err = pipe.communicate()

    if err:
        print("Error:", err)
        return None

    video = np.frombuffer(out, dtype=movie_dtype).reshape(
        (len(frames), frame_size[1], frame_size[0])
    )

    return video.astype("uint16")


def read_mkv(
    filename,
    frames=range(
        0,
    ),
    pixel_format="gray16be",
    movie_dtype="uint16",
    frames_is_timestamp=True,
    timestamps=None,
    **kwargs,
):
    """
    Read in frames from a .mkv file using a pipe from ffmpeg.

    Args:
    filename (str): filename to get frames from
    frames (list or numpy.ndarray): list of frame indices to read
    pixel_format (str): ffmpeg pixel format of data
    movie_dtype (str): An indicator for numpy to store the piped ffmpeg-read video in memory for processing.
    frames_is_timestamp (bool): if False, use machine timestamp if True, use frame as timestamp
    timestamps (list): array of timestamps to slice into using the frame indices
    threads (int): number of threads to use for decode
    fps (int): frame rate of camera in Hz
    frame_size (str): wxh frame size in pixels
    frame_dtype (str): indicates the data type to use when reading the videos
    slices (int): number of slices to use for decode
    slicecrc (int): check integrity of slices
    mapping (int): ffmpeg channel mapping; "o:mapping"; chooses the stream to read from mkv files.
    get_cmd (bool): indicates whether function should return ffmpeg command (instead of executing).

    Returns:
    video (numpy.ndarray):  frames x rows x columns
    """

    if timestamps is None and exists(filename):
        timestamps = load_timestamps_from_movie(
            filename, mapping=kwargs.get("mapping", "DEPTH")
        )

    if timestamps is not None:
        if isinstance(frames, range):
            frames = timestamps[slice(frames.start, frames.stop, frames.step)]
        else:
            frames = [timestamps[frames[0]]]

    return read_frames(
        filename,
        frames,
        pixel_format=pixel_format,
        movie_dtype=movie_dtype,
        frames_is_timestamp=frames_is_timestamp,
        **kwargs,
    )


def write_frames_preview(
    filename,
    frames=np.empty((0,)),
    threads=6,
    fps=30,
    pixel_format="rgb24",
    codec="h264",
    slices=24,
    slicecrc=1,
    frame_size=None,
    depth_min=0,
    depth_max=80,
    get_cmd=False,
    cmap="jet",
    pipe=None,
    close_pipe=True,
    frame_range=None,
    progress_bar=False,
):
    """
    Simple command to pipe frames to an ffv1 file. Writes out a false-colored mp4 video.

    Args:
    filename (str): path to file to write to.
    frames (np.ndarray): frames to write
    threads (int): number of threads to write video
    fps (int): frames per second
    pixel_format (str): format video color scheme
    codec (str): ffmpeg encoding-writer method to use
    slices (int): number of frame slices to write at a time.
    slicecrc (int): check integrity of slices
    frame_size (tuple): shape/dimensions of image.
    depth_min (int): minimum mouse depth from floor in (mm)
    depth_max (int): maximum mouse depth from floor in (mm)
    get_cmd (bool): indicates whether function should return ffmpeg command (instead of executing)
    cmap (str): color map to use.
    pipe (subProcess.Pipe): pipe to currently open video file.
    close_pipe (bool): indicates to close the open pipe to video when done writing.
    frame_range (range()): frame indices to write on video
    progress_bar (bool): If True, displays a TQDM progress bar for the video writing progress.

    Returns:
    pipe (subProcess.Pipe): indicates whether video writing is complete.
    """

    font = cv2.FONT_HERSHEY_SIMPLEX
    white = (255, 255, 255)
    txt_pos = (5, frames.shape[-1] - 40)

    if not np.mod(frames.shape[1], 2) == 0:
        frames = np.pad(frames, ((0, 0), (0, 1), (0, 0)), "constant", constant_values=0)

    if not np.mod(frames.shape[2], 2) == 0:
        frames = np.pad(frames, ((0, 0), (0, 0), (0, 1)), "constant", constant_values=0)

    if not frame_size and type(frames) is np.ndarray:
        frame_size = "{0:d}x{1:d}".format(frames.shape[2], frames.shape[1])
    elif not frame_size and type(frames) is tuple:
        frame_size = "{0:d}x{1:d}".format(frames[0], frames[1])

    command = [
        "ffmpeg",
        "-y",
        "-loglevel",
        "fatal",
        "-threads",
        str(threads),
        "-framerate",
        str(fps),
        "-f",
        "rawvideo",
        "-s",
        frame_size,
        "-pix_fmt",
        pixel_format,
        "-i",
        "-",
        "-an",
        "-vcodec",
        codec,
        "-slices",
        str(slices),
        "-slicecrc",
        str(slicecrc),
        "-r",
        str(fps),
        filename,
    ]

    if get_cmd:
        return command

    if not pipe:
        pipe = subprocess.Popen(command, stdin=subprocess.PIPE, stderr=subprocess.PIPE)

    # scale frames to appropriate depth ranges
    use_cmap = plt.get_cmap(cmap)
    for i in tqdm(
        range(frames.shape[0]),
        disable=not progress_bar,
        desc=f"Writing frames to {filename}",
    ):
        disp_img = frames[i, :].copy().astype("float32")
        disp_img = (disp_img - depth_min) / (depth_max - depth_min)
        disp_img[disp_img < 0] = 0
        disp_img[disp_img > 1] = 1
        disp_img = np.delete(use_cmap(disp_img), 3, 2) * 255
        if frame_range is not None:
            try:
                cv2.putText(
                    disp_img,
                    str(frame_range[i]),
                    txt_pos,
                    font,
                    1,
                    white,
                    2,
                    cv2.LINE_AA,
                )
            except (IndexError, ValueError):
                # len(frame_range) M < len(frames) or
                # txt_pos is outside of the frame dimensions
                print("Could not overlay frame number on preview on video.")

        pipe.stdin.write(disp_img.astype("uint8").tostring())

    if close_pipe:
        pipe.communicate()
        return None
    else:
        return pipe


def load_movie_data(
    filename, frames=None, frame_size=(512, 424), bit_depth=16, **kwargs
):
    """
    Parse file extension and load the movie data into numpy array.

    Args:
    filename (str): Path to video.
    frames (int or list): Frame indices to read in to output array.
    frame_size (tuple): Video dimensions (nrows, ncols)
    bit_depth (int): Number of bits per pixel, corresponds to image resolution.
    kwargs (dict): Any additional parameters that could be required in read_frames_raw().

    Returns:
    frame_data (numpy.ndarray): Read video as numpy array. (nframes, nrows, ncols)
    """

    if type(frames) is int:
        frames = [frames]
    try:
        if type(filename) is tarfile.TarFile:
            frame_data = read_frames_raw(
                filename,
                frames=frames,
                frame_size=frame_size,
                bit_depth=bit_depth,
                **kwargs,
            )
        elif filename.lower().endswith(".dat"):
            frame_data = read_frames_raw(
                filename,
                frames=frames,
                frame_size=frame_size,
                bit_depth=bit_depth,
                **kwargs,
            )
        elif filename.lower().endswith(".mkv"):
            frame_data = read_mkv(filename, frames, frame_size=frame_size, **kwargs)
        elif filename.lower().endswith(".avi"):
            frame_data = read_frames(filename, frames, frame_size=frame_size, **kwargs)

    except AttributeError as e:
        print("Error reading movie:", e)
        frame_data = read_frames_raw(
            filename,
            frames=frames,
            frame_size=frame_size,
            bit_depth=bit_depth,
            **kwargs,
        )

    return frame_data


def get_movie_info(
    filename, frame_size=(512, 424), bit_depth=16, mapping="DEPTH", threads=8, **kwargs
):
    """
    Return dict of movie metadata.

    Args:
    filename (str): path to video file
    frame_dims (tuple): video dimensions
    bit_depth (int): integer indicating data type encoding
    mapping (str): the stream to read from mkv files
    threads (int): number of threads to simultaneously read timestamps stored within the raw data file.

    Returns:
    metadata (dict): dictionary containing video file metadata
    """

    try:
        if type(filename) is tarfile.TarFile:
            metadata = get_raw_info(
                filename, frame_size=frame_size, bit_depth=bit_depth
            )
        elif filename.lower().endswith(".dat"):
            metadata = get_raw_info(
                filename, frame_size=frame_size, bit_depth=bit_depth
            )
        elif filename.lower().endswith((".avi", ".mkv")):
            metadata = get_video_info(
                filename, mapping=mapping, threads=threads, **kwargs
            )
    except AttributeError as e:
        print("Error reading movie metadata:", e)
        metadata = {}

    return metadata


def load_timestamps_from_movie(input_file, threads=8, mapping="DEPTH"):
    """
    Run a ffprobe command to extract the timestamps from the .mkv file, and pipes the
    output data to a csv file.

    Args:
    filename (str): path to input file to extract timestamps from.
    threads (int): number of threads to simultaneously read timestamps
    mapping (str): chooses the stream to read from mkv files. (Will default to if video is not an mkv format)

    Returns:
    timestamps (list): list of float values representing timestamps for each frame.
    """

    print("Loading movie timestamps")

    if isinstance(mapping, str):
        mapping_dict = get_stream_names(input_file)
        mapping = mapping_dict.get(mapping, 0)

    command = [
        "ffprobe",
        "-select_streams",
        f"v:{mapping}",
        "-threads",
        str(threads),
        "-show_entries",
        "frame=pkt_pts_time",
        "-v",
        "quiet",
        input_file,
        "-of",
        "csv=p=0",
    ]

    ffprobe = subprocess.Popen(command, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
    out, err = ffprobe.communicate()

    if err:
        print("Error:", err)
        return None

    timestamps = [float(t) for t in out.split()]

    if len(timestamps) == 0:
        return None

    return timestamps


--- File: moseq2_extract/io/image.py ---
"""
Image reading/writing functionality.
"""

import os
import ast
import json
import numpy as np
from skimage.external import tifffile
from os.path import join, dirname, exists


def read_tiff_files(input_dir):
    """
    Read ROI output results (Tiff files) located in the given input_directory.

    Args:
    input_dir (str): path to directory containing ROI files.

    Returns:
    images (list): list of 2d arrays of the ROIs.
    filenames (list): list of corresponding filenames to each read image.
    """

    images = []
    filenames = []
    for infile in os.listdir(input_dir):
        if infile[-4:] == "tiff":
            im = read_image(join(input_dir, infile))
            if len(im.shape) == 2:
                images.append(im)
            elif len(im.shape) == 3:
                images.append(im[0])
            filenames.append(infile)

    return images, filenames


def write_image(
    filename, image, scale=True, scale_factor=None, frame_dtype="uint16", compress=0
):
    """
    Save image data.

    Args:
    filename (str): path to output file
    image (numpy.ndarray): the (unscaled) 2-D image to save
    scale (bool): flag to scale the image between the bounds of `dtype`
    scale_factor (int): factor by which to scale image
    frame_dtype (str): array data type
    compress (int): image compression level

    """

    file = filename

    metadata = {}

    if scale:
        max_int = np.iinfo(frame_dtype).max

        if not scale_factor:
            # scale image to `dtype`'s full range
            scale_factor = int(
                max_int / (np.nanmax(image) + 1e-25)
            )  # adding very small value to avoid divide by 0
            image = image * scale_factor
        elif isinstance(scale_factor, tuple):
            image = np.float32(image)
            image = (image - scale_factor[0]) / (scale_factor[1] - scale_factor[0])
            image = np.clip(image, 0, 1) * max_int

        metadata = {"scale_factor": str(scale_factor)}

    directory = dirname(file)
    if not exists(directory):
        os.makedirs(directory)

    tifffile.imsave(
        file, image.astype(frame_dtype), compress=compress, metadata=metadata
    )


def read_image(filename, scale=True, scale_key="scale_factor"):
    """
    Load image data

    Args:
    filename (str): path to output file
    scale (bool): flag that indicates whether to scale image
    scale_key (str): indicates scale factor.

    Returns:
    image (numpy.ndarray): loaded image
    """

    with tifffile.TiffFile(filename) as tif:
        tmp = tif

    image = tmp.asarray()

    if scale:
        image_desc = json.loads(tmp.pages[0].tags["image_description"].as_str()[2:-1])

        try:
            scale_factor = int(image_desc[scale_key])
        except ValueError:
            scale_factor = ast.literal_eval(image_desc[scale_key])

        if type(scale_factor) is int:
            image = image / scale_factor
        elif type(scale_factor) is tuple:
            iinfo = np.iinfo(image.dtype)
            image = image.astype("float32") / iinfo.max
            image = image * (scale_factor[1] - scale_factor[0]) + scale_factor[0]

    return image


--- File: moseq2_extract/extract/roi.py ---
"""
ROI detection pre-processing utilities for fitting a plane to an input depth image.
"""

import numpy as np
from tqdm.auto import tqdm


def plane_fit3(points):
    """
    Fit a plane to 3 points (min number of points for fitting a plane)

    Args:
    points (numpy.ndarray): each row is a group of points, columns correspond to x,y,z.

    Returns:
    plane (numpy.array): linear plane fit-->a*x+b*y+c*z+d
    """

    a = points[1] - points[0]
    b = points[2] - points[0]
    # cross prod to make sure the three points make an area, hence a plane.
    normal = np.array(
        [
            [a[1] * b[2] - a[2] * b[1]],
            [a[2] * b[0] - a[0] * b[2]],
            [a[0] * b[1] - a[1] * b[0]],
        ]
    ).astype("float")
    denom = np.sum(np.square(normal)).astype("float")
    if denom < np.spacing(1):
        plane = np.empty((4,))
        plane[:] = np.nan
    else:
        normal /= np.sqrt(denom)
        d = np.dot(-points[0], normal)
        plane = np.hstack((normal.flatten(), d))

    return plane


def plane_ransac(
    depth_image,
    bg_roi_depth_range=(650, 750),
    iters=1000,
    noise_tolerance=30,
    in_ratio=0.1,
    progress_bar=False,
    mask=None,
    **kwargs,
):
    """
    Fit a plane using a naive RANSAC implementation

    Args:
    depth_image (numpy.ndarray): background image to fit plane to
    bg_roi_depth_range (tuple): min/max depth (mm) to consider pixels for plane
    iters (int): number of RANSAC iterations
    noise_tolerance (float): distance from plane to consider a point an inlier
    in_ratio (float): fraction of points required to consider a plane fit good
    progress_bar (bool): display progress bar
    mask (numpy.ndarray): boolean mask to find region to use
    kwargs (dict): dictionary containing extra keyword arguments from moseq2_extract.proc.get_roi()

    Returns:
    best_plane (numpy.array): plane fit to data
    dist (numpy.array): distance of the calculated coordinates and "best plane"
    """

    use_points = np.logical_and(
        depth_image > bg_roi_depth_range[0], depth_image < bg_roi_depth_range[1]
    )
    if np.sum(use_points) <= 10:
        raise ValueError(
            f'Too few datapoints exist within given "bg roi depth range" {bg_roi_depth_range} -- data point count: {np.sum(use_points)}.'
            "Please adjust this parameter to fit your recording sessions."
        )

    if mask is not None:
        use_points = np.logical_and(use_points, mask)

    xx, yy = np.meshgrid(
        np.arange(depth_image.shape[1]), np.arange(depth_image.shape[0])
    )

    coords = np.vstack(
        (
            xx[use_points].ravel(),
            yy[use_points].ravel(),
            depth_image[use_points].ravel(),
        )
    )
    coords = coords.T

    best_dist = np.inf
    best_num = 0

    npoints = np.sum(use_points)

    for _ in tqdm(range(iters), disable=not progress_bar, desc="Finding plane"):

        sel = coords[np.random.choice(coords.shape[0], 3, replace=True)]
        tmp_plane = plane_fit3(sel)

        if np.all(np.isnan(tmp_plane)):
            continue

        dist = np.abs(np.dot(coords, tmp_plane[:3]) + tmp_plane[3])
        inliers = dist < noise_tolerance
        ninliers = np.sum(inliers)

        if (
            (ninliers / npoints) > in_ratio
            and ninliers > best_num
            and np.mean(dist) < best_dist
        ):
            best_dist = np.mean(dist)
            best_num = ninliers
            best_plane = tmp_plane

    # fit the plane to our x,y,z coordinates
    coords = np.vstack((xx.ravel(), yy.ravel(), depth_image.ravel())).T
    dist = np.abs(np.dot(coords, best_plane[:3]) + best_plane[3])

    return best_plane, dist


--- File: moseq2_extract/extract/proc.py ---
"""
Video pre-processing utilities for detecting ROIs and extracting raw data.
"""

import cv2
import joblib
import tarfile
import scipy.stats
import numpy as np
import scipy.signal
import skimage.measure
import scipy.interpolate
import skimage.morphology
from copy import deepcopy
from tqdm.auto import tqdm
import moseq2_extract.io.video
import moseq2_extract.extract.roi
from os.path import exists, join, dirname
from moseq2_extract.io.image import read_image, write_image
from moseq2_extract.util import convert_pxs_to_mm, strided_app


def get_flips(frames, flip_file=None, smoothing=None):
    """
    Predict frames where mouse orientation is flipped to later correct.

    Args:
    frames (numpy.ndarray): frames x rows x columns, cropped mouse
    flip_file (str): path to pre-trained scipy random forest classifier
    smoothing (int): kernel size for median filter smoothing of random forest probabilities

    Returns:
    flips (numpy.array):  array for flips
    """

    try:
        clf = joblib.load(flip_file)
    except IOError:
        print(f"Could not open file {flip_file}")
        raise

    flip_class = np.where(clf.classes_ == 1)[0]

    try:
        probas = clf.predict_proba(
            frames.reshape((-1, frames.shape[1] * frames.shape[2]))
        )
    except ValueError:
        if hasattr(clf, "n_features_") and int(np.sqrt(clf.n_features_)) != frames.shape[-1]:
            print('WARNING: Input crop-size is not compatible with flip classifier.')
            accepted_crop = int(np.sqrt(clf.n_features_))
            print(f'Adjust the crop-size to ({accepted_crop}, {accepted_crop}) to use this flip classifier.')
        print("Frames shape:", frames.shape)
        print('The extracted data will NOT be flipped!')
        probas = np.array([[0]*len(frames), [1]*len(frames)]).T # default output; indicating no flips

    if smoothing:
        for i in range(probas.shape[1]):
            probas[:, i] = scipy.signal.medfilt(probas[:, i], smoothing)

    flips = probas.argmax(axis=1) == flip_class

    return flips


def get_largest_cc(frames, progress_bar=False):
    """
    Returns largest connected component blob in image

    Args:
    frames (numpy.ndarray): frames x rows x columns, uncropped mouse
    progress_bar (bool): display progress bar

    Returns:
    foreground_obj (numpy.ndarray):  frames x rows x columns, true where blob was found
    """

    foreground_obj = np.zeros((frames.shape), 'bool')

    for i in tqdm(range(frames.shape[0]), disable=not progress_bar, desc='Computing largest Connected Component'):
        nb_components, output, stats, centroids =\
            cv2.connectedComponentsWithStats(frames[i], connectivity=4)
        szs = stats[:, -1]
        foreground_obj[i] = output == szs[1:].argmax()+1

    return foreground_obj


def get_bground_im_file(frames_file, frame_stride=500, med_scale=5, output_dir=None, **kwargs):
    """
    Load or compute background from file.

    Args:
    frames_file (str): path to the depth video
    frame_stride (int): stride size between frames for median bground calculation
    med_scale (int): kernel size for median blur for background images.
    kwargs (dict): extra keyword arguments

    Returns:
    bground (numpy.ndarray): background image
    """

    if output_dir is None:
        bground_path = join(dirname(frames_file), 'proc', 'bground.tiff')
    else:
        bground_path = join(output_dir, 'bground.tiff')

    if type(frames_file) is not tarfile.TarFile:
        kwargs = deepcopy(kwargs)
    finfo = kwargs.pop('finfo', None)

    # Compute background image if it doesn't exist. Otherwise, load from file
    if not exists(bground_path) or kwargs.get('recompute_bg', False):
        if finfo is None:
            finfo = moseq2_extract.io.video.get_movie_info(frames_file, **kwargs)

        frame_idx = np.arange(0, finfo['nframes'], frame_stride)
        frame_store = []
        for i, frame in enumerate(frame_idx):
            frs = moseq2_extract.io.video.load_movie_data(frames_file,
                                                          [int(frame)], 
                                                          frame_size=finfo['dims'], 
                                                          finfo=finfo, 
                                                          **kwargs).squeeze()
            frame_store.append(cv2.medianBlur(frs, med_scale))

        bground = np.nanmedian(frame_store, axis=0)

        write_image(bground_path, bground, scale=True)
    else:
        bground = read_image(bground_path, scale=True)
        
    return bground


def get_bbox(roi):
    """
    return an array with the x and y boundaries given ROI.

    Args:
    roi (np.ndarray): ROI boolean mask to calculate bounding box.

    Returns:
    bbox (np.ndarray): Bounding Box around ROI
    """

    y, x = np.where(roi > 0)

    if len(y) == 0 or len(x) == 0:
        return None
    else:
        bbox = np.array([[y.min(), x.min()], [y.max(), x.max()]])
        return bbox

def threshold_chunk(chunk, min_height, max_height):
    """
    Threshold out depth values that are less than min_height and larger than
    max_height.

    Args:
    chunk (np.ndarray): Chunk of frames to threshold (nframes, width, height)
    min_height (int): Minimum depth values to include after thresholding.
    max_height (int): Maximum depth values to include after thresholding.
    dilate_iterations (int): Number of iterations the ROI was dilated.

    Returns:
    chunk (3D np.ndarray): Updated frame chunk.
    """

    chunk[chunk < min_height] = 0
    chunk[chunk > max_height] = 0

    return chunk

def get_roi(depth_image,
            strel_dilate=cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15)),
            dilate_iterations=0,
            erode_iterations=0,
            strel_erode=None,
            noise_tolerance=30,
            bg_roi_weights=(1, .1, 1),
            overlap_roi=None,
            bg_roi_gradient_filter=False,
            bg_roi_gradient_kernel=7,
            bg_roi_gradient_threshold=3000,
            bg_roi_fill_holes=True,
            get_all_data=False,
            **kwargs):
    """
    Compute an ROI using RANSAC plane fitting and simple blob features.

    Args:
    depth_image (np.ndarray): Singular depth image frame.
    strel_dilate (cv2.StructuringElement): dilation shape to use.
    dilate_iterations (int): number of dilation iterations.
    erode_iterations (int): number of erosion iterations.
    strel_erode (int): image erosion kernel size.
    noise_tolerance (int): threshold to use for noise filtering.
    bg_roi_weights (tuple): weights describing threshold to accept ROI.
    overlap_roi (np.ndarray): list of ROI boolean arrays to possibly combine.
    bg_roi_gradient_filter (bool): Boolean for whether to use a gradient filter.
    bg_roi_gradient_kernel (tuple): Kernel size of length 2, e.g. (1, 1.5)
    bg_roi_gradient_threshold (int): Threshold for noise gradient filtering
    bg_roi_fill_holes (bool): Boolean to fill any missing regions within the ROI.
    get_all_data (bool): If True, returns all ROI data, else, only return ROIs and computed Planes
    kwargs (dict) Dictionary containing `bg_roi_depth_range` parameter for plane_ransac()

    Returns:
    rois (list): list of detected roi images.
    roi_plane (np.ndarray): computed ROI Plane using RANSAC.
    bboxes (list): list of computed bounding boxes for each respective ROI.
    label_im (list): list of scikit-image image properties
    ranks (list): list of ROI ranks.
    shape_index (list): list of rank means.
    """

    if bg_roi_gradient_filter:
        gradient_x = np.abs(cv2.Sobel(depth_image, cv2.CV_64F,
                                      1, 0, ksize=bg_roi_gradient_kernel))
        gradient_y = np.abs(cv2.Sobel(depth_image, cv2.CV_64F,
                                      0, 1, ksize=bg_roi_gradient_kernel))
        mask = np.logical_and(gradient_x < bg_roi_gradient_threshold, gradient_y < bg_roi_gradient_threshold)
    else:
        mask = None

    roi_plane, dists = moseq2_extract.extract.roi.plane_ransac(
        depth_image, noise_tolerance=noise_tolerance, mask=mask, **kwargs)
    dist_ims = dists.reshape(depth_image.shape)

    if bg_roi_gradient_filter:
        dist_ims[~mask] = np.inf

    bin_im = dist_ims < noise_tolerance

    # anything < noise_tolerance from the plane is part of it
    label_im = skimage.measure.label(bin_im)
    region_properties = skimage.measure.regionprops(label_im)

    areas = np.zeros((len(region_properties),))
    extents = np.zeros_like(areas)
    dists = np.zeros_like(extents)

    # get the max distance from the center, area and extent
    center = np.array(depth_image.shape)/2

    for i, props in enumerate(region_properties):
        areas[i] = props.area
        extents[i] = props.extent
        tmp_dists = np.sqrt(np.sum(np.square(props.coords-center), 1))
        dists[i] = tmp_dists.max()

    # rank features
    ranks = np.vstack((scipy.stats.rankdata(-areas, method='max'),
                       scipy.stats.rankdata(-extents, method='max'),
                       scipy.stats.rankdata(dists, method='max')))
    weight_array = np.array(bg_roi_weights, 'float32')
    shape_index = np.mean(np.multiply(ranks.astype('float32'), weight_array[:, np.newaxis]), 0).argsort()

    # expansion microscopy on the roi
    rois = []
    bboxes = []

    # Perform image processing on each found ROI
    for shape in shape_index:
        roi = np.zeros_like(depth_image)
        roi[region_properties[shape].coords[:, 0],
            region_properties[shape].coords[:, 1]] = 1
        if strel_dilate is not None:
            roi = cv2.dilate(roi, strel_dilate, iterations=dilate_iterations) # Dilate
        if strel_erode is not None:
            roi = cv2.erode(roi, strel_erode, iterations=erode_iterations) # Erode
        if bg_roi_fill_holes:
            roi = scipy.ndimage.morphology.binary_fill_holes(roi) # Fill Holes

        rois.append(roi)
        bboxes.append(get_bbox(roi))

    # Remove largest overlapping found ROI
    if overlap_roi is not None:
        overlaps = np.zeros_like(areas)

        for i in range(len(rois)):
            overlaps[i] = np.sum(np.logical_and(overlap_roi, rois[i]))

        del_roi = np.argmax(overlaps)
        del rois[del_roi]
        del bboxes[del_roi]

    if get_all_data == True:
        return rois, roi_plane, bboxes, label_im, ranks, shape_index
    else:
        return rois, roi_plane


def apply_roi(frames, roi):
    """
    Apply ROI to data.

    Args:
    frames (np.ndarray): input frames to apply ROI.
    roi (np.ndarray): selected ROI to extract from input images.

    Returns:
    cropped_frames (np.ndarray): Frames cropped around ROI Bounding Box.
    """

    # yeah so fancy indexing slows us down by 3-5x
    cropped_frames = frames*roi
    bbox = get_bbox(roi)

    cropped_frames = cropped_frames[:, bbox[0, 0]:bbox[1, 0], bbox[0, 1]:bbox[1, 1]]
    return cropped_frames


def im_moment_features(IM):
    """
    Use the method of moments and centralized moments to get image properties.

    Args:
    IM (numpy.ndarray): depth image

    Returns:
    features (dict): returns a dictionary with orientation, centroid, and ellipse axis length
    """

    tmp = cv2.moments(IM)
    num = 2*tmp['mu11']
    den = tmp['mu20']-tmp['mu02']

    common = np.sqrt(4*np.square(tmp['mu11'])+np.square(den))

    if tmp['m00'] == 0:
        features = {
            'orientation': np.nan,
            'centroid': np.nan,
            'axis_length': [np.nan, np.nan]}
    else:
        features = {
            'orientation': -.5*np.arctan2(num, den),
            'centroid': [tmp['m10']/tmp['m00'], tmp['m01']/tmp['m00']],
            'axis_length': [2*np.sqrt(2)*np.sqrt((tmp['mu20']+tmp['mu02']+common)/tmp['m00']),
                            2*np.sqrt(2)*np.sqrt((tmp['mu20']+tmp['mu02']-common)/tmp['m00'])]
        }

    return features


def clean_frames(frames, prefilter_space=(3,), prefilter_time=None,
                 strel_tail=cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7)),
                 iters_tail=None, frame_dtype='uint8',
                 strel_min=cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5)),
                 iters_min=None, progress_bar=False):
    """
    Simple temporal and/or spatial filtering, median filter and morphological opening.

    Args:
    frames (np.ndarray): Frames (frames x rows x columns) to filter.
    prefilter_space (tuple): kernel size for spatial filtering
    prefilter_time (tuple): kernel size for temporal filtering
    strel_tail (cv2.StructuringElement): Element for tail filtering.
    iters_tail (int): number of iterations to run opening
    frame_dtype (str): frame encodings
    strel_min (int): minimum kernel size
    iters_min (int): minimum number of filtering iterations
    progress_bar (bool): display progress bar

    Returns:
    filtered_frames (numpy.ndarray): frames x rows x columns
    """

    # seeing enormous speed gains w/ opencv
    filtered_frames = frames.copy().astype(frame_dtype)

    for i in tqdm(range(frames.shape[0]), disable=not progress_bar, desc='Cleaning frames'):
        # Erode Frames
        if iters_min is not None and iters_min > 0:
            filtered_frames[i] = cv2.erode(filtered_frames[i], strel_min, iters_min)
        # Median Blur
        if prefilter_space is not None and np.all(np.array(prefilter_space) > 0):
            for j in range(len(prefilter_space)):
                filtered_frames[i] = cv2.medianBlur(filtered_frames[i], prefilter_space[j])
        # Tail Filter
        if iters_tail is not None and iters_tail > 0:
            filtered_frames[i] = cv2.morphologyEx(filtered_frames[i], cv2.MORPH_OPEN, strel_tail, iters_tail)

    # Temporal Median Filter
    if prefilter_time is not None and np.all(np.array(prefilter_time) > 0):
        for j in range(len(prefilter_time)):
            filtered_frames = scipy.signal.medfilt(filtered_frames, [prefilter_time[j], 1, 1])

    return filtered_frames


def get_frame_features(frames, frame_threshold=10, mask=np.array([]),
                       mask_threshold=-30, use_cc=False, progress_bar=False):
    """
    Use image moments to compute features of the largest object in the frame

    Args:
    frames (3d np.ndarray): input frames
    frame_threshold (int): threshold in mm separating floor from mouse
    mask (3d np.ndarray): input frame mask for parts not to filter.
    mask_threshold (int): threshold to include regions into mask.
    use_cc (bool): Use connected components.
    progress_bar (bool): Display progress bar.

    Returns:
    features (dict of lists): dictionary with simple image features
    mask (3d np.ndarray): input frame mask.
    """

    nframes = frames.shape[0]

    # Get frame mask
    if type(mask) is np.ndarray and mask.size > 0:
        has_mask = True
    else:
        has_mask = False
        mask = np.zeros((frames.shape), 'uint8')

    # Pack contour features into dict
    features = {
        'centroid': np.full((nframes, 2), np.nan),
        'orientation': np.full((nframes,), np.nan),
        'axis_length': np.full((nframes, 2), np.nan)
    }

    for i in tqdm(range(nframes), disable=not progress_bar, desc='Computing moments'):
        # Threshold frame to compute mask
        frame_mask = frames[i] > frame_threshold

        # Incorporate largest connected component with frame mask
        if use_cc:
            cc_mask = get_largest_cc((frames[[i]] > mask_threshold).astype('uint8')).squeeze()
            frame_mask = np.logical_and(cc_mask, frame_mask)

        # Apply mask
        if has_mask:
            frame_mask = np.logical_and(frame_mask, mask[i] > mask_threshold)
        else:
            mask[i] = frame_mask

        # Get contours in frame
        cnts, hierarchy = cv2.findContours(frame_mask.astype('uint8'), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
        tmp = np.array([cv2.contourArea(x) for x in cnts])

        if tmp.size == 0:
            continue

        mouse_cnt = tmp.argmax()

        # Get features from contours
        for key, value in im_moment_features(cnts[mouse_cnt]).items():
            features[key][i] = value

    return features, mask


def crop_and_rotate_frames(frames, features, crop_size=(80, 80), progress_bar=False):
    """
    Crop mouse from image and orients it such that the head is pointing right

    Args:
    frames (3d np.ndarray): frames to crop and rotate
    features (dict): dict of extracted features, found in result_00.h5 files.
    crop_size (tuple): size of cropped image.
    progress_bar (bool): Display progress bar.

    Returns:
    cropped_frames (3d np.ndarray): Crop and rotated frames.
    """

    nframes = frames.shape[0]

    # Prepare cropped frame array
    cropped_frames = np.zeros((nframes, crop_size[0], crop_size[1]), frames.dtype)

    # Get window dimensions
    win = (crop_size[0] // 2, crop_size[1] // 2 + 1)
    border = (crop_size[1], crop_size[1], crop_size[0], crop_size[0])

    for i in tqdm(range(frames.shape[0]), disable=not progress_bar, desc='Rotating'):

        if np.any(np.isnan(features['centroid'][i])):
            continue

        # Get bounded frames
        use_frame = cv2.copyMakeBorder(frames[i], *border, cv2.BORDER_CONSTANT, 0)

        # Get row and column centroids
        rr = np.arange(features['centroid'][i, 1]-win[0],
                       features['centroid'][i, 1]+win[1]).astype('int16')
        cc = np.arange(features['centroid'][i, 0]-win[0],
                       features['centroid'][i, 0]+win[1]).astype('int16')

        rr = rr+crop_size[0]
        cc = cc+crop_size[1]

        # Ensure centroids are in bounded frame
        if (np.any(rr >= use_frame.shape[0]) or np.any(rr < 1)
                or np.any(cc >= use_frame.shape[1]) or np.any(cc < 1)):
            continue

        # Rotate the frame such that the mouse is oriented facing east
        rot_mat = cv2.getRotationMatrix2D((crop_size[0] // 2, crop_size[1] // 2),
                                          -np.rad2deg(features['orientation'][i]), 1)
        cropped_frames[i] = cv2.warpAffine(use_frame[rr[0]:rr[-1], cc[0]:cc[-1]],
                                           rot_mat, (crop_size[0], crop_size[1]))

    return cropped_frames


def compute_scalars(frames, track_features, min_height=10, max_height=100, true_depth=673.1):
    """
    Compute extracted scalars.

    Args:
    frames (np.ndarray): frames x r x c, uncropped mouse
    track_features (dict):  dictionary with tracking variables (centroid and orientation)
    min_height (float): minimum height of the mouse
    max_height (float): maximum height of the mouse
    true_depth (float): detected true depth

    Returns:
    features (dict): dictionary of scalars
    """

    nframes = frames.shape[0]

    # Pack features into dict
    features = {
        'centroid_x_px': np.zeros((nframes,), 'float32'),
        'centroid_y_px': np.zeros((nframes,), 'float32'),
        'velocity_2d_px': np.zeros((nframes,), 'float32'),
        'velocity_3d_px': np.zeros((nframes,), 'float32'),
        'width_px': np.zeros((nframes,), 'float32'),
        'length_px': np.zeros((nframes,), 'float32'),
        'area_px': np.zeros((nframes,)),
        'centroid_x_mm': np.zeros((nframes,), 'float32'),
        'centroid_y_mm': np.zeros((nframes,), 'float32'),
        'velocity_2d_mm': np.zeros((nframes,), 'float32'),
        'velocity_3d_mm': np.zeros((nframes,), 'float32'),
        'width_mm': np.zeros((nframes,), 'float32'),
        'length_mm': np.zeros((nframes,), 'float32'),
        'area_mm': np.zeros((nframes,)),
        'height_ave_mm': np.zeros((nframes,), 'float32'),
        'angle': np.zeros((nframes,), 'float32'),
        'velocity_theta': np.zeros((nframes,)),
    }

    # Get mm centroid
    centroid_mm = convert_pxs_to_mm(track_features['centroid'], true_depth=true_depth)
    centroid_mm_shift = convert_pxs_to_mm(track_features['centroid'] + 1, true_depth=true_depth)

    # Based on the centroid of the mouse, get the mm_to_px conversion
    px_to_mm = np.abs(centroid_mm_shift - centroid_mm)
    masked_frames = np.logical_and(frames > min_height, frames < max_height)

    features['centroid_x_px'] = track_features['centroid'][:, 0]
    features['centroid_y_px'] = track_features['centroid'][:, 1]

    features['centroid_x_mm'] = centroid_mm[:, 0]
    features['centroid_y_mm'] = centroid_mm[:, 1]

    # based on the centroid of the mouse, get the mm_to_px conversion

    features['width_px'] = np.min(track_features['axis_length'], axis=1)
    features['length_px'] = np.max(track_features['axis_length'], axis=1)
    features['area_px'] = np.sum(masked_frames, axis=(1, 2))

    features['width_mm'] = features['width_px'] * px_to_mm[:, 1]
    features['length_mm'] = features['length_px'] * px_to_mm[:, 0]
    features['area_mm'] = features['area_px'] * px_to_mm.mean(axis=1)

    features['angle'] = track_features['orientation']

    nmask = np.sum(masked_frames, axis=(1, 2))

    for i in range(nframes):
        if nmask[i] > 0:
            features['height_ave_mm'][i] = np.mean(
                frames[i, masked_frames[i]])

    vel_x = np.diff(np.concatenate((features['centroid_x_px'][:1], features['centroid_x_px'])))
    vel_y = np.diff(np.concatenate((features['centroid_y_px'][:1], features['centroid_y_px'])))
    vel_z = np.diff(np.concatenate((features['height_ave_mm'][:1], features['height_ave_mm'])))

    features['velocity_2d_px'] = np.hypot(vel_x, vel_y)
    features['velocity_3d_px'] = np.sqrt(
        np.square(vel_x)+np.square(vel_y)+np.square(vel_z))

    vel_x = np.diff(np.concatenate((features['centroid_x_mm'][:1], features['centroid_x_mm'])))
    vel_y = np.diff(np.concatenate((features['centroid_y_mm'][:1], features['centroid_y_mm'])))

    features['velocity_2d_mm'] = np.hypot(vel_x, vel_y)
    features['velocity_3d_mm'] = np.sqrt(
        np.square(vel_x)+np.square(vel_y)+np.square(vel_z))

    features['velocity_theta'] = np.arctan2(vel_y, vel_x)

    return features


def feature_hampel_filter(features, centroid_hampel_span=None, centroid_hampel_sig=3,
                          angle_hampel_span=None, angle_hampel_sig=3):
    """
    Filter computed extraction features using Hampel Filtering.

    Args:
    features (dict): dictionary of video features
    centroid_hampel_span (int): Centroid Hampel Span Filtering Kernel Size
    centroid_hampel_sig (int): Centroid Hampel Signal Filtering Kernel Size
    angle_hampel_span (int): Angle Hampel Span Filtering Kernel Size
    angle_hampel_sig (int): Angle Hampel Span Filtering Kernel Size

    Returns:
    features (dict): filtered version of input dict.
    """
    if centroid_hampel_span is not None and centroid_hampel_span > 0:
        padded_centroids = np.pad(features['centroid'],
                                  (((centroid_hampel_span // 2, centroid_hampel_span // 2)),
                                   (0, 0)),
                                  'constant', constant_values = np.nan)
        for i in range(1):
            vws = strided_app(padded_centroids[:, i], centroid_hampel_span, 1)
            med = np.nanmedian(vws, axis=1)
            mad = np.nanmedian(np.abs(vws - med[:, None]), axis=1)
            vals = np.abs(features['centroid'][:, i] - med)
            fill_idx = np.where(vals > med + centroid_hampel_sig * mad)[0]
            features['centroid'][fill_idx, i] = med[fill_idx]

        padded_orientation = np.pad(features['orientation'],
                                    (angle_hampel_span // 2, angle_hampel_span // 2),
                                    'constant', constant_values = np.nan)

    if angle_hampel_span is not None and angle_hampel_span > 0:
        vws = strided_app(padded_orientation, angle_hampel_span, 1)
        med = np.nanmedian(vws, axis=1)
        mad = np.nanmedian(np.abs(vws - med[:, None]), axis=1)
        vals = np.abs(features['orientation'] - med)
        fill_idx = np.where(vals > med + angle_hampel_sig * mad)[0]
        features['orientation'][fill_idx] = med[fill_idx]

    return features


def model_smoother(features, ll=None, clips=(-300, -125)):
    """
    Apply spatial feature filtering.

    Args:
    features (dict): dictionary of extraction scalar features
    ll (numpy.array): array of loglikelihoods of pixels in frame
    clips (tuple): tuple to ensure video is indexed properly

    Returns:
    features (dict): smoothed version of input features
    """

    if ll is None or clips is None or (clips[0] >= clips[1]):
        return features

    ave_ll = np.zeros((ll.shape[0], ))
    for i, ll_frame in enumerate(ll):

        max_mu = clips[1]
        min_mu = clips[0]

        smoother = np.mean(ll[i])
        smoother -= min_mu
        smoother /= (max_mu - min_mu)

        smoother = np.clip(smoother, 0, 1)
        ave_ll[i] = smoother

    for k, v in features.items():
        nans = np.isnan(v)
        ndims = len(v.shape)
        xvec = np.arange(len(v))
        if nans.any():
            if ndims == 2:
                for i in range(v.shape[1]):
                    f = scipy.interpolate.interp1d(xvec[~nans[:, i]], v[~nans[:, i], i],
                                                   kind='nearest', fill_value='extrapolate')
                    fill_vals = f(xvec[nans[:, i]])
                    features[k][xvec[nans[:, i]], i] = fill_vals
            else:
                f = scipy.interpolate.interp1d(xvec[~nans], v[~nans],
                                               kind='nearest', fill_value='extrapolate')
                fill_vals = f(xvec[nans])
                features[k][nans] = fill_vals

    for i in range(2, len(ave_ll)):
        smoother = ave_ll[i]
        for k, v in features.items():
            features[k][i] = (1 - smoother) * v[i - 1] + smoother * v[i]

    for i in reversed(range(len(ave_ll) - 1)):
        smoother = ave_ll[i]
        for k, v in features.items():
            features[k][i] = (1 - smoother) * v[i + 1] + smoother * v[i]

    return features

--- File: moseq2_extract/extract/track.py ---
"""
Expectation-Maximization mouse tracking utilities.
"""

import cv2
import numpy as np
import scipy.stats
from tqdm.auto import tqdm
import statsmodels.stats.correlation_tools as stats_tools


def em_iter(data, mean, cov, lamd=0.1, epsilon=1e-1, max_iter=25):
    """
    Use EM tracker to iteratively update the mean and covariance variables using Expectation Maximization up to the max inputted number
    of iterations.

    Args:
    data (numpy.ndarray): frame, x, y, z coordinates to use
    mean (numpy.array): dx1, current mean estimate
    cov (numpy.array): current covariance estimate
    lambd (float): constant to add to diagonal of covariance matrix
    epsilon (float): tolerance on change in likelihood to terminate iteration
    max_iter (int): maximum number of EM iterations

    Returns:
    mean (numpy.array): updated mean
    cov (numpy.array): updated covariance
    """

    prev_likelihood = 0
    ll = 0

    ndatapoints = data.shape[1]
    pxtheta_raw = np.zeros((ndatapoints,), dtype="float64")

    for i in range(max_iter):
        pxtheta_raw = scipy.stats.multivariate_normal.pdf(x=data, mean=mean, cov=cov)
        pxtheta_raw /= np.sum(pxtheta_raw)

        mean = np.sum(data.T * pxtheta_raw, axis=1)
        dx = (data - mean).T
        cov = stats_tools.cov_nearest(np.dot(dx * pxtheta_raw, dx.T) + lamd * np.eye(3))

        ll = np.sum(np.log(pxtheta_raw + 1e-300))
        delta_likelihood = ll - prev_likelihood

        if delta_likelihood >= 0 and delta_likelihood < epsilon * abs(prev_likelihood):
            break

        prev_likelihood = ll

    return mean, cov


def em_init(
    depth_frame,
    depth_floor,
    depth_ceiling,
    init_strel=cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9)),
    strel_iters=1,
):
    """
    Estimate depth frame contours using OpenCV, and select the largest chosen contour to initialize a mask for EM tracking.

    Args:
    depth_frame (numpy.ndarray): depth frame to initialize mask with.
    depth_floor (float): distance from camera to bucket floor.
    depth_ceiling (float): max depth value.
    init_strel (cv2.structuringElement): structuring Element to compute mask.
    strel_iters (int): number of morphological iterations.

    Returns:
    mouse_mask (numpy.ndarray): mask of depth frame.
    """

    mask = np.logical_and(depth_frame > depth_floor, depth_frame < depth_ceiling)
    mask = cv2.morphologyEx(
        mask.astype("uint8"), cv2.MORPH_OPEN, init_strel, strel_iters
    )

    cnts, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    tmp = np.array([cv2.contourArea(x) for x in cnts])

    try:
        use_cnt = tmp.argmax()
        mouse_mask = np.zeros_like(mask)
        cv2.drawContours(mouse_mask, cnts, use_cnt, (255), cv2.FILLED)
        mouse_mask = mouse_mask > 0
    except Exception:
        mouse_mask = mask > 0

    return mouse_mask


def em_tracking(
    frames,
    raw_frames,
    segment=True,
    ll_threshold=-30,
    rho_mean=0,
    rho_cov=0,
    depth_floor=10,
    depth_ceiling=100,
    progress_bar=True,
    init_mean=None,
    init_cov=None,
    init_frames=10,
    init_method="raw",
    init_strel=cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9)),
):
    """
    Naive tracker, use EM update rules to follow a 3D Gaussian around the room.

    Args:
    frames (numpy.ndarray): filtered frames.
    raw_frames (numpy.ndarray): chunk to track mouse in.
    segment (bool): use only the largest blob for em updates
    ll_threshold (float): threshold on log likelihood for segmentation
    rho_mean (float): smoothing parameter for the mean
    rho_cov (float): smoothing parameter for the covariance
    depth_floor (float): height in mm for separating mouse from floor
    depth_ceiling (float): max height in mm for mouse from floor.
    progress_bar (bool): display progress bar.
    init_mean (np.ndarray): array of inital frame pixel means.
    init_cov (np.ndarray): array of inital frame pixel covariances.
    init_frames (int): number of frames to include in the init calulation
    init_method (str): mode in which to process inputs
    init_strel (cv2.structuringElement): structuring Element to compute mask.

    Returns:
    model_parameters (dict): mean and covariance estimates for each frame
    """

    # initialize the mean and covariance

    nframes, r, c = frames.shape
    xx, yy = np.meshgrid(np.arange(frames.shape[2]), np.arange(frames.shape[1]))
    coords = np.vstack((xx.ravel(), yy.ravel()))
    xyz = np.vstack((coords, frames[0].ravel()))

    if init_mean is None or init_cov is None:
        if init_method == "min":
            use_frame = np.min(frames[:init_frames], axis=0)
        elif init_method == "med":
            use_frame = np.median(frames[:init_frames], axis=0)
        elif init_method == "raw":
            use_frame = frames[0]

        mouse_mask = em_init(
            use_frame,
            depth_floor=depth_floor,
            depth_ceiling=depth_ceiling,
            init_strel=init_strel,
        )
        include_pixels = mouse_mask.ravel()

        if init_mean is None:
            try:
                mean = np.mean(xyz[:, include_pixels], axis=1)
            except Exception:
                mean = np.mean(xyz, axis=1)

        if init_cov is None:
            try:
                cov = stats_tools.cov_nearest(np.cov(xyz[:, include_pixels]))
            except Exception:
                cov = np.eye(3) * 20

        if np.any(np.isnan(mean)):
            mean = np.mean(xyz, axis=1)
    else:
        mean = init_mean
        cov = init_cov

    model_parameters = {
        "mean": np.empty((nframes, 3), "float64"),
        "cov": np.empty((nframes, 3, 3), "float64"),
    }

    for k, v in model_parameters.items():
        model_parameters[k][:] = np.nan

    frames = frames.reshape(frames.shape[0], frames.shape[1] * frames.shape[2])
    pbar = tqdm(total=nframes, disable=not progress_bar, desc="Computing EM")
    i = 0
    repeat = False
    while i < nframes:

        if repeat:
            xyz = np.vstack((coords, raw_frames[i].ravel()))
        else:
            xyz = np.vstack((coords, frames[i].ravel()))

        pxtheta_im = scipy.stats.multivariate_normal.logpdf(xyz.T, mean, cov).reshape(
            (r, c)
        )

        # segment to find pixels with likely mice, only use those for updating

        # if we try to find contours and we fail, repeat with the base initialization
        # if THAT fails, go back to the unfiltered frame and repeat base initialization
        # if THAT fails, just set all the pixels to true (tracking is hopeless, get the mouse in later frames)
        if segment and not repeat:
            try:
                cnts, hierarchy = cv2.findContours(
                    (pxtheta_im > ll_threshold).astype("uint8"),
                    cv2.RETR_TREE,
                    cv2.CHAIN_APPROX_SIMPLE,
                )
                tmp = np.array([cv2.contourArea(x) for x in cnts])
            except Exception:
                tmp = np.array([])

            if tmp.size == 0:
                repeat = True
                continue
            else:
                use_cnt = tmp.argmax()
                mask = np.zeros_like(pxtheta_im)
                cv2.drawContours(mask, cnts, use_cnt, (255), cv2.FILLED)
        elif segment and repeat:
            # basically try each step in succession, first try to get contours
            # if that fails try re-initialization, if that fails try re-initialization
            # with raw data, if that fails give up and use all of the pixels
            mask = em_init(
                frames[i],
                depth_floor=depth_floor,
                depth_ceiling=depth_ceiling,
                init_strel=init_strel,
            )
            if np.all(mask == 0):
                mask = em_init(
                    raw_frames[i],
                    depth_floor=depth_floor,
                    depth_ceiling=depth_ceiling,
                    init_strel=init_strel,
                )
                if np.all(mask == 0):
                    mask = np.ones(pxtheta_im.shape, dtype="bool")
        else:
            mask = pxtheta_im > ll_threshold

        tmp = mask.ravel() > 0
        tmp[np.logical_or(xyz[2] <= depth_floor, xyz[2] >= depth_ceiling)] = 0

        try:
            mean_update, cov_update = em_iter(
                xyz[:, tmp.astype("bool")].T,
                mean=mean,
                cov=cov,
                epsilon=0.25,
                max_iter=15,
                lamd=30,
            )
        except Exception:
            if not repeat:
                repeat = True
                continue
            else:
                mean_update = mean
                cov_update = cov

        if (np.all(mean_update == 0) or np.all(cov_update.ravel() == 0)) and not repeat:
            print("Backing off...")
            repeat = True
            continue
        elif np.all(mean_update == 0) or np.all(cov_update.ravel() == 0):
            mean_update = np.mean(xyz, axis=1)
            cov_update = np.eye(3) * 30

        # exponential smoothers for mean and covariance if
        # you want (easier to do this offline)
        # leave these set to 0 for now
        mean = (1 - rho_mean) * mean_update + rho_mean * mean
        cov = (1 - rho_cov) * cov_update + rho_cov * cov

        model_parameters["mean"][i] = mean
        model_parameters["cov"][i] = cov

        # TODO: add the walk-back where we use the
        # raw frames in case our update craps out...

        repeat = False
        i += 1
        pbar.update(1)

    pbar.close()

    return model_parameters


def em_get_ll(frames, mean, cov, progress_bar=False):
    """
    Return mouse tracking log-likelihoods for each frame given tracking parameters.

    Args:
    frames (numpy.ndarray): depth frames
    mean (numpy.array): frames x d, mean estimates
    cov (numpy.array): frames x d x d, covariance estimates
    progress_bar (bool): use a progress bar

    Returns:
    ll (numpy.ndarray): frames x rows x columns, log likelihood of all pixels in each frame
    """

    xx, yy = np.meshgrid(np.arange(frames.shape[2]), np.arange(frames.shape[1]))
    coords = np.vstack((xx.ravel(), yy.ravel()))

    nframes, r, c = frames.shape

    ll = np.zeros(frames.shape, dtype="float64")

    for i in tqdm(
        range(nframes), disable=not progress_bar, desc="Computing EM likelihoods"
    ):
        xyz = np.vstack((coords, frames[i].ravel()))
        ll[i] = scipy.stats.multivariate_normal.logpdf(xyz.T, mean[i], cov[i]).reshape(
            (r, c)
        )

    return ll


--- File: moseq2_extract/extract/__init__.py ---


--- File: moseq2_extract/extract/extract.py ---
"""
Extraction helper utility for computing scalar feature values performing cleaning, cropping and rotating operations.
"""

import cv2
import numpy as np
from copy import deepcopy
from moseq2_extract.extract.track import em_tracking, em_get_ll
from moseq2_extract.extract.proc import (
    crop_and_rotate_frames,
    threshold_chunk,
    clean_frames,
    apply_roi,
    get_frame_features,
    get_flips,
    compute_scalars,
    feature_hampel_filter,
    model_smoother,
)


# one stop shopping for taking some frames and doing stuff
def extract_chunk(
    chunk,
    use_tracking_model=False,
    spatial_filter_size=(3,),
    temporal_filter_size=None,
    tail_filter_iters=1,
    iters_min=0,
    strel_tail=cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9)),
    strel_min=cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5)),
    min_height=10,
    max_height=100,
    mask_threshold=-20,
    use_cc=False,
    bground=None,
    roi=None,
    rho_mean=0,
    rho_cov=0,
    tracking_ll_threshold=-100,
    tracking_model_segment=True,
    tracking_init_mean=None,
    tracking_init_cov=None,
    tracking_init_strel=cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9)),
    flip_classifier=None,
    flip_classifier_smoothing=51,
    frame_dtype="uint8",
    progress_bar=True,
    crop_size=(80, 80),
    true_depth=673.1,
    centroid_hampel_span=5,
    centroid_hampel_sig=3,
    angle_hampel_span=5,
    angle_hampel_sig=3,
    model_smoothing_clips=(-300, -150),
    tracking_model_init="raw",
    compute_raw_scalars=False,
    **kwargs
):
    """
    Extract mouse from the depth videos.

    Args:
    chunk (np.ndarray): chunk to extract - (chunksize, height, width)
    use_tracking_model (bool): The EM tracker uses expectation-maximization to fit improve mouse detection.
    spatial_filter_size (tuple): spatial kernel size used in median filtering.
    temporal_filter_size (tuple): temporal kernel size used in median filtering.
    tail_filter_iters (int): number of filtering iterations on mouse tail
    iters_min (int): minimum tail filtering filter kernel size
    strel_tail (cv2::StructuringElement): filtering kernel size to filter out mouse tail.
    strel_min (cv2::StructuringElement): filtering kernel size to filter mouse body in cable recording cases.
    min_height (int): minimum (mm) distance of mouse to floor.
    max_height (int): maximum (mm) distance of mouse to floor.
    mask_threshold (int): Threshold on log-likelihood to include pixels for centroid and angle calculation
    use_cc (bool): boolean to use connected components in cv2 structuring elements
    bground (np.ndarray): 2D numpy array representing previously computed median background image of entire extracted recording.
    roi (np.ndarray): 2D numpy array representing previously computed roi (area of bucket floor) to search for mouse within.
    rho_mean (int): smoothing parameter for the mean
    rho_cov (int): smoothing parameter for the covariance
    tracking_ll_threshold (float):  threshold for calling pixels a cable vs a mouse (usually between -16 to -12).
    tracking_model_segment (bool): boolean for whether to use only the largest blob for EM updates.
    tracking_init_mean (float): Initialized mean value for EM Tracking
    tracking_init_cov (float): Initialized covariance value for EM Tracking
    tracking_init_strel (cv2::StructuringElement - Ellipse): initial structuring element to use in EM tracking model.
    flip_classifier (str): path to pre-selected flip classifier.
    flip_classifier_smoothing (int): amount of smoothing to use for flip classifier.
    frame_dtype (str): Data type for processed frames
    save_path: (str): Path to save extracted results
    progress_bar (bool): Display progress bar
    crop_size (tuple): size of the cropped mouse image.
    true_depth (float): the computed detected true depth value for the middle of the arena
    centroid_hampel_span (int): Hampel filter span kernel size
    centroid_hampel_sig (int):  Hampel filter standard deviation
    angle_hampel_span (int): Angle filter span kernel size
    angle_hampel_sig (int): Angle filter standard deviation
    model_smoothing_clips (tuple): Model smoothing clips
    tracking_model_init (str): Method for tracking model initialization
    compute_raw_scalars (bool): Compute scalars from unfiltered crop-rotated data.

    Returns:
    results (dict): dict object containing the following keys:
    chunk (numpy.ndarray): bg subtracted and applied ROI version of original video chunk
    depth_frames(numpy.ndarray): cropped and oriented mouse video chunk
    mask_frames (numpy.ndarray): cropped and oriented mouse video chunk
    scalars (dict): computed scalars (str) mapped to 1d numpy arrays of length=nframes.
    flips(1d array): list of frame indices where the mouse orientation was flipped.
    parameters (dict): mean and covariance estimates for each frame (if em_tracking=True), otherwise None.
    """

    if bground is not None:
        # Perform background subtraction
        if not kwargs.get('graduate_walls', False):
            # pixels with a value of 0 are depth values that could not be computed
            chunk = ((bground - chunk) * (chunk != 0)).astype(frame_dtype)
        else:
            # Subtracting only background area where mouse is not on the bucket edge
            mouse_on_edge = (bground < true_depth) & (chunk < bground)
            chunk = (bground - chunk) * np.logical_not(mouse_on_edge) + (
                true_depth - chunk
            ) * mouse_on_edge

        # Threshold chunk depth values at min and max heights
        chunk = threshold_chunk(chunk, min_height, max_height).astype(frame_dtype)

    # Apply ROI mask
    if roi is not None:
        chunk = apply_roi(chunk, roi)

    # Denoise the frames before we do anything else
    filtered_frames = clean_frames(
        chunk,
        prefilter_space=spatial_filter_size,
        prefilter_time=temporal_filter_size,
        iters_tail=tail_filter_iters,
        strel_tail=strel_tail,
        iters_min=iters_min,
        strel_min=strel_min,
        frame_dtype=frame_dtype,
        progress_bar=progress_bar,
    )

    # If we need it, compute the EM parameters (for tracking in presence of occluders)
    if use_tracking_model:
        parameters = em_tracking(
            filtered_frames,
            chunk,
            rho_mean=rho_mean,
            rho_cov=rho_cov,
            progress_bar=progress_bar,
            ll_threshold=tracking_ll_threshold,
            segment=tracking_model_segment,
            init_mean=tracking_init_mean,
            init_cov=tracking_init_cov,
            depth_floor=min_height,
            depth_ceiling=max_height,
            init_strel=tracking_init_strel,
            init_method=tracking_model_init,
        )
        ll = em_get_ll(filtered_frames, progress_bar=progress_bar, **parameters)
    else:
        ll = None
        parameters = None

    # now get the centroid and orientation of the mouse
    features, mask = get_frame_features(
        filtered_frames,
        frame_threshold=min_height,
        mask=ll,
        mask_threshold=mask_threshold,
        use_cc=use_cc,
        progress_bar=progress_bar,
    )

    incl = ~np.isnan(features["orientation"])
    features["orientation"][incl] = np.unwrap(features["orientation"][incl] * 2) / 2

    # Detect and filter out any mouse-centering outlier frames
    features = feature_hampel_filter(
        features,
        centroid_hampel_span=centroid_hampel_span,
        centroid_hampel_sig=centroid_hampel_sig,
        angle_hampel_span=angle_hampel_span,
        angle_hampel_sig=angle_hampel_sig,
    )

    # Smooth EM tracker results if they exist
    if ll is not None:
        features = model_smoother(features, ll=ll, clips=model_smoothing_clips)

    # Crop and rotate the original frames
    cropped_frames = crop_and_rotate_frames(
        chunk, features, crop_size=crop_size, progress_bar=progress_bar
    )

    # Crop and rotate the filtered frames to be returned and later written
    cropped_filtered_frames = crop_and_rotate_frames(
        filtered_frames, features, crop_size=crop_size, progress_bar=progress_bar
    )

    # Compute crop-rotated frame mask
    if use_tracking_model:
        use_parameters = deepcopy(parameters)
        use_parameters["mean"][:, 0] = crop_size[1] // 2
        use_parameters["mean"][:, 1] = crop_size[0] // 2
        mask = em_get_ll(cropped_frames, progress_bar=progress_bar, **use_parameters)
    else:
        mask = crop_and_rotate_frames(
            mask, features, crop_size=crop_size, progress_bar=progress_bar
        )

    # Orient mouse to face east
    if flip_classifier:
        # get frame indices of incorrectly orientation
        flips = get_flips(
            cropped_filtered_frames, flip_classifier, flip_classifier_smoothing
        )
        flip_indices = np.where(flips)

        # apply flips
        cropped_frames[flip_indices] = np.rot90(
            cropped_frames[flip_indices], k=2, axes=(1, 2)
        )
        cropped_filtered_frames[flip_indices] = np.rot90(
            cropped_filtered_frames[flip_indices], k=2, axes=(1, 2)
        )
        mask[flip_indices] = np.rot90(mask[flip_indices], k=2, axes=(1, 2))
        features["orientation"][flips] += np.pi

    else:
        flips = None

    if compute_raw_scalars:
        # Computing scalars from raw data
        scalars = compute_scalars(
            cropped_frames,
            features,
            min_height=min_height,
            max_height=max_height,
            true_depth=true_depth,
        )
    else:
        # Computing scalars from filtered data
        scalars = compute_scalars(
            cropped_filtered_frames,
            features,
            min_height=min_height,
            max_height=max_height,
            true_depth=true_depth,
        )

    # Store all results in a dictionary
    results = {
        "chunk": chunk,
        "depth_frames": cropped_frames,
        "mask_frames": mask,
        "scalars": scalars,
        "flips": flips,
        "parameters": parameters,
    }

    return results


--- File: moseq2_extract/helpers/__init__.py ---


--- File: moseq2_extract/helpers/wrappers.py ---
"""
Wrapper functions for data processing in extraction.
"""

import os
import sys
import uuid
import h5py
import shutil
import warnings
from glob import glob
import numpy as np
import urllib.request
from copy import deepcopy
import ruamel.yaml as yaml
from tqdm.auto import tqdm
from cytoolz import partial
from moseq2_extract.io.image import write_image
from moseq2_extract.helpers.extract import process_extract_batches
from moseq2_extract.extract.proc import get_roi, get_bground_im_file
from os.path import join, exists, dirname, basename, abspath, splitext
from moseq2_extract.io.video import load_movie_data, get_movie_info, write_frames
from moseq2_extract.util import mouse_threshold_filter, filter_warnings, read_yaml
from moseq2_extract.helpers.data import (
    handle_extract_metadata,
    create_extract_h5,
    build_index_dict,
    load_extraction_meta_from_h5s,
    build_manifest,
    copy_manifest_results,
    check_completion_status,
)
from moseq2_extract.util import (
    select_strel,
    gen_batch_sequence,
    scalar_attributes,
    convert_raw_to_avi_function,
    set_bground_to_plane_fit,
    recursive_find_h5s,
    clean_dict,
    graduate_dilated_wall_area,
    get_bucket_center,
    h5_to_dict,
    detect_and_set_camera_parameters,
    get_frame_range_indices,
    check_filter_sizes,
    get_strels,
)


def copy_h5_metadata_to_yaml_wrapper(input_dir, h5_metadata_path):
    """
    Copy user specified metadata from h5path to a yaml file.

    Args:
    input_dir (str): path to directory containing h5 files
    h5_metadata_path (str): path within h5 to desired metadata to copy to yaml.

    """

    h5s, dicts, yamls = recursive_find_h5s(input_dir)
    to_load = [
        (tmp, yml, file)
        for tmp, yml, file in zip(dicts, yamls, h5s)
        if tmp["complete"] and not tmp["skip"]
    ]

    # load in all of the h5 files, grab the extraction metadata, reformat to make nice 'n pretty
    # then stage the copy

    for tup in tqdm(to_load, desc="Copying data to yamls"):
        with h5py.File(tup[2], "r") as f:
            tmp = clean_dict(h5_to_dict(f, h5_metadata_path))
            tup[0]["metadata"] = dict(tmp)

        new_file = f"{basename(tup[1])}_update.yaml"
        with open(new_file, "w+") as f:
            yaml.safe_dump(tup[0], f)

        if new_file != tup[1]:
            shutil.move(new_file, tup[1])


@filter_warnings
def generate_index_wrapper(input_dir, output_file):
    """
    Generate index file containing a summary of all extracted sessions.

    Args:
    input_dir (str): directory to search for extracted sessions.
    output_file (str): preferred name of the index file.

    Returns:
    output_file (str): path to index file (moseq2-index.yaml).
    """

    # gather the h5s and the pca scores file
    # uuids should match keys in the scores file
    h5s, dicts, yamls = recursive_find_h5s(input_dir)

    file_with_uuids = [
        (abspath(h5), abspath(yml), meta) for h5, yml, meta in zip(h5s, yamls, dicts)
    ]

    # Ensuring all retrieved extracted session h5s have the appropriate metadata
    # included in their results_00.h5 file
    for file in file_with_uuids:
        try:
            if "metadata" not in file[2]:
                copy_h5_metadata_to_yaml_wrapper(input_dir, file[0])
        except:
            warnings.warn(
                f"Metadata for session {file[0]} not found. \
            File may be listed with minimal/defaulted metadata in index file."
            )

    print(f"Number of sessions included in index file: {len(file_with_uuids)}")

    # Create index file in dict form
    output_dict = build_index_dict(file_with_uuids)

    # write out index yaml
    with open(output_file, "w") as f:
        yaml.safe_dump(output_dict, f)

    return output_file


def aggregate_extract_results_wrapper(
    input_dir, format, output_dir, mouse_threshold=0.0
):
    """
    Aggregate results to one folder and generate index file (moseq2-index.yaml).

    Args:
    input_dir (str): path to base directory containing all session folders
    format (str): string format for metadata to use as the new aggregated filename
    output_dir (str): name of the directory to create and store all results in
    mouse_threshold (float): threshold value of mean frame depth to include session frames

    Returns:
    indexpath (str): path to generated index file including all aggregated session information.
    """

    h5s, dicts, _ = recursive_find_h5s(input_dir)

    not_in_output = lambda f: not exists(join(output_dir, basename(f)))
    complete = lambda d: d["complete"] and not d["skip"]

    # only include real extracted mice with this filter func
    mtf = partial(mouse_threshold_filter, thresh=mouse_threshold)

    def filter_h5(args):
        """remove h5's that should be skipped or extraction wasn't complete"""
        _dict, _h5 = args
        return (
            complete(_dict)
            and not_in_output(_h5)
            and mtf(_h5)
            and ("sample" not in _dict)
        )

    # load in all of the h5 files, grab the extraction metadata, reformat to make nice 'n pretty
    # then stage the copy
    to_load = list(filter(filter_h5, zip(dicts, h5s)))

    loaded = load_extraction_meta_from_h5s(to_load)

    manifest = build_manifest(loaded, format=format)

    copy_manifest_results(manifest, output_dir)

    print("Results successfully aggregated in", output_dir)

    indexpath = generate_index_wrapper(output_dir, join(input_dir, "moseq2-index.yaml"))

    print(f"Index file path: {indexpath}")
    return indexpath


def generate_index_from_agg_res_wrapper(input_dir):
    """
    Generate index file from aggregated results folder.

    Args:
    input_dir (str): path to aggregated results folder
    """

    # find the yaml files
    yaml_paths = glob(os.path.join(input_dir, "*.yaml"))
    # setup pca path
    pca_path = os.path.join(os.path.dirname(input_dir), "_pca", "pca_scores.h5")
    if os.path.exists(pca_path):
        # point pca_path to pca scores you have
        index_data = {
            "files": [],
            "pca_path": pca_path,
        }
    else:
        index_data = {
            "files": [],
            "pca_path": "",
        }

    for p in yaml_paths:
        temp_yaml = read_yaml(p)
        file_dict = {
            "group": "default",
            "metadata": temp_yaml["metadata"],
            "path": [p[:-4] + "h5", p],
            "uuid": temp_yaml["uuid"],
        }
        index_data["files"].append(file_dict)

    # find output filename
    output_file = os.path.join(os.path.dirname(input_dir), "moseq2-index.yaml")

    # write out index yaml
    with open(output_file, "w") as f:
        yaml.safe_dump(index_data, f)


def get_roi_wrapper(input_file, config_data, output_dir=None):
    """
    Compute ROI given depth file.

    Args:
    input_file (str): path to depth file.
    config_data (dict): dictionary of ROI extraction parameters.
    output_dir (str): path to desired directory to save results in.

    Returns:
    roi (numpy.ndarray): ROI image to plot in GUI
    bground_im (numpy.ndarray): Background image to plot in GUI
    first_frame (numpy.ndarray): First frame image to plot in GUI
    """

    if output_dir is None:
        output_dir = join(dirname(input_file), "proc")
    elif exists(output_dir):
        pass
    elif dirname(output_dir) == "" or dirname(output_dir) not in input_file:
        output_dir = join(dirname(input_file), output_dir)

    os.makedirs(output_dir, exist_ok=True)
    config_data["output_dir"] = output_dir

    if config_data.get("finfo") is None:
        # when depth video is .avi, frame_size/dim is read directly from the video
        config_data["finfo"] = get_movie_info(input_file, **config_data)

    # checks camera type to set appropriate bg_roi_weights
    config_data = detect_and_set_camera_parameters(config_data, input_file)

    print("Getting background...")
    bground_im = get_bground_im_file(input_file, **config_data)
    write_image(join(output_dir, "bground.tiff"), bground_im, scale=True)

    # readjust depth range
    if not config_data.get("manual_set_depth_range", False):
        # search for depth values between the max distance and the halfway point to the camera
        print(
            "Automatically setting depth range. To manually set range values,"
            ' set "manual_set_depth_range" to True.\n For CLI users: use the --manual-set-depth-range flag.'
        )
        print(
            "To manually set a correct --bg-roi-depth-range value, "
            "set the min and max range values to +/-50mm of the actual camera height."
        )

        cX, cY = get_bucket_center(
            bground_im, bground_im.max(), threshold=int(np.median(bground_im) / 2)
        )
        adjusted_bg_depth_range = bground_im[cY][cX]
        config_data["bg_roi_depth_range"] = [
            int(adjusted_bg_depth_range - 50),
            int(adjusted_bg_depth_range + 50),
        ]

    # pass in config_data['finfo']['dims'] for frame size otherwise frame size is hard coded to 512x424
    first_frame = load_movie_data(
        input_file, 0, frame_size=config_data["finfo"]["dims"], **config_data
    )  # there is a tar object flag that must be set!!
    write_image(
        join(output_dir, "first_frame.tiff"),
        first_frame,
        scale=True,
        scale_factor=config_data["bg_roi_depth_range"],
    )

    print("Getting roi...")
    strel_dilate = select_strel(
        config_data["bg_roi_shape"], tuple(config_data["bg_roi_dilate"])
    )
    strel_erode = select_strel(
        config_data["bg_roi_shape"], tuple(config_data["bg_roi_erode"])
    )

    rois, plane = get_roi(
        bground_im,
        **config_data,
        strel_dilate=strel_dilate,
        strel_erode=strel_erode,
        get_all_data=False,
    )

    if config_data["use_plane_bground"]:
        print("Using plane fit for background...")
        bground_im = set_bground_to_plane_fit(bground_im, plane, output_dir)

    # Sort ROIs by largest mean area to later select largest one (bg_roi_index)
    if config_data["bg_sort_roi_by_position"]:
        rois = rois[: config_data["bg_sort_roi_by_position_max_rois"]]
        rois = [
            rois[i] for i in np.argsort([np.nonzero(roi)[0].mean() for roi in rois])
        ]

    if type(config_data["bg_roi_index"]) == int:
        config_data["bg_roi_index"] = [config_data["bg_roi_index"]]

    bg_roi_index = [
        idx for idx in config_data["bg_roi_index"] if idx in range(len(rois))
    ]
    roi = rois[bg_roi_index[0]]

    for idx in bg_roi_index:
        roi_filename = f"roi_{idx:02d}.tiff"
        write_image(join(output_dir, roi_filename), rois[idx], scale=True)

    return roi, bground_im, first_frame


def extract_wrapper(input_file, output_dir, config_data, num_frames=None, skip=False):
    """
    Extract depth videos.

    Args:
    input_file (str): path to depth file
    output_dir (str): path to directory to save results in.
    config_data (dict): dictionary containing extraction parameters.
    num_frames (int): number of frames to extract.
    skip (bool): indicates whether to skip file if already extracted
    extract (function): extraction function state

    Returns:
    output_dir (str): path to directory containing extraction
    """
    print("Processing:", input_file)
    # get the basic metadata

    # ensure 'get_cmd' and 'run_cmd' are not in config_data or get_bground_im_file will fail
    config_data = {
        k: v
        for k, v in config_data.items()
        if k not in ("get_cmd", "run_cmd", "extensions")
    }

    status_dict = {
        "complete": False,
        "skip": False,
        "uuid": str(uuid.uuid4()),
        "metadata": "",
        "parameters": deepcopy(config_data),
    }

    # save input directory path
    in_dirname = dirname(input_file)

    # If input file is compressed (tarFile), returns decompressed file path and tar bool indicator.
    # Also gets loads respective metadata dictionary and timestamp array.
    acquisition_metadata, config_data["timestamps"], config_data["tar"] = (
        handle_extract_metadata(input_file, in_dirname)
    )

    # updating input_file reference to open tar file object if input file ends with [.tar/.tar.gz]
    if config_data["tar"] is not None:
        input_file = config_data["tar"]

    config_data["finfo"] = get_movie_info(input_file, **config_data)

    if config_data["finfo"]["nframes"] is None:
        config_data["finfo"]["nframes"] = len(config_data["timestamps"])

    status_dict["metadata"] = acquisition_metadata  # update status dict

    # Getting number of frames to extract
    if num_frames is None:
        nframes = int(config_data["finfo"]["nframes"])
    elif num_frames > config_data["finfo"]["nframes"]:
        warnings.warn(
            "Requested more frames than video includes, extracting whole recording..."
        )
        nframes = int(config_data["finfo"]["nframes"])
    elif isinstance(num_frames, int):
        nframes = num_frames

    config_data = check_filter_sizes(config_data)

    # Compute total number of frames to include from an initial starting point.
    total_frames, first_frame_idx, last_frame_idx = get_frame_range_indices(
        *config_data["frame_trim"], nframes
    )

    scalars_attrs = scalar_attributes()
    scalars = list(scalars_attrs)

    # Get frame chunks to extract
    frame_batches = gen_batch_sequence(
        last_frame_idx,
        config_data["chunk_size"],
        config_data["chunk_overlap"],
        offset=first_frame_idx,
    )

    # set up the output directory
    if output_dir is None:
        output_dir = join(in_dirname, "proc")
    else:
        if in_dirname not in output_dir:
            output_dir = join(in_dirname, output_dir)

    if not exists(output_dir):
        os.makedirs(output_dir)

    # Ensure index is int
    if isinstance(config_data["bg_roi_index"], list):
        config_data["bg_roi_index"] = config_data["bg_roi_index"][0]

    output_filename = f'results_{config_data["bg_roi_index"]:02d}'
    status_filename = join(output_dir, f"{output_filename}.yaml")
    movie_filename = join(output_dir, f"{output_filename}.mp4")
    results_filename = join(output_dir, f"{output_filename}.h5")

    # Check if session has already been extracted
    if check_completion_status(status_filename) and skip:
        print("Skipping...")
        return

    with open(status_filename, "w") as f:
        yaml.safe_dump(status_dict, f)

    # Get Structuring Elements for extraction
    str_els = get_strels(config_data)

    # Compute ROIs
    roi, bground_im, first_frame = get_roi_wrapper(
        input_file, config_data, output_dir=output_dir
    )

    # Debugging option: DTD has no effect on extraction results unless dilate iterations > 1
    if config_data.get("detected_true_depth", "auto") == "auto":
        config_data["true_depth"] = np.median(bground_im[roi > 0])
    else:
        config_data["true_depth"] = int(config_data["detected_true_depth"])

    print("Detected true depth:", config_data["true_depth"])

    if config_data.get("dilate_iterations", 0) > 1 and config_data.get(
        "graduate_walls"
    ):
        print("Dilating background and graduating walls")
        bground_im = graduate_dilated_wall_area(
            bground_im, config_data, str_els["strel_dilate"], output_dir
        )

    extraction_data = {
        "bground_im": bground_im,
        "roi": roi,
        "first_frame": first_frame,
        "first_frame_idx": first_frame_idx,
        "last_frame_idx": last_frame_idx,
        "nframes": total_frames,
        "frame_batches": frame_batches,
    }

    # farm out the batches and write to an hdf5 file
    with h5py.File(results_filename, "w") as f:
        # Write scalars, roi, acquisition metadata, etc. to h5 file
        create_extract_h5(
            **extraction_data,
            h5_file=f,
            acquisition_metadata=acquisition_metadata,
            config_data=config_data,
            status_dict=status_dict,
            scalars_attrs=scalars_attrs,
        )

        # Write crop-rotated results to h5 file and write video preview mp4 file
        process_extract_batches(
            **extraction_data,
            h5_file=f,
            input_file=input_file,
            config_data=config_data,
            scalars=scalars,
            str_els=str_els,
            output_mov_path=movie_filename,
        )

    print()

    # Compress the depth file to avi format; compresses original raw file by ~8x.
    try:
        if config_data["tar"] is None:
            if input_file.endswith("dat") and config_data["compress"]:
                convert_raw_to_avi_function(
                    input_file,
                    chunk_size=config_data["compress_chunk_size"],
                    fps=config_data["fps"],
                    delete=False,  # to be changed when we're ready!
                    threads=config_data["compress_threads"],
                )
    except AttributeError as e:
        print("Error converting raw video to avi format, continuing anyway...")
        print(e)

    status_dict["complete"] = True
    if status_dict["parameters"].get("true_depth") is None:
        # config_data.get('true_depth') is numpy.float64 and yaml.safe_dump can't represent the object
        status_dict["parameters"]["true_depth"] = float(config_data.get("true_depth"))
    with open(status_filename, "w") as f:
        yaml.safe_dump(status_dict, f)

    return output_dir


@filter_warnings
def flip_file_wrapper(config_file, output_dir, selected_flip=None):
    """
    Download and save flip classifiers.

    Args:
    config_file (str): path to config file
    output_dir (str): path to directory to save classifier in.
    selected_flip (int or str): int: index of desired flip classifier; str: path to flip file

    Returns:
    None
    """

    flip_files = {
        "large mice with fibers (K2)": "https://storage.googleapis.com/flip-classifiers/flip_classifier_k2_largemicewithfiber.pkl",
        "adult male c57s (K2)": "https://storage.googleapis.com/flip-classifiers/flip_classifier_k2_c57_10to13weeks.pkl",
        "mice with Inscopix cables (K2)": "https://storage.googleapis.com/flip-classifiers/flip_classifier_k2_inscopix.pkl",
        "adult male c57s (Azure)": "https://moseq-data.s3.amazonaws.com/flip-classifier-azure-temp.pkl",
    }

    key_list = list(flip_files)

    if selected_flip is None:
        for idx, (k, v) in enumerate(flip_files.items()):
            print(f"[{idx}] {k} ---> {v}")
    else:
        selected_flip = key_list[selected_flip]

    # prompt for user selection if not already inputted
    while selected_flip is None:
        try:
            selected_flip = key_list[int(input("Enter a selection "))]
        except ValueError:
            print("Please enter a valid number listed above")
            continue

    if not exists(output_dir):
        os.makedirs(output_dir)

    selection = flip_files[selected_flip]

    output_filename = join(output_dir, basename(selection))

    urllib.request.urlretrieve(selection, output_filename)
    print("Successfully downloaded flip file to", output_filename)

    # Update the config file with the latest path to the flip classifier
    try:
        config_data = read_yaml(config_file)
        config_data["flip_classifier"] = output_filename

        with open(config_file, "w") as f:
            yaml.safe_dump(config_data, f)
    except Exception as e:
        print("Could not update configuration file flip classifier path")
        print("Unexpected error:", e)


def convert_raw_to_avi_wrapper(
    input_file, output_file, chunk_size, fps, delete, threads, mapping
):
    """
    compress a raw depth file into an avi file (with depth values) that is 8x smaller.

    Args:
    input_file (str): Path to depth file to convert
    output_file (str): Path to output avi file
    chunk_size (int): Size of frame chunks to iteratively process
    fps (int): frame rate.
    delete (bool): Delete the original depth file if True.
    threads (int): Number of threads used to encode video.
    mapping (str or int): Indicate which video stream to from the inputted file

    Returns:
    """

    if output_file is None:
        base_filename = splitext(basename(input_file))[0]
        output_file = join(dirname(input_file), f"{base_filename}.avi")

    vid_info = get_movie_info(input_file, mapping=mapping)
    frame_batches = gen_batch_sequence(vid_info["nframes"], chunk_size, 0)
    video_pipe = None

    for batch in tqdm(frame_batches, desc="Encoding batches"):
        frames = load_movie_data(input_file, batch, mapping=mapping)
        video_pipe = write_frames(
            output_file,
            frames,
            pipe=video_pipe,
            close_pipe=False,
            threads=threads,
            fps=fps,
        )

    if video_pipe:
        video_pipe.communicate()

    for batch in tqdm(frame_batches, desc="Checking data integrity"):
        raw_frames = load_movie_data(input_file, batch, mapping=mapping)
        encoded_frames = load_movie_data(output_file, batch, mapping=mapping)

        if not np.array_equal(raw_frames, encoded_frames):
            raise RuntimeError(
                f"Raw frames and encoded frames not equal from {batch[0]} to {batch[-1]}"
            )

    print("Encoding successful")

    if delete:
        print("Deleting", input_file)
        os.remove(input_file)


def copy_slice_wrapper(
    input_file, output_file, copy_slice, chunk_size, fps, delete, threads, mapping
):
    """
    Copy a segment of an input depth recording into a new video file.

    Args:
    input_file (str): Path to depth file to read segment from
    output_file (str): Path to outputted video file with copied slice.
    copy_slice (2-tuple): Frame range to copy from input file.
    chunk_size (int): Size of frame chunks to iteratively process
    fps (int): Frames per second.
    delete (bool): Delete the original depth file if True.
    threads (int): Number of threads used to encode video.
    mapping (str or int): Indicate which video stream to from the inputted file

    Returns:
    """

    if output_file is None:
        base_filename = splitext(basename(input_file))[0]
        avi_encode = True
        output_file = join(dirname(input_file), f"{base_filename}.avi")
    else:
        output_filename, ext = splitext(basename(output_file))
        if ext == ".avi":
            avi_encode = True
        else:
            avi_encode = False

    vid_info = get_movie_info(input_file)
    copy_slice = (copy_slice[0], np.minimum(copy_slice[1], vid_info["nframes"]))
    nframes = copy_slice[1] - copy_slice[0]
    offset = copy_slice[0]

    frame_batches = gen_batch_sequence(nframes, chunk_size, 0, offset)
    video_pipe = None

    if exists(output_file):
        overwrite = input(
            "Press ENTER to overwrite your previous extraction, else to end the process."
        )
        if overwrite != "":
            sys.exit(0)

    for batch in tqdm(frame_batches, desc="Encoding batches"):
        frames = load_movie_data(input_file, batch, mapping=mapping)
        if avi_encode:
            video_pipe = write_frames(
                output_file,
                frames,
                pipe=video_pipe,
                close_pipe=False,
                threads=threads,
                fps=fps,
            )
        else:
            with open(output_file, "ab") as f:
                f.write(frames.astype("uint16").tobytes())

    if avi_encode and video_pipe:
        video_pipe.communicate()

    for batch in tqdm(frame_batches, desc="Checking data integrity"):
        raw_frames = load_movie_data(input_file, batch, mapping=mapping)
        encoded_frames = load_movie_data(output_file, batch, mapping=mapping)

        if not np.array_equal(raw_frames, encoded_frames):
            raise RuntimeError(
                f"Raw frames and encoded frames not equal from {batch[0]} to {batch[-1]}"
            )

    print("Encoding successful")

    if delete:
        print("Deleting", input_file)
        os.remove(input_file)


--- File: moseq2_extract/helpers/extract.py ---
"""
Extraction-helper utility functions.
"""

import numpy as np
import ruamel.yaml as yaml
from os.path import exists, basename, dirname, join, abspath
from os import makedirs, system
from tqdm.auto import tqdm
from moseq2_extract.extract.extract import extract_chunk
from moseq2_extract.util import read_yaml
from moseq2_extract.io.video import load_movie_data, write_frames_preview
from moseq2_extract.helpers.data import check_completion_status


def write_extracted_chunk_to_h5(
    h5_file, results, config_data, scalars, frame_range, offset
):
    """

    Write extracted frames, frame masks, and scalars to an open h5 file.

    Args:
    h5_file (H5py.File): open results_00 h5 file to save data in.
    results (dict): extraction results dict.
    config_data (dict): dictionary containing extraction parameters (autogenerated)
    scalars (list): list of keys to scalar attribute values
    frame_range (range object): current chunk frame range
    offset (int): frame offset

    Returns:
    """

    # Writing computed scalars to h5 file
    for scalar in scalars:
        h5_file[f"scalars/{scalar}"][frame_range] = results["scalars"][scalar][offset:]

    # Writing frames and mask to h5
    h5_file["frames"][frame_range] = results["depth_frames"][offset:]
    h5_file["frames_mask"][frame_range] = results["mask_frames"][offset:]

    # Writing flip classifier results to h5
    if config_data["flip_classifier"]:
        h5_file["metadata/extraction/flips"][frame_range] = results["flips"][offset:]


def set_tracking_model_parameters(
    results, min_height, tracking_model_ll_clip, chunk_overlap, **kwargs
):
    """
    Threshold and clip the masked frame data if use_tracking_model = True and update results.

    Args:
    results (dict): dict of extracted depth frames and mask frames to threshold to update; output of extract_chunk().
    min_height (int): distance from floor to threshold out of extracted image.
    tracking_model_ll_clip (numpy.ndarray): clipped frame regions based on EM loglikelihoods computed in extract_chunk().
    chunk_overlap (int): number of frames each extracted chunk is overlapping with the next.

    Returns:
    results (dict): updated results dict with thresholded and clipped mask_frames.
    tracking_init_mean (float): mean value for EM Tracking
    tracking_init_cov (float): covariance value for EM Tracking
    """

    # Thresholding and clipping EM-tracked frame mask data
    results["mask_frames"][
        results["depth_frames"] < min_height
    ] = tracking_model_ll_clip
    results["mask_frames"][
        results["mask_frames"] < tracking_model_ll_clip
    ] = tracking_model_ll_clip

    # Updating EM tracking estimators
    tracking_init_mean = results["parameters"]["mean"][-(chunk_overlap + 1)]
    tracking_init_cov = results["parameters"]["cov"][-(chunk_overlap + 1)]

    return results, tracking_init_mean, tracking_init_cov


def make_output_movie(results, config_data, offset=0):
    """
    Create an array for output movie with filtered video and cropped mouse on the top left

    Args:
    results (dict): dict of extracted depth frames, and original raw chunk to create an output movie.
    config_data (dict): dict of extraction parameters containing the crop sizes used in the extraction.
    offset (int): current offset being used, automatically set if chunk_overlap > 0

    Returns:
    output_movie (numpy.ndarray): output movie to write to mp4 file.
    """

    # Create empty array for output movie with filtered video and cropped mouse on the top left
    nframes, rows, cols = results["chunk"][offset:].shape
    output_movie = np.zeros(
        (
            nframes,
            rows + config_data["crop_size"][0],
            cols + config_data["crop_size"][1],
        ),
        "uint16",
    )

    # Populating array with filtered and cropped videos
    output_movie[:, : config_data["crop_size"][0], : config_data["crop_size"][1]] = (
        results["depth_frames"][offset:]
    )
    output_movie[:, config_data["crop_size"][0] :, config_data["crop_size"][1] :] = (
        results["chunk"][offset:]
    )

    return output_movie


def process_extract_batches(
    input_file,
    config_data,
    bground_im,
    roi,
    frame_batches,
    str_els,
    output_mov_path,
    scalars=None,
    h5_file=None,
    video_pipe=None,
    **kwargs,
):
    """
    Compute extracted frames and save them to h5 files and avi files.

    Args:
    input_file (str): path to depth file
    config_data (dict): dictionary containing extraction parameters (autogenerated)
    bground_im (numpy.ndarray):  background image
    roi (numpy.ndarray): roi image
    frame_batches (list): list of batches of frames to serially process.
    str_els (dict): dictionary containing OpenCV StructuringElements
    output_mov_path (str): path and filename of the output movie generated by the extraction
    scalars (list): list of keys to scalar attribute values
    h5file (h5py.File): opened h5 file to write extracted batches to
    video_pipe (subprocess.PIPE): open pipe to location where preview extraction is being written.
    kwargs (dict): Extra keyword arguments.

    Returns:
    """

    tracking_init_mean = config_data.pop("tracking_init_mean", None)
    tracking_init_cov = config_data.pop("tracking_init_cov", None)

    for i, frame_range in enumerate(tqdm(frame_batches, desc="Processing batches")):
        raw_chunk = load_movie_data(
            input_file, frame_range, frame_size=bground_im.shape[::-1], **config_data
        )

        offset = config_data["chunk_overlap"] if i > 0 else 0

        # Get crop-rotated frame batch
        results = extract_chunk(
            **config_data,
            **str_els,
            chunk=raw_chunk,
            roi=roi,
            bground=bground_im,
            tracking_init_mean=tracking_init_mean,
            tracking_init_cov=tracking_init_cov,
        )

        if config_data["use_tracking_model"]:
            # threshold and clip mask frames from EM tracking results
            results, tracking_init_mean, tracking_init_cov = (
                set_tracking_model_parameters(results, **config_data)
            )

        # Offsetting frame chunk by CLI parameter defined option: chunk_overlap
        frame_range = frame_range[offset:]

        if h5_file is not None:
            write_extracted_chunk_to_h5(
                h5_file, results, config_data, scalars, frame_range, offset
            )

        # Create array for output movie with filtered video and cropped mouse on the top left
        output_movie = make_output_movie(results, config_data, offset)

        # Writing frame batch to mp4 file
        video_pipe = write_frames_preview(
            output_mov_path,
            output_movie,
            pipe=video_pipe,
            close_pipe=False,
            fps=config_data["fps"],
            frame_range=list(frame_range),
            depth_max=config_data["max_height"],
            depth_min=config_data["min_height"],
            progress_bar=config_data.get("progress_bar", False),
        )

    # Check if video is done writing. If not, wait.
    if video_pipe is not None:
        video_pipe.communicate()


def run_local_extract(to_extract, config_file, skip_extracted=False):
    """
    Run the extract command on given list of sessions to extract on a local platform.

    Args:
    to_extract (list): list of paths to files to extract
    config_file (str): path to configuration file containing pre-configured extract and ROI
    skip_extracted (bool): Whether to skip already extracted session.

    """
    from moseq2_extract.gui import extract_command

    for ext in tqdm(to_extract, desc="Extracting Sessions"):
        try:
            extract_command(ext, None, config_file=config_file, skip=skip_extracted)
        except Exception as e:
            print("Unexpected error:", e)
            print("could not extract", ext)


def run_slurm_extract(input_dir, to_extract, config_data, skip_extracted=False):

    assert (
        "extract_out_script" in config_data
    ), "Need to supply extract_out_script to save extract commands"
    # expand input_dir absolute path
    input_dir = abspath(input_dir)

    # make session_specific config file and save it in proc folder if session config exists
    if exists(config_data.get("session_config_path", "")):
        session_configs = read_yaml(config_data["session_config_path"])
        for depth_file in to_extract:
            output_dir = join(dirname(depth_file), config_data["output_dir"])

            # ensure output_dir exists
            if not exists(output_dir):
                makedirs(output_dir)

            # get and write session-specific parameters
            output_file = join(output_dir, "config.yaml")
            session_key = basename(dirname(depth_file))

            with open(output_file, "w") as f:
                yaml.safe_dump(session_configs.get(session_key, config_data), f)

    # Construct sbatch command for slurm
    commands = ""
    for depth_file in to_extract:
        output_dir = join(dirname(depth_file), config_data["output_dir"])

        # skip session if skip_extracted is true and the session is already extracted
        if skip_extracted and check_completion_status(
            join(output_dir, "results_00.yaml")
        ):
            continue

        # set up config file
        if exists(config_data.get("session_config_path", "")):
            config_file = join(output_dir, "config.yaml")
        else:
            config_file = config_data["config_file"]

        # construct command
        base_command = (
            f'moseq2-extract extract --config-file {config_file} {depth_file}; "\n'
        )
        prefix = f'sbatch -c {config_data["ncpus"] if config_data["ncpus"] > 0 else 1} --mem={config_data["memory"]} '
        prefix += f'-p {config_data["partition"]} -t {config_data["wall_time"]} --wrap "{config_data["prefix"]}'
        commands += prefix + base_command

    # Ensure output directory exists
    config_data["extract_out_script"] = join(
        input_dir, config_data["extract_out_script"]
    )
    with open(config_data["extract_out_script"], "w") as f:
        f.write(commands)
    print("Commands saved to:", config_data["extract_out_script"])

    # Print command
    if config_data["get_cmd"]:
        print("Listing extract commands...\n")
        print(commands)

    # Run command using system
    if config_data["run_cmd"]:
        print("Running extract commands")
        system(commands)


--- File: moseq2_extract/helpers/data.py ---
"""
Contains helper functions for handling/storing data during extraction.
"""

import os
import h5py
import shutil
import tarfile
import warnings
import numpy as np
import ruamel.yaml as yaml
from tqdm.auto import tqdm
from cytoolz import keymap
from pkg_resources import get_distribution
from moseq2_extract.io.video import load_timestamps_from_movie
from os.path import exists, join, dirname, basename, splitext
from moseq2_extract.util import (
    h5_to_dict,
    load_timestamps,
    load_metadata,
    read_yaml,
    camel_to_snake,
    load_textdata,
    build_path,
    dict_to_h5,
    click_param_annot,
)


def check_completion_status(status_filename):
    """
    Read a results_00.yaml (status file) and checks whether the session has been
    fully extracted.

    Args:
    status_filename (str): path to results_00.yaml

    Returns:
    complete (bool): If True, data has been extracted to completion.
    """

    if exists(status_filename):
        return read_yaml(status_filename)["complete"]
    return False


def build_index_dict(files_to_use):
    """
    Create a dictionary for the index file from a list of files and respective metadatas.

    Args:
    files_to_use (list): list of paths to extracted h5 files.

    Returns:
    output_dict (dict): index-file dictionary containing all aggregated extractions.
    """

    output_dict = {"files": [], "pca_path": ""}

    index_uuids = []
    for i, file_tup in enumerate(files_to_use):
        if file_tup[2]["uuid"] not in index_uuids:
            tmp = {
                "path": (file_tup[0], file_tup[1]),
                "uuid": file_tup[2]["uuid"],
                "group": "default",
                "metadata": {
                    "SessionName": f"default_{i}",
                    "SubjectName": f"default_{i}",
                },  # fallback metadata
            }

            # handling metadata sub-dictionary values
            if "metadata" in file_tup[2]:
                tmp["metadata"].update(file_tup[2]["metadata"])
            else:
                warnings.warn(
                    f"Could not locate metadata for {file_tup[0]}! File will be listed with minimal default metadata."
                )

            index_uuids.append(file_tup[2]["uuid"])
            # appending file with default information
            output_dict["files"].append(tmp)

    return output_dict


def load_extraction_meta_from_h5s(to_load, snake_case=True):
    """
    Load extraction metadata from h5 files.

    Args:
    to_load (list): list of paths to h5 files.
    snake_case (bool): whether to save the files using snake_case

    Returns:
    loaded (list): list of loaded h5 dicts.
    """

    loaded = []
    for _dict, _h5f in tqdm(to_load, desc="Scanning data"):
        try:
            # v0.1.3 introduced a change - acq. metadata now here
            tmp = h5_to_dict(_h5f, "/metadata/acquisition")
        except KeyError:
            # if it doesn't exist it's likely from an older moseq version. Try loading it here
            try:
                tmp = h5_to_dict(_h5f, "/metadata/extraction")
            except KeyError:
                # if all else fails, abandon all hope
                tmp = {}

        # note that everything going into here must be a string (no bytes!)
        tmp = {k: str(v) for k, v in tmp.items()}
        if snake_case:
            tmp = keymap(camel_to_snake, tmp)

        # Specific use case block: Behavior reinforcement experiments
        feedback_file = join(dirname(_h5f), "..", "feedback_ts.txt")
        if exists(feedback_file):
            timestamps = map(int, load_timestamps(feedback_file, 0))
            feedback_status = map(int, load_timestamps(feedback_file, 1))
            _dict["feedback_timestamps"] = list(zip(timestamps, feedback_status))

        _dict["extraction_metadata"] = tmp
        loaded += [(_dict, _h5f)]

    return loaded


def build_manifest(loaded, format, snake_case=True):
    """
    Build a manifest file used to contain extraction result metadata from h5 and yaml files.

    Args:
    loaded (list of dicts): list of dicts containing loaded h5 data.
    format (str): filename format indicating the new name for the metadata files in the aggregate_results dir.
    snake_case (bool): whether to save the files using snake_case

    Returns:
    manifest (dict): dictionary of extraction metadata.
    """

    manifest = {}
    fallback = "session_{:03d}"
    fallback_count = 0

    # Additional metadata for certain use cases
    additional_meta = []

    # Behavior reinforcement metadata
    additional_meta.append(
        {
            "filename": "feedback_ts.txt",
            "var_name": "realtime_feedback",
            "dtype": np.bool,
        }
    )

    # Pre-trained model real-time syllable classification results
    additional_meta.append(
        {
            "filename": "predictions.txt",
            "var_name": "realtime_predictions",
            "dtype": np.int,
        }
    )

    # Real-Time Recorded/Computed PC Scores
    additional_meta.append(
        {
            "filename": "pc_scores.txt",
            "var_name": "realtime_pc_scores",
            "dtype": np.float32,
        }
    )

    for _dict, _h5f in loaded:
        print_format = f"{format}_{splitext(basename(_h5f))[0]}"
        if not _dict["extraction_metadata"]:
            copy_path = fallback.format(fallback_count)
            fallback_count += 1
        else:
            try:
                copy_path = build_path(
                    _dict["extraction_metadata"], print_format, snake_case=snake_case
                )
            except:
                copy_path = fallback.format(fallback_count)
                fallback_count += 1
                pass

        # add a bonus dictionary here to be copied to h5 file itself
        manifest[_h5f] = {
            "copy_path": copy_path,
            "yaml_dict": _dict,
            "additional_metadata": {},
        }
        for meta in additional_meta:
            filename = join(dirname(_h5f), "..", meta["filename"])
            if exists(filename):
                try:
                    data, timestamps = load_textdata(filename, dtype=meta["dtype"])
                    manifest[_h5f]["additional_metadata"][meta["var_name"]] = {
                        "data": data,
                        "timestamps": timestamps,
                    }
                except:
                    warnings.warn(
                        "WARNING: Did not load timestamps! This may cause issues if total dropped frames > 2% of the session."
                    )

    return manifest


def copy_manifest_results(manifest, output_dir):
    """
    Copy all consolidated manifest results to their respective output files.

    Args:
    manifest (dict): manifest dictionary containing all extraction h5 metadata to save
    output_dir (str): path to directory where extraction results will be aggregated.

    """

    if not exists(output_dir):
        os.makedirs(output_dir)

    # now the key is the source h5 file and the value is the path to copy to
    for k, v in tqdm(manifest.items(), desc="Copying files"):

        if exists(join(output_dir, f'{v["copy_path"]}.h5')):
            continue

        in_basename = splitext(basename(k))[0]
        in_dirname = dirname(k)

        h5_path = k
        mp4_path = join(in_dirname, f"{in_basename}.mp4")

        if exists(h5_path):
            new_h5_path = join(output_dir, f'{v["copy_path"]}.h5')
            shutil.copyfile(h5_path, new_h5_path)

        # if we have additional_meta then crack open the h5py and write to a safe place
        if len(v["additional_metadata"]) > 0:
            for k2, v2 in v["additional_metadata"].items():
                new_key = f"/metadata/misc/{k2}"
                with h5py.File(new_h5_path, "a") as f:
                    f.create_dataset(f"{new_key}/data", data=v2["data"])
                    f.create_dataset(f"{new_key}/timestamps", data=v2["timestamps"])

        if exists(mp4_path):
            shutil.copyfile(mp4_path, join(output_dir, f'{v["copy_path"]}.mp4'))

        v["yaml_dict"].pop("extraction_metadata", None)
        with open(f'{join(output_dir, v["copy_path"])}.yaml', "w") as f:
            yaml.safe_dump(v["yaml_dict"], f)


def handle_extract_metadata(input_file, dirname):
    """
    Extract metadata and timestamp in the extraction.

    Args:
    input_file (str): path to input file to extract
    dirname (str): path to directory where extraction files reside.

    Returns:
    acquisition_metadata (dict): key-value pairs of JSON contents
    timestamps (1D array): list of loaded timestamps
    tar (bool): indicator for whether the file is compressed.
    """

    tar = None
    tar_members = None
    alternate_correct = False
    from_depth_file = False

    # Handle TAR files
    if input_file.endswith((".tar.gz", ".tgz")):
        print(f"Scanning tarball {input_file} (this will take a minute)")
        # compute NEW psuedo-dirname now, `input_file` gets overwritten below with test_vid.dat tarinfo...
        dirname = join(
            dirname, basename(input_file).replace(".tar.gz", "").replace(".tgz", "")
        )

        tar = tarfile.open(input_file, "r:gz")
        tar_members = tar.getmembers()
        tar_names = [_.name for _ in tar_members]

    if tar is not None:
        # Handling tar paths
        metadata_path = tar.extractfile(tar_members[tar_names.index("metadata.json")])
        if "depth_ts.txt" in tar_names:
            timestamp_path = tar.extractfile(
                tar_members[tar_names.index("depth_ts.txt")]
            )
        elif "timestamps.csv" in tar_names:
            timestamp_path = tar.extractfile(
                tar_members[tar_names.index("timestamps.csv")]
            )
            alternate_correct = True
    else:
        # Handling non-compressed session paths
        metadata_path = join(dirname, "metadata.json")
        timestamp_path = join(dirname, "depth_ts.txt")
        alternate_timestamp_path = join(dirname, "timestamps.csv")
        # Checks for alternative timestamp file if original .txt extension does not exist
        if not exists(timestamp_path) and exists(alternate_timestamp_path):
            timestamp_path = alternate_timestamp_path
            alternate_correct = True
        elif not (
            exists(timestamp_path) or exists(alternate_timestamp_path)
        ) and input_file.endswith(".mkv"):
            from_depth_file = True

    acquisition_metadata = load_metadata(metadata_path)
    if not from_depth_file:
        timestamps = load_timestamps(timestamp_path, col=0, alternate=alternate_correct)
    else:
        timestamps = load_timestamps_from_movie(input_file)

    return acquisition_metadata, timestamps, tar


# extract h5 helper function
def create_extract_h5(
    h5_file,
    acquisition_metadata,
    config_data,
    status_dict,
    scalars_attrs,
    nframes,
    roi,
    bground_im,
    first_frame,
    first_frame_idx,
    last_frame_idx,
    **kwargs,
):
    """
    write acquisition metadata, extraction metadata, computed scalars, timestamps, and original frames/frames_mask to extracted h5.

    Args:
    h5_file (h5py.File object): opened h5 file object to write to.
    acquisition_metadata (dict): Dictionary containing extracted session acquisition metadata.
    config_data (dict): dictionary object containing all required extraction parameters. (auto generated)
    status_dict (dict): dictionary that helps indicate if the session has been extracted fully.
    scalars_attrs (dict): dict of computed scalar attributes and descriptions to save.
    nframes (int): number of frames being recorded
    roi (np.ndarray): Computed 2D ROI Image.
    bground_im (np.ndarray): Computed 2D Background Image.
    first_frame (np.ndarray): Computed 2D First Frame Image.
    timestamps (numpy.array): Array of session timestamps.
    kwargs (dict): additional keyword arguments.

    """

    h5_file.create_dataset("metadata/uuid", data=status_dict["uuid"])

    # Creating scalar dataset
    for scalar in list(scalars_attrs.keys()):
        h5_file.create_dataset(
            f"scalars/{scalar}", (nframes,), "float32", compression="gzip"
        )
        h5_file[f"scalars/{scalar}"].attrs["description"] = scalars_attrs[scalar]

    # Timestamps
    if config_data.get("timestamps") is not None:
        h5_file.create_dataset(
            "timestamps",
            compression="gzip",
            data=config_data["timestamps"][first_frame_idx:last_frame_idx],
        )
        h5_file["timestamps"].attrs["description"] = "Depth video timestamps"

    # Cropped Frames
    h5_file.create_dataset(
        "frames",
        (nframes, config_data["crop_size"][0], config_data["crop_size"][1]),
        config_data["frame_dtype"],
        compression="gzip",
    )
    h5_file["frames"].attrs["description"] = (
        "3D Numpy array of depth frames (nframes x w x h)." + " Depth values are in mm."
    )
    # Frame Masks for EM Tracking
    if config_data["use_tracking_model"]:
        h5_file.create_dataset(
            "frames_mask",
            (nframes, config_data["crop_size"][0], config_data["crop_size"][1]),
            "float32",
            compression="gzip",
        )
        h5_file["frames_mask"].attrs[
            "description"
        ] = "Log-likelihood values from the tracking model (nframes x w x h)"
    else:
        h5_file.create_dataset(
            "frames_mask",
            (nframes, config_data["crop_size"][0], config_data["crop_size"][1]),
            "bool",
            compression="gzip",
        )
        h5_file["frames_mask"].attrs[
            "description"
        ] = "Boolean mask, false=not mouse, true=mouse"

    # Flip Classifier
    if config_data["flip_classifier"] is not None:
        h5_file.create_dataset(
            "metadata/extraction/flips", (nframes,), "bool", compression="gzip"
        )
        h5_file["metadata/extraction/flips"].attrs[
            "description"
        ] = "Output from flip classifier, false=no flip, true=flip"

    # True Depth
    h5_file.create_dataset(
        "metadata/extraction/true_depth", data=config_data["true_depth"]
    )
    h5_file["metadata/extraction/true_depth"].attrs[
        "description"
    ] = "Detected true depth of arena floor in mm"

    # ROI
    h5_file.create_dataset("metadata/extraction/roi", data=roi, compression="gzip")
    h5_file["metadata/extraction/roi"].attrs["description"] = "ROI mask"

    # First Frame
    h5_file.create_dataset(
        "metadata/extraction/first_frame", data=first_frame[0], compression="gzip"
    )
    h5_file["metadata/extraction/first_frame"].attrs[
        "description"
    ] = "First frame of depth dataset"

    # First Frame index
    h5_file.create_dataset(
        "metadata/extraction/first_frame_idx",
        data=[first_frame_idx],
        compression="gzip",
    )
    h5_file["metadata/extraction/first_frame_idx"].attrs[
        "description"
    ] = "First frame index of this dataset"

    # Last Frame index
    h5_file.create_dataset(
        "metadata/extraction/last_frame_idx", data=[last_frame_idx], compression="gzip"
    )
    h5_file["metadata/extraction/last_frame_idx"].attrs[
        "description"
    ] = "Last frame index of this dataset"

    # Background
    h5_file.create_dataset(
        "metadata/extraction/background", data=bground_im, compression="gzip"
    )
    h5_file["metadata/extraction/background"].attrs[
        "description"
    ] = "Computed background image"

    # Extract Version
    extract_version = np.string_(get_distribution("moseq2-extract").version)
    h5_file.create_dataset("metadata/extraction/extract_version", data=extract_version)
    h5_file["metadata/extraction/extract_version"].attrs[
        "description"
    ] = "Version of moseq2-extract"

    # Extraction Parameters
    from moseq2_extract.cli import extract

    dict_to_h5(
        h5_file,
        status_dict["parameters"],
        "metadata/extraction/parameters",
        click_param_annot(extract),
    )

    # Acquisition Metadata
    for key, value in acquisition_metadata.items():
        if type(value) is list and len(value) > 0 and type(value[0]) is str:
            value = [n.encode("utf8") for n in value]

        if value is not None:
            h5_file.create_dataset(f"metadata/acquisition/{key}", data=value)
        else:
            h5_file.create_dataset(f"metadata/acquisition/{key}", dtype="f")


--- File: docs/moseq2_extract.helpers.rst ---
moseq2\_extract.helpers package
===============================================

Helpers - Data Module
---------------------------------------------------

.. automodule:: moseq2_extract.helpers.data
   :members:
   :undoc-members:
   :show-inheritance:

Helpers - Extract Module
------------------------------------------------------

.. automodule:: moseq2_extract.helpers.extract
   :members:
   :undoc-members:
   :show-inheritance:

Helpers - Wrappers Module
-------------------------------------------------------

.. automodule:: moseq2_extract.helpers.wrappers
   :members:
   :undoc-members:
   :show-inheritance:



--- File: docs/index.rst ---
.. moseq2-extract documentation master file, created by
   sphinx-quickstart on Tue Apr 14 11:01:14 2020.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Welcome to moseq2-extract's documentation!
==========================================

.. toctree::
   :maxdepth: 3
   :caption: Contents:

   modules

Index
=====

* :ref:`genindex`

--- File: docs/Makefile ---
# Minimal makefile for Sphinx documentation
#

# You can set these variables from the command line, and also
# from the environment for the first two.
SPHINXOPTS    ?=
SPHINXBUILD   ?= sphinx-build
SOURCEDIR     = .
BUILDDIR      = _build

# Put it first so that "make" without argument is like "make help".
help:
	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

.PHONY: help Makefile

# Catch-all target: route all unknown targets to Sphinx using the new
# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
%: Makefile
	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)


--- File: docs/conf.py ---
# Configuration file for the Sphinx documentation builder.
#
# This file only contains a selection of the most common options. For a full
# list see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import os
import sys
sys.path.insert(0, os.path.abspath('../moseq2_extract/'))


# -- Project information -----------------------------------------------------

project = 'moseq2-extract'
author = 'Datta Lab'


# -- General configuration ---------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    'sphinx.ext.napoleon',
    'sphinx.ext.autodoc',
    'sphinx.ext.autosummary',
    'sphinx_click.ext'
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']

autosummary_generate = True  # Make _autosummary files and include them
napoleon_google_docstring = False
napoleon_use_param = False
napoleon_use_ivar = True

autodoc_default_flags = [
         # Make sure that any autodoc declarations show the right members
         "members",
         "inherited-members",
         "private-members",
         "show-inheritance",
]

# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = 'sphinx_rtd_theme'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = []

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',
    # 'fontpkg': '\\usepackage{euler}'

    # The font size ('10pt', '11pt' or '12pt').
    
    # 'pointsize': '12pt',

    # fontpkg': r'''
    # \setmainfont{Arial Regular}
    # \setsansfont{Arial Regular}
    # \setmonofont{Menlo Regular}
    # ''',

    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',

    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}

latex_documents = [
    ('index', u'moseq2-extract.tex', u'moseq2-extract Documentation', u'Datta Lab', 'manual'),
]

--- File: docs/moseq2_extract.io.rst ---
moseq2\_extract.io package
==========================================

IO - Image Module
-----------------------------------------------

.. automodule:: moseq2_extract.io.image
   :members:
   :undoc-members:
   :show-inheritance:

IO - Video Module
-----------------------------------------------

.. automodule:: moseq2_extract.io.video
   :members:
   :undoc-members:
   :show-inheritance:


--- File: docs/modules.rst ---
moseq2\_extract package
=======================================

CLI Module
------------------------------------------

.. click:: moseq2_extract.cli:cli
    :prog: moseq2-extract
    :show-nested:

GUI Module
------------------------------------------

.. automodule:: moseq2_extract.gui
   :members:
   :undoc-members:
   :show-inheritance:

General Utilities Module
-------------------------------------------

.. automodule:: moseq2_extract.util
   :members:
   :undoc-members:
   :show-inheritance:

Subpackages
-----------

.. toctree::

   moseq2_extract.extract
   moseq2_extract.helpers
   moseq2_extract.io


--- File: docs/make.bat ---
@ECHO OFF

pushd %~dp0

REM Command file for Sphinx documentation

if "%SPHINXBUILD%" == "" (
	set SPHINXBUILD=sphinx-build
)
set SOURCEDIR=.
set BUILDDIR=_build

if "%1" == "" goto help

%SPHINXBUILD% >NUL 2>NUL
if errorlevel 9009 (
	echo.
	echo.The 'sphinx-build' command was not found. Make sure you have Sphinx
	echo.installed, then set the SPHINXBUILD environment variable to point
	echo.to the full path of the 'sphinx-build' executable. Alternatively you
	echo.may add the Sphinx directory to PATH.
	echo.
	echo.If you don't have Sphinx installed, grab it from
	echo.http://sphinx-doc.org/
	exit /b 1
)

%SPHINXBUILD% -M %1 %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%
goto end

:help
%SPHINXBUILD% -M help %SOURCEDIR% %BUILDDIR% %SPHINXOPTS% %O%

:end
popd


--- File: docs/make_doc.md ---
# How to update docs
## install packages
`pip install -U sphinx sphinx-click  sphinx-rtd-theme`
Optional for pdf generation
`pip install pdflatex`

# go to the right directory
`cd ./docs`
# run the makefile to make html
`make html`

# run the makefile to make pdf
`make latexpdf`

--- File: docs/moseq2_extract.extract.rst ---
moseq2\_extract.extract package
===============================================

Extract - Extract Module
------------------------------------------------------

.. automodule:: moseq2_extract.extract.extract
   :members:
   :undoc-members:
   :show-inheritance:

Extract - Proc Module
---------------------------------------------------

.. automodule:: moseq2_extract.extract.proc
   :members:
   :undoc-members:
   :show-inheritance:

Extract - ROI Module
--------------------------------------------------

.. automodule:: moseq2_extract.extract.roi
   :members:
   :undoc-members:
   :show-inheritance:

Extract - Track Module
----------------------------------------------------

.. automodule:: moseq2_extract.extract.track
   :members:
   :undoc-members:
   :show-inheritance:


--- File: scripts/download_test_dataset.sh ---
curl -L -o data.zip https://www.dropbox.com/sh/lwg9l1u80ma3buv/AABQ0gpUA89LakWEjx1kdmWva?dl=0 && unzip data.zip -d data
rm data.zip


--- File: .github/pull_request_template.md ---
<!--- Provide a general summary of your changes in the Title above -->
<!--- Pull request titles must use the [conventional commits](https://www.conventionalcommits.org/en/v1.0.0/#summary) format -->

## Issue being fixed or feature implemented
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->


## What was done?
<!--- Describe your changes in detail -->


## How Has This Been Tested?
<!--- Please describe in detail how you tested your changes. -->
<!--- Include details of your testing environment, and the tests you ran to -->
<!--- see how your change affects other areas of the code, etc. -->


## Breaking Changes
<!--- Please describe any breaking changes your code introduces and verify that -->
<!--- the title includes "!" following the conventional commit type (e.g. "feat!: ..."-->


## Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have added or updated relevant unit/integration/functional/e2e tests
- [ ] I have made corresponding changes to the documentation


